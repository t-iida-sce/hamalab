{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Classification label : {'Lateral': 0, 'Posterior': 1, 'Anterior': 2, 'Inferior': 3}\n",
      "torch.Size([149, 12, 500])\n",
      "149\n",
      "test1\n",
      "dataset len : 149\n",
      "labels len : 149\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3])\n",
      "test2\n",
      "indices\n",
      "119\n",
      "30\n",
      "<torch.utils.data.dataset.Subset object at 0x775c6d047f70>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x775c6d09a980>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader_csv import create_dataloader_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# CPUデバイスを指定\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "path = os.getcwd()\n",
    "path = '/root/notebooks/sample/dataset/CSV_4label'\n",
    "train_dataloader,test_dataloader,labels=create_dataloader_csv(path,1,None)\n",
    "print(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "\n",
    "class Attention_Input(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_Input, self).__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x と cls_token を連結\n",
    "        z_0 = torch.cat([self.cls_token.unsqueeze(0), x], dim=0)\n",
    "        return z_0\n",
    "# AttentionInputをnn.Moduleに変更\n",
    "class AttentionInput(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionInput, self).__init__()\n",
    "        self.attention_input = Attention_Input()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # AttentionInputのforwardメソッドを呼び出す\n",
    "        output = self.attention_input(x)\n",
    "        return output\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # MultiheadAttentionのためにクエリ、キー、バリューの変換を行う層を定義\n",
    "        self.query_transform = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_transform = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_transform = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # MultiheadAttentionレイヤー\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # クエリ、キー、バリューの変換\n",
    "        query = self.query_transform(x)\n",
    "        key = self.key_transform(x)\n",
    "        value = self.value_transform(x)\n",
    "\n",
    "        # MultiheadAttentionの適用\n",
    "        attention_output, _ = self.multihead_attention(query, key, value)\n",
    "        return attention_output\n",
    "\n",
    "# # Attentionの出力からCLSトークンを取り出す    \n",
    "class Take_cls(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x[0, :].reshape(1, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
