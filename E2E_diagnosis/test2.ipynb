{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/python/build/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification lab : {'Lateral': 0, 'Posterior': 1, 'Anterior': 2, 'Inferior': 3, 'Normal': 4}\n",
      "Number of datas 378\n",
      "torch.Size([12, 378, 3, 112, 224])\n",
      "378\n",
      "378\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader_e2e import create_dataset_12\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "path = os.getcwd()\n",
    "path = '/root/notebooks/sample/dataset/ECG100_224/'\n",
    "train_dataloader,test_dataloader,label_lead=create_dataset_12(path,1,None)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  (faltten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=5, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): CNNModel(\n",
      "    (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "    (faltten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (fc2): Linear(in_features=16, out_features=5, bias=True)\n",
      "  )\n",
      "  (1): MeanLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# モデルクラス\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # 畳み込み層や全結合層などを定義する\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.faltten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16,16)\n",
    "        self.fc2 = nn.Linear(16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.gap(x)\n",
    "        x = self.faltten(x)\n",
    "        #x = x.view(-1, 16 * 56 * 112)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model = CNNModel().to(device)\n",
    "print(model)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    model,\n",
    "    MeanLayer()\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score_dx!!\n",
      "Epoch [1/20000] Train-Loss: 1.7278143433715947 Test-Loss: 0.4339102976594917 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [2/20000] Train-Loss: 1.7183745000214692 Test-Loss: 0.43141360911264753 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [3/20000] Train-Loss: 1.7079326320585246 Test-Loss: 0.42862139749288647 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [4/20000] Train-Loss: 1.6955336160588559 Test-Loss: 0.42524422020786345 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [5/20000] Train-Loss: 1.6850118673469143 Test-Loss: 0.4232538364520874 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [6/20000] Train-Loss: 1.6765562325300514 Test-Loss: 0.4209401460292057 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [7/20000] Train-Loss: 1.6668349138770584 Test-Loss: 0.41830021035232035 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [8/20000] Train-Loss: 1.6554347613625646 Test-Loss: 0.4152164813179703 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [9/20000] Train-Loss: 1.6429837570114425 Test-Loss: 0.41223302826627345 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [10/20000] Train-Loss: 1.631363560940034 Test-Loss: 0.40914643576851895 Train-f1: 0.006514657980456026 Test-f1: 0.005194805194805195 Train-acc: 0.016556291390728478 Test-acc: 0.013157894736842105\n",
      "best score_dx!!\n",
      "Epoch [11/20000] Train-Loss: 1.6170824074839671 Test-Loss: 0.40475365758616716 Train-f1: 0.006514657980456026 Test-f1: 0.03436853002070393 Train-acc: 0.016556291390728478 Test-acc: 0.05263157894736842\n",
      "best score_dx!!\n",
      "Epoch [12/20000] Train-Loss: 1.5979752604753492 Test-Loss: 0.3995207086999134 Train-f1: 0.11980676328502415 Test-f1: 0.12363636363636363 Train-acc: 0.4105960264900662 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [13/20000] Train-Loss: 1.5753198758013034 Test-Loss: 0.3934522606101351 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [14/20000] Train-Loss: 1.5501397652193651 Test-Loss: 0.38680634794089935 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [15/20000] Train-Loss: 1.5235817915859657 Test-Loss: 0.3800314172972582 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [16/20000] Train-Loss: 1.4962061455820033 Test-Loss: 0.373020552510508 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [17/20000] Train-Loss: 1.4691202692926055 Test-Loss: 0.3663135370012385 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [18/20000] Train-Loss: 1.4435394785757323 Test-Loss: 0.3599819329925452 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [19/20000] Train-Loss: 1.420083654189879 Test-Loss: 0.3543431497819448 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [20/20000] Train-Loss: 1.3993226289210108 Test-Loss: 0.3495166962836699 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [21/20000] Train-Loss: 1.3814533469986394 Test-Loss: 0.34528760736715136 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [22/20000] Train-Loss: 1.36638090462304 Test-Loss: 0.34190052234956136 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [23/20000] Train-Loss: 1.3540733560177036 Test-Loss: 0.3390492403740706 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [24/20000] Train-Loss: 1.3439756074280285 Test-Loss: 0.33670570276980266 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [25/20000] Train-Loss: 1.3360293953908984 Test-Loss: 0.33479661315026443 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [26/20000] Train-Loss: 1.3295734612490282 Test-Loss: 0.33334741049464234 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [27/20000] Train-Loss: 1.324404265289538 Test-Loss: 0.332142828480549 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [28/20000] Train-Loss: 1.3200804185437212 Test-Loss: 0.33113124434491487 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [29/20000] Train-Loss: 1.3165161640601473 Test-Loss: 0.33025423065961956 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [30/20000] Train-Loss: 1.3137398441990962 Test-Loss: 0.3296659265875875 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [31/20000] Train-Loss: 1.311287616549825 Test-Loss: 0.3290901503336589 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [32/20000] Train-Loss: 1.3093411280699598 Test-Loss: 0.32856602772342586 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [33/20000] Train-Loss: 1.3075838467096554 Test-Loss: 0.32816679512521985 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [34/20000] Train-Loss: 1.306273105497485 Test-Loss: 0.32779490900499375 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [35/20000] Train-Loss: 1.3049044270689607 Test-Loss: 0.3274560555903496 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [36/20000] Train-Loss: 1.3037687376630172 Test-Loss: 0.3271863074257258 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [37/20000] Train-Loss: 1.3027645227485873 Test-Loss: 0.32693964979036616 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [38/20000] Train-Loss: 1.3019171855978702 Test-Loss: 0.3266875243758722 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [39/20000] Train-Loss: 1.3010854914576364 Test-Loss: 0.3264829913383994 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [40/20000] Train-Loss: 1.3003540578594905 Test-Loss: 0.3262974839604282 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [41/20000] Train-Loss: 1.2996165165260238 Test-Loss: 0.3261205480126834 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [42/20000] Train-Loss: 1.2990164456037736 Test-Loss: 0.3259446165215379 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [43/20000] Train-Loss: 1.298363744983109 Test-Loss: 0.32577451111207184 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [44/20000] Train-Loss: 1.2979636900611617 Test-Loss: 0.32564027388536804 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [45/20000] Train-Loss: 1.2973604014591822 Test-Loss: 0.325496088341677 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [46/20000] Train-Loss: 1.2970147604919198 Test-Loss: 0.3253784195698427 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [47/20000] Train-Loss: 1.2964846487460628 Test-Loss: 0.32525768331533855 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [48/20000] Train-Loss: 1.296119543256925 Test-Loss: 0.32513795814689195 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [49/20000] Train-Loss: 1.2958110881849114 Test-Loss: 0.32502914136888206 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [50/20000] Train-Loss: 1.2954363309145072 Test-Loss: 0.3249326330507372 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [51/20000] Train-Loss: 1.2951121433171606 Test-Loss: 0.3248382248669148 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [52/20000] Train-Loss: 1.2946167520918728 Test-Loss: 0.3247477044131763 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [53/20000] Train-Loss: 1.2943000329169958 Test-Loss: 0.324649871277612 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [54/20000] Train-Loss: 1.2941189932929977 Test-Loss: 0.32455522542993337 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [55/20000] Train-Loss: 1.2939316065447095 Test-Loss: 0.3244918988535296 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [56/20000] Train-Loss: 1.2935531744255289 Test-Loss: 0.3244148400995177 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [57/20000] Train-Loss: 1.2934888627841592 Test-Loss: 0.32435139807436997 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [58/20000] Train-Loss: 1.2930974388937848 Test-Loss: 0.3242887558126125 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [59/20000] Train-Loss: 1.292937066431815 Test-Loss: 0.3242199388615026 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [60/20000] Train-Loss: 1.2926935970466453 Test-Loss: 0.3241623916748013 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [61/20000] Train-Loss: 1.2925198858743636 Test-Loss: 0.3240827449680235 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [62/20000] Train-Loss: 1.2923292646241102 Test-Loss: 0.3240366162642235 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [63/20000] Train-Loss: 1.2921611129427377 Test-Loss: 0.3239862997773138 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [64/20000] Train-Loss: 1.2919918122224905 Test-Loss: 0.32392683882928774 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [65/20000] Train-Loss: 1.2917830918943163 Test-Loss: 0.32387087080876964 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [66/20000] Train-Loss: 1.2917899622288014 Test-Loss: 0.32382233811266736 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [67/20000] Train-Loss: 1.2915520443123296 Test-Loss: 0.32375822818479705 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [68/20000] Train-Loss: 1.2913173625574867 Test-Loss: 0.323700750661069 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [69/20000] Train-Loss: 1.2912116891901677 Test-Loss: 0.32365154555085324 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [70/20000] Train-Loss: 1.2909889262190704 Test-Loss: 0.3235888117104803 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [71/20000] Train-Loss: 1.2907477863457189 Test-Loss: 0.32354525332257683 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [72/20000] Train-Loss: 1.2908067676834458 Test-Loss: 0.32350249232505834 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [73/20000] Train-Loss: 1.2905741091211669 Test-Loss: 0.32345116836013565 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [74/20000] Train-Loss: 1.2903904618600455 Test-Loss: 0.3234040021848272 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [75/20000] Train-Loss: 1.2903390062168898 Test-Loss: 0.3233604624080278 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [76/20000] Train-Loss: 1.2900465090631799 Test-Loss: 0.3233188892325297 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [77/20000] Train-Loss: 1.289996257546314 Test-Loss: 0.32328258318494446 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [78/20000] Train-Loss: 1.289897410041226 Test-Loss: 0.3232409558725817 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [79/20000] Train-Loss: 1.2898294246632087 Test-Loss: 0.3231940060739613 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [80/20000] Train-Loss: 1.2896827419677326 Test-Loss: 0.32316856442320313 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [81/20000] Train-Loss: 1.2896590192165212 Test-Loss: 0.3231305124530271 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [82/20000] Train-Loss: 1.2895470408680278 Test-Loss: 0.323096753148404 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [83/20000] Train-Loss: 1.2895348644658615 Test-Loss: 0.3230698185196313 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [84/20000] Train-Loss: 1.2893190730169317 Test-Loss: 0.32303308191850066 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [85/20000] Train-Loss: 1.289334951608847 Test-Loss: 0.32301418617584254 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [86/20000] Train-Loss: 1.2891109937812921 Test-Loss: 0.32297668188746104 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [87/20000] Train-Loss: 1.2890016190845008 Test-Loss: 0.3229448033533536 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [88/20000] Train-Loss: 1.2888844453212882 Test-Loss: 0.3229193647655283 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [89/20000] Train-Loss: 1.288876535157548 Test-Loss: 0.3228914370575914 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [90/20000] Train-Loss: 1.2889393766511146 Test-Loss: 0.32286912445926164 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [91/20000] Train-Loss: 1.28874235670728 Test-Loss: 0.32284062007138126 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [92/20000] Train-Loss: 1.2887713748260046 Test-Loss: 0.3228162313929092 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [93/20000] Train-Loss: 1.2885504710503697 Test-Loss: 0.32279935434520496 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [94/20000] Train-Loss: 1.2885361978391168 Test-Loss: 0.3227782567158936 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [95/20000] Train-Loss: 1.2885232983546109 Test-Loss: 0.3227568475292488 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [96/20000] Train-Loss: 1.2884892577665479 Test-Loss: 0.3227359591572453 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [97/20000] Train-Loss: 1.2884031829211968 Test-Loss: 0.3227154020650688 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [98/20000] Train-Loss: 1.2883164001862117 Test-Loss: 0.3227001448814385 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [99/20000] Train-Loss: 1.2882289891747485 Test-Loss: 0.32268294004187115 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [100/20000] Train-Loss: 1.288233153227736 Test-Loss: 0.3226603648886282 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [101/20000] Train-Loss: 1.288095992734241 Test-Loss: 0.32264222788317914 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [102/20000] Train-Loss: 1.2882270761903312 Test-Loss: 0.3226264587756256 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [103/20000] Train-Loss: 1.2881069734274724 Test-Loss: 0.3226062629694803 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [104/20000] Train-Loss: 1.2880632693634426 Test-Loss: 0.3225887895929474 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [105/20000] Train-Loss: 1.2879949996456102 Test-Loss: 0.32258197335978245 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [106/20000] Train-Loss: 1.2880174743035622 Test-Loss: 0.32255787667966673 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [107/20000] Train-Loss: 1.2878878165590582 Test-Loss: 0.32254870483322207 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [108/20000] Train-Loss: 1.2878568298674928 Test-Loss: 0.32253291645456017 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [109/20000] Train-Loss: 1.2878041549075068 Test-Loss: 0.3225137161799791 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [110/20000] Train-Loss: 1.2879513824371216 Test-Loss: 0.322502730517708 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [111/20000] Train-Loss: 1.287730319587481 Test-Loss: 0.32248708719735375 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [112/20000] Train-Loss: 1.2877467875251727 Test-Loss: 0.32247695694598677 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [113/20000] Train-Loss: 1.2877523810248572 Test-Loss: 0.3224673252206186 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [114/20000] Train-Loss: 1.2876361314197793 Test-Loss: 0.32244983050730336 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [115/20000] Train-Loss: 1.2876565611509472 Test-Loss: 0.32243812837739205 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [116/20000] Train-Loss: 1.2878436194112322 Test-Loss: 0.32242673696636587 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [117/20000] Train-Loss: 1.2876272248513265 Test-Loss: 0.32241566864782595 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [118/20000] Train-Loss: 1.2876375784016658 Test-Loss: 0.32240174670769717 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [119/20000] Train-Loss: 1.287584116903665 Test-Loss: 0.3223879752614149 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [120/20000] Train-Loss: 1.2875380758925583 Test-Loss: 0.32238225181175767 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [121/20000] Train-Loss: 1.2874759418971715 Test-Loss: 0.3223723147230743 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [122/20000] Train-Loss: 1.287536652176177 Test-Loss: 0.32236151233119015 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [123/20000] Train-Loss: 1.2874183292311816 Test-Loss: 0.322349251451165 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [124/20000] Train-Loss: 1.2874264570470153 Test-Loss: 0.32234103459750607 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [125/20000] Train-Loss: 1.2874073371380086 Test-Loss: 0.3223386989201425 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [126/20000] Train-Loss: 1.287351997139378 Test-Loss: 0.3223259791272445 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [127/20000] Train-Loss: 1.2873539366697022 Test-Loss: 0.3223160953955767 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [128/20000] Train-Loss: 1.287377895684575 Test-Loss: 0.32230926125196885 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [129/20000] Train-Loss: 1.2872795669660841 Test-Loss: 0.3222947104161293 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [130/20000] Train-Loss: 1.287405831785339 Test-Loss: 0.32228769073236957 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [131/20000] Train-Loss: 1.287218267086502 Test-Loss: 0.322281921540034 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [132/20000] Train-Loss: 1.287361851135492 Test-Loss: 0.32228013401329764 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [133/20000] Train-Loss: 1.2872810637554282 Test-Loss: 0.3222687297383488 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [134/20000] Train-Loss: 1.2872592317367884 Test-Loss: 0.3222573201103431 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [135/20000] Train-Loss: 1.2872450947223955 Test-Loss: 0.32225698777591516 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [136/20000] Train-Loss: 1.2873515126918982 Test-Loss: 0.3222483093276832 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [137/20000] Train-Loss: 1.2871931692811402 Test-Loss: 0.32223608601324655 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [138/20000] Train-Loss: 1.2871150959534716 Test-Loss: 0.322229849589789 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [139/20000] Train-Loss: 1.2871677278954474 Test-Loss: 0.3222252219971408 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [140/20000] Train-Loss: 1.2870134499649344 Test-Loss: 0.3222141683384066 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [141/20000] Train-Loss: 1.2871176913908402 Test-Loss: 0.32220883458885535 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [142/20000] Train-Loss: 1.287056368783362 Test-Loss: 0.3222015183175324 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [143/20000] Train-Loss: 1.2870731236772692 Test-Loss: 0.3221982004164607 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [144/20000] Train-Loss: 1.2871301649710245 Test-Loss: 0.3221958180545454 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [145/20000] Train-Loss: 1.2870604101983887 Test-Loss: 0.322190762455789 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [146/20000] Train-Loss: 1.2869961636807097 Test-Loss: 0.3221916027772455 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [147/20000] Train-Loss: 1.2870921105999664 Test-Loss: 0.32217771182333477 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [148/20000] Train-Loss: 1.2870744228921183 Test-Loss: 0.3221713351396557 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [149/20000] Train-Loss: 1.2870948257646662 Test-Loss: 0.32217107937968764 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [150/20000] Train-Loss: 1.2869702982892102 Test-Loss: 0.3221611274111909 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [151/20000] Train-Loss: 1.286990288195853 Test-Loss: 0.32215476506200164 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [152/20000] Train-Loss: 1.2870340175865123 Test-Loss: 0.3221547686326234 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [153/20000] Train-Loss: 1.2869915364671376 Test-Loss: 0.32215018533409795 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [154/20000] Train-Loss: 1.287078132344861 Test-Loss: 0.3221422945582389 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [155/20000] Train-Loss: 1.2869492682043202 Test-Loss: 0.32213983961404985 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [156/20000] Train-Loss: 1.287000736200494 Test-Loss: 0.3221369695336865 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [157/20000] Train-Loss: 1.287000383545164 Test-Loss: 0.3221354467924077 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [158/20000] Train-Loss: 1.286910232794283 Test-Loss: 0.3221214374114676 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [159/20000] Train-Loss: 1.2869319551424763 Test-Loss: 0.3221227259589069 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [160/20000] Train-Loss: 1.286904179558634 Test-Loss: 0.32211934009055954 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [161/20000] Train-Loss: 1.2869149325217264 Test-Loss: 0.3221138662061897 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [162/20000] Train-Loss: 1.2870114099754597 Test-Loss: 0.3221114441967154 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [163/20000] Train-Loss: 1.2869647869355327 Test-Loss: 0.32210798117901646 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [164/20000] Train-Loss: 1.2867970670958404 Test-Loss: 0.32210634172896707 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [165/20000] Train-Loss: 1.2868754091800942 Test-Loss: 0.3220983362625083 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [166/20000] Train-Loss: 1.2869262065647487 Test-Loss: 0.3220905045773468 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [167/20000] Train-Loss: 1.2868177103435423 Test-Loss: 0.32209400047209596 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [168/20000] Train-Loss: 1.286833963233894 Test-Loss: 0.32208905237959917 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [169/20000] Train-Loss: 1.2868353159012884 Test-Loss: 0.3220817462560737 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [170/20000] Train-Loss: 1.286905285495983 Test-Loss: 0.3220790650797141 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [171/20000] Train-Loss: 1.2868317409309418 Test-Loss: 0.3220799562707652 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [172/20000] Train-Loss: 1.2868167085184736 Test-Loss: 0.32207283460475167 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [173/20000] Train-Loss: 1.2868505501198377 Test-Loss: 0.3220705353991345 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [174/20000] Train-Loss: 1.2868608620689599 Test-Loss: 0.3220689312236385 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [175/20000] Train-Loss: 1.2868114127996217 Test-Loss: 0.32206517045388866 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [176/20000] Train-Loss: 1.2867910810793681 Test-Loss: 0.32206111240337504 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [177/20000] Train-Loss: 1.2867520782027244 Test-Loss: 0.3220595442916439 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [178/20000] Train-Loss: 1.286943734812981 Test-Loss: 0.3220589899987646 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [179/20000] Train-Loss: 1.2867206421374808 Test-Loss: 0.32205265484546763 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [180/20000] Train-Loss: 1.2868600137149873 Test-Loss: 0.32205007540240194 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [181/20000] Train-Loss: 1.2867882091236378 Test-Loss: 0.3220488728455932 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [182/20000] Train-Loss: 1.2868522019758957 Test-Loss: 0.32204250724599376 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [183/20000] Train-Loss: 1.2868806638147567 Test-Loss: 0.3220456250695785 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [184/20000] Train-Loss: 1.286711955497024 Test-Loss: 0.32204693776180093 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [185/20000] Train-Loss: 1.2867558836575788 Test-Loss: 0.3220355502139042 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [186/20000] Train-Loss: 1.286667997500915 Test-Loss: 0.3220327831644205 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [187/20000] Train-Loss: 1.2867592300191741 Test-Loss: 0.32203656714292556 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [188/20000] Train-Loss: 1.2867579121542718 Test-Loss: 0.32203111152666974 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [189/20000] Train-Loss: 1.2868172578245738 Test-Loss: 0.3220312600914771 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [190/20000] Train-Loss: 1.2867958711601648 Test-Loss: 0.32202808500979996 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [191/20000] Train-Loss: 1.286834775543845 Test-Loss: 0.32202287861407797 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [192/20000] Train-Loss: 1.2866876372805423 Test-Loss: 0.3220294295149099 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [193/20000] Train-Loss: 1.286698382742275 Test-Loss: 0.32202216060968336 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [194/20000] Train-Loss: 1.2867137162018318 Test-Loss: 0.322019449011806 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [195/20000] Train-Loss: 1.2867291649681374 Test-Loss: 0.3220121326726911 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [196/20000] Train-Loss: 1.2867856447426005 Test-Loss: 0.3220194520589871 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [197/20000] Train-Loss: 1.2867507793052109 Test-Loss: 0.32201209528614394 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [198/20000] Train-Loss: 1.2866748648711135 Test-Loss: 0.32200991698828635 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [199/20000] Train-Loss: 1.2866589046551113 Test-Loss: 0.32201500884236905 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [200/20000] Train-Loss: 1.286661041877759 Test-Loss: 0.3220095397061531 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [201/20000] Train-Loss: 1.2867064318959442 Test-Loss: 0.3220040089236252 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [202/20000] Train-Loss: 1.286777534356419 Test-Loss: 0.32200420740521324 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [203/20000] Train-Loss: 1.2866798085889766 Test-Loss: 0.32200476935425115 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [204/20000] Train-Loss: 1.2867017964624465 Test-Loss: 0.32199951428250506 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [205/20000] Train-Loss: 1.286741578660478 Test-Loss: 0.32200163218495725 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [206/20000] Train-Loss: 1.2866517269063256 Test-Loss: 0.32199637169232553 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [207/20000] Train-Loss: 1.2866902705965437 Test-Loss: 0.3219946346139967 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [208/20000] Train-Loss: 1.2867412918916683 Test-Loss: 0.32199218881546227 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [209/20000] Train-Loss: 1.2866012213745937 Test-Loss: 0.32199264967221064 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [210/20000] Train-Loss: 1.2867278001897626 Test-Loss: 0.321988337714183 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [211/20000] Train-Loss: 1.2866191763322747 Test-Loss: 0.3219909666801429 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [212/20000] Train-Loss: 1.2866800868240766 Test-Loss: 0.3219867738075757 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [213/20000] Train-Loss: 1.2868397209550457 Test-Loss: 0.3219868462052483 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [214/20000] Train-Loss: 1.286650193386343 Test-Loss: 0.3219897637378593 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [215/20000] Train-Loss: 1.2865876804324 Test-Loss: 0.32198131835735244 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [216/20000] Train-Loss: 1.2866519200655975 Test-Loss: 0.3219864357866427 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [217/20000] Train-Loss: 1.286653562924671 Test-Loss: 0.3219820370446455 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [218/20000] Train-Loss: 1.286664079667197 Test-Loss: 0.32197349671937564 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [219/20000] Train-Loss: 1.2866720374300806 Test-Loss: 0.3219769826721179 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [220/20000] Train-Loss: 1.2866680275519224 Test-Loss: 0.32197222659921415 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [221/20000] Train-Loss: 1.286580757615394 Test-Loss: 0.32197918280745336 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [222/20000] Train-Loss: 1.2865771660730319 Test-Loss: 0.3219799656104082 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [223/20000] Train-Loss: 1.2867494593133029 Test-Loss: 0.32197419495872026 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [224/20000] Train-Loss: 1.2866801906726568 Test-Loss: 0.321971122887306 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [225/20000] Train-Loss: 1.2867272437271473 Test-Loss: 0.32198026352803544 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [226/20000] Train-Loss: 1.2866322866915076 Test-Loss: 0.3219726874274764 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [227/20000] Train-Loss: 1.2865741847961305 Test-Loss: 0.321967430355131 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [228/20000] Train-Loss: 1.2865978678057222 Test-Loss: 0.3219712288597987 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [229/20000] Train-Loss: 1.2866643434175353 Test-Loss: 0.3219670993274462 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [230/20000] Train-Loss: 1.2865855227234624 Test-Loss: 0.3219657271175752 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [231/20000] Train-Loss: 1.286688696913262 Test-Loss: 0.3219636978357249 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [232/20000] Train-Loss: 1.2866415841140806 Test-Loss: 0.3219650491445671 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [233/20000] Train-Loss: 1.2866814428350837 Test-Loss: 0.32195918183641986 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [234/20000] Train-Loss: 1.2866430703351177 Test-Loss: 0.3219555903139105 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [235/20000] Train-Loss: 1.286665044273253 Test-Loss: 0.32195743325843673 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [236/20000] Train-Loss: 1.286621707664336 Test-Loss: 0.32195862504137474 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [237/20000] Train-Loss: 1.286610340263486 Test-Loss: 0.32195525218432497 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [238/20000] Train-Loss: 1.2866112626500688 Test-Loss: 0.32194964188870956 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [239/20000] Train-Loss: 1.2865586879696438 Test-Loss: 0.3219533344969785 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [240/20000] Train-Loss: 1.2866879947134942 Test-Loss: 0.3219527572917816 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [241/20000] Train-Loss: 1.2865630320137518 Test-Loss: 0.3219469270462431 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [242/20000] Train-Loss: 1.2866182337808034 Test-Loss: 0.3219484756893046 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [243/20000] Train-Loss: 1.286594751378218 Test-Loss: 0.32194841280920267 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [244/20000] Train-Loss: 1.286720954615334 Test-Loss: 0.3219544338667367 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [245/20000] Train-Loss: 1.2866290518692454 Test-Loss: 0.3219465562948636 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [246/20000] Train-Loss: 1.28673041540401 Test-Loss: 0.32194237535036596 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [247/20000] Train-Loss: 1.2865948188659613 Test-Loss: 0.32194349293419966 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [248/20000] Train-Loss: 1.2866017461605412 Test-Loss: 0.3219435588832388 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [249/20000] Train-Loss: 1.2866316557414275 Test-Loss: 0.32193899348533395 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [250/20000] Train-Loss: 1.2865928506536928 Test-Loss: 0.32194245272848016 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [251/20000] Train-Loss: 1.2866733808820527 Test-Loss: 0.32193986510084127 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [252/20000] Train-Loss: 1.2866007657665313 Test-Loss: 0.3219355566657931 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [253/20000] Train-Loss: 1.2865919965966677 Test-Loss: 0.32193754925676155 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [254/20000] Train-Loss: 1.286597994515396 Test-Loss: 0.32193542179898776 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [255/20000] Train-Loss: 1.286553780023208 Test-Loss: 0.32193619283129377 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [256/20000] Train-Loss: 1.2865068534208977 Test-Loss: 0.32193917557491697 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [257/20000] Train-Loss: 1.2865346782401035 Test-Loss: 0.32193613107212105 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [258/20000] Train-Loss: 1.2864756783570679 Test-Loss: 0.3219365576010218 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [259/20000] Train-Loss: 1.2865158581979215 Test-Loss: 0.32192937521167725 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [260/20000] Train-Loss: 1.2865217866020837 Test-Loss: 0.3219311853536336 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [261/20000] Train-Loss: 1.2865919115836775 Test-Loss: 0.32193621374160614 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [262/20000] Train-Loss: 1.2865710287012913 Test-Loss: 0.3219352677440051 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [263/20000] Train-Loss: 1.2865501220090543 Test-Loss: 0.32192463426180407 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [264/20000] Train-Loss: 1.286520471606888 Test-Loss: 0.3219260647636418 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [265/20000] Train-Loss: 1.286541186269986 Test-Loss: 0.3219278326140875 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [266/20000] Train-Loss: 1.2865378319937077 Test-Loss: 0.3219220276060361 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [267/20000] Train-Loss: 1.2864447381518629 Test-Loss: 0.3219229772575589 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [268/20000] Train-Loss: 1.2865999592261255 Test-Loss: 0.3219245277645411 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [269/20000] Train-Loss: 1.2865069329639898 Test-Loss: 0.3219243312270122 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [270/20000] Train-Loss: 1.2865425158996522 Test-Loss: 0.3219268927750414 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [271/20000] Train-Loss: 1.2866880029671857 Test-Loss: 0.3219203509878224 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [272/20000] Train-Loss: 1.2865174992948287 Test-Loss: 0.32192580390263914 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [273/20000] Train-Loss: 1.2866761916550578 Test-Loss: 0.32192381583821206 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [274/20000] Train-Loss: 1.2865179237103985 Test-Loss: 0.32192023944948767 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [275/20000] Train-Loss: 1.2865583372776939 Test-Loss: 0.32191925203536187 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [276/20000] Train-Loss: 1.2865547235519543 Test-Loss: 0.3219157486095428 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [277/20000] Train-Loss: 1.2865216655933784 Test-Loss: 0.3219206978955923 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [278/20000] Train-Loss: 1.28648507924154 Test-Loss: 0.3219200154442575 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [279/20000] Train-Loss: 1.2864552318125493 Test-Loss: 0.3219195921474271 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [280/20000] Train-Loss: 1.286509541824207 Test-Loss: 0.32191760111242645 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [281/20000] Train-Loss: 1.286517727944084 Test-Loss: 0.3219091536833456 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [282/20000] Train-Loss: 1.2864611018285854 Test-Loss: 0.3219090163267121 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [283/20000] Train-Loss: 1.2865910255976174 Test-Loss: 0.3219102443066141 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [284/20000] Train-Loss: 1.2865011124748567 Test-Loss: 0.3219035788015588 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [285/20000] Train-Loss: 1.286462033175014 Test-Loss: 0.32190593492186487 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [286/20000] Train-Loss: 1.2865239309966003 Test-Loss: 0.3219115650189784 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [287/20000] Train-Loss: 1.2865270174214394 Test-Loss: 0.3219049125060671 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [288/20000] Train-Loss: 1.2866057788214886 Test-Loss: 0.321905279000497 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [289/20000] Train-Loss: 1.286564511246765 Test-Loss: 0.32190095295694127 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [290/20000] Train-Loss: 1.2865009449859766 Test-Loss: 0.3219028139326847 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [291/20000] Train-Loss: 1.2864531514908286 Test-Loss: 0.3218992645185686 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [292/20000] Train-Loss: 1.2864616578628651 Test-Loss: 0.32190646615560153 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [293/20000] Train-Loss: 1.2864717072868628 Test-Loss: 0.321900701719813 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [294/20000] Train-Loss: 1.2864738122945516 Test-Loss: 0.3218986498926911 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [295/20000] Train-Loss: 1.2865104358471224 Test-Loss: 0.3218958524740113 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [296/20000] Train-Loss: 1.286485118810817 Test-Loss: 0.32190084352749115 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [297/20000] Train-Loss: 1.2865632239627358 Test-Loss: 0.3218885690105792 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [298/20000] Train-Loss: 1.2865232061551537 Test-Loss: 0.32188833141998324 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [299/20000] Train-Loss: 1.2864291869889657 Test-Loss: 0.32189280268426435 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [300/20000] Train-Loss: 1.2863618739555247 Test-Loss: 0.3218904468740442 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [301/20000] Train-Loss: 1.2864684396352384 Test-Loss: 0.32188564266037106 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [302/20000] Train-Loss: 1.2864048309976515 Test-Loss: 0.32188581624503393 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [303/20000] Train-Loss: 1.28649869414888 Test-Loss: 0.3218924520358528 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [304/20000] Train-Loss: 1.2864094863531073 Test-Loss: 0.3218849566724559 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [305/20000] Train-Loss: 1.2863769822778717 Test-Loss: 0.32188447315939617 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [306/20000] Train-Loss: 1.2864901477524953 Test-Loss: 0.3218820093981574 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [307/20000] Train-Loss: 1.286459833192357 Test-Loss: 0.3218815837795698 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [308/20000] Train-Loss: 1.28658378419438 Test-Loss: 0.32187787601574175 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [309/20000] Train-Loss: 1.2864844198270198 Test-Loss: 0.3218851242064327 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [310/20000] Train-Loss: 1.2864647736410735 Test-Loss: 0.3218795278818539 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [311/20000] Train-Loss: 1.2864661669572433 Test-Loss: 0.3218796587888437 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [312/20000] Train-Loss: 1.286419645529108 Test-Loss: 0.32188374099934686 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [313/20000] Train-Loss: 1.2863816514225386 Test-Loss: 0.32188050536492424 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [314/20000] Train-Loss: 1.2864413715389817 Test-Loss: 0.3218812182846537 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [315/20000] Train-Loss: 1.286453933590555 Test-Loss: 0.32187789865164834 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [316/20000] Train-Loss: 1.2863809512975466 Test-Loss: 0.32188725520590467 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [317/20000] Train-Loss: 1.2864185542012867 Test-Loss: 0.3218772526486883 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [318/20000] Train-Loss: 1.2864199954049726 Test-Loss: 0.3218767293781281 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [319/20000] Train-Loss: 1.2863770718440581 Test-Loss: 0.3218806287669409 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [320/20000] Train-Loss: 1.2864088350028449 Test-Loss: 0.32187559962194945 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [321/20000] Train-Loss: 1.2864417354148392 Test-Loss: 0.3218776979797423 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [322/20000] Train-Loss: 1.2864668272215067 Test-Loss: 0.3218770805787805 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [323/20000] Train-Loss: 1.286547966204824 Test-Loss: 0.3218738460328985 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [324/20000] Train-Loss: 1.2865035847210657 Test-Loss: 0.3218719158307617 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [325/20000] Train-Loss: 1.2863954547875003 Test-Loss: 0.3218709998387488 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [326/20000] Train-Loss: 1.2864443862893222 Test-Loss: 0.32187194779021455 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [327/20000] Train-Loss: 1.286400942626663 Test-Loss: 0.321867966665497 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [328/20000] Train-Loss: 1.2862913807389185 Test-Loss: 0.32186796637073184 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [329/20000] Train-Loss: 1.2863396132098395 Test-Loss: 0.3218675521888172 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [330/20000] Train-Loss: 1.286388867689369 Test-Loss: 0.32187114893671914 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [331/20000] Train-Loss: 1.2865209068149672 Test-Loss: 0.32186861359638086 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [332/20000] Train-Loss: 1.286443243577795 Test-Loss: 0.3218662669959947 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [333/20000] Train-Loss: 1.286429625391354 Test-Loss: 0.3218629186045501 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [334/20000] Train-Loss: 1.2864237904773865 Test-Loss: 0.3218608841574421 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [335/20000] Train-Loss: 1.2863986485673102 Test-Loss: 0.3218632828205812 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [336/20000] Train-Loss: 1.286349824908797 Test-Loss: 0.3218675405094892 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [337/20000] Train-Loss: 1.2863924333642822 Test-Loss: 0.3218671224272158 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [338/20000] Train-Loss: 1.2863380252443342 Test-Loss: 0.3218621356426847 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [339/20000] Train-Loss: 1.2863426474365913 Test-Loss: 0.32185762210139784 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [340/20000] Train-Loss: 1.286458718516498 Test-Loss: 0.3218621986129856 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [341/20000] Train-Loss: 1.2863751022929333 Test-Loss: 0.3218622770742291 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [342/20000] Train-Loss: 1.2863692180484039 Test-Loss: 0.3218630458964341 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [343/20000] Train-Loss: 1.2862744110135627 Test-Loss: 0.32186021493789846 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [344/20000] Train-Loss: 1.2863312561249036 Test-Loss: 0.3218612329574421 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [345/20000] Train-Loss: 1.286512958666589 Test-Loss: 0.3218617707471074 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [346/20000] Train-Loss: 1.2863024363006785 Test-Loss: 0.3218594478179463 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [347/20000] Train-Loss: 1.2863250955473813 Test-Loss: 0.3218586046662931 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [348/20000] Train-Loss: 1.2864495487542174 Test-Loss: 0.32185436764591774 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [349/20000] Train-Loss: 1.2864172997306456 Test-Loss: 0.321852974561543 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [350/20000] Train-Loss: 1.2863998566643022 Test-Loss: 0.3218567031449829 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [351/20000] Train-Loss: 1.286309543963107 Test-Loss: 0.32185386038373837 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [352/20000] Train-Loss: 1.2863699395870176 Test-Loss: 0.32185269122229804 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [353/20000] Train-Loss: 1.2864924016751413 Test-Loss: 0.3218551477185825 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [354/20000] Train-Loss: 1.286377762693511 Test-Loss: 0.32185516919732804 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [355/20000] Train-Loss: 1.286324757661669 Test-Loss: 0.32185635254066947 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [356/20000] Train-Loss: 1.2863178070433883 Test-Loss: 0.3218508159690964 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [357/20000] Train-Loss: 1.2863810899431563 Test-Loss: 0.3218558486779522 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [358/20000] Train-Loss: 1.2863008251810608 Test-Loss: 0.3218566256784151 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [359/20000] Train-Loss: 1.2863144498216401 Test-Loss: 0.32185427399115846 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [360/20000] Train-Loss: 1.2863901221050587 Test-Loss: 0.3218551651336438 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [361/20000] Train-Loss: 1.2863073395823579 Test-Loss: 0.3218502338127777 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [362/20000] Train-Loss: 1.2863193624945783 Test-Loss: 0.3218515233685043 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [363/20000] Train-Loss: 1.286234154954771 Test-Loss: 0.32184842710143846 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [364/20000] Train-Loss: 1.2863933262509346 Test-Loss: 0.3218497614549707 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [365/20000] Train-Loss: 1.286426848604075 Test-Loss: 0.32185269206543143 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [366/20000] Train-Loss: 1.2863174005398612 Test-Loss: 0.32184552331504096 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [367/20000] Train-Loss: 1.2863274371472142 Test-Loss: 0.3218482816818006 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [368/20000] Train-Loss: 1.286257975915811 Test-Loss: 0.3218429651885625 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [369/20000] Train-Loss: 1.2863216655659635 Test-Loss: 0.3218499577504889 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [370/20000] Train-Loss: 1.2863928337864872 Test-Loss: 0.3218421072560335 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [371/20000] Train-Loss: 1.286260205345733 Test-Loss: 0.32184227412698646 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [372/20000] Train-Loss: 1.2863285969172393 Test-Loss: 0.321848084012257 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [373/20000] Train-Loss: 1.286458389286182 Test-Loss: 0.32184821629876487 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [374/20000] Train-Loss: 1.2863088897030117 Test-Loss: 0.3218429127009761 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [375/20000] Train-Loss: 1.2864839741232927 Test-Loss: 0.3218386546711572 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [376/20000] Train-Loss: 1.286397231873705 Test-Loss: 0.3218357255039025 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [377/20000] Train-Loss: 1.2863536790969554 Test-Loss: 0.3218359328699139 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [378/20000] Train-Loss: 1.2862309977682922 Test-Loss: 0.3218336439983202 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [379/20000] Train-Loss: 1.2864164090083263 Test-Loss: 0.3218327303050481 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [380/20000] Train-Loss: 1.2862375279437437 Test-Loss: 0.3218365957999382 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [381/20000] Train-Loss: 1.2863776459412517 Test-Loss: 0.3218340811348163 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [382/20000] Train-Loss: 1.28635923409897 Test-Loss: 0.32183409288465575 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [383/20000] Train-Loss: 1.2863321132171528 Test-Loss: 0.3218372122133582 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [384/20000] Train-Loss: 1.2864019457792346 Test-Loss: 0.32183449908023165 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [385/20000] Train-Loss: 1.2862809524579402 Test-Loss: 0.3218278675629471 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [386/20000] Train-Loss: 1.2862205564608882 Test-Loss: 0.32182761627545636 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [387/20000] Train-Loss: 1.2863704162944867 Test-Loss: 0.3218289090329666 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [388/20000] Train-Loss: 1.2862806325748515 Test-Loss: 0.321829010427033 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [389/20000] Train-Loss: 1.2861784592891456 Test-Loss: 0.32182822425144103 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [390/20000] Train-Loss: 1.2862953764899743 Test-Loss: 0.3218196345131229 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [391/20000] Train-Loss: 1.2862568082195542 Test-Loss: 0.3218224695627416 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [392/20000] Train-Loss: 1.2862758731058157 Test-Loss: 0.3218193655736257 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [393/20000] Train-Loss: 1.2862550625634213 Test-Loss: 0.321820667056866 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [394/20000] Train-Loss: 1.286174057339909 Test-Loss: 0.3218111504735693 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [395/20000] Train-Loss: 1.2861932454433118 Test-Loss: 0.3218168323576179 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [396/20000] Train-Loss: 1.2864219115607023 Test-Loss: 0.32181298601177954 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [397/20000] Train-Loss: 1.2862364284432817 Test-Loss: 0.3218113924629621 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [398/20000] Train-Loss: 1.2861295132243753 Test-Loss: 0.32181077505148903 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [399/20000] Train-Loss: 1.2861811622313235 Test-Loss: 0.3218078349625328 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [400/20000] Train-Loss: 1.2862477519396014 Test-Loss: 0.32181509834127464 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [401/20000] Train-Loss: 1.286192120050014 Test-Loss: 0.32180772050036405 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [402/20000] Train-Loss: 1.286244212415042 Test-Loss: 0.3218045630110147 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [403/20000] Train-Loss: 1.2862994361670732 Test-Loss: 0.3218078667403698 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [404/20000] Train-Loss: 1.2863210822936684 Test-Loss: 0.321805371826752 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [405/20000] Train-Loss: 1.2861127543731932 Test-Loss: 0.32180315739323423 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [406/20000] Train-Loss: 1.286170569806017 Test-Loss: 0.32179747749304644 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [407/20000] Train-Loss: 1.2861755175367064 Test-Loss: 0.32180237357558544 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [408/20000] Train-Loss: 1.2862142921256816 Test-Loss: 0.32179862740037896 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [409/20000] Train-Loss: 1.2861142292556207 Test-Loss: 0.3217992754308849 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [410/20000] Train-Loss: 1.2862291910371197 Test-Loss: 0.3217942605110348 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [411/20000] Train-Loss: 1.2860436379402485 Test-Loss: 0.3217947168220395 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [412/20000] Train-Loss: 1.2861486880802764 Test-Loss: 0.3217948398947917 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [413/20000] Train-Loss: 1.2861681768684419 Test-Loss: 0.3217853778763883 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [414/20000] Train-Loss: 1.2861547546194216 Test-Loss: 0.32178694370761074 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [415/20000] Train-Loss: 1.286155017159312 Test-Loss: 0.3217938455434878 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [416/20000] Train-Loss: 1.2861899212998098 Test-Loss: 0.32178542424849715 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [417/20000] Train-Loss: 1.286172136451224 Test-Loss: 0.3217810770183205 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [418/20000] Train-Loss: 1.2861353610532433 Test-Loss: 0.3217817356060005 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [419/20000] Train-Loss: 1.2861491254579809 Test-Loss: 0.3217848245231471 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [420/20000] Train-Loss: 1.28612314736413 Test-Loss: 0.32178275224294167 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [421/20000] Train-Loss: 1.286163400629257 Test-Loss: 0.3217749119204953 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [422/20000] Train-Loss: 1.2861468059479289 Test-Loss: 0.3217772160101271 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [423/20000] Train-Loss: 1.2861344149449365 Test-Loss: 0.32178034277388307 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [424/20000] Train-Loss: 1.2860602040291549 Test-Loss: 0.3217802277908224 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [425/20000] Train-Loss: 1.2861273353438951 Test-Loss: 0.32177770306934983 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [426/20000] Train-Loss: 1.2861674010400839 Test-Loss: 0.32178266119297355 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [427/20000] Train-Loss: 1.286146179977171 Test-Loss: 0.32177389275223517 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [428/20000] Train-Loss: 1.2861197348513878 Test-Loss: 0.32177533271030595 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [429/20000] Train-Loss: 1.2860431980072031 Test-Loss: 0.32177424507896163 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [430/20000] Train-Loss: 1.2861441499717015 Test-Loss: 0.32177315907538856 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [431/20000] Train-Loss: 1.2861469735582174 Test-Loss: 0.32177035331696563 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [432/20000] Train-Loss: 1.2860937324652375 Test-Loss: 0.3217693283714009 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [433/20000] Train-Loss: 1.2860738029228673 Test-Loss: 0.3217687373450712 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [434/20000] Train-Loss: 1.2860539419965906 Test-Loss: 0.3217650061265941 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [435/20000] Train-Loss: 1.286058526309406 Test-Loss: 0.3217657963071645 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [436/20000] Train-Loss: 1.2861557929482086 Test-Loss: 0.3217688819914483 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [437/20000] Train-Loss: 1.2860267225202222 Test-Loss: 0.3217621448309466 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [438/20000] Train-Loss: 1.2860125881749491 Test-Loss: 0.32176867427896677 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [439/20000] Train-Loss: 1.2860944686989328 Test-Loss: 0.32175835815840326 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [440/20000] Train-Loss: 1.2860362726821228 Test-Loss: 0.3217649379186733 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [441/20000] Train-Loss: 1.2861148951607384 Test-Loss: 0.32176404767069655 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [442/20000] Train-Loss: 1.286122511817515 Test-Loss: 0.32176498665009806 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [443/20000] Train-Loss: 1.286071658973371 Test-Loss: 0.32175812669363857 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [444/20000] Train-Loss: 1.2862076846970236 Test-Loss: 0.32176892315006933 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [445/20000] Train-Loss: 1.286054645399781 Test-Loss: 0.32175556017384827 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [446/20000] Train-Loss: 1.2861656276861626 Test-Loss: 0.32175778626518897 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [447/20000] Train-Loss: 1.286021009549677 Test-Loss: 0.32176504093001856 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [448/20000] Train-Loss: 1.2862447084737476 Test-Loss: 0.3217672289075351 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [449/20000] Train-Loss: 1.2860195561316123 Test-Loss: 0.3217573216051667 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [450/20000] Train-Loss: 1.28607877951157 Test-Loss: 0.3217587470209871 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [451/20000] Train-Loss: 1.286098086892643 Test-Loss: 0.32175934518731397 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [452/20000] Train-Loss: 1.2861063230184533 Test-Loss: 0.3217621979233287 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [453/20000] Train-Loss: 1.2859737401358406 Test-Loss: 0.32175899502542543 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [454/20000] Train-Loss: 1.2860704146857256 Test-Loss: 0.3217526587884402 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [455/20000] Train-Loss: 1.2860466162902997 Test-Loss: 0.32175061919091796 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [456/20000] Train-Loss: 1.286093772637519 Test-Loss: 0.3217575179954726 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [457/20000] Train-Loss: 1.2860317398442997 Test-Loss: 0.3217596450230181 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [458/20000] Train-Loss: 1.2860935551951171 Test-Loss: 0.3217579806493891 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [459/20000] Train-Loss: 1.2860769346353895 Test-Loss: 0.32175204852130823 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [460/20000] Train-Loss: 1.286076643221135 Test-Loss: 0.3217557264422075 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [461/20000] Train-Loss: 1.2860125843712769 Test-Loss: 0.3217511896805854 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [462/20000] Train-Loss: 1.2860412084110389 Test-Loss: 0.3217521478429267 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [463/20000] Train-Loss: 1.2860632847444724 Test-Loss: 0.32175205821616537 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [464/20000] Train-Loss: 1.2860208809304712 Test-Loss: 0.3217481308330803 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [465/20000] Train-Loss: 1.2859555968830012 Test-Loss: 0.3217471381584551 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [466/20000] Train-Loss: 1.2860001975314508 Test-Loss: 0.32174494450199853 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [467/20000] Train-Loss: 1.2859848926814994 Test-Loss: 0.321744257466348 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [468/20000] Train-Loss: 1.286047414119975 Test-Loss: 0.32175207973226333 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [469/20000] Train-Loss: 1.2860217387439317 Test-Loss: 0.321747794890634 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [470/20000] Train-Loss: 1.2861020032290105 Test-Loss: 0.3217573671249789 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [471/20000] Train-Loss: 1.2860087220237684 Test-Loss: 0.32175313685812706 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [472/20000] Train-Loss: 1.2861115554398594 Test-Loss: 0.3217421488909673 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [473/20000] Train-Loss: 1.2859668467893512 Test-Loss: 0.32174820674645715 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [474/20000] Train-Loss: 1.2860734730263348 Test-Loss: 0.3217548451515757 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [475/20000] Train-Loss: 1.2859671597985503 Test-Loss: 0.3217550587225859 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [476/20000] Train-Loss: 1.286019198073583 Test-Loss: 0.32174317945364833 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [477/20000] Train-Loss: 1.285967441882289 Test-Loss: 0.3217436220150274 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [478/20000] Train-Loss: 1.2860163733686218 Test-Loss: 0.3217384635009857 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [479/20000] Train-Loss: 1.2860275015925344 Test-Loss: 0.3217449380865136 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [480/20000] Train-Loss: 1.2859810761641448 Test-Loss: 0.3217387818390498 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [481/20000] Train-Loss: 1.2860348802635309 Test-Loss: 0.32173631449992707 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [482/20000] Train-Loss: 1.2859916403226084 Test-Loss: 0.3217331227970832 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [483/20000] Train-Loss: 1.2859566903504243 Test-Loss: 0.321742423904326 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [484/20000] Train-Loss: 1.286013068587655 Test-Loss: 0.3217323551712351 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [485/20000] Train-Loss: 1.2860466784214732 Test-Loss: 0.3217365681599686 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [486/20000] Train-Loss: 1.2860480995869328 Test-Loss: 0.32173916932772295 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [487/20000] Train-Loss: 1.2860051549544254 Test-Loss: 0.321732167036416 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [488/20000] Train-Loss: 1.2860194271995076 Test-Loss: 0.3217333725662577 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [489/20000] Train-Loss: 1.285966069954764 Test-Loss: 0.32172862497603066 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [490/20000] Train-Loss: 1.2861150692776246 Test-Loss: 0.32173237413398603 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [491/20000] Train-Loss: 1.2859724322401886 Test-Loss: 0.32172869816745536 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [492/20000] Train-Loss: 1.2858992013001884 Test-Loss: 0.3217302023303239 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [493/20000] Train-Loss: 1.2860439200482292 Test-Loss: 0.3217357820254943 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [494/20000] Train-Loss: 1.2859105380028264 Test-Loss: 0.32173119342153456 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [495/20000] Train-Loss: 1.2858927622543492 Test-Loss: 0.3217247988549377 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [496/20000] Train-Loss: 1.2859982710911737 Test-Loss: 0.3217275522303516 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [497/20000] Train-Loss: 1.2860053951255705 Test-Loss: 0.3217185003594824 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [498/20000] Train-Loss: 1.286014548449397 Test-Loss: 0.32172663216073927 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [499/20000] Train-Loss: 1.286047166825878 Test-Loss: 0.3217257819200822 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [500/20000] Train-Loss: 1.2859484324308108 Test-Loss: 0.32172198617979253 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [501/20000] Train-Loss: 1.2859054912217458 Test-Loss: 0.32172104359070536 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [502/20000] Train-Loss: 1.2859803003475136 Test-Loss: 0.32172686010907203 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [503/20000] Train-Loss: 1.285915891703717 Test-Loss: 0.3217258356298726 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [504/20000] Train-Loss: 1.2858476690945515 Test-Loss: 0.3217268826193152 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [505/20000] Train-Loss: 1.285959704551975 Test-Loss: 0.321727985936779 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [506/20000] Train-Loss: 1.2858849646100472 Test-Loss: 0.3217209880047681 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [507/20000] Train-Loss: 1.2858990726719117 Test-Loss: 0.32172446675262995 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [508/20000] Train-Loss: 1.2859730391925985 Test-Loss: 0.32171918636816516 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [509/20000] Train-Loss: 1.285907225880428 Test-Loss: 0.32172108040868164 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [510/20000] Train-Loss: 1.2860807090754094 Test-Loss: 0.3217162243301867 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [511/20000] Train-Loss: 1.2859450101175165 Test-Loss: 0.32171615814056415 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [512/20000] Train-Loss: 1.2858819825259138 Test-Loss: 0.3217126473398519 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [513/20000] Train-Loss: 1.285905272087261 Test-Loss: 0.321712595293391 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [514/20000] Train-Loss: 1.2859162291090138 Test-Loss: 0.3217115491849494 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [515/20000] Train-Loss: 1.2859116885079334 Test-Loss: 0.3217088820091413 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [516/20000] Train-Loss: 1.2858687476074415 Test-Loss: 0.32171203510382657 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [517/20000] Train-Loss: 1.2859531207440218 Test-Loss: 0.3217099586428322 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [518/20000] Train-Loss: 1.2859963265958656 Test-Loss: 0.32171142232163263 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [519/20000] Train-Loss: 1.2859022338256776 Test-Loss: 0.32171208389491124 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [520/20000] Train-Loss: 1.2860279823016867 Test-Loss: 0.3217022293508319 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [521/20000] Train-Loss: 1.285954113281386 Test-Loss: 0.32170715912293313 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [522/20000] Train-Loss: 1.2859656729515814 Test-Loss: 0.32170809906707204 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [523/20000] Train-Loss: 1.2858629046313752 Test-Loss: 0.32170579813641037 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [524/20000] Train-Loss: 1.2858744784670633 Test-Loss: 0.32171126791378263 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [525/20000] Train-Loss: 1.285958480068979 Test-Loss: 0.3217046768550649 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [526/20000] Train-Loss: 1.2860443681097828 Test-Loss: 0.3217132287139391 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [527/20000] Train-Loss: 1.2857908184266442 Test-Loss: 0.3217031570863787 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [528/20000] Train-Loss: 1.2859119859611672 Test-Loss: 0.32170303370444964 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [529/20000] Train-Loss: 1.2858030795062685 Test-Loss: 0.32170236511351336 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [530/20000] Train-Loss: 1.2858036516004383 Test-Loss: 0.3217050817558816 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [531/20000] Train-Loss: 1.2859709948425577 Test-Loss: 0.3217007298960887 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [532/20000] Train-Loss: 1.2858179994372971 Test-Loss: 0.3216925891545514 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [533/20000] Train-Loss: 1.2858652432515594 Test-Loss: 0.32169090355130986 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [534/20000] Train-Loss: 1.2859016961531613 Test-Loss: 0.3216929554236493 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [535/20000] Train-Loss: 1.2859420581135566 Test-Loss: 0.3216927950575076 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [536/20000] Train-Loss: 1.2860019811863233 Test-Loss: 0.321690050559383 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [537/20000] Train-Loss: 1.2857316139770503 Test-Loss: 0.32168656899902864 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [538/20000] Train-Loss: 1.2860130453714458 Test-Loss: 0.3216827075055612 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [539/20000] Train-Loss: 1.285835519735512 Test-Loss: 0.3216821137314836 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [540/20000] Train-Loss: 1.2858767961845763 Test-Loss: 0.3216759266294339 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [541/20000] Train-Loss: 1.2858392775289018 Test-Loss: 0.32167279407548194 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [542/20000] Train-Loss: 1.2858301334754096 Test-Loss: 0.3216803186641505 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [543/20000] Train-Loss: 1.285897868543586 Test-Loss: 0.3216760706973809 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [544/20000] Train-Loss: 1.2858676747920241 Test-Loss: 0.3216856533004523 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [545/20000] Train-Loss: 1.2858628204479103 Test-Loss: 0.321674755036326 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [546/20000] Train-Loss: 1.2858226450830983 Test-Loss: 0.321676936521163 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [547/20000] Train-Loss: 1.2858783886783014 Test-Loss: 0.32166871338822295 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [548/20000] Train-Loss: 1.285741255076298 Test-Loss: 0.32167514322602553 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [549/20000] Train-Loss: 1.285782771746373 Test-Loss: 0.32166956028458177 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [550/20000] Train-Loss: 1.2858015714140556 Test-Loss: 0.321676478145502 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [551/20000] Train-Loss: 1.2857897761691632 Test-Loss: 0.32166989131595347 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [552/20000] Train-Loss: 1.2857750385093112 Test-Loss: 0.32167022804025885 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [553/20000] Train-Loss: 1.2858765810329866 Test-Loss: 0.3216731112237514 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [554/20000] Train-Loss: 1.2858210946017665 Test-Loss: 0.3216685358672845 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [555/20000] Train-Loss: 1.28587812610914 Test-Loss: 0.32167421344639197 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [556/20000] Train-Loss: 1.28582772499195 Test-Loss: 0.321674103351131 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [557/20000] Train-Loss: 1.2857452885624006 Test-Loss: 0.32166774893632677 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [558/20000] Train-Loss: 1.285816477761916 Test-Loss: 0.32167716145528547 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [559/20000] Train-Loss: 1.2857894425518936 Test-Loss: 0.321665458545197 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [560/20000] Train-Loss: 1.2857906808538984 Test-Loss: 0.3216621002676009 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [561/20000] Train-Loss: 1.2857744007175216 Test-Loss: 0.32166755931646884 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [562/20000] Train-Loss: 1.2858185828509645 Test-Loss: 0.32166856323780924 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [563/20000] Train-Loss: 1.2858379768500066 Test-Loss: 0.3216677057790329 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [564/20000] Train-Loss: 1.2857156197748196 Test-Loss: 0.3216657717426912 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [565/20000] Train-Loss: 1.2857045994259955 Test-Loss: 0.3216586920929408 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [566/20000] Train-Loss: 1.2857653371143407 Test-Loss: 0.32165916195908617 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [567/20000] Train-Loss: 1.2856459280797605 Test-Loss: 0.32166414017548295 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [568/20000] Train-Loss: 1.2857876619606239 Test-Loss: 0.32166444137811145 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [569/20000] Train-Loss: 1.28570288119859 Test-Loss: 0.3216678820009528 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [570/20000] Train-Loss: 1.285732414356182 Test-Loss: 0.32166055200162774 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [571/20000] Train-Loss: 1.285839033620377 Test-Loss: 0.3216631299816645 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [572/20000] Train-Loss: 1.2857088550422493 Test-Loss: 0.32165206909062183 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [573/20000] Train-Loss: 1.2856938149801531 Test-Loss: 0.321652509833286 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [574/20000] Train-Loss: 1.2856719285943543 Test-Loss: 0.32166181237137575 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [575/20000] Train-Loss: 1.2856672204181363 Test-Loss: 0.3216532462519877 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [576/20000] Train-Loss: 1.2859179695019478 Test-Loss: 0.3216511503935392 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [577/20000] Train-Loss: 1.285722968169784 Test-Loss: 0.32165130582674906 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [578/20000] Train-Loss: 1.2856717410502165 Test-Loss: 0.32165293219173596 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [579/20000] Train-Loss: 1.285845581217195 Test-Loss: 0.3216464378577446 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [580/20000] Train-Loss: 1.2857438024217478 Test-Loss: 0.3216536379622177 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [581/20000] Train-Loss: 1.2857087199739081 Test-Loss: 0.321650590130021 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [582/20000] Train-Loss: 1.2857434766038123 Test-Loss: 0.32165272744064466 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [583/20000] Train-Loss: 1.285636853586834 Test-Loss: 0.3216501015948275 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [584/20000] Train-Loss: 1.2857555129734732 Test-Loss: 0.32164668934355095 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [585/20000] Train-Loss: 1.2856796723540937 Test-Loss: 0.3216508251160636 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [586/20000] Train-Loss: 1.2857983881150843 Test-Loss: 0.32164806792663103 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [587/20000] Train-Loss: 1.2856652477120052 Test-Loss: 0.3216393223412096 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [588/20000] Train-Loss: 1.2857974670759469 Test-Loss: 0.321642184413274 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [589/20000] Train-Loss: 1.2856707783855765 Test-Loss: 0.32164266392632074 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [590/20000] Train-Loss: 1.285711490875588 Test-Loss: 0.321641629895273 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [591/20000] Train-Loss: 1.2857617009215199 Test-Loss: 0.32163753803877193 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [592/20000] Train-Loss: 1.2857354655976747 Test-Loss: 0.3216398086268704 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [593/20000] Train-Loss: 1.2856930827895863 Test-Loss: 0.321638017062363 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [594/20000] Train-Loss: 1.285780424149172 Test-Loss: 0.32164182145382425 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [595/20000] Train-Loss: 1.2856294194551103 Test-Loss: 0.32163670646200776 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [596/20000] Train-Loss: 1.2857356147223045 Test-Loss: 0.3216351256215912 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [597/20000] Train-Loss: 1.2856063650807297 Test-Loss: 0.32163337033395495 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [598/20000] Train-Loss: 1.2856581361517188 Test-Loss: 0.32163380982477546 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [599/20000] Train-Loss: 1.2856082793753776 Test-Loss: 0.3216322657012931 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [600/20000] Train-Loss: 1.2856539025313867 Test-Loss: 0.32163101192725285 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [601/20000] Train-Loss: 1.285696771502664 Test-Loss: 0.3216284000234062 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [602/20000] Train-Loss: 1.2856688389296542 Test-Loss: 0.32163111502755093 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [603/20000] Train-Loss: 1.2855807743971222 Test-Loss: 0.3216302880947873 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [604/20000] Train-Loss: 1.2857458018614307 Test-Loss: 0.3216293364612767 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [605/20000] Train-Loss: 1.2856946508979838 Test-Loss: 0.3216233615419045 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [606/20000] Train-Loss: 1.2857159442577653 Test-Loss: 0.321624325700053 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [607/20000] Train-Loss: 1.2857080179977805 Test-Loss: 0.32162542642375913 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [608/20000] Train-Loss: 1.2856240877761937 Test-Loss: 0.3216206641586691 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [609/20000] Train-Loss: 1.2856375925512125 Test-Loss: 0.3216220855250738 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [610/20000] Train-Loss: 1.2855970401048689 Test-Loss: 0.32162433580883504 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [611/20000] Train-Loss: 1.2856166110510951 Test-Loss: 0.32162315300751865 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [612/20000] Train-Loss: 1.2856427141599054 Test-Loss: 0.32162479618415984 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [613/20000] Train-Loss: 1.2856436068909822 Test-Loss: 0.32162113755649163 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [614/20000] Train-Loss: 1.2857038154228215 Test-Loss: 0.32162865680173014 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [615/20000] Train-Loss: 1.285542218565403 Test-Loss: 0.32162376475520044 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [616/20000] Train-Loss: 1.285650610887847 Test-Loss: 0.32161980955554176 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [617/20000] Train-Loss: 1.2857670265690222 Test-Loss: 0.32162322326807363 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [618/20000] Train-Loss: 1.2856065996482964 Test-Loss: 0.321622380495609 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [619/20000] Train-Loss: 1.285571547619952 Test-Loss: 0.32162197463639347 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [620/20000] Train-Loss: 1.2855210309481995 Test-Loss: 0.32161699945245115 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [621/20000] Train-Loss: 1.2856268122769086 Test-Loss: 0.3216208160716786 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [622/20000] Train-Loss: 1.2855795529785798 Test-Loss: 0.3216171737908867 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [623/20000] Train-Loss: 1.2855875673823918 Test-Loss: 0.32161452645619887 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [624/20000] Train-Loss: 1.2855285708622783 Test-Loss: 0.3216149336664614 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [625/20000] Train-Loss: 1.2855963771551306 Test-Loss: 0.32161618916779644 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [626/20000] Train-Loss: 1.285595658432491 Test-Loss: 0.3216046679353765 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [627/20000] Train-Loss: 1.2855380306005386 Test-Loss: 0.3216129544918982 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [628/20000] Train-Loss: 1.2855813810749064 Test-Loss: 0.3216104524536342 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [629/20000] Train-Loss: 1.2855407995446495 Test-Loss: 0.3216063460891895 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [630/20000] Train-Loss: 1.2855266857809613 Test-Loss: 0.32160674869161393 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [631/20000] Train-Loss: 1.2855002232492163 Test-Loss: 0.32160547447694976 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [632/20000] Train-Loss: 1.2855031106250967 Test-Loss: 0.32160262063261374 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [633/20000] Train-Loss: 1.285574774715146 Test-Loss: 0.32159790092841706 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [634/20000] Train-Loss: 1.2855301696035604 Test-Loss: 0.3216033795286776 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [635/20000] Train-Loss: 1.2856997149604823 Test-Loss: 0.3216012091972308 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [636/20000] Train-Loss: 1.285600203081489 Test-Loss: 0.3215984959746283 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [637/20000] Train-Loss: 1.2855158498070043 Test-Loss: 0.3215998811690919 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [638/20000] Train-Loss: 1.2856049606032907 Test-Loss: 0.32159809671671136 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [639/20000] Train-Loss: 1.285488812593588 Test-Loss: 0.32159971882999516 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [640/20000] Train-Loss: 1.285559771037166 Test-Loss: 0.3215990677150533 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [641/20000] Train-Loss: 1.2854915696496911 Test-Loss: 0.32159246493334614 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [642/20000] Train-Loss: 1.2855324389477907 Test-Loss: 0.32159543244333216 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [643/20000] Train-Loss: 1.2855168347066674 Test-Loss: 0.3215906115737553 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [644/20000] Train-Loss: 1.2854092012536684 Test-Loss: 0.32158973333456153 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [645/20000] Train-Loss: 1.2854922599579692 Test-Loss: 0.32159369410833033 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [646/20000] Train-Loss: 1.285595702437963 Test-Loss: 0.3215931749758097 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [647/20000] Train-Loss: 1.2855314751724665 Test-Loss: 0.32159224164095923 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [648/20000] Train-Loss: 1.2855028013534737 Test-Loss: 0.3215870934777518 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [649/20000] Train-Loss: 1.2854324078144488 Test-Loss: 0.32158733768170217 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [650/20000] Train-Loss: 1.2854626325765042 Test-Loss: 0.32158729762736804 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [651/20000] Train-Loss: 1.285449396657342 Test-Loss: 0.32158840266700917 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [652/20000] Train-Loss: 1.2854782675996737 Test-Loss: 0.32158532692388947 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [653/20000] Train-Loss: 1.2854449392158933 Test-Loss: 0.3215836597390567 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [654/20000] Train-Loss: 1.285361425782148 Test-Loss: 0.32157756830111023 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [655/20000] Train-Loss: 1.2855843737057242 Test-Loss: 0.3215871820563979 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [656/20000] Train-Loss: 1.2855163669178138 Test-Loss: 0.3215838209978417 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [657/20000] Train-Loss: 1.2854804426349755 Test-Loss: 0.321580603632113 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [658/20000] Train-Loss: 1.2854977535169336 Test-Loss: 0.3215798736597757 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [659/20000] Train-Loss: 1.285409589938075 Test-Loss: 0.3215821946867523 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [660/20000] Train-Loss: 1.2854386872754004 Test-Loss: 0.32157417683480893 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [661/20000] Train-Loss: 1.285439102372879 Test-Loss: 0.3215781369269846 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [662/20000] Train-Loss: 1.28537319185742 Test-Loss: 0.3215748489168896 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [663/20000] Train-Loss: 1.2854570206393636 Test-Loss: 0.3215753747352266 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [664/20000] Train-Loss: 1.2855813744353244 Test-Loss: 0.32157729579275046 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [665/20000] Train-Loss: 1.2853822945945519 Test-Loss: 0.3215743228603336 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [666/20000] Train-Loss: 1.2854645058785286 Test-Loss: 0.32156923416054656 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [667/20000] Train-Loss: 1.2854017848118573 Test-Loss: 0.3215718680154184 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [668/20000] Train-Loss: 1.2854486140958208 Test-Loss: 0.3215688072964076 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [669/20000] Train-Loss: 1.2854044754699983 Test-Loss: 0.321562417250206 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [670/20000] Train-Loss: 1.2855254572077368 Test-Loss: 0.3215699560053191 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [671/20000] Train-Loss: 1.285425122547581 Test-Loss: 0.3215660776990557 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [672/20000] Train-Loss: 1.2853507060211304 Test-Loss: 0.3215676974779015 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [673/20000] Train-Loss: 1.2853879950193012 Test-Loss: 0.3215671626155728 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [674/20000] Train-Loss: 1.2854662214844914 Test-Loss: 0.32156230347649006 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [675/20000] Train-Loss: 1.2854458951610643 Test-Loss: 0.3215613136368354 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [676/20000] Train-Loss: 1.2855083208396836 Test-Loss: 0.3215639312512364 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [677/20000] Train-Loss: 1.2854628827919314 Test-Loss: 0.32155966791440754 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [678/20000] Train-Loss: 1.2854140826375795 Test-Loss: 0.3215595326482492 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [679/20000] Train-Loss: 1.2855367183689754 Test-Loss: 0.3215587017366776 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [680/20000] Train-Loss: 1.285546857750711 Test-Loss: 0.32155946400220015 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [681/20000] Train-Loss: 1.2854302791634347 Test-Loss: 0.3215596732759553 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [682/20000] Train-Loss: 1.2854202378469115 Test-Loss: 0.32156221804957275 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [683/20000] Train-Loss: 1.285371100004913 Test-Loss: 0.3215612309501936 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [684/20000] Train-Loss: 1.2854146258649561 Test-Loss: 0.32155185587571555 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [685/20000] Train-Loss: 1.2853949842324814 Test-Loss: 0.321560181005384 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [686/20000] Train-Loss: 1.2853407192707538 Test-Loss: 0.32155142295673544 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [687/20000] Train-Loss: 1.2854085460684248 Test-Loss: 0.3215542400058438 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [688/20000] Train-Loss: 1.2852482405636134 Test-Loss: 0.32155486943088607 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [689/20000] Train-Loss: 1.2854244427455253 Test-Loss: 0.3215451780286036 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [690/20000] Train-Loss: 1.2853529155659766 Test-Loss: 0.32154759559494256 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [691/20000] Train-Loss: 1.2854149968633346 Test-Loss: 0.3215472358544131 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [692/20000] Train-Loss: 1.2854335754679462 Test-Loss: 0.3215481852781563 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [693/20000] Train-Loss: 1.285256316775297 Test-Loss: 0.3215448008878232 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [694/20000] Train-Loss: 1.2853978702856568 Test-Loss: 0.32155149746735995 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [695/20000] Train-Loss: 1.2852078227790062 Test-Loss: 0.3215450044966264 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [696/20000] Train-Loss: 1.2854672478703295 Test-Loss: 0.3215460606180248 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [697/20000] Train-Loss: 1.2852671785850207 Test-Loss: 0.3215428015786898 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [698/20000] Train-Loss: 1.2853085844175645 Test-Loss: 0.32153720001913666 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [699/20000] Train-Loss: 1.2854070821552386 Test-Loss: 0.3215428053195505 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [700/20000] Train-Loss: 1.285215319853266 Test-Loss: 0.32153986434009785 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [701/20000] Train-Loss: 1.2852596857964766 Test-Loss: 0.32154432436129293 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [702/20000] Train-Loss: 1.2852930935518707 Test-Loss: 0.32153934209410906 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [703/20000] Train-Loss: 1.2852420802776041 Test-Loss: 0.32153357707617014 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [704/20000] Train-Loss: 1.285301678318026 Test-Loss: 0.32153894983632625 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [705/20000] Train-Loss: 1.2852139923697725 Test-Loss: 0.3215317828189876 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [706/20000] Train-Loss: 1.2851955520990426 Test-Loss: 0.32153149039550455 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [707/20000] Train-Loss: 1.2851876211344013 Test-Loss: 0.3215238680684122 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [708/20000] Train-Loss: 1.2852982337275798 Test-Loss: 0.3215309871695566 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [709/20000] Train-Loss: 1.2853201191396946 Test-Loss: 0.3215299740118105 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [710/20000] Train-Loss: 1.2852623549701268 Test-Loss: 0.32152676060974517 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [711/20000] Train-Loss: 1.2853232025878638 Test-Loss: 0.32152917882417126 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [712/20000] Train-Loss: 1.2852572527469708 Test-Loss: 0.32152929088953086 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [713/20000] Train-Loss: 1.2853269285115083 Test-Loss: 0.3215229204783112 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [714/20000] Train-Loss: 1.2853516423030247 Test-Loss: 0.3215195226629457 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [715/20000] Train-Loss: 1.2851791966137602 Test-Loss: 0.3215219834533713 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [716/20000] Train-Loss: 1.285200438151157 Test-Loss: 0.3215231293160798 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [717/20000] Train-Loss: 1.2852747337272863 Test-Loss: 0.32152921661287015 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [718/20000] Train-Loss: 1.2852780690328485 Test-Loss: 0.321522636842308 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [719/20000] Train-Loss: 1.2853257561219116 Test-Loss: 0.32152508139944236 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [720/20000] Train-Loss: 1.2853420172034224 Test-Loss: 0.3215181730443043 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [721/20000] Train-Loss: 1.2851270585320054 Test-Loss: 0.3215210447761439 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [722/20000] Train-Loss: 1.2852499518380918 Test-Loss: 0.32151930649891675 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [723/20000] Train-Loss: 1.285125959024046 Test-Loss: 0.3215133279555268 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [724/20000] Train-Loss: 1.2853876025999214 Test-Loss: 0.32151444390520684 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [725/20000] Train-Loss: 1.2851259623764717 Test-Loss: 0.3215130111298615 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [726/20000] Train-Loss: 1.2851710454018437 Test-Loss: 0.3215075618594733 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [727/20000] Train-Loss: 1.2852680834841017 Test-Loss: 0.32151287244034654 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [728/20000] Train-Loss: 1.2853008464177724 Test-Loss: 0.32150594325072945 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [729/20000] Train-Loss: 1.2852284915537784 Test-Loss: 0.3215054111147381 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [730/20000] Train-Loss: 1.2851302994171474 Test-Loss: 0.3215015814542211 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [731/20000] Train-Loss: 1.2851057127753105 Test-Loss: 0.32150326651720595 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [732/20000] Train-Loss: 1.2852341020547553 Test-Loss: 0.32150277417607437 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [733/20000] Train-Loss: 1.2852927681194426 Test-Loss: 0.3215004442667419 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [734/20000] Train-Loss: 1.2851428557224371 Test-Loss: 0.3214954223499623 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [735/20000] Train-Loss: 1.285280682566924 Test-Loss: 0.32149939251006354 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [736/20000] Train-Loss: 1.2851957655553061 Test-Loss: 0.3214996557630448 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [737/20000] Train-Loss: 1.285125171376957 Test-Loss: 0.32150212621850155 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [738/20000] Train-Loss: 1.285183033672013 Test-Loss: 0.3214980468943419 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [739/20000] Train-Loss: 1.285172033344447 Test-Loss: 0.3214981554857478 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [740/20000] Train-Loss: 1.2852090566664118 Test-Loss: 0.32149645685819705 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [741/20000] Train-Loss: 1.2851370123798305 Test-Loss: 0.3214946563861441 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [742/20000] Train-Loss: 1.2850926853215718 Test-Loss: 0.3215005441952415 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [743/20000] Train-Loss: 1.2851105955837903 Test-Loss: 0.3214910732858229 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [744/20000] Train-Loss: 1.2850843781201102 Test-Loss: 0.32149260168353627 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [745/20000] Train-Loss: 1.2851832087061839 Test-Loss: 0.32149166583332794 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [746/20000] Train-Loss: 1.285132136133142 Test-Loss: 0.3214943749670273 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [747/20000] Train-Loss: 1.2851524705379684 Test-Loss: 0.3214843254013385 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [748/20000] Train-Loss: 1.2851012839916822 Test-Loss: 0.3214871769634456 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [749/20000] Train-Loss: 1.2851601766961311 Test-Loss: 0.32148986169659094 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [750/20000] Train-Loss: 1.2851001152308708 Test-Loss: 0.3214848440940258 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [751/20000] Train-Loss: 1.2852410930354152 Test-Loss: 0.32149012366740926 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [752/20000] Train-Loss: 1.285203263804911 Test-Loss: 0.3214805928753987 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [753/20000] Train-Loss: 1.285081021479979 Test-Loss: 0.3214799628244646 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [754/20000] Train-Loss: 1.2851516321749599 Test-Loss: 0.3214898242688489 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [755/20000] Train-Loss: 1.2850260626160603 Test-Loss: 0.3214777579469226 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [756/20000] Train-Loss: 1.2850949081559724 Test-Loss: 0.32147766643398457 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [757/20000] Train-Loss: 1.285094633302655 Test-Loss: 0.3214810199112959 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [758/20000] Train-Loss: 1.2850439172912789 Test-Loss: 0.3214740578668454 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [759/20000] Train-Loss: 1.2850542014688435 Test-Loss: 0.3214773330597453 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [760/20000] Train-Loss: 1.2851317839640901 Test-Loss: 0.32147383054347023 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [761/20000] Train-Loss: 1.2851866825502491 Test-Loss: 0.32148506024174 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [762/20000] Train-Loss: 1.2850609434943652 Test-Loss: 0.3214814140986702 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [763/20000] Train-Loss: 1.2850088660510852 Test-Loss: 0.3214760197807333 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [764/20000] Train-Loss: 1.2850032186743725 Test-Loss: 0.32147104427816425 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [765/20000] Train-Loss: 1.2850777555093646 Test-Loss: 0.32146888669434676 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [766/20000] Train-Loss: 1.2850091589780885 Test-Loss: 0.3214695168545028 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [767/20000] Train-Loss: 1.285133262337405 Test-Loss: 0.3214628982690194 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [768/20000] Train-Loss: 1.2850007138654118 Test-Loss: 0.32145937400921354 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [769/20000] Train-Loss: 1.2851583616112403 Test-Loss: 0.3214621264332905 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [770/20000] Train-Loss: 1.2850034161883284 Test-Loss: 0.3214572861354921 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [771/20000] Train-Loss: 1.2850995597933856 Test-Loss: 0.3214630401980021 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [772/20000] Train-Loss: 1.2851188889270617 Test-Loss: 0.32146213365407456 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [773/20000] Train-Loss: 1.285017045755577 Test-Loss: 0.32145672180001683 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [774/20000] Train-Loss: 1.2848993546544487 Test-Loss: 0.3214584232301439 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [775/20000] Train-Loss: 1.2850231227273061 Test-Loss: 0.3214599488583531 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [776/20000] Train-Loss: 1.2851001066660166 Test-Loss: 0.32145812685089686 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [777/20000] Train-Loss: 1.2850367692211928 Test-Loss: 0.321456421763039 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [778/20000] Train-Loss: 1.2849584343119933 Test-Loss: 0.3214577201559764 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [779/20000] Train-Loss: 1.2850177358444415 Test-Loss: 0.3214513954102083 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [780/20000] Train-Loss: 1.2850731750178834 Test-Loss: 0.32144486826533203 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [781/20000] Train-Loss: 1.2849441842354805 Test-Loss: 0.3214560106015626 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [782/20000] Train-Loss: 1.2848561302193966 Test-Loss: 0.3214473525956803 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [783/20000] Train-Loss: 1.284858878425138 Test-Loss: 0.3214450070602573 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [784/20000] Train-Loss: 1.2850010878361409 Test-Loss: 0.3214409721524878 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [785/20000] Train-Loss: 1.2848973577530625 Test-Loss: 0.321441507077759 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [786/20000] Train-Loss: 1.2849221124997503 Test-Loss: 0.3214391204170985 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [787/20000] Train-Loss: 1.2849748526669984 Test-Loss: 0.3214389573555362 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [788/20000] Train-Loss: 1.2849638037539646 Test-Loss: 0.3214386322517648 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [789/20000] Train-Loss: 1.2850188806253022 Test-Loss: 0.32143337308400194 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [790/20000] Train-Loss: 1.285031672382282 Test-Loss: 0.3214365918348216 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [791/20000] Train-Loss: 1.2848364425906849 Test-Loss: 0.32143429627447284 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [792/20000] Train-Loss: 1.2847962996243119 Test-Loss: 0.3214371465567537 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [793/20000] Train-Loss: 1.2848225132438589 Test-Loss: 0.3214295628095146 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [794/20000] Train-Loss: 1.2851574565009483 Test-Loss: 0.32143876606696103 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [795/20000] Train-Loss: 1.2850071270283163 Test-Loss: 0.32143302567120824 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [796/20000] Train-Loss: 1.2849287781226175 Test-Loss: 0.3214296640936829 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [797/20000] Train-Loss: 1.2849538041669428 Test-Loss: 0.3214341402010203 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [798/20000] Train-Loss: 1.284931590671194 Test-Loss: 0.32143370473569705 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [799/20000] Train-Loss: 1.2848748482074028 Test-Loss: 0.32142851925671306 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [800/20000] Train-Loss: 1.284873794524157 Test-Loss: 0.3214316727122087 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [801/20000] Train-Loss: 1.2849380047418562 Test-Loss: 0.32143147288698404 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [802/20000] Train-Loss: 1.2849756689097578 Test-Loss: 0.3214172378344161 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [803/20000] Train-Loss: 1.2847846820767592 Test-Loss: 0.32142892755730235 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [804/20000] Train-Loss: 1.28498698156556 Test-Loss: 0.3214301028932665 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [805/20000] Train-Loss: 1.2847844397133799 Test-Loss: 0.32142125627124885 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [806/20000] Train-Loss: 1.2849738481993553 Test-Loss: 0.321416074262622 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [807/20000] Train-Loss: 1.284931590456163 Test-Loss: 0.32142323507668796 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [808/20000] Train-Loss: 1.2849016996865303 Test-Loss: 0.321421158948973 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [809/20000] Train-Loss: 1.2847762284542223 Test-Loss: 0.321422339730921 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [810/20000] Train-Loss: 1.2848327684578071 Test-Loss: 0.321420673251706 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [811/20000] Train-Loss: 1.284889461099266 Test-Loss: 0.3214114370129255 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [812/20000] Train-Loss: 1.284803891672049 Test-Loss: 0.32141609452839337 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [813/20000] Train-Loss: 1.2847290086144167 Test-Loss: 0.32141463013181704 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [814/20000] Train-Loss: 1.2848431789222439 Test-Loss: 0.32141940655204176 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [815/20000] Train-Loss: 1.2847331042324677 Test-Loss: 0.32141824351765236 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [816/20000] Train-Loss: 1.2847816830354493 Test-Loss: 0.3214111074769662 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [817/20000] Train-Loss: 1.2848378635164834 Test-Loss: 0.32140877694929826 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [818/20000] Train-Loss: 1.2847201511767903 Test-Loss: 0.3214053168819913 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [819/20000] Train-Loss: 1.2847321242524659 Test-Loss: 0.32140916191657254 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [820/20000] Train-Loss: 1.284915094863272 Test-Loss: 0.3214076649975994 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [821/20000] Train-Loss: 1.2848621902380852 Test-Loss: 0.3214065457439886 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [822/20000] Train-Loss: 1.2848373780144735 Test-Loss: 0.3213982281055243 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [823/20000] Train-Loss: 1.2848990102737867 Test-Loss: 0.32140137918797507 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [824/20000] Train-Loss: 1.2847033001219397 Test-Loss: 0.32139766304533796 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [825/20000] Train-Loss: 1.2846802839112839 Test-Loss: 0.32139709371403574 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [826/20000] Train-Loss: 1.284642863143858 Test-Loss: 0.3213971400277478 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [827/20000] Train-Loss: 1.2847339026874034 Test-Loss: 0.321392496195317 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [828/20000] Train-Loss: 1.284719660696906 Test-Loss: 0.32139563519527 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [829/20000] Train-Loss: 1.2847437483247846 Test-Loss: 0.32138705350850366 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [830/20000] Train-Loss: 1.2847046341624428 Test-Loss: 0.32138567621833497 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [831/20000] Train-Loss: 1.284754884149737 Test-Loss: 0.32138849692340843 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [832/20000] Train-Loss: 1.2847141583338944 Test-Loss: 0.32138948497429826 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [833/20000] Train-Loss: 1.2846453932437558 Test-Loss: 0.3213852308050038 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [834/20000] Train-Loss: 1.2846859398613575 Test-Loss: 0.32138552684837 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [835/20000] Train-Loss: 1.2847478670275985 Test-Loss: 0.3213781336305513 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [836/20000] Train-Loss: 1.2847107106763722 Test-Loss: 0.3213788700698293 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [837/20000] Train-Loss: 1.2846489273131259 Test-Loss: 0.3213811194870005 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [838/20000] Train-Loss: 1.284597062741315 Test-Loss: 0.32137773472530584 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [839/20000] Train-Loss: 1.2847280173145357 Test-Loss: 0.3213759717638687 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [840/20000] Train-Loss: 1.2847480688669384 Test-Loss: 0.32137549116173797 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [841/20000] Train-Loss: 1.2846909563699402 Test-Loss: 0.32137954196834956 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [842/20000] Train-Loss: 1.2846948573811274 Test-Loss: 0.3213748166571372 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [843/20000] Train-Loss: 1.2846593585894748 Test-Loss: 0.32136351424579074 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [844/20000] Train-Loss: 1.284752040622665 Test-Loss: 0.32137523030302323 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [845/20000] Train-Loss: 1.2846879027460871 Test-Loss: 0.3213661422190117 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [846/20000] Train-Loss: 1.2846530533771947 Test-Loss: 0.3213646698192794 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [847/20000] Train-Loss: 1.2846123149730522 Test-Loss: 0.3213669557182298 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [848/20000] Train-Loss: 1.2847067762352968 Test-Loss: 0.321357624751585 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [849/20000] Train-Loss: 1.2847284815340736 Test-Loss: 0.3213666756846038 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [850/20000] Train-Loss: 1.284641133847608 Test-Loss: 0.32136345656018905 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [851/20000] Train-Loss: 1.284652154711073 Test-Loss: 0.32136106314824925 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [852/20000] Train-Loss: 1.2847238515744575 Test-Loss: 0.32135296439998806 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [853/20000] Train-Loss: 1.2847961013046216 Test-Loss: 0.321361214061994 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [854/20000] Train-Loss: 1.2846491522398862 Test-Loss: 0.32136078550136804 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [855/20000] Train-Loss: 1.2845804724745813 Test-Loss: 0.3213563727885749 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [856/20000] Train-Loss: 1.284571244612834 Test-Loss: 0.32135510286720415 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [857/20000] Train-Loss: 1.2847292963344337 Test-Loss: 0.32134898658303357 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [858/20000] Train-Loss: 1.2846972917862818 Test-Loss: 0.32134598854582636 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [859/20000] Train-Loss: 1.2845273624666673 Test-Loss: 0.3213499789581456 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [860/20000] Train-Loss: 1.2845225927482502 Test-Loss: 0.3213487556780402 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [861/20000] Train-Loss: 1.2845629505666918 Test-Loss: 0.32134522533172744 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [862/20000] Train-Loss: 1.2845937632609221 Test-Loss: 0.32134652584851586 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [863/20000] Train-Loss: 1.2845058110508014 Test-Loss: 0.3213421206043802 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [864/20000] Train-Loss: 1.2844317213502814 Test-Loss: 0.321349029075944 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [865/20000] Train-Loss: 1.2845533093463686 Test-Loss: 0.3213399888063977 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [866/20000] Train-Loss: 1.2844687296837576 Test-Loss: 0.32133686842905734 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [867/20000] Train-Loss: 1.284440842735023 Test-Loss: 0.3213399107881304 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [868/20000] Train-Loss: 1.2847548992216309 Test-Loss: 0.32133840264077707 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [869/20000] Train-Loss: 1.2844319984886041 Test-Loss: 0.32133814104609504 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [870/20000] Train-Loss: 1.284518850262055 Test-Loss: 0.3213401333484537 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [871/20000] Train-Loss: 1.2845141514872944 Test-Loss: 0.3213291904821275 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [872/20000] Train-Loss: 1.2845292152662606 Test-Loss: 0.32132942231461525 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [873/20000] Train-Loss: 1.2844695001413025 Test-Loss: 0.3213289016977922 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [874/20000] Train-Loss: 1.284471428309197 Test-Loss: 0.32132690289990523 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [875/20000] Train-Loss: 1.284396296056972 Test-Loss: 0.32132878024760414 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [876/20000] Train-Loss: 1.284469294776096 Test-Loss: 0.32132920396311776 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [877/20000] Train-Loss: 1.2843829218427392 Test-Loss: 0.3213204507589008 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [878/20000] Train-Loss: 1.284406434581797 Test-Loss: 0.3213286559409604 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [879/20000] Train-Loss: 1.2844628771368847 Test-Loss: 0.3213086627549909 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [880/20000] Train-Loss: 1.284469020644902 Test-Loss: 0.32131767942953127 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [881/20000] Train-Loss: 1.2844918825749398 Test-Loss: 0.3213198104195244 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [882/20000] Train-Loss: 1.284536218732707 Test-Loss: 0.3213262633971652 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [883/20000] Train-Loss: 1.284365282585988 Test-Loss: 0.3213139271988868 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [884/20000] Train-Loss: 1.284482853724427 Test-Loss: 0.3213115889257673 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [885/20000] Train-Loss: 1.2845947314104396 Test-Loss: 0.3213162694527561 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [886/20000] Train-Loss: 1.2843974257575936 Test-Loss: 0.3213156405997209 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [887/20000] Train-Loss: 1.2843599496421565 Test-Loss: 0.32130653582968194 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [888/20000] Train-Loss: 1.2844443659774523 Test-Loss: 0.32131052777889657 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [889/20000] Train-Loss: 1.2844243094790995 Test-Loss: 0.3213075299761804 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [890/20000] Train-Loss: 1.2844577857191888 Test-Loss: 0.3213013251581215 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [891/20000] Train-Loss: 1.284378840662887 Test-Loss: 0.32130422120479296 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [892/20000] Train-Loss: 1.2842993868925054 Test-Loss: 0.3213001676312549 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [893/20000] Train-Loss: 1.2844747112764812 Test-Loss: 0.32129562144009716 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [894/20000] Train-Loss: 1.284363695503241 Test-Loss: 0.3212957757786859 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [895/20000] Train-Loss: 1.2843323513687726 Test-Loss: 0.32129404196620626 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [896/20000] Train-Loss: 1.2844248945481653 Test-Loss: 0.32129621953880516 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [897/20000] Train-Loss: 1.2844300573017304 Test-Loss: 0.321294991870604 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [898/20000] Train-Loss: 1.2842341811768414 Test-Loss: 0.32129086015418445 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [899/20000] Train-Loss: 1.2844468851657365 Test-Loss: 0.32128701167436857 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [900/20000] Train-Loss: 1.2843418365368828 Test-Loss: 0.32129187440787 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [901/20000] Train-Loss: 1.2843563500002737 Test-Loss: 0.3212821963338356 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [902/20000] Train-Loss: 1.2842652792777707 Test-Loss: 0.32127842823138275 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [903/20000] Train-Loss: 1.2842808501862566 Test-Loss: 0.32128594921209835 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [904/20000] Train-Loss: 1.2844045065471292 Test-Loss: 0.32128747786464673 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [905/20000] Train-Loss: 1.2843422280277395 Test-Loss: 0.3212821106306487 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [906/20000] Train-Loss: 1.2843241734696906 Test-Loss: 0.32128502637135037 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [907/20000] Train-Loss: 1.2842155323805553 Test-Loss: 0.32128021702065573 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [908/20000] Train-Loss: 1.2842892192490374 Test-Loss: 0.3212779391277003 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [909/20000] Train-Loss: 1.2842404995513312 Test-Loss: 0.3212667479081811 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [910/20000] Train-Loss: 1.2842489364733933 Test-Loss: 0.3212695500870062 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [911/20000] Train-Loss: 1.2842924170531078 Test-Loss: 0.32127236194310405 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [912/20000] Train-Loss: 1.2842571631167337 Test-Loss: 0.32126902272029123 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [913/20000] Train-Loss: 1.2843056706743397 Test-Loss: 0.3212678777310736 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [914/20000] Train-Loss: 1.2844201347126396 Test-Loss: 0.3212608100786406 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [915/20000] Train-Loss: 1.2842474577632492 Test-Loss: 0.3212633305107637 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [916/20000] Train-Loss: 1.2842788800152363 Test-Loss: 0.32126384634208976 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [917/20000] Train-Loss: 1.2841982635744829 Test-Loss: 0.32126295320246495 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [918/20000] Train-Loss: 1.2843729441930631 Test-Loss: 0.3212612522607075 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [919/20000] Train-Loss: 1.2842283546657294 Test-Loss: 0.3212610964844746 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [920/20000] Train-Loss: 1.2843219595926865 Test-Loss: 0.3212574700625517 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [921/20000] Train-Loss: 1.284215998098885 Test-Loss: 0.3212605640365924 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [922/20000] Train-Loss: 1.2841833757131629 Test-Loss: 0.32125273683922634 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [923/20000] Train-Loss: 1.2842471666930593 Test-Loss: 0.3212555598651144 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [924/20000] Train-Loss: 1.284258222175859 Test-Loss: 0.3212525329346286 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [925/20000] Train-Loss: 1.284088244096287 Test-Loss: 0.3212511157839952 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [926/20000] Train-Loss: 1.2841611508552648 Test-Loss: 0.32124872857235526 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [927/20000] Train-Loss: 1.28422746504176 Test-Loss: 0.32124367403925697 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [928/20000] Train-Loss: 1.2841387839335827 Test-Loss: 0.32124900874297685 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [929/20000] Train-Loss: 1.28413414355878 Test-Loss: 0.3212424210376019 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [930/20000] Train-Loss: 1.2841878701444513 Test-Loss: 0.32124404431433934 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [931/20000] Train-Loss: 1.2841257892163416 Test-Loss: 0.32124242375669854 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [932/20000] Train-Loss: 1.2840539101941455 Test-Loss: 0.32123573015196927 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [933/20000] Train-Loss: 1.2841628222697927 Test-Loss: 0.32123403712104587 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [934/20000] Train-Loss: 1.2840963735382134 Test-Loss: 0.3212342930564315 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [935/20000] Train-Loss: 1.2840925228028572 Test-Loss: 0.32122847994455594 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [936/20000] Train-Loss: 1.2841834390236806 Test-Loss: 0.32123278760592217 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [937/20000] Train-Loss: 1.28414774078273 Test-Loss: 0.3212330813478645 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [938/20000] Train-Loss: 1.2842321197317292 Test-Loss: 0.321231914248312 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [939/20000] Train-Loss: 1.284087648247632 Test-Loss: 0.32122795909198665 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [940/20000] Train-Loss: 1.2841963376510461 Test-Loss: 0.32122747171828553 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [941/20000] Train-Loss: 1.2842376790781644 Test-Loss: 0.32122133760396476 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [942/20000] Train-Loss: 1.2840443686993037 Test-Loss: 0.32122085014021395 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [943/20000] Train-Loss: 1.2840531582439179 Test-Loss: 0.3212193304358721 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [944/20000] Train-Loss: 1.2840264278760392 Test-Loss: 0.3212181646580757 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [945/20000] Train-Loss: 1.2841248181037783 Test-Loss: 0.3212160804411706 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [946/20000] Train-Loss: 1.284025273771841 Test-Loss: 0.32121593388749153 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [947/20000] Train-Loss: 1.283978799249102 Test-Loss: 0.32121105462754324 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [948/20000] Train-Loss: 1.283946861272689 Test-Loss: 0.32121547001330863 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [949/20000] Train-Loss: 1.283997175162959 Test-Loss: 0.3212098880120271 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [950/20000] Train-Loss: 1.2841162786486597 Test-Loss: 0.32120573727238694 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [951/20000] Train-Loss: 1.2839533921817199 Test-Loss: 0.32120693348233503 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [952/20000] Train-Loss: 1.284171126343741 Test-Loss: 0.32120827877355523 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [953/20000] Train-Loss: 1.2838788681380433 Test-Loss: 0.32120255193200814 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [954/20000] Train-Loss: 1.2840087963588962 Test-Loss: 0.3212049656014303 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [955/20000] Train-Loss: 1.2839489945369644 Test-Loss: 0.32119802652504587 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [956/20000] Train-Loss: 1.284044571953723 Test-Loss: 0.3211928419076107 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [957/20000] Train-Loss: 1.2841152241109428 Test-Loss: 0.3211921296172057 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [958/20000] Train-Loss: 1.284071420076794 Test-Loss: 0.32119968652922604 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [959/20000] Train-Loss: 1.2838898128021812 Test-Loss: 0.32119533302018577 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [960/20000] Train-Loss: 1.2839563832523906 Test-Loss: 0.3211892209061189 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [961/20000] Train-Loss: 1.2838835451684156 Test-Loss: 0.3211900344497107 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [962/20000] Train-Loss: 1.2838713152768746 Test-Loss: 0.3211836173321045 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [963/20000] Train-Loss: 1.2839658138283283 Test-Loss: 0.32118072595719965 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [964/20000] Train-Loss: 1.283870151194744 Test-Loss: 0.32117614779210846 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [965/20000] Train-Loss: 1.2839213275992511 Test-Loss: 0.3211744829135527 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [966/20000] Train-Loss: 1.2839175172243877 Test-Loss: 0.32116861260088714 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [967/20000] Train-Loss: 1.2837718019476207 Test-Loss: 0.3211725073168224 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [968/20000] Train-Loss: 1.2838858141100693 Test-Loss: 0.32116943146767263 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [969/20000] Train-Loss: 1.2838975088490454 Test-Loss: 0.32117645557275826 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [970/20000] Train-Loss: 1.2839579360813953 Test-Loss: 0.32116964594652175 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [971/20000] Train-Loss: 1.2838685901829798 Test-Loss: 0.3211651364944048 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [972/20000] Train-Loss: 1.2838191714989442 Test-Loss: 0.3211720711965277 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [973/20000] Train-Loss: 1.283940499342642 Test-Loss: 0.32116322377852 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [974/20000] Train-Loss: 1.283745866666227 Test-Loss: 0.3211627444608818 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [975/20000] Train-Loss: 1.2838960284175542 Test-Loss: 0.3211617530984861 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [976/20000] Train-Loss: 1.2838762027234092 Test-Loss: 0.321153684831243 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [977/20000] Train-Loss: 1.2838404447585037 Test-Loss: 0.3211489193034939 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [978/20000] Train-Loss: 1.283723498648954 Test-Loss: 0.3211508124342674 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [979/20000] Train-Loss: 1.283716243204839 Test-Loss: 0.32115074107900876 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [980/20000] Train-Loss: 1.2837250096152313 Test-Loss: 0.3211467807589552 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [981/20000] Train-Loss: 1.283876439734742 Test-Loss: 0.3211500003305028 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [982/20000] Train-Loss: 1.283727146522097 Test-Loss: 0.3211457711584695 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [983/20000] Train-Loss: 1.2837758491070765 Test-Loss: 0.3211504677681952 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [984/20000] Train-Loss: 1.2837523627999583 Test-Loss: 0.32114348672579757 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [985/20000] Train-Loss: 1.2839596654612133 Test-Loss: 0.3211433436024553 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [986/20000] Train-Loss: 1.2837894649515427 Test-Loss: 0.32113853733851916 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [987/20000] Train-Loss: 1.2837541187362491 Test-Loss: 0.3211366514647309 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [988/20000] Train-Loss: 1.283771742441111 Test-Loss: 0.32113815357279035 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [989/20000] Train-Loss: 1.28372127982067 Test-Loss: 0.32113367649870683 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [990/20000] Train-Loss: 1.283637327765646 Test-Loss: 0.32113728842699574 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [991/20000] Train-Loss: 1.2837391305133603 Test-Loss: 0.3211373829189779 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [992/20000] Train-Loss: 1.283692467630811 Test-Loss: 0.32113311330428373 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [993/20000] Train-Loss: 1.2838043596084017 Test-Loss: 0.3211312753565364 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [994/20000] Train-Loss: 1.283735179508156 Test-Loss: 0.3211273209671326 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [995/20000] Train-Loss: 1.2836886346299987 Test-Loss: 0.32113117888694453 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [996/20000] Train-Loss: 1.2836349163192282 Test-Loss: 0.3211220570340804 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [997/20000] Train-Loss: 1.2835990142280578 Test-Loss: 0.3211176515172986 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [998/20000] Train-Loss: 1.283628355002253 Test-Loss: 0.32112141342544637 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [999/20000] Train-Loss: 1.2837192557912354 Test-Loss: 0.32111972815037193 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1000/20000] Train-Loss: 1.283779122662302 Test-Loss: 0.32112325089605465 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1001/20000] Train-Loss: 1.2837398903051245 Test-Loss: 0.32111167497047466 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1002/20000] Train-Loss: 1.283650924390794 Test-Loss: 0.32112283937053754 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1003/20000] Train-Loss: 1.2836499998841044 Test-Loss: 0.3211146140565122 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1004/20000] Train-Loss: 1.2835805725880522 Test-Loss: 0.3211200716249033 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1005/20000] Train-Loss: 1.2835526721095891 Test-Loss: 0.3211103798376012 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1006/20000] Train-Loss: 1.2837528841285006 Test-Loss: 0.3211019817642723 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1007/20000] Train-Loss: 1.2835855762735882 Test-Loss: 0.32109897271102017 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1008/20000] Train-Loss: 1.2835828888464094 Test-Loss: 0.3210926040039114 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1009/20000] Train-Loss: 1.28370858183828 Test-Loss: 0.3210966329690972 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1010/20000] Train-Loss: 1.2835110770654092 Test-Loss: 0.32109084529920817 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1011/20000] Train-Loss: 1.2834350647170394 Test-Loss: 0.32109410296772395 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1012/20000] Train-Loss: 1.2835004424342682 Test-Loss: 0.32108827842716675 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1013/20000] Train-Loss: 1.2835832248323378 Test-Loss: 0.3210833390177324 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1014/20000] Train-Loss: 1.2835128922132606 Test-Loss: 0.32108440461505633 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1015/20000] Train-Loss: 1.283651728232575 Test-Loss: 0.3210720622495587 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1016/20000] Train-Loss: 1.2834974537335038 Test-Loss: 0.3210757330813073 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1017/20000] Train-Loss: 1.2835591534649045 Test-Loss: 0.3210774622410171 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1018/20000] Train-Loss: 1.283411700695589 Test-Loss: 0.32107038052909076 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1019/20000] Train-Loss: 1.2834265740243807 Test-Loss: 0.3210601179521121 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1020/20000] Train-Loss: 1.2834050666642427 Test-Loss: 0.32106572185897453 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1021/20000] Train-Loss: 1.2833880006972127 Test-Loss: 0.32106721557388024 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1022/20000] Train-Loss: 1.2834068388775646 Test-Loss: 0.32105910093907125 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1023/20000] Train-Loss: 1.2835708916688087 Test-Loss: 0.3210636672902266 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1024/20000] Train-Loss: 1.283495745094829 Test-Loss: 0.3210521637106459 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1025/20000] Train-Loss: 1.2834722987000549 Test-Loss: 0.3210545113503777 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1026/20000] Train-Loss: 1.28339273556252 Test-Loss: 0.3210574987373093 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1027/20000] Train-Loss: 1.2834595524926422 Test-Loss: 0.32105142938427345 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1028/20000] Train-Loss: 1.2833671534616098 Test-Loss: 0.3210449920482587 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1029/20000] Train-Loss: 1.283443231492295 Test-Loss: 0.3210492885968085 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1030/20000] Train-Loss: 1.2834871270087314 Test-Loss: 0.3210379271304482 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1031/20000] Train-Loss: 1.2832743975206033 Test-Loss: 0.3210435443867463 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1032/20000] Train-Loss: 1.2834628250109557 Test-Loss: 0.321044240864162 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1033/20000] Train-Loss: 1.2834599888986282 Test-Loss: 0.32103298417856196 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1034/20000] Train-Loss: 1.283393552513252 Test-Loss: 0.32104255882514493 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1035/20000] Train-Loss: 1.2834331876143763 Test-Loss: 0.3210327595273158 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1036/20000] Train-Loss: 1.2833939244713524 Test-Loss: 0.32103331952869024 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1037/20000] Train-Loss: 1.2832243655276632 Test-Loss: 0.32103790141293814 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1038/20000] Train-Loss: 1.2834254440934147 Test-Loss: 0.32102558266729647 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1039/20000] Train-Loss: 1.28327595168269 Test-Loss: 0.32102397981644604 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1040/20000] Train-Loss: 1.2833077359931884 Test-Loss: 0.3210254095496596 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1041/20000] Train-Loss: 1.2834439035165979 Test-Loss: 0.32102013705651833 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1042/20000] Train-Loss: 1.2833543604786184 Test-Loss: 0.3210196041895643 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1043/20000] Train-Loss: 1.283243447839502 Test-Loss: 0.32102182850905325 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1044/20000] Train-Loss: 1.2831477628040495 Test-Loss: 0.3210126652620594 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1045/20000] Train-Loss: 1.2833101648017404 Test-Loss: 0.3210163784374861 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1046/20000] Train-Loss: 1.2832150765879193 Test-Loss: 0.32101038754082173 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1047/20000] Train-Loss: 1.2831961295007992 Test-Loss: 0.3210188246297899 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1048/20000] Train-Loss: 1.2831556058678697 Test-Loss: 0.3210054750555938 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1049/20000] Train-Loss: 1.2831937342958044 Test-Loss: 0.3210108818122923 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1050/20000] Train-Loss: 1.2831619261859533 Test-Loss: 0.3210065522233957 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1051/20000] Train-Loss: 1.283161531357973 Test-Loss: 0.32100186160738514 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1052/20000] Train-Loss: 1.2831204484913052 Test-Loss: 0.32099620346827434 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1053/20000] Train-Loss: 1.2833695068258428 Test-Loss: 0.3209965563985254 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1054/20000] Train-Loss: 1.2831349523640962 Test-Loss: 0.32099578357648956 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1055/20000] Train-Loss: 1.2831765068852599 Test-Loss: 0.320992447310845 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1056/20000] Train-Loss: 1.2832323969543664 Test-Loss: 0.32098772597154784 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1057/20000] Train-Loss: 1.2831406980150828 Test-Loss: 0.32097946469382094 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1058/20000] Train-Loss: 1.2831572804341136 Test-Loss: 0.32098553295437965 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1059/20000] Train-Loss: 1.2830961143279274 Test-Loss: 0.3209792492650279 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1060/20000] Train-Loss: 1.283138537471278 Test-Loss: 0.3209725932680503 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1061/20000] Train-Loss: 1.2831973444144742 Test-Loss: 0.32098527871595434 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1062/20000] Train-Loss: 1.2831343606763481 Test-Loss: 0.32097663330738146 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1063/20000] Train-Loss: 1.2830722875517064 Test-Loss: 0.32098383969873323 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1064/20000] Train-Loss: 1.28305560761932 Test-Loss: 0.32096969616412047 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1065/20000] Train-Loss: 1.2831446968840112 Test-Loss: 0.32097067138405583 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1066/20000] Train-Loss: 1.2830680749630285 Test-Loss: 0.3209634940297001 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1067/20000] Train-Loss: 1.283087136717882 Test-Loss: 0.32096477993652905 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1068/20000] Train-Loss: 1.282989454180089 Test-Loss: 0.32095636895177504 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1069/20000] Train-Loss: 1.2831031396280965 Test-Loss: 0.3209589338442737 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1070/20000] Train-Loss: 1.2830727473508683 Test-Loss: 0.3209494729087086 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1071/20000] Train-Loss: 1.2831122592189579 Test-Loss: 0.32095135679997017 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1072/20000] Train-Loss: 1.283094460640812 Test-Loss: 0.3209426146192494 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1073/20000] Train-Loss: 1.2828249106247722 Test-Loss: 0.3209437093137318 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1074/20000] Train-Loss: 1.283031962973002 Test-Loss: 0.3209431423871234 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1075/20000] Train-Loss: 1.2829722233579286 Test-Loss: 0.32094132532091774 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1076/20000] Train-Loss: 1.2829365180477217 Test-Loss: 0.320943660858634 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1077/20000] Train-Loss: 1.282802791927393 Test-Loss: 0.3209358184397914 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1078/20000] Train-Loss: 1.282910817855232 Test-Loss: 0.32093798323018086 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1079/20000] Train-Loss: 1.282973819313934 Test-Loss: 0.32092807727144407 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1080/20000] Train-Loss: 1.2828643468441012 Test-Loss: 0.3209323744216309 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1081/20000] Train-Loss: 1.2829169781474192 Test-Loss: 0.32092673397223265 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1082/20000] Train-Loss: 1.2828284697019228 Test-Loss: 0.3209286133181669 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1083/20000] Train-Loss: 1.2828961152769036 Test-Loss: 0.3209234159217436 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1084/20000] Train-Loss: 1.2827397557973959 Test-Loss: 0.32091825627267484 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1085/20000] Train-Loss: 1.2828511458220309 Test-Loss: 0.32092005707512106 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1086/20000] Train-Loss: 1.282802077596688 Test-Loss: 0.32091144942984406 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1087/20000] Train-Loss: 1.282778477845681 Test-Loss: 0.3209032070337826 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1088/20000] Train-Loss: 1.2828432241407237 Test-Loss: 0.32091063401471015 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1089/20000] Train-Loss: 1.282861157847786 Test-Loss: 0.32090219828676636 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1090/20000] Train-Loss: 1.2827699390730622 Test-Loss: 0.32089788095995414 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1091/20000] Train-Loss: 1.2829523174775301 Test-Loss: 0.320903093222396 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1092/20000] Train-Loss: 1.282773283451166 Test-Loss: 0.3209064456691649 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1093/20000] Train-Loss: 1.2828007721469448 Test-Loss: 0.3209012928304125 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1094/20000] Train-Loss: 1.2828435423432107 Test-Loss: 0.32089394269079285 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1095/20000] Train-Loss: 1.2829327654449176 Test-Loss: 0.32088952947581534 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1096/20000] Train-Loss: 1.2827335185484976 Test-Loss: 0.3208863875820466 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1097/20000] Train-Loss: 1.282891089767499 Test-Loss: 0.32088092468589713 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1098/20000] Train-Loss: 1.282735444516594 Test-Loss: 0.32088133548523723 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1099/20000] Train-Loss: 1.2828266626869549 Test-Loss: 0.3208827953927608 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1100/20000] Train-Loss: 1.2826558599743574 Test-Loss: 0.3208761644231925 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1101/20000] Train-Loss: 1.282600390265955 Test-Loss: 0.3208674314612744 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1102/20000] Train-Loss: 1.282849042998397 Test-Loss: 0.32086881134703227 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1103/20000] Train-Loss: 1.2827512620790307 Test-Loss: 0.3208629751066763 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1104/20000] Train-Loss: 1.2826702718809282 Test-Loss: 0.32086095197280734 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1105/20000] Train-Loss: 1.2827466203507263 Test-Loss: 0.3208624342584505 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1106/20000] Train-Loss: 1.2828372078655677 Test-Loss: 0.320861535733717 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1107/20000] Train-Loss: 1.2825882858142001 Test-Loss: 0.3208484067548141 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1108/20000] Train-Loss: 1.2825598454831326 Test-Loss: 0.3208589634190964 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1109/20000] Train-Loss: 1.282573277835965 Test-Loss: 0.32085228093284773 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1110/20000] Train-Loss: 1.2827221743087456 Test-Loss: 0.3208520822656321 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1111/20000] Train-Loss: 1.2825175317274593 Test-Loss: 0.3208351236409617 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1112/20000] Train-Loss: 1.2825993366389337 Test-Loss: 0.3208433755901519 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1113/20000] Train-Loss: 1.2825231113194533 Test-Loss: 0.32084679240174385 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1114/20000] Train-Loss: 1.2825536727215308 Test-Loss: 0.3208375765889678 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1115/20000] Train-Loss: 1.2825562517053568 Test-Loss: 0.3208456829641515 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1116/20000] Train-Loss: 1.2825153567399346 Test-Loss: 0.3208322285018808 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1117/20000] Train-Loss: 1.282553592082693 Test-Loss: 0.32083639456332097 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1118/20000] Train-Loss: 1.2826448337558762 Test-Loss: 0.32082536964775993 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1119/20000] Train-Loss: 1.2826211386367148 Test-Loss: 0.32082751184489733 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1120/20000] Train-Loss: 1.2825206363331823 Test-Loss: 0.32082032381303194 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1121/20000] Train-Loss: 1.2823891667678389 Test-Loss: 0.3208211680481729 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1122/20000] Train-Loss: 1.2824625136350232 Test-Loss: 0.32081282827906804 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1123/20000] Train-Loss: 1.2824844459738034 Test-Loss: 0.32082413078593136 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1124/20000] Train-Loss: 1.2824115110469796 Test-Loss: 0.32080419415554234 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1125/20000] Train-Loss: 1.2824197436193627 Test-Loss: 0.32080785384217253 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1126/20000] Train-Loss: 1.2823880186427927 Test-Loss: 0.32079912120750786 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1127/20000] Train-Loss: 1.2823302913628143 Test-Loss: 0.32079351261710604 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1128/20000] Train-Loss: 1.2823824831280124 Test-Loss: 0.32078450856338353 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1129/20000] Train-Loss: 1.2824042472750892 Test-Loss: 0.3207861860138934 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1130/20000] Train-Loss: 1.2824214956306272 Test-Loss: 0.3207845914759984 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1131/20000] Train-Loss: 1.2824247436387564 Test-Loss: 0.3207784330973137 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1132/20000] Train-Loss: 1.2823708718975995 Test-Loss: 0.32077365264682434 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1133/20000] Train-Loss: 1.282334967386633 Test-Loss: 0.32078166541090686 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1134/20000] Train-Loss: 1.2824573864151239 Test-Loss: 0.3207723904465559 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1135/20000] Train-Loss: 1.2823558867314184 Test-Loss: 0.3207671251969541 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1136/20000] Train-Loss: 1.2823529331011112 Test-Loss: 0.32076122066634793 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1137/20000] Train-Loss: 1.2823034686915415 Test-Loss: 0.3207559531545233 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1138/20000] Train-Loss: 1.2823019508289377 Test-Loss: 0.320764918583218 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1139/20000] Train-Loss: 1.2822867487160792 Test-Loss: 0.3207578634304742 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1140/20000] Train-Loss: 1.2822602926186737 Test-Loss: 0.32076145842867526 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1141/20000] Train-Loss: 1.2821853094475144 Test-Loss: 0.32074336795288216 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1142/20000] Train-Loss: 1.2821784547133763 Test-Loss: 0.3207488052988442 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1143/20000] Train-Loss: 1.2821671850838912 Test-Loss: 0.3207410826595187 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1144/20000] Train-Loss: 1.282240624464032 Test-Loss: 0.3207354789744007 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1145/20000] Train-Loss: 1.2822883135926941 Test-Loss: 0.32073174473533017 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1146/20000] Train-Loss: 1.2821196424998074 Test-Loss: 0.3207341209912881 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1147/20000] Train-Loss: 1.2821183584041693 Test-Loss: 0.3207389159877751 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1148/20000] Train-Loss: 1.282168864127895 Test-Loss: 0.3207244054260085 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1149/20000] Train-Loss: 1.2821296341316581 Test-Loss: 0.3207295884282241 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1150/20000] Train-Loss: 1.2822025676215185 Test-Loss: 0.3207218632799293 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1151/20000] Train-Loss: 1.2820218307486455 Test-Loss: 0.32071197769619547 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1152/20000] Train-Loss: 1.2822517756881429 Test-Loss: 0.32071570624778495 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1153/20000] Train-Loss: 1.2820813880878927 Test-Loss: 0.32070866298852463 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1154/20000] Train-Loss: 1.2820543737320163 Test-Loss: 0.32070279955043085 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1155/20000] Train-Loss: 1.2821782347156119 Test-Loss: 0.3207069449676844 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1156/20000] Train-Loss: 1.2820260040109712 Test-Loss: 0.32070567022487967 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1157/20000] Train-Loss: 1.2819842588363195 Test-Loss: 0.32070907028336637 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1158/20000] Train-Loss: 1.2819417975445908 Test-Loss: 0.32070066069374004 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1159/20000] Train-Loss: 1.2819230678641207 Test-Loss: 0.32069822097772194 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1160/20000] Train-Loss: 1.282018489976217 Test-Loss: 0.3206945088270868 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1161/20000] Train-Loss: 1.2819466735216538 Test-Loss: 0.3206858841783002 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1162/20000] Train-Loss: 1.2819854123125305 Test-Loss: 0.32068133105119934 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1163/20000] Train-Loss: 1.2819377687122926 Test-Loss: 0.32067050491969373 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1164/20000] Train-Loss: 1.282061329964304 Test-Loss: 0.32067129434017566 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1165/20000] Train-Loss: 1.281851178580165 Test-Loss: 0.3206754061002101 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1166/20000] Train-Loss: 1.2819353778818867 Test-Loss: 0.3206639810547247 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1167/20000] Train-Loss: 1.2820097120643341 Test-Loss: 0.32066494180560534 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1168/20000] Train-Loss: 1.2819333485511764 Test-Loss: 0.32066093882856944 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1169/20000] Train-Loss: 1.2819082331054623 Test-Loss: 0.32066035587087255 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1170/20000] Train-Loss: 1.28184653461185 Test-Loss: 0.3206481625537102 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1171/20000] Train-Loss: 1.28168992145383 Test-Loss: 0.3206457608280555 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1172/20000] Train-Loss: 1.2819005385053572 Test-Loss: 0.32065186008301017 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1173/20000] Train-Loss: 1.2817855033158456 Test-Loss: 0.3206433871981684 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1174/20000] Train-Loss: 1.2816877879832245 Test-Loss: 0.32063450119663717 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1175/20000] Train-Loss: 1.2817500830071864 Test-Loss: 0.32063588356341727 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1176/20000] Train-Loss: 1.281825588839163 Test-Loss: 0.3206188735249379 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1177/20000] Train-Loss: 1.2817162764606596 Test-Loss: 0.32062697894225234 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1178/20000] Train-Loss: 1.2816601728085184 Test-Loss: 0.32061302725558455 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1179/20000] Train-Loss: 1.2818339663003175 Test-Loss: 0.3206133599582949 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1180/20000] Train-Loss: 1.2818535150828432 Test-Loss: 0.32060798282273767 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1181/20000] Train-Loss: 1.2816543358786192 Test-Loss: 0.3206086440367939 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1182/20000] Train-Loss: 1.281819702557852 Test-Loss: 0.3206084799512686 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1183/20000] Train-Loss: 1.2816480412612978 Test-Loss: 0.3206019033869622 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1184/20000] Train-Loss: 1.2816706552251673 Test-Loss: 0.3206036041810506 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1185/20000] Train-Loss: 1.2815939296496255 Test-Loss: 0.32060256609221566 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1186/20000] Train-Loss: 1.2816493255774422 Test-Loss: 0.3206008299431654 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1187/20000] Train-Loss: 1.2816640740966534 Test-Loss: 0.32059777397308287 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1188/20000] Train-Loss: 1.2815612698552805 Test-Loss: 0.3205850899421762 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1189/20000] Train-Loss: 1.281597479305696 Test-Loss: 0.32058662958820655 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1190/20000] Train-Loss: 1.2815876928343253 Test-Loss: 0.320585966651679 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1191/20000] Train-Loss: 1.2815411077683694 Test-Loss: 0.32057997421285905 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1192/20000] Train-Loss: 1.2815536531209966 Test-Loss: 0.3205715666488493 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1193/20000] Train-Loss: 1.2814472902775669 Test-Loss: 0.32056757980699435 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1194/20000] Train-Loss: 1.2815734787062725 Test-Loss: 0.3205631016806022 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1195/20000] Train-Loss: 1.281493218833361 Test-Loss: 0.3205681400881996 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1196/20000] Train-Loss: 1.2814109344011282 Test-Loss: 0.3205532125466457 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1197/20000] Train-Loss: 1.2814217111156199 Test-Loss: 0.3205528733178505 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1198/20000] Train-Loss: 1.2813251987908203 Test-Loss: 0.3205489549474005 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1199/20000] Train-Loss: 1.2814186058876245 Test-Loss: 0.320548907165283 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1200/20000] Train-Loss: 1.281336478070535 Test-Loss: 0.320549644934519 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1201/20000] Train-Loss: 1.2813543224399644 Test-Loss: 0.3205367669219662 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1202/20000] Train-Loss: 1.2812905025395585 Test-Loss: 0.3205322912234997 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1203/20000] Train-Loss: 1.281289943451471 Test-Loss: 0.32052785036486936 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1204/20000] Train-Loss: 1.2813370560817738 Test-Loss: 0.32052554371745695 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1205/20000] Train-Loss: 1.2813102348765228 Test-Loss: 0.3205175089096859 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1206/20000] Train-Loss: 1.2814140032986365 Test-Loss: 0.3205176950875997 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1207/20000] Train-Loss: 1.2811944234761463 Test-Loss: 0.3205184200678371 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1208/20000] Train-Loss: 1.281184687030277 Test-Loss: 0.3205120265687467 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1209/20000] Train-Loss: 1.2812401969578266 Test-Loss: 0.3205110063420557 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1210/20000] Train-Loss: 1.281374247851846 Test-Loss: 0.32050719143058237 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1211/20000] Train-Loss: 1.2812078539026246 Test-Loss: 0.32049786678001946 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1212/20000] Train-Loss: 1.281264992770609 Test-Loss: 0.32049247272017883 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1213/20000] Train-Loss: 1.281252217582018 Test-Loss: 0.3204975935227605 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1214/20000] Train-Loss: 1.2812203252757377 Test-Loss: 0.32049031826867425 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1215/20000] Train-Loss: 1.281157493843264 Test-Loss: 0.32048543925569734 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1216/20000] Train-Loss: 1.2812246454162144 Test-Loss: 0.3204699589671974 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1217/20000] Train-Loss: 1.281315025733084 Test-Loss: 0.32047638474817614 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1218/20000] Train-Loss: 1.2810871050858148 Test-Loss: 0.3204713515768602 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1219/20000] Train-Loss: 1.281074566126771 Test-Loss: 0.32046376045924724 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1220/20000] Train-Loss: 1.2810302375823006 Test-Loss: 0.3204634612834818 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1221/20000] Train-Loss: 1.2809898043007941 Test-Loss: 0.32045946755508753 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1222/20000] Train-Loss: 1.2811897486779864 Test-Loss: 0.3204538416682624 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1223/20000] Train-Loss: 1.2809931662058307 Test-Loss: 0.3204421954574217 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1224/20000] Train-Loss: 1.2810845261210053 Test-Loss: 0.3204531281543435 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1225/20000] Train-Loss: 1.2809126388493093 Test-Loss: 0.320442874293349 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1226/20000] Train-Loss: 1.2809776645272564 Test-Loss: 0.3204370383743629 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1227/20000] Train-Loss: 1.2809690271744045 Test-Loss: 0.3204365255391284 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1228/20000] Train-Loss: 1.2809353322116428 Test-Loss: 0.320424608232425 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1229/20000] Train-Loss: 1.2809035965166453 Test-Loss: 0.32041912241690806 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1230/20000] Train-Loss: 1.2810017827485818 Test-Loss: 0.32041591898552063 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1231/20000] Train-Loss: 1.2809448701945383 Test-Loss: 0.32041095541298825 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1232/20000] Train-Loss: 1.2808656748303422 Test-Loss: 0.3204067079768793 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1233/20000] Train-Loss: 1.2810170178105798 Test-Loss: 0.32040843723341716 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1234/20000] Train-Loss: 1.281042766379184 Test-Loss: 0.3203983582225789 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1235/20000] Train-Loss: 1.280867793071322 Test-Loss: 0.3203938041694552 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1236/20000] Train-Loss: 1.2809740424452014 Test-Loss: 0.32038567715222876 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1237/20000] Train-Loss: 1.2808068695989345 Test-Loss: 0.3203894302058257 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1238/20000] Train-Loss: 1.280840905351147 Test-Loss: 0.32037845074973564 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1239/20000] Train-Loss: 1.2809198759232316 Test-Loss: 0.3203706886876876 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1240/20000] Train-Loss: 1.2808317180488746 Test-Loss: 0.32037318931131603 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1241/20000] Train-Loss: 1.2807930385346382 Test-Loss: 0.3203679711925535 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1242/20000] Train-Loss: 1.280829534914841 Test-Loss: 0.3203639417471157 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1243/20000] Train-Loss: 1.2808194315536354 Test-Loss: 0.3203567338834952 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1244/20000] Train-Loss: 1.2806347976166734 Test-Loss: 0.3203549029034869 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1245/20000] Train-Loss: 1.2806918977454076 Test-Loss: 0.3203508621268757 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1246/20000] Train-Loss: 1.2806742032394953 Test-Loss: 0.3203442265010142 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1247/20000] Train-Loss: 1.2806715046608232 Test-Loss: 0.32034066703521136 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1248/20000] Train-Loss: 1.280562799433945 Test-Loss: 0.3203410508329659 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1249/20000] Train-Loss: 1.2806038073566002 Test-Loss: 0.32033268985991936 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1250/20000] Train-Loss: 1.2806706353340307 Test-Loss: 0.3203252387444247 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1251/20000] Train-Loss: 1.2805022662512209 Test-Loss: 0.3203210870536487 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1252/20000] Train-Loss: 1.2806449618135884 Test-Loss: 0.3203158163906081 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1253/20000] Train-Loss: 1.2808144174369354 Test-Loss: 0.3203130717629799 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1254/20000] Train-Loss: 1.2805952834938834 Test-Loss: 0.32030681802472677 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1255/20000] Train-Loss: 1.280562948903841 Test-Loss: 0.32029814647656196 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1256/20000] Train-Loss: 1.2803541058963666 Test-Loss: 0.32029604211753926 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1257/20000] Train-Loss: 1.2804737770428078 Test-Loss: 0.3202931755652578 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1258/20000] Train-Loss: 1.280586121776599 Test-Loss: 0.32029218445938784 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1259/20000] Train-Loss: 1.2803238514813908 Test-Loss: 0.320279445639847 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1260/20000] Train-Loss: 1.2803313690990972 Test-Loss: 0.3202706158596937 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1261/20000] Train-Loss: 1.2805552529802608 Test-Loss: 0.3202664031243553 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1262/20000] Train-Loss: 1.2805077545023835 Test-Loss: 0.32026789469409606 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1263/20000] Train-Loss: 1.28047804546591 Test-Loss: 0.32026270029656345 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1264/20000] Train-Loss: 1.280382784638081 Test-Loss: 0.32026369484297174 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1265/20000] Train-Loss: 1.280290282456415 Test-Loss: 0.320256852598008 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1266/20000] Train-Loss: 1.2802992256754255 Test-Loss: 0.32025174580080207 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1267/20000] Train-Loss: 1.280491942863996 Test-Loss: 0.3202490241547878 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1268/20000] Train-Loss: 1.280164736082416 Test-Loss: 0.32023897406263524 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1269/20000] Train-Loss: 1.280220895373907 Test-Loss: 0.32023819767854284 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1270/20000] Train-Loss: 1.2801941585064696 Test-Loss: 0.3202320365146492 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1271/20000] Train-Loss: 1.2804407991022833 Test-Loss: 0.3202299539941199 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1272/20000] Train-Loss: 1.2800974074190934 Test-Loss: 0.32022097267513383 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1273/20000] Train-Loss: 1.2803510318362754 Test-Loss: 0.3202338634953452 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1274/20000] Train-Loss: 1.280113811391597 Test-Loss: 0.3202072893973304 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1275/20000] Train-Loss: 1.2801960043310583 Test-Loss: 0.3202091838105698 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1276/20000] Train-Loss: 1.2800626287933192 Test-Loss: 0.32019869840445897 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1277/20000] Train-Loss: 1.2801907043600789 Test-Loss: 0.3202025193680493 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1278/20000] Train-Loss: 1.2801214025836694 Test-Loss: 0.3202042946209077 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1279/20000] Train-Loss: 1.280226708819807 Test-Loss: 0.3201884564492145 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1280/20000] Train-Loss: 1.2800163788628633 Test-Loss: 0.32017831569537747 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1281/20000] Train-Loss: 1.280211688311436 Test-Loss: 0.3201765690798029 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1282/20000] Train-Loss: 1.279941905703051 Test-Loss: 0.32018208120484876 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1283/20000] Train-Loss: 1.2802186955822543 Test-Loss: 0.3201775997985514 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1284/20000] Train-Loss: 1.2799482366602053 Test-Loss: 0.3201597987678229 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1285/20000] Train-Loss: 1.2800098503769837 Test-Loss: 0.3201544878476637 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1286/20000] Train-Loss: 1.2801582201396073 Test-Loss: 0.32015706743492284 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1287/20000] Train-Loss: 1.2797813800566875 Test-Loss: 0.3201544485370297 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1288/20000] Train-Loss: 1.2801040110769146 Test-Loss: 0.32013985353735425 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1289/20000] Train-Loss: 1.2799434379867116 Test-Loss: 0.320141685163957 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1290/20000] Train-Loss: 1.2799947045349886 Test-Loss: 0.3201357875424548 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1291/20000] Train-Loss: 1.2798149616090095 Test-Loss: 0.32012508076790613 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1292/20000] Train-Loss: 1.2797721760351102 Test-Loss: 0.3201312693632018 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1293/20000] Train-Loss: 1.2798174531099704 Test-Loss: 0.3201078703569424 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1294/20000] Train-Loss: 1.279874843834365 Test-Loss: 0.3201242187744117 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1295/20000] Train-Loss: 1.2796835417387311 Test-Loss: 0.32011088817687394 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1296/20000] Train-Loss: 1.2798604169778252 Test-Loss: 0.32010589253077115 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1297/20000] Train-Loss: 1.2796852760933355 Test-Loss: 0.3200910988996906 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1298/20000] Train-Loss: 1.2795862532561526 Test-Loss: 0.32008690436503506 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1299/20000] Train-Loss: 1.2796508286514903 Test-Loss: 0.3200878235644426 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1300/20000] Train-Loss: 1.2795490396536762 Test-Loss: 0.3200824380860791 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1301/20000] Train-Loss: 1.2796573710202135 Test-Loss: 0.3200787797460137 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1302/20000] Train-Loss: 1.2796140945085204 Test-Loss: 0.3200636122898544 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1303/20000] Train-Loss: 1.279619661414538 Test-Loss: 0.3200565090121602 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1304/20000] Train-Loss: 1.279390822673101 Test-Loss: 0.32006027792753583 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1305/20000] Train-Loss: 1.279563124407352 Test-Loss: 0.32004178774942055 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1306/20000] Train-Loss: 1.2795998405181765 Test-Loss: 0.32004857920549024 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1307/20000] Train-Loss: 1.2796793837882023 Test-Loss: 0.32004732534964897 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1308/20000] Train-Loss: 1.2794593189087349 Test-Loss: 0.3200352862291007 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1309/20000] Train-Loss: 1.2794469314452268 Test-Loss: 0.3200289591945456 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1310/20000] Train-Loss: 1.2792922047152366 Test-Loss: 0.3200261156692572 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1311/20000] Train-Loss: 1.2795509466178734 Test-Loss: 0.3200240734287751 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1312/20000] Train-Loss: 1.279386209600504 Test-Loss: 0.3199987490575709 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1313/20000] Train-Loss: 1.27947953011369 Test-Loss: 0.3200156111323153 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1314/20000] Train-Loss: 1.2791888675221843 Test-Loss: 0.31999549936141297 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1315/20000] Train-Loss: 1.2791616250224416 Test-Loss: 0.3199911053744654 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1316/20000] Train-Loss: 1.279313059258915 Test-Loss: 0.3199876832720034 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1317/20000] Train-Loss: 1.2792075078698273 Test-Loss: 0.3199968590701613 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1318/20000] Train-Loss: 1.2792958510407686 Test-Loss: 0.31998288184750606 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1319/20000] Train-Loss: 1.2792304447367524 Test-Loss: 0.3199808577424089 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1320/20000] Train-Loss: 1.2793226676493399 Test-Loss: 0.31997071413461936 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1321/20000] Train-Loss: 1.2790981658001392 Test-Loss: 0.31995392651929616 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1322/20000] Train-Loss: 1.2790156073996606 Test-Loss: 0.31994681614012016 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1323/20000] Train-Loss: 1.279030096080753 Test-Loss: 0.31994833498103475 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1324/20000] Train-Loss: 1.279029605552818 Test-Loss: 0.3199444304789613 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1325/20000] Train-Loss: 1.2790248598746765 Test-Loss: 0.3199347897836853 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1326/20000] Train-Loss: 1.279233980896905 Test-Loss: 0.31992259434203746 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1327/20000] Train-Loss: 1.2790699412268964 Test-Loss: 0.3199221350676877 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1328/20000] Train-Loss: 1.2788860387684273 Test-Loss: 0.31991653822380034 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1329/20000] Train-Loss: 1.2789560539017297 Test-Loss: 0.31991511089883096 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1330/20000] Train-Loss: 1.27879173287197 Test-Loss: 0.3199113905714371 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1331/20000] Train-Loss: 1.2790738860829185 Test-Loss: 0.3198988459578903 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1332/20000] Train-Loss: 1.2789341165225636 Test-Loss: 0.3199074649634013 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1333/20000] Train-Loss: 1.2789719604800813 Test-Loss: 0.3198944664002075 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1334/20000] Train-Loss: 1.2788402426234986 Test-Loss: 0.3198944487103458 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1335/20000] Train-Loss: 1.2790330436611372 Test-Loss: 0.31989614779003234 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1336/20000] Train-Loss: 1.2788895553570034 Test-Loss: 0.3198748831373918 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1337/20000] Train-Loss: 1.278878228331791 Test-Loss: 0.3198703242160986 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1338/20000] Train-Loss: 1.2787372518899518 Test-Loss: 0.319857384556625 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1339/20000] Train-Loss: 1.278897423835955 Test-Loss: 0.3198528618290106 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1340/20000] Train-Loss: 1.2786182983981855 Test-Loss: 0.3198444634244885 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1341/20000] Train-Loss: 1.278769890251801 Test-Loss: 0.3198301467391019 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1342/20000] Train-Loss: 1.2785965445384906 Test-Loss: 0.3198329096037463 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1343/20000] Train-Loss: 1.2786725617002672 Test-Loss: 0.31982452023443314 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1344/20000] Train-Loss: 1.2785874416945626 Test-Loss: 0.31981721572226385 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1345/20000] Train-Loss: 1.278687195126446 Test-Loss: 0.3198202704584312 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1346/20000] Train-Loss: 1.278507298374027 Test-Loss: 0.31981451425498486 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1347/20000] Train-Loss: 1.278548454908352 Test-Loss: 0.3198079423385213 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1348/20000] Train-Loss: 1.2785575988570703 Test-Loss: 0.3198033341247655 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1349/20000] Train-Loss: 1.2784211801858683 Test-Loss: 0.31980196525059196 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1350/20000] Train-Loss: 1.278501964518354 Test-Loss: 0.3197843472041322 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1351/20000] Train-Loss: 1.2784813742139078 Test-Loss: 0.3197910448853623 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1352/20000] Train-Loss: 1.2784025701768094 Test-Loss: 0.31977473610963736 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1353/20000] Train-Loss: 1.2784341159600079 Test-Loss: 0.3197676196561348 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1354/20000] Train-Loss: 1.278597648801659 Test-Loss: 0.31976690293021465 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1355/20000] Train-Loss: 1.2783482363695204 Test-Loss: 0.31974479029940645 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1356/20000] Train-Loss: 1.2784244506300075 Test-Loss: 0.3197471436374307 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1357/20000] Train-Loss: 1.278366465840858 Test-Loss: 0.3197458338302192 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1358/20000] Train-Loss: 1.2783072935918525 Test-Loss: 0.3197429305696476 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1359/20000] Train-Loss: 1.278096156576191 Test-Loss: 0.3197290748990535 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1360/20000] Train-Loss: 1.2781729669416129 Test-Loss: 0.31972482636402844 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1361/20000] Train-Loss: 1.2782789613679622 Test-Loss: 0.31972296324146743 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1362/20000] Train-Loss: 1.278173249499926 Test-Loss: 0.31970725104975406 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1363/20000] Train-Loss: 1.2783556815785262 Test-Loss: 0.3196989717598783 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1364/20000] Train-Loss: 1.2781029278420504 Test-Loss: 0.31968155764181216 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1365/20000] Train-Loss: 1.2779783989655145 Test-Loss: 0.31967458785573255 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1366/20000] Train-Loss: 1.2780423432142791 Test-Loss: 0.31967162437393465 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1367/20000] Train-Loss: 1.2779591699374877 Test-Loss: 0.3196634620252257 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1368/20000] Train-Loss: 1.2780063784154025 Test-Loss: 0.31965995356809535 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1369/20000] Train-Loss: 1.278079694411946 Test-Loss: 0.3196504190602505 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1370/20000] Train-Loss: 1.2779986490439885 Test-Loss: 0.3196443821125304 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1371/20000] Train-Loss: 1.2780175856933849 Test-Loss: 0.3196433647152858 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1372/20000] Train-Loss: 1.2778447880845951 Test-Loss: 0.3196338766182332 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1373/20000] Train-Loss: 1.277977409626007 Test-Loss: 0.31963589710127494 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1374/20000] Train-Loss: 1.2778573196030245 Test-Loss: 0.3196231766738781 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1375/20000] Train-Loss: 1.2777885459420588 Test-Loss: 0.31962206285855005 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1376/20000] Train-Loss: 1.277897569639845 Test-Loss: 0.319619382363062 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1377/20000] Train-Loss: 1.277825087284308 Test-Loss: 0.3195939548047502 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1378/20000] Train-Loss: 1.2777504560010895 Test-Loss: 0.31959074838804424 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1379/20000] Train-Loss: 1.2775845523643274 Test-Loss: 0.3195861003101199 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1380/20000] Train-Loss: 1.2776589811908037 Test-Loss: 0.31957816101647357 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1381/20000] Train-Loss: 1.277744538461172 Test-Loss: 0.31957471393635284 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1382/20000] Train-Loss: 1.2777352566078057 Test-Loss: 0.31956648936045556 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1383/20000] Train-Loss: 1.277708848566773 Test-Loss: 0.31956781238871357 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1384/20000] Train-Loss: 1.277676466587003 Test-Loss: 0.3195571939383277 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1385/20000] Train-Loss: 1.2775180105351955 Test-Loss: 0.3195402552156675 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1386/20000] Train-Loss: 1.277551974064833 Test-Loss: 0.3195332646756896 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1387/20000] Train-Loss: 1.277444251380193 Test-Loss: 0.31952910551971375 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1388/20000] Train-Loss: 1.2775959943489419 Test-Loss: 0.31951776626635403 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1389/20000] Train-Loss: 1.2780292321716118 Test-Loss: 0.3195081070437137 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1390/20000] Train-Loss: 1.2773499929835308 Test-Loss: 0.3195053234683923 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1391/20000] Train-Loss: 1.2773187118378413 Test-Loss: 0.3194921009190412 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1392/20000] Train-Loss: 1.2773993353429 Test-Loss: 0.3194888267344151 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1393/20000] Train-Loss: 1.2773187616764439 Test-Loss: 0.3194854760218022 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1394/20000] Train-Loss: 1.277376534694202 Test-Loss: 0.31947828750515345 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1395/20000] Train-Loss: 1.2771375113449122 Test-Loss: 0.3194615615737884 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1396/20000] Train-Loss: 1.2771652394528412 Test-Loss: 0.31945223794843325 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1397/20000] Train-Loss: 1.2773202199234766 Test-Loss: 0.3194605705637191 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1398/20000] Train-Loss: 1.2771272122265866 Test-Loss: 0.31944515064099926 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1399/20000] Train-Loss: 1.2772319409722783 Test-Loss: 0.3194410631028557 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1400/20000] Train-Loss: 1.2770408657010006 Test-Loss: 0.3194266945077524 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1401/20000] Train-Loss: 1.277059098672187 Test-Loss: 0.31941182454961925 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1402/20000] Train-Loss: 1.2770775634476548 Test-Loss: 0.3194015440538642 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1403/20000] Train-Loss: 1.2771652972692207 Test-Loss: 0.3193899403196237 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1404/20000] Train-Loss: 1.2771835425201696 Test-Loss: 0.31939706125089623 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1405/20000] Train-Loss: 1.2768842141074443 Test-Loss: 0.31938028942545776 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1406/20000] Train-Loss: 1.2770536823319785 Test-Loss: 0.31937514926698335 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1407/20000] Train-Loss: 1.276744802389353 Test-Loss: 0.31937868096155 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1408/20000] Train-Loss: 1.276871306383812 Test-Loss: 0.3193645164245278 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1409/20000] Train-Loss: 1.276885251078434 Test-Loss: 0.31936488566638277 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1410/20000] Train-Loss: 1.2767415632059806 Test-Loss: 0.3193621711556583 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1411/20000] Train-Loss: 1.2767292484535104 Test-Loss: 0.31935476488319636 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1412/20000] Train-Loss: 1.2766513537067323 Test-Loss: 0.31933997031935724 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1413/20000] Train-Loss: 1.2768144002580029 Test-Loss: 0.3193271699458262 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1414/20000] Train-Loss: 1.2766111054180893 Test-Loss: 0.31932294038523235 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1415/20000] Train-Loss: 1.2765631473704995 Test-Loss: 0.3193160217692415 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1416/20000] Train-Loss: 1.2766165319497458 Test-Loss: 0.3193104861334636 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1417/20000] Train-Loss: 1.2764858438911288 Test-Loss: 0.3192992684003305 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1418/20000] Train-Loss: 1.2766101021684073 Test-Loss: 0.31929054351304514 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1419/20000] Train-Loss: 1.2764290126757223 Test-Loss: 0.31929011986055567 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1420/20000] Train-Loss: 1.2765401268176835 Test-Loss: 0.3192856249901239 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1421/20000] Train-Loss: 1.2764176189952523 Test-Loss: 0.31927011819661966 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1422/20000] Train-Loss: 1.2766026680568803 Test-Loss: 0.3192652267695442 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1423/20000] Train-Loss: 1.2765357736405951 Test-Loss: 0.31924978457825215 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1424/20000] Train-Loss: 1.2762982276085624 Test-Loss: 0.3192409326100115 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1425/20000] Train-Loss: 1.276383725682111 Test-Loss: 0.3192344796133332 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1426/20000] Train-Loss: 1.2763247270921565 Test-Loss: 0.31921900138110026 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1427/20000] Train-Loss: 1.2762537973113586 Test-Loss: 0.3192180532763163 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1428/20000] Train-Loss: 1.2760991255081973 Test-Loss: 0.3191939259311401 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1429/20000] Train-Loss: 1.2761766713717617 Test-Loss: 0.31917862328762114 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1430/20000] Train-Loss: 1.2760241801619012 Test-Loss: 0.3191787696012957 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1431/20000] Train-Loss: 1.276019829518121 Test-Loss: 0.31917102382334517 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1432/20000] Train-Loss: 1.2760493423744173 Test-Loss: 0.3191640804275967 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1433/20000] Train-Loss: 1.275912419373394 Test-Loss: 0.3191580011408356 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1434/20000] Train-Loss: 1.2759323854443896 Test-Loss: 0.3191427438224554 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1435/20000] Train-Loss: 1.2758952442137823 Test-Loss: 0.3191283006791714 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1436/20000] Train-Loss: 1.2758988482850808 Test-Loss: 0.3191145638642515 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1437/20000] Train-Loss: 1.2758502953475515 Test-Loss: 0.31911162771861035 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1438/20000] Train-Loss: 1.2757151101696234 Test-Loss: 0.31910525157691855 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1439/20000] Train-Loss: 1.2757727944283328 Test-Loss: 0.3190904987510783 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1440/20000] Train-Loss: 1.275783695263808 Test-Loss: 0.31908462890544625 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1441/20000] Train-Loss: 1.2757368030906826 Test-Loss: 0.3190790284640885 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1442/20000] Train-Loss: 1.2756479107754986 Test-Loss: 0.3190554177138016 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1443/20000] Train-Loss: 1.2757773932729324 Test-Loss: 0.3190451789907228 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1444/20000] Train-Loss: 1.2757431867022413 Test-Loss: 0.3190471163981303 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1445/20000] Train-Loss: 1.275578069148773 Test-Loss: 0.3190383309162391 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1446/20000] Train-Loss: 1.2755768562697511 Test-Loss: 0.3190236850156774 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1447/20000] Train-Loss: 1.2754540591621557 Test-Loss: 0.3190134852286113 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1448/20000] Train-Loss: 1.2754790194230168 Test-Loss: 0.31900568664667117 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1449/20000] Train-Loss: 1.2754691067086013 Test-Loss: 0.31899936686076025 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1450/20000] Train-Loss: 1.2754650221455301 Test-Loss: 0.3189842208272354 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1451/20000] Train-Loss: 1.2752238734974628 Test-Loss: 0.3189803836057128 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1452/20000] Train-Loss: 1.2751861088904681 Test-Loss: 0.3189620001736423 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1453/20000] Train-Loss: 1.2753028552265604 Test-Loss: 0.318960167171872 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1454/20000] Train-Loss: 1.2750358969931455 Test-Loss: 0.31895875184208705 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1455/20000] Train-Loss: 1.2751304573304372 Test-Loss: 0.31893108936515024 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1456/20000] Train-Loss: 1.2751870263977558 Test-Loss: 0.31892525205137023 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1457/20000] Train-Loss: 1.2750389708118228 Test-Loss: 0.3189226718322501 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1458/20000] Train-Loss: 1.275010892378489 Test-Loss: 0.3189075775078663 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1459/20000] Train-Loss: 1.2749042564495094 Test-Loss: 0.31891137026325417 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1460/20000] Train-Loss: 1.2748885172674231 Test-Loss: 0.31890238571583485 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1461/20000] Train-Loss: 1.2748122900561212 Test-Loss: 0.3188792990116493 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1462/20000] Train-Loss: 1.2748864584818675 Test-Loss: 0.318873941694308 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1463/20000] Train-Loss: 1.2747613565420297 Test-Loss: 0.318859517960412 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1464/20000] Train-Loss: 1.2748237985037592 Test-Loss: 0.3188480949138001 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1465/20000] Train-Loss: 1.274897853162897 Test-Loss: 0.3188348960248192 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1466/20000] Train-Loss: 1.2746701200090382 Test-Loss: 0.3188279643892271 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1467/20000] Train-Loss: 1.2746898720091908 Test-Loss: 0.31881238464864 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1468/20000] Train-Loss: 1.2747056613585819 Test-Loss: 0.3187998615556108 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1469/20000] Train-Loss: 1.2745652788336743 Test-Loss: 0.31880363433857084 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1470/20000] Train-Loss: 1.2745559641946103 Test-Loss: 0.3187775268901408 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1471/20000] Train-Loss: 1.27453161563359 Test-Loss: 0.31876705623541385 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1472/20000] Train-Loss: 1.2743371641488874 Test-Loss: 0.31875039880611444 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1473/20000] Train-Loss: 1.2747151413258124 Test-Loss: 0.3187502442811982 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1474/20000] Train-Loss: 1.2744537109477139 Test-Loss: 0.31873341428384466 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1475/20000] Train-Loss: 1.2744431824968838 Test-Loss: 0.31871548178505443 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1476/20000] Train-Loss: 1.2742274945579424 Test-Loss: 0.31871081842058546 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1477/20000] Train-Loss: 1.2741836848680392 Test-Loss: 0.31870284680310357 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1478/20000] Train-Loss: 1.2743252779908298 Test-Loss: 0.3187061754391285 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1479/20000] Train-Loss: 1.274124319662647 Test-Loss: 0.31870014699558824 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1480/20000] Train-Loss: 1.2740866477451658 Test-Loss: 0.3186578113246878 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1481/20000] Train-Loss: 1.2739237030381876 Test-Loss: 0.3186574678957169 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1482/20000] Train-Loss: 1.2739985054984364 Test-Loss: 0.31865944739752583 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1483/20000] Train-Loss: 1.2739259460072743 Test-Loss: 0.31864612666329173 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1484/20000] Train-Loss: 1.273728676798887 Test-Loss: 0.3186178302627619 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1485/20000] Train-Loss: 1.2739716912734418 Test-Loss: 0.3186093933723607 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1486/20000] Train-Loss: 1.273620857170718 Test-Loss: 0.3185797883054178 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1487/20000] Train-Loss: 1.2739492959524803 Test-Loss: 0.318581486623536 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1488/20000] Train-Loss: 1.273819731947276 Test-Loss: 0.3185837068349519 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1489/20000] Train-Loss: 1.2734093511630493 Test-Loss: 0.3185459511060424 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1490/20000] Train-Loss: 1.2736785356018239 Test-Loss: 0.3185353540573588 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1491/20000] Train-Loss: 1.2736570321501588 Test-Loss: 0.3185221010751142 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1492/20000] Train-Loss: 1.2735260690530008 Test-Loss: 0.31850695383136524 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1493/20000] Train-Loss: 1.2733841904651875 Test-Loss: 0.318499402192826 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1494/20000] Train-Loss: 1.273261242652814 Test-Loss: 0.31848512015547376 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1495/20000] Train-Loss: 1.2732371064456884 Test-Loss: 0.31847836030831206 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1496/20000] Train-Loss: 1.2733620823606724 Test-Loss: 0.318453276630602 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1497/20000] Train-Loss: 1.2730234735339172 Test-Loss: 0.3184491793864141 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1498/20000] Train-Loss: 1.2730159648369153 Test-Loss: 0.31843818365314186 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1499/20000] Train-Loss: 1.2729772475367211 Test-Loss: 0.31842692472903805 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1500/20000] Train-Loss: 1.2729462401851643 Test-Loss: 0.3184039789298981 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1501/20000] Train-Loss: 1.2729039713077106 Test-Loss: 0.3183946829239652 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1502/20000] Train-Loss: 1.2728566759755664 Test-Loss: 0.3183925688305517 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1503/20000] Train-Loss: 1.272638167884319 Test-Loss: 0.31835886909076444 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1504/20000] Train-Loss: 1.2727181695931034 Test-Loss: 0.31835788101967455 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1505/20000] Train-Loss: 1.272746547982737 Test-Loss: 0.31836845989380713 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1506/20000] Train-Loss: 1.2726772484894413 Test-Loss: 0.31833762727241294 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1507/20000] Train-Loss: 1.2727586499019399 Test-Loss: 0.31834047487438644 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1508/20000] Train-Loss: 1.2725285893016225 Test-Loss: 0.31831828299935894 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1509/20000] Train-Loss: 1.272546869172164 Test-Loss: 0.31830203857403083 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1510/20000] Train-Loss: 1.272619917421216 Test-Loss: 0.3182870777146479 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1511/20000] Train-Loss: 1.2723123087703618 Test-Loss: 0.3182697007624787 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1512/20000] Train-Loss: 1.272361001856831 Test-Loss: 0.31827626799892544 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1513/20000] Train-Loss: 1.2723587384172517 Test-Loss: 0.31826087612952497 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1514/20000] Train-Loss: 1.272209702205726 Test-Loss: 0.31824805079599683 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1515/20000] Train-Loss: 1.272334694655551 Test-Loss: 0.3182469982699998 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1516/20000] Train-Loss: 1.2722416253174345 Test-Loss: 0.31823281620192456 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1517/20000] Train-Loss: 1.271936547633021 Test-Loss: 0.31820904113073784 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1518/20000] Train-Loss: 1.2720390709851934 Test-Loss: 0.3182228764750619 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1519/20000] Train-Loss: 1.2721452269152804 Test-Loss: 0.31818317551499775 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1520/20000] Train-Loss: 1.2721245402782255 Test-Loss: 0.31818271092204187 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1521/20000] Train-Loss: 1.2720302494569926 Test-Loss: 0.318178097017302 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1522/20000] Train-Loss: 1.2718554664889519 Test-Loss: 0.3181579673748336 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1523/20000] Train-Loss: 1.2717387456679823 Test-Loss: 0.3181388667051058 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1524/20000] Train-Loss: 1.2716882414914157 Test-Loss: 0.3181413730899471 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1525/20000] Train-Loss: 1.2715179244881252 Test-Loss: 0.3181351073352073 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1526/20000] Train-Loss: 1.271676783174783 Test-Loss: 0.31811637661251974 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1527/20000] Train-Loss: 1.2715854713958992 Test-Loss: 0.3180834998290101 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1528/20000] Train-Loss: 1.2715184555998165 Test-Loss: 0.3180865044320136 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1529/20000] Train-Loss: 1.2715761002274248 Test-Loss: 0.318065650138695 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1530/20000] Train-Loss: 1.2714578682937732 Test-Loss: 0.31805603727173326 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1531/20000] Train-Loss: 1.2712897471509028 Test-Loss: 0.31805127947430284 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1532/20000] Train-Loss: 1.2711319055224293 Test-Loss: 0.31802747079284727 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1533/20000] Train-Loss: 1.2712299822695132 Test-Loss: 0.3180172164119861 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1534/20000] Train-Loss: 1.2713244355781796 Test-Loss: 0.3180087301816037 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1535/20000] Train-Loss: 1.271240868174413 Test-Loss: 0.31799143042486977 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1536/20000] Train-Loss: 1.2712035030758262 Test-Loss: 0.317981887310525 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1537/20000] Train-Loss: 1.271061499148188 Test-Loss: 0.3179695285261147 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1538/20000] Train-Loss: 1.2713136364055841 Test-Loss: 0.31795934280059346 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1539/20000] Train-Loss: 1.2709357806253787 Test-Loss: 0.317942087092016 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1540/20000] Train-Loss: 1.2708274684829122 Test-Loss: 0.3179284095223969 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1541/20000] Train-Loss: 1.27094082382713 Test-Loss: 0.3179400199713973 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1542/20000] Train-Loss: 1.2708205066751734 Test-Loss: 0.3179254609203829 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1543/20000] Train-Loss: 1.2705591667404637 Test-Loss: 0.31789424236083647 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1544/20000] Train-Loss: 1.27061371951973 Test-Loss: 0.31789273692169473 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1545/20000] Train-Loss: 1.270799793921361 Test-Loss: 0.31785809126905923 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1546/20000] Train-Loss: 1.2705266240251436 Test-Loss: 0.31785728206324304 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1547/20000] Train-Loss: 1.2703548834331726 Test-Loss: 0.31784061929894863 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1548/20000] Train-Loss: 1.270854063238575 Test-Loss: 0.3178112786180654 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1549/20000] Train-Loss: 1.2706582101899435 Test-Loss: 0.3178009624175384 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1550/20000] Train-Loss: 1.270181672860711 Test-Loss: 0.31778483553591264 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1551/20000] Train-Loss: 1.2702521397874997 Test-Loss: 0.3177723899057893 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1552/20000] Train-Loss: 1.2702055878062513 Test-Loss: 0.31776526891083756 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1553/20000] Train-Loss: 1.2701049792620542 Test-Loss: 0.31773908055103733 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1554/20000] Train-Loss: 1.270126257095499 Test-Loss: 0.31772873303831256 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1555/20000] Train-Loss: 1.2701289208936721 Test-Loss: 0.31772983065617955 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1556/20000] Train-Loss: 1.2699357326381517 Test-Loss: 0.317723082004093 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1557/20000] Train-Loss: 1.2700941859962904 Test-Loss: 0.3177204769890691 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1558/20000] Train-Loss: 1.2698035771906713 Test-Loss: 0.31769502905769814 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1559/20000] Train-Loss: 1.2697600322321902 Test-Loss: 0.3176709065507653 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1560/20000] Train-Loss: 1.2697294476296148 Test-Loss: 0.31765360094150924 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1561/20000] Train-Loss: 1.2696064303656636 Test-Loss: 0.3175857389186549 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1562/20000] Train-Loss: 1.2694836275377488 Test-Loss: 0.31758752246346605 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1563/20000] Train-Loss: 1.2695139000492282 Test-Loss: 0.31756222460492767 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1564/20000] Train-Loss: 1.2691839768259183 Test-Loss: 0.31754076007799137 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1565/20000] Train-Loss: 1.2691510769626708 Test-Loss: 0.31752029245535884 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1566/20000] Train-Loss: 1.2689616661650658 Test-Loss: 0.3175018790052414 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1567/20000] Train-Loss: 1.269116556288951 Test-Loss: 0.31748558926923504 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1568/20000] Train-Loss: 1.2693173606292303 Test-Loss: 0.3174839494963794 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1569/20000] Train-Loss: 1.268870803828604 Test-Loss: 0.31745266922954624 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1570/20000] Train-Loss: 1.2689594660599903 Test-Loss: 0.31743583978354345 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1571/20000] Train-Loss: 1.2690280871939301 Test-Loss: 0.3174061809462984 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1572/20000] Train-Loss: 1.2686774090366204 Test-Loss: 0.3174098467773322 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1573/20000] Train-Loss: 1.2686062155534417 Test-Loss: 0.3173952387764916 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1574/20000] Train-Loss: 1.2686607174971223 Test-Loss: 0.317368764239019 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1575/20000] Train-Loss: 1.2683689257334743 Test-Loss: 0.3173524924795731 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1576/20000] Train-Loss: 1.268288252632978 Test-Loss: 0.3173414902578847 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1577/20000] Train-Loss: 1.26820819183917 Test-Loss: 0.31731672217302637 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1578/20000] Train-Loss: 1.2682834026064322 Test-Loss: 0.3172844101261077 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1579/20000] Train-Loss: 1.2680469971548676 Test-Loss: 0.31724663126115404 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1580/20000] Train-Loss: 1.268142390630208 Test-Loss: 0.3172320992250794 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1581/20000] Train-Loss: 1.267917349602098 Test-Loss: 0.3172178246773194 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1582/20000] Train-Loss: 1.2679121725889237 Test-Loss: 0.31719602235940064 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1583/20000] Train-Loss: 1.2679918997812585 Test-Loss: 0.31718131521348153 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1584/20000] Train-Loss: 1.2677540427023315 Test-Loss: 0.31716660187845636 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1585/20000] Train-Loss: 1.2676610275499203 Test-Loss: 0.31711846843057107 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1586/20000] Train-Loss: 1.2675137321202494 Test-Loss: 0.3170962739436157 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1587/20000] Train-Loss: 1.267303925907638 Test-Loss: 0.31706507282666846 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1588/20000] Train-Loss: 1.267345866607888 Test-Loss: 0.31703966223124264 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1589/20000] Train-Loss: 1.2670859987777805 Test-Loss: 0.3170415199123668 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1590/20000] Train-Loss: 1.2672488621224822 Test-Loss: 0.31701041490055065 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1591/20000] Train-Loss: 1.267191015463607 Test-Loss: 0.3169999955142664 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1592/20000] Train-Loss: 1.2669038585103767 Test-Loss: 0.3169692765362489 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1593/20000] Train-Loss: 1.26679122049423 Test-Loss: 0.31696374914304504 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1594/20000] Train-Loss: 1.2672082779876366 Test-Loss: 0.3169438181807506 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1595/20000] Train-Loss: 1.2668040685297384 Test-Loss: 0.3169063232985433 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1596/20000] Train-Loss: 1.2666125922940872 Test-Loss: 0.3169032088272934 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1597/20000] Train-Loss: 1.266742035285108 Test-Loss: 0.31687707646644936 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1598/20000] Train-Loss: 1.2664559273701785 Test-Loss: 0.3168485805486731 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1599/20000] Train-Loss: 1.2664509837627578 Test-Loss: 0.3168286039591178 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1600/20000] Train-Loss: 1.2665738751910836 Test-Loss: 0.31680326177650353 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1601/20000] Train-Loss: 1.2664126865159637 Test-Loss: 0.31679654817176384 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1602/20000] Train-Loss: 1.2663314854767536 Test-Loss: 0.31678390431499803 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1603/20000] Train-Loss: 1.2661800808020083 Test-Loss: 0.31676880793759626 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1604/20000] Train-Loss: 1.2661371605945921 Test-Loss: 0.31674906257628166 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1605/20000] Train-Loss: 1.2659141216147756 Test-Loss: 0.3167181879019865 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1606/20000] Train-Loss: 1.2658818499334812 Test-Loss: 0.31670181970931355 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1607/20000] Train-Loss: 1.2660261118946143 Test-Loss: 0.3166944948322896 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1608/20000] Train-Loss: 1.2657321081997501 Test-Loss: 0.31667543653331987 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1609/20000] Train-Loss: 1.2657683033076146 Test-Loss: 0.31663977552885314 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1610/20000] Train-Loss: 1.2656265335334176 Test-Loss: 0.31661922568919537 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1611/20000] Train-Loss: 1.2656447589492792 Test-Loss: 0.31660643957368884 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1612/20000] Train-Loss: 1.265470647055381 Test-Loss: 0.31659732288823883 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1613/20000] Train-Loss: 1.2653351468360758 Test-Loss: 0.3165803742830128 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1614/20000] Train-Loss: 1.2652129157499623 Test-Loss: 0.3165520162367765 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1615/20000] Train-Loss: 1.2653642101358675 Test-Loss: 0.31654041363822544 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1616/20000] Train-Loss: 1.265254545762891 Test-Loss: 0.31651700417956163 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1617/20000] Train-Loss: 1.2651132621998256 Test-Loss: 0.3165197527221932 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1618/20000] Train-Loss: 1.2648287316654774 Test-Loss: 0.3165036046485085 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1619/20000] Train-Loss: 1.2649628347805664 Test-Loss: 0.3164764140207048 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1620/20000] Train-Loss: 1.2648751864037924 Test-Loss: 0.3164499084077835 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1621/20000] Train-Loss: 1.2648201101318395 Test-Loss: 0.3164505353535199 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1622/20000] Train-Loss: 1.2645862642246881 Test-Loss: 0.31641660540866057 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1623/20000] Train-Loss: 1.2645902616793339 Test-Loss: 0.3164117196256073 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1624/20000] Train-Loss: 1.2643252313760227 Test-Loss: 0.3163651383380702 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1625/20000] Train-Loss: 1.2644123268248637 Test-Loss: 0.3163455435789972 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1626/20000] Train-Loss: 1.2642936650427123 Test-Loss: 0.316314278724384 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1627/20000] Train-Loss: 1.2643450845905937 Test-Loss: 0.3163077873701235 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1628/20000] Train-Loss: 1.2641178352969746 Test-Loss: 0.3162916252527715 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1629/20000] Train-Loss: 1.2641431563201206 Test-Loss: 0.3162816688358341 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1630/20000] Train-Loss: 1.263821535787714 Test-Loss: 0.316264457205077 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1631/20000] Train-Loss: 1.2639666513267118 Test-Loss: 0.31624994148147767 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1632/20000] Train-Loss: 1.263839466027232 Test-Loss: 0.31621906678679396 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1633/20000] Train-Loss: 1.263842813947184 Test-Loss: 0.31622221766305625 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1634/20000] Train-Loss: 1.2634640213784016 Test-Loss: 0.31619107107939515 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1635/20000] Train-Loss: 1.2636330384601293 Test-Loss: 0.3161612394355818 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1636/20000] Train-Loss: 1.2633383463253982 Test-Loss: 0.31613217958968415 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1637/20000] Train-Loss: 1.263870576700343 Test-Loss: 0.3161256602229524 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1638/20000] Train-Loss: 1.2633926048876742 Test-Loss: 0.3160806985697514 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1639/20000] Train-Loss: 1.2632048643874827 Test-Loss: 0.3160590511993362 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1640/20000] Train-Loss: 1.263116702220529 Test-Loss: 0.31603324665126137 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1641/20000] Train-Loss: 1.2634310051386666 Test-Loss: 0.3160012815998685 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1642/20000] Train-Loss: 1.262965896127608 Test-Loss: 0.3159973124777406 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1643/20000] Train-Loss: 1.2629373573449065 Test-Loss: 0.31598191861429564 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1644/20000] Train-Loss: 1.2626115998900151 Test-Loss: 0.31595787038709594 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1645/20000] Train-Loss: 1.2626041805215693 Test-Loss: 0.31593686500928536 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1646/20000] Train-Loss: 1.2628481100763307 Test-Loss: 0.315926934952745 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1647/20000] Train-Loss: 1.2624722555732735 Test-Loss: 0.31590853544875347 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1648/20000] Train-Loss: 1.262251639740954 Test-Loss: 0.31589408629580434 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1649/20000] Train-Loss: 1.2624945321268766 Test-Loss: 0.3158806425477208 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1650/20000] Train-Loss: 1.2623212605319785 Test-Loss: 0.31583114506179993 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1651/20000] Train-Loss: 1.2621321624488058 Test-Loss: 0.3158229341096483 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1652/20000] Train-Loss: 1.2619591365681508 Test-Loss: 0.31581603661735375 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1653/20000] Train-Loss: 1.2622718983387176 Test-Loss: 0.31579000749952674 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1654/20000] Train-Loss: 1.2618036025928336 Test-Loss: 0.31575274004703957 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1655/20000] Train-Loss: 1.2618248228028088 Test-Loss: 0.3157459701130264 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1656/20000] Train-Loss: 1.2616867338146425 Test-Loss: 0.3157058771296767 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1657/20000] Train-Loss: 1.2617167825064188 Test-Loss: 0.3156777946591841 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1658/20000] Train-Loss: 1.2616289331884303 Test-Loss: 0.3156767832997174 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1659/20000] Train-Loss: 1.2615157446711982 Test-Loss: 0.3156463577523234 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1660/20000] Train-Loss: 1.2613070530140862 Test-Loss: 0.31561883221792325 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1661/20000] Train-Loss: 1.2615146230403542 Test-Loss: 0.31559518358362565 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1662/20000] Train-Loss: 1.2612005387769636 Test-Loss: 0.31557989936386377 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1663/20000] Train-Loss: 1.261109333197436 Test-Loss: 0.3155629025558335 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1664/20000] Train-Loss: 1.260990968563658 Test-Loss: 0.3155250107515582 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1665/20000] Train-Loss: 1.2610607828825875 Test-Loss: 0.3155282976525784 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1666/20000] Train-Loss: 1.2609583419651078 Test-Loss: 0.3154770576043265 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1667/20000] Train-Loss: 1.260634637684814 Test-Loss: 0.3154646019489517 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1668/20000] Train-Loss: 1.2605267004773733 Test-Loss: 0.31543451344820833 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1669/20000] Train-Loss: 1.26099400630301 Test-Loss: 0.3154154917379408 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1670/20000] Train-Loss: 1.2604966843120027 Test-Loss: 0.31539731555891526 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1671/20000] Train-Loss: 1.2604573406077841 Test-Loss: 0.31538581086029227 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1672/20000] Train-Loss: 1.2603680799707329 Test-Loss: 0.315358736066604 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1673/20000] Train-Loss: 1.2604398073552567 Test-Loss: 0.3153509150121631 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1674/20000] Train-Loss: 1.2600428278562565 Test-Loss: 0.3153157447717773 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1675/20000] Train-Loss: 1.2603992610489227 Test-Loss: 0.3152851515023756 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1676/20000] Train-Loss: 1.2600750059276755 Test-Loss: 0.3152877117389629 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1677/20000] Train-Loss: 1.2602612593973417 Test-Loss: 0.3152599525861984 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1678/20000] Train-Loss: 1.259818057359078 Test-Loss: 0.31523195771644147 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1679/20000] Train-Loss: 1.259955207057715 Test-Loss: 0.3151938529195594 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1680/20000] Train-Loss: 1.2594799861783976 Test-Loss: 0.31518287509971704 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1681/20000] Train-Loss: 1.2594544256342295 Test-Loss: 0.31516010586453624 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1682/20000] Train-Loss: 1.2596875092216666 Test-Loss: 0.31513270824704886 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1683/20000] Train-Loss: 1.2592139301129186 Test-Loss: 0.315132737172318 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1684/20000] Train-Loss: 1.259224267437205 Test-Loss: 0.3151173174383154 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1685/20000] Train-Loss: 1.2591846704557117 Test-Loss: 0.3150764121353529 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1686/20000] Train-Loss: 1.2587632332126357 Test-Loss: 0.31505732122911223 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1687/20000] Train-Loss: 1.2589374008875591 Test-Loss: 0.3150612608798967 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1688/20000] Train-Loss: 1.258678927694184 Test-Loss: 0.31501948404355823 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1689/20000] Train-Loss: 1.2588928013132337 Test-Loss: 0.31498541148607956 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1690/20000] Train-Loss: 1.2588869275255037 Test-Loss: 0.31498469207964447 Train-f1: 0.12420091324200913 Test-f1: 0.12363636363636363 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1691/20000] Train-Loss: 1.2586019152492338 Test-Loss: 0.3149177680791405 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1692/20000] Train-Loss: 1.2583369379963711 Test-Loss: 0.31489385066372955 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1693/20000] Train-Loss: 1.2585362920870562 Test-Loss: 0.3148854261981497 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1694/20000] Train-Loss: 1.258255478412805 Test-Loss: 0.3148750061832382 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1695/20000] Train-Loss: 1.2582618680963493 Test-Loss: 0.3148615201006185 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1696/20000] Train-Loss: 1.2578679981218632 Test-Loss: 0.3148463251504484 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1697/20000] Train-Loss: 1.25798947801717 Test-Loss: 0.3148267917648667 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1698/20000] Train-Loss: 1.2579362952005957 Test-Loss: 0.31477613504803315 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1699/20000] Train-Loss: 1.2578734916029135 Test-Loss: 0.3147520001365651 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1700/20000] Train-Loss: 1.2576528754605927 Test-Loss: 0.31472953902532613 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1701/20000] Train-Loss: 1.2574040621724125 Test-Loss: 0.31470468052796563 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1702/20000] Train-Loss: 1.2576838054100152 Test-Loss: 0.3147037976823179 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1703/20000] Train-Loss: 1.2571105826894196 Test-Loss: 0.3146920762469186 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1704/20000] Train-Loss: 1.2571991117311496 Test-Loss: 0.31464919132132413 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1705/20000] Train-Loss: 1.2571898492367546 Test-Loss: 0.3146196169118332 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1706/20000] Train-Loss: 1.2569343127962525 Test-Loss: 0.3146034898156573 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1707/20000] Train-Loss: 1.2566826545714427 Test-Loss: 0.3145553875866086 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1708/20000] Train-Loss: 1.2569669951213787 Test-Loss: 0.314513467806603 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1709/20000] Train-Loss: 1.256622710783161 Test-Loss: 0.314493122667541 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1710/20000] Train-Loss: 1.2565707962126291 Test-Loss: 0.31446304699007244 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1711/20000] Train-Loss: 1.2565727823408495 Test-Loss: 0.3144291114643284 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1712/20000] Train-Loss: 1.2563601966040885 Test-Loss: 0.3144133839864775 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1713/20000] Train-Loss: 1.2560422451959126 Test-Loss: 0.31440472875666947 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1714/20000] Train-Loss: 1.2562938527860696 Test-Loss: 0.3143633066775143 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1715/20000] Train-Loss: 1.2560582573003651 Test-Loss: 0.31436790840511025 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1716/20000] Train-Loss: 1.2561845302958345 Test-Loss: 0.31431253252715086 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1717/20000] Train-Loss: 1.2557036302731153 Test-Loss: 0.31432304611581596 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1718/20000] Train-Loss: 1.2557420598435496 Test-Loss: 0.3142801226408722 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1719/20000] Train-Loss: 1.2555415171547228 Test-Loss: 0.31425036908599385 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1720/20000] Train-Loss: 1.2555448370795763 Test-Loss: 0.31420128379499596 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1721/20000] Train-Loss: 1.255471237691237 Test-Loss: 0.3141881377855641 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1722/20000] Train-Loss: 1.2553414988780813 Test-Loss: 0.31415891515595096 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1723/20000] Train-Loss: 1.2553697877642829 Test-Loss: 0.31414987770803565 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1724/20000] Train-Loss: 1.2551775766239721 Test-Loss: 0.31413630005160076 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1725/20000] Train-Loss: 1.2548436469693132 Test-Loss: 0.3141113314018644 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1726/20000] Train-Loss: 1.2546466883599992 Test-Loss: 0.31405974845714163 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1727/20000] Train-Loss: 1.254695550381955 Test-Loss: 0.31403350474761077 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1728/20000] Train-Loss: 1.2547241780745577 Test-Loss: 0.3140218586135583 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1729/20000] Train-Loss: 1.2544268302811608 Test-Loss: 0.31403113140372096 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1730/20000] Train-Loss: 1.2545198386296916 Test-Loss: 0.3139692948413414 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1731/20000] Train-Loss: 1.254557118484783 Test-Loss: 0.3139275062854621 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1732/20000] Train-Loss: 1.254186167004554 Test-Loss: 0.31391214117329386 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1733/20000] Train-Loss: 1.2541508976965134 Test-Loss: 0.31386202531187624 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1734/20000] Train-Loss: 1.2540905711453862 Test-Loss: 0.31383902085082904 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1735/20000] Train-Loss: 1.2539353528240067 Test-Loss: 0.31383704080480895 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1736/20000] Train-Loss: 1.25373419674801 Test-Loss: 0.31383819974926 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1737/20000] Train-Loss: 1.253782118265628 Test-Loss: 0.31379908155647457 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1738/20000] Train-Loss: 1.253444743599239 Test-Loss: 0.3137778343232702 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1739/20000] Train-Loss: 1.2533861664618386 Test-Loss: 0.313746108786502 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1740/20000] Train-Loss: 1.2531788450375696 Test-Loss: 0.31373382651809906 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1741/20000] Train-Loss: 1.2532324970608144 Test-Loss: 0.31369255533426316 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1742/20000] Train-Loss: 1.2530982679263398 Test-Loss: 0.3136670008606635 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1743/20000] Train-Loss: 1.2530069369917598 Test-Loss: 0.3136229682153772 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1744/20000] Train-Loss: 1.2528229098179278 Test-Loss: 0.313612368486336 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1745/20000] Train-Loss: 1.252663491806681 Test-Loss: 0.31356811632960746 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1746/20000] Train-Loss: 1.2526427231652228 Test-Loss: 0.313518566244994 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1747/20000] Train-Loss: 1.2525929250688292 Test-Loss: 0.31349499662230157 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1748/20000] Train-Loss: 1.2524238126211205 Test-Loss: 0.31345268788588326 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1749/20000] Train-Loss: 1.2522262217465205 Test-Loss: 0.31342686202016445 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1750/20000] Train-Loss: 1.2526594921792369 Test-Loss: 0.31339237489646204 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1751/20000] Train-Loss: 1.2518805900195635 Test-Loss: 0.3133982954767543 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1752/20000] Train-Loss: 1.2518002405405206 Test-Loss: 0.31339162214450517 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1753/20000] Train-Loss: 1.251734589488915 Test-Loss: 0.31334371283331747 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1754/20000] Train-Loss: 1.2515395262688354 Test-Loss: 0.31332936810432743 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1755/20000] Train-Loss: 1.2516728766928789 Test-Loss: 0.3132718475628135 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1756/20000] Train-Loss: 1.2511592038163633 Test-Loss: 0.31325651888431866 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1757/20000] Train-Loss: 1.2512971313897812 Test-Loss: 0.3132539573365326 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1758/20000] Train-Loss: 1.2513013500776724 Test-Loss: 0.3132396990043201 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1759/20000] Train-Loss: 1.2513680372106253 Test-Loss: 0.31318038993963 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1760/20000] Train-Loss: 1.2509006824297955 Test-Loss: 0.3131387294041189 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1761/20000] Train-Loss: 1.250933897140645 Test-Loss: 0.3130960866406923 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "Epoch [1762/20000] Train-Loss: 1.2505827890369132 Test-Loss: 0.31310735751827795 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1763/20000] Train-Loss: 1.2503720512822707 Test-Loss: 0.31305920087767525 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1764/20000] Train-Loss: 1.250342305133257 Test-Loss: 0.3129927427933149 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1765/20000] Train-Loss: 1.2507306013738948 Test-Loss: 0.3129863600405165 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1766/20000] Train-Loss: 1.2504232971829536 Test-Loss: 0.31297400291432687 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1767/20000] Train-Loss: 1.2501999608096905 Test-Loss: 0.31293086051442365 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1768/20000] Train-Loss: 1.250036154947803 Test-Loss: 0.3129088321768249 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1769/20000] Train-Loss: 1.2499936530142508 Test-Loss: 0.31286966838754054 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1770/20000] Train-Loss: 1.2497202284778377 Test-Loss: 0.31284086634363556 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1771/20000] Train-Loss: 1.2494509964383438 Test-Loss: 0.3128311421905204 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1772/20000] Train-Loss: 1.2497296441108556 Test-Loss: 0.3127713274626624 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1773/20000] Train-Loss: 1.2490281536301089 Test-Loss: 0.3127417707009926 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1774/20000] Train-Loss: 1.2490444635662867 Test-Loss: 0.3126933967245325 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1775/20000] Train-Loss: 1.249215670542052 Test-Loss: 0.31269119227065506 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1776/20000] Train-Loss: 1.2490511770572468 Test-Loss: 0.3126547379791149 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1777/20000] Train-Loss: 1.2487695562926489 Test-Loss: 0.31261907493162455 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1778/20000] Train-Loss: 1.2486156606870444 Test-Loss: 0.312607279549123 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1779/20000] Train-Loss: 1.2484006331976896 Test-Loss: 0.3125605243765081 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1780/20000] Train-Loss: 1.248269601642543 Test-Loss: 0.3125128517314831 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1781/20000] Train-Loss: 1.2482757052050897 Test-Loss: 0.3124976480336011 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1782/20000] Train-Loss: 1.248083515723178 Test-Loss: 0.3124504880993097 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1783/20000] Train-Loss: 1.2479136394902526 Test-Loss: 0.31242557471709154 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1784/20000] Train-Loss: 1.247830530589768 Test-Loss: 0.3124176587959955 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1785/20000] Train-Loss: 1.2479540113038041 Test-Loss: 0.3123722864125215 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1786/20000] Train-Loss: 1.2474649561947664 Test-Loss: 0.312356183050632 Train-f1: 0.13543730886850153 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "Epoch [1787/20000] Train-Loss: 1.2473848115458215 Test-Loss: 0.31238119512735996 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1788/20000] Train-Loss: 1.2472733939025904 Test-Loss: 0.31228703764181587 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "Epoch [1789/20000] Train-Loss: 1.2471085427878643 Test-Loss: 0.3123018359944454 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1790/20000] Train-Loss: 1.2471303965353084 Test-Loss: 0.3122227465661849 Train-f1: 0.12420091324200913 Test-f1: 0.12477064220183484 Train-acc: 0.4503311258278146 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1791/20000] Train-Loss: 1.2469692104388423 Test-Loss: 0.31218438567026546 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1792/20000] Train-Loss: 1.2467103924485479 Test-Loss: 0.31217601476764867 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1793/20000] Train-Loss: 1.246455945423517 Test-Loss: 0.3121085916545163 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1794/20000] Train-Loss: 1.2465641160996512 Test-Loss: 0.31206214729169923 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1795/20000] Train-Loss: 1.2464298521199164 Test-Loss: 0.3120251962618152 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1796/20000] Train-Loss: 1.2462560420632245 Test-Loss: 0.3120132934287374 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1797/20000] Train-Loss: 1.246021452085311 Test-Loss: 0.3119823551187821 Train-f1: 0.13543730886850153 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1798/20000] Train-Loss: 1.2458798028219655 Test-Loss: 0.3119632162166446 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1799/20000] Train-Loss: 1.2463792711626085 Test-Loss: 0.3119247425139265 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1800/20000] Train-Loss: 1.2455013780219408 Test-Loss: 0.31188735590681654 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1801/20000] Train-Loss: 1.2456096220082782 Test-Loss: 0.3118316688380438 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1802/20000] Train-Loss: 1.2451290132100448 Test-Loss: 0.3118207898749449 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "Epoch [1803/20000] Train-Loss: 1.2450360498492794 Test-Loss: 0.3118372228166563 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1804/20000] Train-Loss: 1.244937111259326 Test-Loss: 0.3117639821873264 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1805/20000] Train-Loss: 1.2452128507452074 Test-Loss: 0.3117527999687768 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1806/20000] Train-Loss: 1.2449452411642368 Test-Loss: 0.3117250539536364 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1807/20000] Train-Loss: 1.2445461448922692 Test-Loss: 0.3116763253172043 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "Epoch [1808/20000] Train-Loss: 1.2443807630032642 Test-Loss: 0.31167724280937764 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1809/20000] Train-Loss: 1.2443123284044433 Test-Loss: 0.31159053246411345 Train-f1: 0.12989053126352895 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1810/20000] Train-Loss: 1.2442193732817863 Test-Loss: 0.3115680077249048 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1811/20000] Train-Loss: 1.2439681578308803 Test-Loss: 0.3115174515396431 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1812/20000] Train-Loss: 1.2438175755305114 Test-Loss: 0.311512405504395 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1813/20000] Train-Loss: 1.2439721144735036 Test-Loss: 0.3114595543729219 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1814/20000] Train-Loss: 1.2434219406432332 Test-Loss: 0.3114316320562199 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1815/20000] Train-Loss: 1.2437804232444378 Test-Loss: 0.3114269098820311 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1816/20000] Train-Loss: 1.243245103898298 Test-Loss: 0.31137200475166527 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1817/20000] Train-Loss: 1.2430357885816339 Test-Loss: 0.3113499066789739 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1818/20000] Train-Loss: 1.2430402682450856 Test-Loss: 0.31128645825714807 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1819/20000] Train-Loss: 1.2429978968102022 Test-Loss: 0.3112616176526837 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1820/20000] Train-Loss: 1.242892463183961 Test-Loss: 0.31120037908587445 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1821/20000] Train-Loss: 1.2426784736266479 Test-Loss: 0.31118052496512016 Train-f1: 0.1461479244389175 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1822/20000] Train-Loss: 1.2422759848462888 Test-Loss: 0.31114847883569857 Train-f1: 0.1461479244389175 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1823/20000] Train-Loss: 1.2421981892840699 Test-Loss: 0.3111397118439039 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "Epoch [1824/20000] Train-Loss: 1.2425771001324624 Test-Loss: 0.3111496225405645 Train-f1: 0.1410197193107124 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1825/20000] Train-Loss: 1.2420066554065252 Test-Loss: 0.31108024242763993 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1826/20000] Train-Loss: 1.2421841656261263 Test-Loss: 0.3110296601008819 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1827/20000] Train-Loss: 1.241458004368069 Test-Loss: 0.3109962167934851 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1828/20000] Train-Loss: 1.2417054017169196 Test-Loss: 0.3109276832763459 Train-f1: 0.1410197193107124 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1829/20000] Train-Loss: 1.2411029185642501 Test-Loss: 0.3108731180507602 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1830/20000] Train-Loss: 1.241305838559558 Test-Loss: 0.3108655298899458 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1831/20000] Train-Loss: 1.240862152774909 Test-Loss: 0.31083583398633124 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1832/20000] Train-Loss: 1.2411327258077156 Test-Loss: 0.31083101597328067 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1833/20000] Train-Loss: 1.2408698590672866 Test-Loss: 0.3107781218001523 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1834/20000] Train-Loss: 1.2406786726455228 Test-Loss: 0.31075726613343946 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1835/20000] Train-Loss: 1.2405111757534573 Test-Loss: 0.3107190289372925 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1836/20000] Train-Loss: 1.2400810936565645 Test-Loss: 0.3106557179370267 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1837/20000] Train-Loss: 1.2401274536424294 Test-Loss: 0.3105750094766114 Train-f1: 0.1355837870538415 Test-f1: 0.12477064220183484 Train-acc: 0.45695364238410596 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1838/20000] Train-Loss: 1.2398760060245633 Test-Loss: 0.31054900226200727 Train-f1: 0.1410197193107124 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1839/20000] Train-Loss: 1.2396898331791837 Test-Loss: 0.3105067191355909 Train-f1: 0.1410197193107124 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "Epoch [1840/20000] Train-Loss: 1.2394165407019604 Test-Loss: 0.3105114649208526 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1841/20000] Train-Loss: 1.2399398851169428 Test-Loss: 0.31043998426256675 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1842/20000] Train-Loss: 1.2390504681324848 Test-Loss: 0.3103908668584572 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "Epoch [1843/20000] Train-Loss: 1.2387892353500929 Test-Loss: 0.31040987699852046 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1844/20000] Train-Loss: 1.2392090296048395 Test-Loss: 0.31037917891853495 Train-f1: 0.1512029859316681 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1845/20000] Train-Loss: 1.238558596918696 Test-Loss: 0.310345646138531 Train-f1: 0.14093003770423126 Test-f1: 0.12477064220183484 Train-acc: 0.4602649006622517 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1846/20000] Train-Loss: 1.2383527456351673 Test-Loss: 0.31029769610101876 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1847/20000] Train-Loss: 1.2383882654823295 Test-Loss: 0.3102326027271926 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1848/20000] Train-Loss: 1.238406456253018 Test-Loss: 0.3102081685948289 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1849/20000] Train-Loss: 1.2383406240665478 Test-Loss: 0.3101759221152039 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1850/20000] Train-Loss: 1.237896238963799 Test-Loss: 0.3101255557193836 Train-f1: 0.1461479244389175 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "Epoch [1851/20000] Train-Loss: 1.2377935180891353 Test-Loss: 0.31013719790446564 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1852/20000] Train-Loss: 1.2375539526371038 Test-Loss: 0.3100125112697706 Train-f1: 0.13010397553516817 Test-f1: 0.12477064220183484 Train-acc: 0.45364238410596025 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1853/20000] Train-Loss: 1.2376033956264725 Test-Loss: 0.30998985121661776 Train-f1: 0.15119918928048642 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "Epoch [1854/20000] Train-Loss: 1.2371429418072524 Test-Loss: 0.3099947137632489 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1855/20000] Train-Loss: 1.2372461332352302 Test-Loss: 0.3099887450501235 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1856/20000] Train-Loss: 1.2369142538195674 Test-Loss: 0.3098668692335635 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1857/20000] Train-Loss: 1.236850137402721 Test-Loss: 0.30984346588601114 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "Epoch [1858/20000] Train-Loss: 1.236589712875586 Test-Loss: 0.30987784973733756 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1859/20000] Train-Loss: 1.2362993354224703 Test-Loss: 0.30978045573665053 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1860/20000] Train-Loss: 1.2364136809317623 Test-Loss: 0.30974063550127795 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1861/20000] Train-Loss: 1.2357084224438122 Test-Loss: 0.3096570074451586 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1862/20000] Train-Loss: 1.2360223618611417 Test-Loss: 0.30964227187769705 Train-f1: 0.14626471432672983 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1863/20000] Train-Loss: 1.2355215784749212 Test-Loss: 0.30959483532138976 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1864/20000] Train-Loss: 1.2352115876242122 Test-Loss: 0.30957223265375966 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1865/20000] Train-Loss: 1.2353779976404071 Test-Loss: 0.3095243341063383 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1866/20000] Train-Loss: 1.2349438921931064 Test-Loss: 0.30951548990804023 Train-f1: 0.14626471432672983 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1867/20000] Train-Loss: 1.2348862025865255 Test-Loss: 0.30949319849957 Train-f1: 0.15119918928048642 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "Epoch [1868/20000] Train-Loss: 1.2352762199218377 Test-Loss: 0.30951108983600295 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1869/20000] Train-Loss: 1.2346345616287473 Test-Loss: 0.309443323168273 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1870/20000] Train-Loss: 1.2341343800639926 Test-Loss: 0.3093502745635314 Train-f1: 0.14626471432672983 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1871/20000] Train-Loss: 1.2344184548571973 Test-Loss: 0.3092682675604537 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1872/20000] Train-Loss: 1.2343072897271388 Test-Loss: 0.30923801334105416 Train-f1: 0.1512029859316681 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "Epoch [1873/20000] Train-Loss: 1.2341079804657284 Test-Loss: 0.30925956768306917 Train-f1: 0.16064070698702015 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1874/20000] Train-Loss: 1.2339269422303236 Test-Loss: 0.3091684374404921 Train-f1: 0.14617909048288796 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1875/20000] Train-Loss: 1.2337234540143354 Test-Loss: 0.30915030588899495 Train-f1: 0.1512029859316681 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1876/20000] Train-Loss: 1.233457300194337 Test-Loss: 0.30913586563445916 Train-f1: 0.14626471432672983 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1877/20000] Train-Loss: 1.2332015435213939 Test-Loss: 0.30912510118835 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1878/20000] Train-Loss: 1.2331476322904187 Test-Loss: 0.3090168286159109 Train-f1: 0.15121809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1879/20000] Train-Loss: 1.232735430025418 Test-Loss: 0.30894622313368547 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1880/20000] Train-Loss: 1.2327082437347554 Test-Loss: 0.30891782648789706 Train-f1: 0.15121809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "Epoch [1881/20000] Train-Loss: 1.2323389989548506 Test-Loss: 0.3089470893818619 Train-f1: 0.16064070698702015 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1882/20000] Train-Loss: 1.2324774631886335 Test-Loss: 0.3088265957150776 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "Epoch [1883/20000] Train-Loss: 1.2319554047020846 Test-Loss: 0.30885945426696365 Train-f1: 0.15593482463407898 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1884/20000] Train-Loss: 1.2318849220154207 Test-Loss: 0.3087987920200955 Train-f1: 0.14626471432672983 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1885/20000] Train-Loss: 1.2314731872231355 Test-Loss: 0.30877991279079076 Train-f1: 0.1512029859316681 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1886/20000] Train-Loss: 1.2319145079853908 Test-Loss: 0.3086866605166683 Train-f1: 0.15119918928048642 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1887/20000] Train-Loss: 1.2311225156049586 Test-Loss: 0.30865195868280154 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1888/20000] Train-Loss: 1.2308758074950235 Test-Loss: 0.30855102658462696 Train-f1: 0.14621809744779582 Test-f1: 0.12477064220183484 Train-acc: 0.46357615894039733 Test-acc: 0.4473684210526316\n",
      "Epoch [1889/20000] Train-Loss: 1.2316858428020556 Test-Loss: 0.3086135847986529 Train-f1: 0.16042338276380827 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1890/20000] Train-Loss: 1.2306384320111112 Test-Loss: 0.3085357085605541 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1891/20000] Train-Loss: 1.230503071727514 Test-Loss: 0.30847454688019643 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1892/20000] Train-Loss: 1.2304473206164939 Test-Loss: 0.3083936056546236 Train-f1: 0.15119918928048642 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "Epoch [1893/20000] Train-Loss: 1.2306674020566517 Test-Loss: 0.30839388020718517 Train-f1: 0.15119918928048642 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1894/20000] Train-Loss: 1.2298598219217867 Test-Loss: 0.30835534417934024 Train-f1: 0.15119918928048642 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1895/20000] Train-Loss: 1.2297229440879323 Test-Loss: 0.3083272358900314 Train-f1: 0.15119918928048642 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1896/20000] Train-Loss: 1.2295327561100697 Test-Loss: 0.30823409546014274 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1897/20000] Train-Loss: 1.2295925486468495 Test-Loss: 0.30821658240567285 Train-f1: 0.15601846638892017 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "Epoch [1898/20000] Train-Loss: 1.2290066010859229 Test-Loss: 0.30821893454514643 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1899/20000] Train-Loss: 1.2287782864203989 Test-Loss: 0.308095257164633 Train-f1: 0.1512029859316681 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1900/20000] Train-Loss: 1.2294971532398458 Test-Loss: 0.3080551423844846 Train-f1: 0.15607481948945365 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1901/20000] Train-Loss: 1.2284390959890559 Test-Loss: 0.3080386301912312 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1902/20000] Train-Loss: 1.2283275938216969 Test-Loss: 0.30796316662592066 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "Epoch [1903/20000] Train-Loss: 1.2281642972639073 Test-Loss: 0.3079791295277163 Train-f1: 0.16064070698702015 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1904/20000] Train-Loss: 1.2277436131475146 Test-Loss: 0.30785874181201633 Train-f1: 0.1512029859316681 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1905/20000] Train-Loss: 1.2276690177769114 Test-Loss: 0.30778708038731933 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1906/20000] Train-Loss: 1.2281003951627285 Test-Loss: 0.30772698940494836 Train-f1: 0.1512029859316681 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "Epoch [1907/20000] Train-Loss: 1.2275845655607802 Test-Loss: 0.30774424639472275 Train-f1: 0.16534658933996133 Test-f1: 0.12477064220183484 Train-acc: 0.4768211920529801 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1908/20000] Train-Loss: 1.2270043858095039 Test-Loss: 0.30763651909053785 Train-f1: 0.16064070698702015 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "Epoch [1909/20000] Train-Loss: 1.2266084830653148 Test-Loss: 0.3076624550580438 Train-f1: 0.1740597464001719 Test-f1: 0.12477064220183484 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1910/20000] Train-Loss: 1.2267090772992506 Test-Loss: 0.3075773791859506 Train-f1: 0.16064070698702015 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1911/20000] Train-Loss: 1.226364693415335 Test-Loss: 0.307524615178712 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1912/20000] Train-Loss: 1.2262686986776823 Test-Loss: 0.30751088712339764 Train-f1: 0.16064070698702015 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1913/20000] Train-Loss: 1.226014409785946 Test-Loss: 0.3074718946765839 Train-f1: 0.15119918928048642 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1914/20000] Train-Loss: 1.2263075777865347 Test-Loss: 0.3074343402908118 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1915/20000] Train-Loss: 1.225392975980002 Test-Loss: 0.3074087697104952 Train-f1: 0.16064070698702015 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1916/20000] Train-Loss: 1.2254373958762976 Test-Loss: 0.3073572648892489 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1917/20000] Train-Loss: 1.2255627616645415 Test-Loss: 0.3072649865515223 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1918/20000] Train-Loss: 1.2250010944003589 Test-Loss: 0.3072208995816917 Train-f1: 0.16073380171740825 Test-f1: 0.12477064220183484 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "Epoch [1919/20000] Train-Loss: 1.224847294386203 Test-Loss: 0.30724233389226735 Train-f1: 0.16951429185471736 Test-f1: 0.12477064220183484 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1920/20000] Train-Loss: 1.2244311334602058 Test-Loss: 0.3071058592549187 Train-f1: 0.15119677070896584 Test-f1: 0.12477064220183484 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1921/20000] Train-Loss: 1.2242081470458916 Test-Loss: 0.3070193215404857 Train-f1: 0.15119918928048642 Test-f1: 0.12592592592592594 Train-acc: 0.46688741721854304 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1922/20000] Train-Loss: 1.2247964903470734 Test-Loss: 0.30701069110546425 Train-f1: 0.17347985347985345 Test-f1: 0.12477064220183484 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1923/20000] Train-Loss: 1.2240477453735001 Test-Loss: 0.3069597306331463 Train-f1: 0.16520930232558137 Test-f1: 0.12477064220183484 Train-acc: 0.4768211920529801 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1924/20000] Train-Loss: 1.223930760013562 Test-Loss: 0.306904932934737 Train-f1: 0.16968119713728042 Test-f1: 0.12477064220183484 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1925/20000] Train-Loss: 1.2237810093600907 Test-Loss: 0.3068892273409746 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1926/20000] Train-Loss: 1.223112040919568 Test-Loss: 0.30679792714778 Train-f1: 0.16064070698702015 Test-f1: 0.12592592592592594 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1927/20000] Train-Loss: 1.2230961848807065 Test-Loss: 0.306738622105398 Train-f1: 0.15593482463407898 Test-f1: 0.12592592592592594 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "Epoch [1928/20000] Train-Loss: 1.2231125135723924 Test-Loss: 0.30676517580996515 Train-f1: 0.17366059646344684 Test-f1: 0.12477064220183484 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1929/20000] Train-Loss: 1.2226960709047674 Test-Loss: 0.30671507527118475 Train-f1: 0.16968119713728042 Test-f1: 0.12477064220183484 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1930/20000] Train-Loss: 1.2223744410809092 Test-Loss: 0.3066677322433302 Train-f1: 0.1559718969555035 Test-f1: 0.12477064220183484 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1931/20000] Train-Loss: 1.2221323277600709 Test-Loss: 0.30660609370483954 Train-f1: 0.16508349598785513 Test-f1: 0.12477064220183484 Train-acc: 0.4768211920529801 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1932/20000] Train-Loss: 1.221953780179943 Test-Loss: 0.3065551480432068 Train-f1: 0.16520930232558137 Test-f1: 0.12477064220183484 Train-acc: 0.4768211920529801 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1933/20000] Train-Loss: 1.221960739066439 Test-Loss: 0.30648742685058483 Train-f1: 0.16968119713728042 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "Epoch [1934/20000] Train-Loss: 1.221834615009141 Test-Loss: 0.30655749280328504 Train-f1: 0.17347985347985345 Test-f1: 0.12477064220183484 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1935/20000] Train-Loss: 1.2214732936242247 Test-Loss: 0.3064306736695718 Train-f1: 0.16520930232558137 Test-f1: 0.12477064220183484 Train-acc: 0.4768211920529801 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1936/20000] Train-Loss: 1.2208984758898447 Test-Loss: 0.30636246657041216 Train-f1: 0.1692161520190024 Test-f1: 0.12477064220183484 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1937/20000] Train-Loss: 1.220882491651025 Test-Loss: 0.3063600420902688 Train-f1: 0.16508349598785513 Test-f1: 0.12477064220183484 Train-acc: 0.4768211920529801 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1938/20000] Train-Loss: 1.220726171531957 Test-Loss: 0.30621466527789254 Train-f1: 0.1559718969555035 Test-f1: 0.12592592592592594 Train-acc: 0.47019867549668876 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1939/20000] Train-Loss: 1.220351636252685 Test-Loss: 0.3061482493083888 Train-f1: 0.17385377283135417 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1940/20000] Train-Loss: 1.2204057516645321 Test-Loss: 0.30612355766330934 Train-f1: 0.1692161520190024 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1941/20000] Train-Loss: 1.2200953500665257 Test-Loss: 0.3060399165191852 Train-f1: 0.16064070698702015 Test-f1: 0.12592592592592594 Train-acc: 0.4735099337748344 Test-acc: 0.4473684210526316\n",
      "Epoch [1942/20000] Train-Loss: 1.2201409889847568 Test-Loss: 0.3060437991915748 Train-f1: 0.1731542933580285 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1943/20000] Train-Loss: 1.2197392888752017 Test-Loss: 0.3059717356889751 Train-f1: 0.16508349598785513 Test-f1: 0.12592592592592594 Train-acc: 0.4768211920529801 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1944/20000] Train-Loss: 1.2193172794188532 Test-Loss: 0.3058993329558406 Train-f1: 0.17385377283135417 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1945/20000] Train-Loss: 1.21907227136089 Test-Loss: 0.3058593093822761 Train-f1: 0.17347985347985345 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "Epoch [1946/20000] Train-Loss: 1.218455701001906 Test-Loss: 0.3058726940205424 Train-f1: 0.17765902251738094 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1947/20000] Train-Loss: 1.218482186906096 Test-Loss: 0.3057756211059914 Train-f1: 0.1740597464001719 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1948/20000] Train-Loss: 1.2183376730830109 Test-Loss: 0.3057510060294932 Train-f1: 0.16935939080888227 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1949/20000] Train-Loss: 1.2185220295743102 Test-Loss: 0.3057006223929428 Train-f1: 0.17708502024291498 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1950/20000] Train-Loss: 1.2179098543397884 Test-Loss: 0.3056244763927111 Train-f1: 0.16935939080888227 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1951/20000] Train-Loss: 1.2180256748096105 Test-Loss: 0.30554401535773534 Train-f1: 0.1817564438956629 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1952/20000] Train-Loss: 1.2179520660988945 Test-Loss: 0.3055163459447265 Train-f1: 0.16951429185471736 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1953/20000] Train-Loss: 1.217340065951645 Test-Loss: 0.30542689996440975 Train-f1: 0.16951429185471736 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1954/20000] Train-Loss: 1.216967741623053 Test-Loss: 0.3053711440108315 Train-f1: 0.16951429185471736 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1955/20000] Train-Loss: 1.2168120318827316 Test-Loss: 0.30531599674087706 Train-f1: 0.1774553686268457 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1956/20000] Train-Loss: 1.216479621035794 Test-Loss: 0.305270976300279 Train-f1: 0.17726414612990457 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "Epoch [1957/20000] Train-Loss: 1.2162533673880864 Test-Loss: 0.3053219054914259 Train-f1: 0.17726414612990457 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1958/20000] Train-Loss: 1.2160512789868874 Test-Loss: 0.3052312275687762 Train-f1: 0.17331119643042442 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1959/20000] Train-Loss: 1.2157614041914386 Test-Loss: 0.3051240472184729 Train-f1: 0.17347985347985345 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1960/20000] Train-Loss: 1.2155527144802145 Test-Loss: 0.3050639233576541 Train-f1: 0.17385377283135417 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1961/20000] Train-Loss: 1.2158219897786078 Test-Loss: 0.30499064758893135 Train-f1: 0.1728744939271255 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1962/20000] Train-Loss: 1.2149991168771535 Test-Loss: 0.30497747247681645 Train-f1: 0.18052368343630482 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1963/20000] Train-Loss: 1.2149146598191531 Test-Loss: 0.3049297876778075 Train-f1: 0.17765902251738094 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "Epoch [1964/20000] Train-Loss: 1.2148070112390987 Test-Loss: 0.3049991534312653 Train-f1: 0.18500921360625527 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1965/20000] Train-Loss: 1.2146467513047956 Test-Loss: 0.3048163657542938 Train-f1: 0.16520930232558137 Test-f1: 0.12592592592592594 Train-acc: 0.4768211920529801 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1966/20000] Train-Loss: 1.2140395890489197 Test-Loss: 0.3047891072965238 Train-f1: 0.17347985347985345 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1967/20000] Train-Loss: 1.2141777879105913 Test-Loss: 0.30467037826062077 Train-f1: 0.1731542933580285 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "Epoch [1968/20000] Train-Loss: 1.2139894070028048 Test-Loss: 0.3047710888466508 Train-f1: 0.17691767068273093 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1969/20000] Train-Loss: 1.2134158523355816 Test-Loss: 0.30460977251392973 Train-f1: 0.16885321808921128 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1970/20000] Train-Loss: 1.213561667702095 Test-Loss: 0.30456063540614375 Train-f1: 0.1730088269809684 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1971/20000] Train-Loss: 1.2131357110530743 Test-Loss: 0.30451618272110814 Train-f1: 0.17691767068273093 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "Epoch [1972/20000] Train-Loss: 1.2126900720582803 Test-Loss: 0.30452534001533116 Train-f1: 0.17708502024291498 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1973/20000] Train-Loss: 1.2126090222695116 Test-Loss: 0.30443737345521793 Train-f1: 0.17765902251738094 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1974/20000] Train-Loss: 1.2120168429456482 Test-Loss: 0.30429914804889685 Train-f1: 0.1692161520190024 Test-f1: 0.12592592592592594 Train-acc: 0.48013245033112584 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1975/20000] Train-Loss: 1.2120812252591224 Test-Loss: 0.30425150922613936 Train-f1: 0.18129554655870445 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1976/20000] Train-Loss: 1.211492645700046 Test-Loss: 0.3042167018187786 Train-f1: 0.1810843373493976 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "Epoch [1977/20000] Train-Loss: 1.2114025355041445 Test-Loss: 0.3042624436745927 Train-f1: 0.1808855022660491 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1978/20000] Train-Loss: 1.2114195809395976 Test-Loss: 0.3040679899484658 Train-f1: 0.17331119643042442 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1979/20000] Train-Loss: 1.2108773663661172 Test-Loss: 0.30400903862111484 Train-f1: 0.1774553686268457 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1980/20000] Train-Loss: 1.210558354103389 Test-Loss: 0.30398635121489287 Train-f1: 0.18069872016603253 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1981/20000] Train-Loss: 1.2104090852490086 Test-Loss: 0.3038817591933322 Train-f1: 0.17708502024291498 Test-f1: 0.12592592592592594 Train-acc: 0.4867549668874172 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1982/20000] Train-Loss: 1.210792558478538 Test-Loss: 0.3038545788503638 Train-f1: 0.18478035281909372 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1983/20000] Train-Loss: 1.2101491638329736 Test-Loss: 0.303824838845174 Train-f1: 0.18456408747670885 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1984/20000] Train-Loss: 1.209468841401759 Test-Loss: 0.3037030242008715 Train-f1: 0.17347985347985345 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "Epoch [1985/20000] Train-Loss: 1.2091750934166678 Test-Loss: 0.3037228325733022 Train-f1: 0.19183086437508987 Test-f1: 0.12592592592592594 Train-acc: 0.5 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1986/20000] Train-Loss: 1.209348661495996 Test-Loss: 0.30363856397133376 Train-f1: 0.18069872016603253 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "Epoch [1987/20000] Train-Loss: 1.2091404593165744 Test-Loss: 0.3036456292005536 Train-f1: 0.18525100401606426 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1988/20000] Train-Loss: 1.2088639190800567 Test-Loss: 0.3035433679592358 Train-f1: 0.18436009732360098 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "Epoch [1989/20000] Train-Loss: 1.2079427840049983 Test-Loss: 0.3036127466395431 Train-f1: 0.18812847138372374 Test-f1: 0.12592592592592594 Train-acc: 0.4966887417218543 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1990/20000] Train-Loss: 1.2081577522848916 Test-Loss: 0.30348137311398493 Train-f1: 0.1808855022660491 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1991/20000] Train-Loss: 1.207938532776117 Test-Loss: 0.303305956569743 Train-f1: 0.17347985347985345 Test-f1: 0.12592592592592594 Train-acc: 0.48344370860927155 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1992/20000] Train-Loss: 1.207329255008656 Test-Loss: 0.3032593033512326 Train-f1: 0.18836009732360098 Test-f1: 0.12592592592592594 Train-acc: 0.4966887417218543 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1993/20000] Train-Loss: 1.2075135929677108 Test-Loss: 0.30319756156815786 Train-f1: 0.18151946527884072 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "Epoch [1994/20000] Train-Loss: 1.2065309346918014 Test-Loss: 0.30322406297868376 Train-f1: 0.19183086437508987 Test-f1: 0.12592592592592594 Train-acc: 0.5 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1995/20000] Train-Loss: 1.2066433714653286 Test-Loss: 0.3030577704485533 Train-f1: 0.17708502024291498 Test-f1: 0.16285714285714284 Train-acc: 0.4867549668874172 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [1996/20000] Train-Loss: 1.2068107233621435 Test-Loss: 0.30301839448554724 Train-f1: 0.18456408747670885 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1997/20000] Train-Loss: 1.2066302743920128 Test-Loss: 0.3030097659226985 Train-f1: 0.18456408747670885 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1998/20000] Train-Loss: 1.2057726479472803 Test-Loss: 0.3029123566645672 Train-f1: 0.18478035281909372 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [1999/20000] Train-Loss: 1.2061016390844 Test-Loss: 0.30285652707273164 Train-f1: 0.18069872016603253 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2000/20000] Train-Loss: 1.205432380395634 Test-Loss: 0.30282917620436783 Train-f1: 0.19183086437508987 Test-f1: 0.12592592592592594 Train-acc: 0.5 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2001/20000] Train-Loss: 1.2051663112016975 Test-Loss: 0.3028000866130776 Train-f1: 0.18478035281909372 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2002/20000] Train-Loss: 1.2048687375708143 Test-Loss: 0.30265495024640965 Train-f1: 0.18836009732360098 Test-f1: 0.14569319114027893 Train-acc: 0.4966887417218543 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2003/20000] Train-Loss: 1.2044787984279388 Test-Loss: 0.30263553660270764 Train-f1: 0.18836009732360098 Test-f1: 0.12592592592592594 Train-acc: 0.4966887417218543 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2004/20000] Train-Loss: 1.2040366001946492 Test-Loss: 0.3025371591775105 Train-f1: 0.18478035281909372 Test-f1: 0.14569319114027893 Train-acc: 0.49337748344370863 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2005/20000] Train-Loss: 1.2049475962473988 Test-Loss: 0.3024539691304119 Train-f1: 0.18812847138372374 Test-f1: 0.16285714285714284 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "Epoch [2006/20000] Train-Loss: 1.203804872461973 Test-Loss: 0.30252424857547017 Train-f1: 0.19183086437508987 Test-f1: 0.12592592592592594 Train-acc: 0.5 Test-acc: 0.4473684210526316\n",
      "Epoch [2007/20000] Train-Loss: 1.2038722427931638 Test-Loss: 0.3024581173780808 Train-f1: 0.18812847138372374 Test-f1: 0.12592592592592594 Train-acc: 0.4966887417218543 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2008/20000] Train-Loss: 1.203107382003425 Test-Loss: 0.3023703958349101 Train-f1: 0.18069872016603253 Test-f1: 0.12592592592592594 Train-acc: 0.4900662251655629 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2009/20000] Train-Loss: 1.202635578640886 Test-Loss: 0.3022632882803836 Train-f1: 0.18812847138372374 Test-f1: 0.12592592592592594 Train-acc: 0.4966887417218543 Test-acc: 0.4473684210526316\n",
      "Epoch [2010/20000] Train-Loss: 1.2029128498971207 Test-Loss: 0.30236310827092755 Train-f1: 0.18812847138372374 Test-f1: 0.12592592592592594 Train-acc: 0.4966887417218543 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2011/20000] Train-Loss: 1.2023863322554316 Test-Loss: 0.3020858776374066 Train-f1: 0.1810843373493976 Test-f1: 0.16285714285714284 Train-acc: 0.4900662251655629 Test-acc: 0.47368421052631576\n",
      "Epoch [2012/20000] Train-Loss: 1.2020358025989795 Test-Loss: 0.3021258755120319 Train-f1: 0.18812847138372374 Test-f1: 0.12592592592592594 Train-acc: 0.4966887417218543 Test-acc: 0.4473684210526316\n",
      "Epoch [2013/20000] Train-Loss: 1.2015240131584297 Test-Loss: 0.3021279292620797 Train-f1: 0.1920888674233277 Test-f1: 0.12592592592592594 Train-acc: 0.5 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2014/20000] Train-Loss: 1.2012707120039707 Test-Loss: 0.30197647999997973 Train-f1: 0.18500921360625527 Test-f1: 0.12592592592592594 Train-acc: 0.49337748344370863 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2015/20000] Train-Loss: 1.2010448065974406 Test-Loss: 0.30193886832764755 Train-f1: 0.19183086437508987 Test-f1: 0.12592592592592594 Train-acc: 0.5 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2016/20000] Train-Loss: 1.2008325183679278 Test-Loss: 0.30188351072397984 Train-f1: 0.19183086437508987 Test-f1: 0.12592592592592594 Train-acc: 0.5 Test-acc: 0.4473684210526316\n",
      "best score_dx!!\n",
      "Epoch [2017/20000] Train-Loss: 1.2005947611113577 Test-Loss: 0.30178215708821515 Train-f1: 0.18436009732360098 Test-f1: 0.14569319114027893 Train-acc: 0.49337748344370863 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2018/20000] Train-Loss: 1.2005786614163934 Test-Loss: 0.3016704973672154 Train-f1: 0.18812847138372374 Test-f1: 0.14569319114027893 Train-acc: 0.4966887417218543 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2019/20000] Train-Loss: 1.200368060297223 Test-Loss: 0.30165902268290884 Train-f1: 0.18812847138372374 Test-f1: 0.14569319114027893 Train-acc: 0.4966887417218543 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2020/20000] Train-Loss: 1.1995491349305663 Test-Loss: 0.3015670785550215 Train-f1: 0.19183086437508987 Test-f1: 0.14569319114027893 Train-acc: 0.5 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2021/20000] Train-Loss: 1.1995683119931415 Test-Loss: 0.30151003220847566 Train-f1: 0.18500921360625527 Test-f1: 0.14569319114027893 Train-acc: 0.49337748344370863 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2022/20000] Train-Loss: 1.1991173033117222 Test-Loss: 0.30137593054000944 Train-f1: 0.17708502024291498 Test-f1: 0.16276923076923078 Train-acc: 0.4867549668874172 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2023/20000] Train-Loss: 1.1994130414574273 Test-Loss: 0.3013104980280682 Train-f1: 0.18812847138372374 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2024/20000] Train-Loss: 1.1989340523298428 Test-Loss: 0.3012588431613796 Train-f1: 0.19183086437508987 Test-f1: 0.16285714285714284 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2025/20000] Train-Loss: 1.198668588232987 Test-Loss: 0.30119386530122355 Train-f1: 0.1954692556634304 Test-f1: 0.16285714285714284 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2026/20000] Train-Loss: 1.1980297331866245 Test-Loss: 0.301182123799221 Train-f1: 0.1954692556634304 Test-f1: 0.16285714285714284 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2027/20000] Train-Loss: 1.197686007566012 Test-Loss: 0.30111865331374005 Train-f1: 0.18836009732360098 Test-f1: 0.14569319114027893 Train-acc: 0.4966887417218543 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2028/20000] Train-Loss: 1.1974686921586881 Test-Loss: 0.30099069563435066 Train-f1: 0.18812847138372374 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "Epoch [2029/20000] Train-Loss: 1.197406353888357 Test-Loss: 0.3010132974303198 Train-f1: 0.1879092957476389 Test-f1: 0.14569319114027893 Train-acc: 0.4966887417218543 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2030/20000] Train-Loss: 1.1970375159674165 Test-Loss: 0.30085661945906145 Train-f1: 0.19183086437508987 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2031/20000] Train-Loss: 1.1968560438280622 Test-Loss: 0.3008504794843558 Train-f1: 0.19183086437508987 Test-f1: 0.16285714285714284 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "Epoch [2032/20000] Train-Loss: 1.196344276452511 Test-Loss: 0.3008977093772897 Train-f1: 0.1951993951993952 Test-f1: 0.14569319114027893 Train-acc: 0.5033112582781457 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2033/20000] Train-Loss: 1.1955962246189031 Test-Loss: 0.300707860658013 Train-f1: 0.18456408747670885 Test-f1: 0.16285714285714284 Train-acc: 0.49337748344370863 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2034/20000] Train-Loss: 1.195236248198243 Test-Loss: 0.3006357941066784 Train-f1: 0.18812847138372374 Test-f1: 0.16285714285714284 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2035/20000] Train-Loss: 1.1953225762089745 Test-Loss: 0.30054622358691885 Train-f1: 0.19183086437508987 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2036/20000] Train-Loss: 1.1950526715593357 Test-Loss: 0.3005195784166479 Train-f1: 0.19183086437508987 Test-f1: 0.16285714285714284 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2037/20000] Train-Loss: 1.1942618408389516 Test-Loss: 0.30041612159077785 Train-f1: 0.18812847138372374 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2038/20000] Train-Loss: 1.1940981294179547 Test-Loss: 0.30036121176900865 Train-f1: 0.1954692556634304 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2039/20000] Train-Loss: 1.194054613406148 Test-Loss: 0.3002801305142477 Train-f1: 0.18836009732360098 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2040/20000] Train-Loss: 1.1942389008943077 Test-Loss: 0.3001908296433551 Train-f1: 0.19183086437508987 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2041/20000] Train-Loss: 1.1939867510843019 Test-Loss: 0.30012700255473534 Train-f1: 0.19183086437508987 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2042/20000] Train-Loss: 1.1935186874166601 Test-Loss: 0.30007337425798775 Train-f1: 0.19183086437508987 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2043/20000] Train-Loss: 1.1933986208776617 Test-Loss: 0.2999933917504146 Train-f1: 0.1879092957476389 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2044/20000] Train-Loss: 1.1922683351541086 Test-Loss: 0.29999149422124766 Train-f1: 0.1951993951993952 Test-f1: 0.16285714285714284 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2045/20000] Train-Loss: 1.1925607282488513 Test-Loss: 0.29987069581406467 Train-f1: 0.18836009732360098 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2046/20000] Train-Loss: 1.1921101006768975 Test-Loss: 0.29978019877333417 Train-f1: 0.1954692556634304 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "Epoch [2047/20000] Train-Loss: 1.1913947295808314 Test-Loss: 0.2998489776988376 Train-f1: 0.20475175697120834 Test-f1: 0.16285714285714284 Train-acc: 0.5132450331125827 Test-acc: 0.47368421052631576\n",
      "Epoch [2048/20000] Train-Loss: 1.1916104437913035 Test-Loss: 0.2998513229100574 Train-f1: 0.1951993951993952 Test-f1: 0.14569319114027893 Train-acc: 0.5033112582781457 Test-acc: 0.4605263157894737\n",
      "best score_dx!!\n",
      "Epoch [2049/20000] Train-Loss: 1.1909760564283078 Test-Loss: 0.29973081809679414 Train-f1: 0.1951993951993952 Test-f1: 0.16285714285714284 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2050/20000] Train-Loss: 1.1905657005981258 Test-Loss: 0.2995460656304816 Train-f1: 0.18812847138372374 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2051/20000] Train-Loss: 1.1906388249442041 Test-Loss: 0.2994354912743651 Train-f1: 0.19183086437508987 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "Epoch [2052/20000] Train-Loss: 1.1897714119765888 Test-Loss: 0.2994801769599606 Train-f1: 0.194698346144887 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2053/20000] Train-Loss: 1.189700737001737 Test-Loss: 0.2993717177052311 Train-f1: 0.18812847138372374 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "Epoch [2054/20000] Train-Loss: 1.1891412235908099 Test-Loss: 0.2993834848724149 Train-f1: 0.198752052545156 Test-f1: 0.16285714285714284 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2055/20000] Train-Loss: 1.1894319592612985 Test-Loss: 0.2992889993378385 Train-f1: 0.18812847138372374 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2056/20000] Train-Loss: 1.1888246839454426 Test-Loss: 0.29917261545197843 Train-f1: 0.1951993951993952 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2057/20000] Train-Loss: 1.188241254748918 Test-Loss: 0.29911187790738897 Train-f1: 0.198752052545156 Test-f1: 0.16276923076923078 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2058/20000] Train-Loss: 1.1880453655022962 Test-Loss: 0.29903433856099865 Train-f1: 0.19904554904554903 Test-f1: 0.16276923076923078 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2059/20000] Train-Loss: 1.1878242362436016 Test-Loss: 0.29891452617373465 Train-f1: 0.18812847138372374 Test-f1: 0.16276923076923078 Train-acc: 0.4966887417218543 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2060/20000] Train-Loss: 1.1883920197698248 Test-Loss: 0.29888957220325657 Train-f1: 0.1984719310505474 Test-f1: 0.16276923076923078 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2061/20000] Train-Loss: 1.1874088282446533 Test-Loss: 0.2987119514152893 Train-f1: 0.1951993951993952 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2062/20000] Train-Loss: 1.1867973798826128 Test-Loss: 0.2986761487928075 Train-f1: 0.20194318497270286 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "Epoch [2063/20000] Train-Loss: 1.186481011996619 Test-Loss: 0.2987091381350184 Train-f1: 0.19158576051779935 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2064/20000] Train-Loss: 1.1863060359094637 Test-Loss: 0.2985215569911804 Train-f1: 0.19494252873563217 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2065/20000] Train-Loss: 1.185558116476155 Test-Loss: 0.2985184464156473 Train-f1: 0.1951993951993952 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2066/20000] Train-Loss: 1.1852738320459155 Test-Loss: 0.2983763124892536 Train-f1: 0.1951993951993952 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2067/20000] Train-Loss: 1.1849858763878902 Test-Loss: 0.2983684831691171 Train-f1: 0.19820486721569353 Test-f1: 0.16276923076923078 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2068/20000] Train-Loss: 1.1854010022749863 Test-Loss: 0.298283833396866 Train-f1: 0.19183086437508987 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2069/20000] Train-Loss: 1.184446683891739 Test-Loss: 0.2981343908387328 Train-f1: 0.1951993951993952 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2070/20000] Train-Loss: 1.1841963184666704 Test-Loss: 0.298052167232129 Train-f1: 0.19494252873563217 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2071/20000] Train-Loss: 1.1834342502996102 Test-Loss: 0.2979972965830054 Train-f1: 0.1951993951993952 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2072/20000] Train-Loss: 1.1834082207087975 Test-Loss: 0.2979108079520942 Train-f1: 0.20194318497270286 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2073/20000] Train-Loss: 1.1830440098378243 Test-Loss: 0.2978290889197986 Train-f1: 0.1951993951993952 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2074/20000] Train-Loss: 1.1834864799959641 Test-Loss: 0.2977901661257581 Train-f1: 0.20165425971877582 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2075/20000] Train-Loss: 1.1818526511356287 Test-Loss: 0.29768523838944655 Train-f1: 0.19183086437508987 Test-f1: 0.16276923076923078 Train-acc: 0.5 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2076/20000] Train-Loss: 1.1828301526974994 Test-Loss: 0.29760995164801146 Train-f1: 0.20535796342247953 Test-f1: 0.16276923076923078 Train-acc: 0.5132450331125827 Test-acc: 0.47368421052631576\n",
      "Epoch [2077/20000] Train-Loss: 1.1816379483193367 Test-Loss: 0.2977439871525976 Train-f1: 0.2113408521303258 Test-f1: 0.16276923076923078 Train-acc: 0.5198675496688742 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2078/20000] Train-Loss: 1.1813283558376553 Test-Loss: 0.29748820004544146 Train-f1: 0.1954692556634304 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2079/20000] Train-Loss: 1.180891365762049 Test-Loss: 0.2973811183248078 Train-f1: 0.1951993951993952 Test-f1: 0.17819268110530245 Train-acc: 0.5033112582781457 Test-acc: 0.4868421052631579\n",
      "Epoch [2080/20000] Train-Loss: 1.1804807674118158 Test-Loss: 0.29748839486161177 Train-f1: 0.2145595232801174 Test-f1: 0.16276923076923078 Train-acc: 0.5231788079470199 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2081/20000] Train-Loss: 1.1805699004018881 Test-Loss: 0.2973525425223118 Train-f1: 0.1984719310505474 Test-f1: 0.16276923076923078 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "Epoch [2082/20000] Train-Loss: 1.1798646686144503 Test-Loss: 0.29742682818978866 Train-f1: 0.20224551595620777 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2083/20000] Train-Loss: 1.1799488551731216 Test-Loss: 0.2972031511681697 Train-f1: 0.1951993951993952 Test-f1: 0.16276923076923078 Train-acc: 0.5033112582781457 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2084/20000] Train-Loss: 1.179185943026258 Test-Loss: 0.2970501476997816 Train-f1: 0.20165425971877582 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2085/20000] Train-Loss: 1.1791844998724819 Test-Loss: 0.2970187720487888 Train-f1: 0.20535796342247953 Test-f1: 0.16276923076923078 Train-acc: 0.5132450331125827 Test-acc: 0.47368421052631576\n",
      "Epoch [2086/20000] Train-Loss: 1.1784877837476813 Test-Loss: 0.2970603866940293 Train-f1: 0.20194318497270286 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2087/20000] Train-Loss: 1.1790960726603554 Test-Loss: 0.29701481030553856 Train-f1: 0.1984719310505474 Test-f1: 0.16276923076923078 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2088/20000] Train-Loss: 1.177590810152627 Test-Loss: 0.2968164106856507 Train-f1: 0.20194318497270286 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2089/20000] Train-Loss: 1.1779410159355648 Test-Loss: 0.29672466291810656 Train-f1: 0.2116756756756757 Test-f1: 0.16276923076923078 Train-acc: 0.5198675496688742 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2090/20000] Train-Loss: 1.1773273774877016 Test-Loss: 0.29658035067948446 Train-f1: 0.1984719310505474 Test-f1: 0.16276923076923078 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2091/20000] Train-Loss: 1.1776214050979186 Test-Loss: 0.2965160854553395 Train-f1: 0.20568150272971222 Test-f1: 0.16276923076923078 Train-acc: 0.5132450331125827 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2092/20000] Train-Loss: 1.1761515380660614 Test-Loss: 0.29651156544827995 Train-f1: 0.20504815372677898 Test-f1: 0.16276923076923078 Train-acc: 0.5132450331125827 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2093/20000] Train-Loss: 1.1759197759913516 Test-Loss: 0.2963613072737014 Train-f1: 0.20165425971877582 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2094/20000] Train-Loss: 1.1759067975069186 Test-Loss: 0.29627552839546156 Train-f1: 0.20224551595620777 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2095/20000] Train-Loss: 1.1754300796768002 Test-Loss: 0.2961479669332373 Train-f1: 0.1984719310505474 Test-f1: 0.17819268110530245 Train-acc: 0.5066225165562914 Test-acc: 0.4868421052631579\n",
      "Epoch [2096/20000] Train-Loss: 1.1754267763686015 Test-Loss: 0.29615088976560155 Train-f1: 0.20504815372677898 Test-f1: 0.16276923076923078 Train-acc: 0.5132450331125827 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2097/20000] Train-Loss: 1.1749447637479264 Test-Loss: 0.2960564896474046 Train-f1: 0.20165425971877582 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "Epoch [2098/20000] Train-Loss: 1.174652152649262 Test-Loss: 0.2961699761642867 Train-f1: 0.2116756756756757 Test-f1: 0.16276923076923078 Train-acc: 0.5198675496688742 Test-acc: 0.47368421052631576\n",
      "Epoch [2099/20000] Train-Loss: 1.1744055659030377 Test-Loss: 0.2960681204895683 Train-f1: 0.19795055601507214 Test-f1: 0.16276923076923078 Train-acc: 0.5066225165562914 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2100/20000] Train-Loss: 1.173265223498236 Test-Loss: 0.2958369713213491 Train-f1: 0.20535796342247953 Test-f1: 0.16276923076923078 Train-acc: 0.5132450331125827 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2101/20000] Train-Loss: 1.1739187128643809 Test-Loss: 0.29578755500547427 Train-f1: 0.20194318497270286 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2102/20000] Train-Loss: 1.1731981471513373 Test-Loss: 0.2957481542802148 Train-f1: 0.20165425971877582 Test-f1: 0.16276923076923078 Train-acc: 0.5099337748344371 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2103/20000] Train-Loss: 1.1718659909139404 Test-Loss: 0.29550827450459694 Train-f1: 0.20137842895613675 Test-f1: 0.17819268110530245 Train-acc: 0.5099337748344371 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2104/20000] Train-Loss: 1.1722800479755617 Test-Loss: 0.2954297648769528 Train-f1: 0.2113408521303258 Test-f1: 0.17819268110530245 Train-acc: 0.5198675496688742 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2105/20000] Train-Loss: 1.1721105762485466 Test-Loss: 0.2953410382489916 Train-f1: 0.20194318497270286 Test-f1: 0.17819268110530245 Train-acc: 0.5099337748344371 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2106/20000] Train-Loss: 1.1714843940362978 Test-Loss: 0.295269054731843 Train-f1: 0.20475175697120834 Test-f1: 0.17819268110530245 Train-acc: 0.5132450331125827 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2107/20000] Train-Loss: 1.1710820534693973 Test-Loss: 0.2951839504288321 Train-f1: 0.20165425971877582 Test-f1: 0.17819268110530245 Train-acc: 0.5099337748344371 Test-acc: 0.4868421052631579\n",
      "Epoch [2108/20000] Train-Loss: 1.1701657370059488 Test-Loss: 0.2952680369596542 Train-f1: 0.21977832406089623 Test-f1: 0.16276923076923078 Train-acc: 0.5298013245033113 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2109/20000] Train-Loss: 1.1700388173168106 Test-Loss: 0.2951054160920176 Train-f1: 0.1984719310505474 Test-f1: 0.17819268110530245 Train-acc: 0.5066225165562914 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2110/20000] Train-Loss: 1.1695469967883694 Test-Loss: 0.2949341511048089 Train-f1: 0.20504815372677898 Test-f1: 0.17819268110530245 Train-acc: 0.5132450331125827 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2111/20000] Train-Loss: 1.1692133752617584 Test-Loss: 0.2948899903142195 Train-f1: 0.20776942355889724 Test-f1: 0.17819268110530245 Train-acc: 0.5165562913907285 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2112/20000] Train-Loss: 1.1686342525764803 Test-Loss: 0.29477637318482164 Train-f1: 0.20535796342247953 Test-f1: 0.17819268110530245 Train-acc: 0.5132450331125827 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2113/20000] Train-Loss: 1.168761905021907 Test-Loss: 0.2946859079801388 Train-f1: 0.2113408521303258 Test-f1: 0.17819268110530245 Train-acc: 0.5198675496688742 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2114/20000] Train-Loss: 1.1683474580257098 Test-Loss: 0.2946651075372942 Train-f1: 0.20165425971877582 Test-f1: 0.17819268110530245 Train-acc: 0.5099337748344371 Test-acc: 0.4868421052631579\n",
      "Epoch [2115/20000] Train-Loss: 1.1676172196922256 Test-Loss: 0.29483294811226873 Train-f1: 0.22617046818727488 Test-f1: 0.16276923076923078 Train-acc: 0.5364238410596026 Test-acc: 0.47368421052631576\n",
      "best score_dx!!\n",
      "Epoch [2116/20000] Train-Loss: 1.167625234721751 Test-Loss: 0.29455351393640883 Train-f1: 0.1984719310505474 Test-f1: 0.17819268110530245 Train-acc: 0.5066225165562914 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2117/20000] Train-Loss: 1.1669313333893971 Test-Loss: 0.2944041626666646 Train-f1: 0.20504815372677898 Test-f1: 0.17819268110530245 Train-acc: 0.5132450331125827 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2118/20000] Train-Loss: 1.1669712882689895 Test-Loss: 0.29435791562680785 Train-f1: 0.2113408521303258 Test-f1: 0.17819268110530245 Train-acc: 0.5198675496688742 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2119/20000] Train-Loss: 1.1663703429472647 Test-Loss: 0.29428453101118607 Train-f1: 0.2074798772624183 Test-f1: 0.17819268110530245 Train-acc: 0.5165562913907285 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2120/20000] Train-Loss: 1.1663043574192682 Test-Loss: 0.29425076953312285 Train-f1: 0.20165425971877582 Test-f1: 0.17819268110530245 Train-acc: 0.5099337748344371 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2121/20000] Train-Loss: 1.1660938475599885 Test-Loss: 0.2940099094590156 Train-f1: 0.208388120607572 Test-f1: 0.1925925925925926 Train-acc: 0.5165562913907285 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2122/20000] Train-Loss: 1.1658258673809754 Test-Loss: 0.29395589709130626 Train-f1: 0.20807207207207207 Test-f1: 0.17819268110530245 Train-acc: 0.5165562913907285 Test-acc: 0.4868421052631579\n",
      "Epoch [2123/20000] Train-Loss: 1.1657627257426364 Test-Loss: 0.2939561687472604 Train-f1: 0.21703186381492795 Test-f1: 0.17819268110530245 Train-acc: 0.5264900662251656 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2124/20000] Train-Loss: 1.164750235205055 Test-Loss: 0.293807362821356 Train-f1: 0.19795055601507214 Test-f1: 0.17819268110530245 Train-acc: 0.5066225165562914 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2125/20000] Train-Loss: 1.1640792251783032 Test-Loss: 0.2936710712612353 Train-f1: 0.19795055601507214 Test-f1: 0.1925925925925926 Train-acc: 0.5066225165562914 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2126/20000] Train-Loss: 1.1635808357809092 Test-Loss: 0.29366684972966617 Train-f1: 0.21944777911164465 Test-f1: 0.17819268110530245 Train-acc: 0.5298013245033113 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2127/20000] Train-Loss: 1.1628913958435558 Test-Loss: 0.293594890737044 Train-f1: 0.20504815372677898 Test-f1: 0.17819268110530245 Train-acc: 0.5132450331125827 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2128/20000] Train-Loss: 1.1625071764301043 Test-Loss: 0.29345887356023276 Train-f1: 0.20504815372677898 Test-f1: 0.17819268110530245 Train-acc: 0.5132450331125827 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2129/20000] Train-Loss: 1.162338966091806 Test-Loss: 0.2933234777406564 Train-f1: 0.2039400542535687 Test-f1: 0.1925925925925926 Train-acc: 0.5132450331125827 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2130/20000] Train-Loss: 1.1624965149920587 Test-Loss: 0.2932553759043295 Train-f1: 0.21101970027126785 Test-f1: 0.1925925925925926 Train-acc: 0.5198675496688742 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2131/20000] Train-Loss: 1.1618332326270975 Test-Loss: 0.2932079643700873 Train-f1: 0.22213180758635306 Test-f1: 0.17819268110530245 Train-acc: 0.5331125827814569 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2132/20000] Train-Loss: 1.1617543311539102 Test-Loss: 0.2931077627036418 Train-f1: 0.20194318497270286 Test-f1: 0.1925925925925926 Train-acc: 0.5099337748344371 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2133/20000] Train-Loss: 1.1607790545380128 Test-Loss: 0.2929973927665174 Train-f1: 0.20807207207207207 Test-f1: 0.1925925925925926 Train-acc: 0.5165562913907285 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2134/20000] Train-Loss: 1.1613752649765303 Test-Loss: 0.2928962857353527 Train-f1: 0.21670354462232638 Test-f1: 0.1925925925925926 Train-acc: 0.5264900662251656 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2135/20000] Train-Loss: 1.159752650739916 Test-Loss: 0.2928017350823698 Train-f1: 0.20165425971877582 Test-f1: 0.1925925925925926 Train-acc: 0.5099337748344371 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2136/20000] Train-Loss: 1.159647818638057 Test-Loss: 0.2927729259167585 Train-f1: 0.21703186381492795 Test-f1: 0.1925925925925926 Train-acc: 0.5264900662251656 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2137/20000] Train-Loss: 1.159547464106877 Test-Loss: 0.2926758412191838 Train-f1: 0.21041721563460697 Test-f1: 0.1925925925925926 Train-acc: 0.5198675496688742 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2138/20000] Train-Loss: 1.1592599189588169 Test-Loss: 0.2926099672527095 Train-f1: 0.2145595232801174 Test-f1: 0.17819268110530245 Train-acc: 0.5231788079470199 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2139/20000] Train-Loss: 1.1588093150614867 Test-Loss: 0.29248902567440743 Train-f1: 0.21977832406089623 Test-f1: 0.1925925925925926 Train-acc: 0.5298013245033113 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2140/20000] Train-Loss: 1.1581018009943609 Test-Loss: 0.29241144223895343 Train-f1: 0.21638849355242162 Test-f1: 0.1925925925925926 Train-acc: 0.5264900662251656 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2141/20000] Train-Loss: 1.1578358746016133 Test-Loss: 0.2923735795570721 Train-f1: 0.22316815456937075 Test-f1: 0.17819268110530245 Train-acc: 0.5331125827814569 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2142/20000] Train-Loss: 1.1572544855066653 Test-Loss: 0.29217640340644474 Train-f1: 0.21703186381492795 Test-f1: 0.1925925925925926 Train-acc: 0.5264900662251656 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2143/20000] Train-Loss: 1.1569318625664429 Test-Loss: 0.2921669776253281 Train-f1: 0.21670354462232638 Test-f1: 0.1925925925925926 Train-acc: 0.5264900662251656 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2144/20000] Train-Loss: 1.1561595796515312 Test-Loss: 0.2921002147694515 Train-f1: 0.21944777911164465 Test-f1: 0.1925925925925926 Train-acc: 0.5298013245033113 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2145/20000] Train-Loss: 1.1564366782798297 Test-Loss: 0.2920585831921658 Train-f1: 0.21328474120352295 Test-f1: 0.17819268110530245 Train-acc: 0.5231788079470199 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2146/20000] Train-Loss: 1.155744385429371 Test-Loss: 0.291838627717719 Train-f1: 0.21389547650417215 Test-f1: 0.1925925925925926 Train-acc: 0.5231788079470199 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2147/20000] Train-Loss: 1.1559814576555865 Test-Loss: 0.29179016575384864 Train-f1: 0.21703186381492795 Test-f1: 0.1925925925925926 Train-acc: 0.5264900662251656 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2148/20000] Train-Loss: 1.1542816552633581 Test-Loss: 0.29163103992622025 Train-f1: 0.20419799498746866 Test-f1: 0.1925925925925926 Train-acc: 0.5132450331125827 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2149/20000] Train-Loss: 1.1544954868278066 Test-Loss: 0.2915982630201912 Train-f1: 0.22246376811594204 Test-f1: 0.1925925925925926 Train-acc: 0.5331125827814569 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2150/20000] Train-Loss: 1.1540356519367694 Test-Loss: 0.29157959404786726 Train-f1: 0.22874337783428694 Test-f1: 0.1925925925925926 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2151/20000] Train-Loss: 1.1531306234965149 Test-Loss: 0.29147739931660216 Train-f1: 0.22579710144927537 Test-f1: 0.1925925925925926 Train-acc: 0.5364238410596026 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2152/20000] Train-Loss: 1.1529477153380523 Test-Loss: 0.2912672210874344 Train-f1: 0.21389547650417215 Test-f1: 0.1925925925925926 Train-acc: 0.5231788079470199 Test-acc: 0.5\n",
      "Epoch [2153/20000] Train-Loss: 1.1525313569797986 Test-Loss: 0.29137115348724296 Train-f1: 0.23164903704328038 Test-f1: 0.17819268110530245 Train-acc: 0.543046357615894 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2154/20000] Train-Loss: 1.1521508986920959 Test-Loss: 0.2911715345589428 Train-f1: 0.21422069026470458 Test-f1: 0.1925925925925926 Train-acc: 0.5231788079470199 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2155/20000] Train-Loss: 1.1518526061286603 Test-Loss: 0.2911199321564556 Train-f1: 0.22280912364945976 Test-f1: 0.1925925925925926 Train-acc: 0.5331125827814569 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2156/20000] Train-Loss: 1.1512948057752483 Test-Loss: 0.2909677484587665 Train-f1: 0.21389547650417215 Test-f1: 0.1925925925925926 Train-acc: 0.5231788079470199 Test-acc: 0.5\n",
      "Epoch [2157/20000] Train-Loss: 1.1507696443762105 Test-Loss: 0.291015493136383 Train-f1: 0.23089105609735766 Test-f1: 0.17819268110530245 Train-acc: 0.543046357615894 Test-acc: 0.4868421052631579\n",
      "Epoch [2158/20000] Train-Loss: 1.1507700626516684 Test-Loss: 0.2909920891926733 Train-f1: 0.21041721563460697 Test-f1: 0.17819268110530245 Train-acc: 0.5198675496688742 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2159/20000] Train-Loss: 1.1507808432751208 Test-Loss: 0.2906083226787329 Train-f1: 0.21703186381492795 Test-f1: 0.1925925925925926 Train-acc: 0.5264900662251656 Test-acc: 0.5\n",
      "Epoch [2160/20000] Train-Loss: 1.1495368294083845 Test-Loss: 0.2907295411687706 Train-f1: 0.23451512865644122 Test-f1: 0.1925925925925926 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2161/20000] Train-Loss: 1.1493905252644374 Test-Loss: 0.29047706227023784 Train-f1: 0.21944777911164465 Test-f1: 0.1925925925925926 Train-acc: 0.5298013245033113 Test-acc: 0.5\n",
      "Epoch [2162/20000] Train-Loss: 1.1484923909874623 Test-Loss: 0.2905988265713189 Train-f1: 0.22280912364945976 Test-f1: 0.17819268110530245 Train-acc: 0.5331125827814569 Test-acc: 0.4868421052631579\n",
      "best score_dx!!\n",
      "Epoch [2163/20000] Train-Loss: 1.1486994845585299 Test-Loss: 0.2902582970675152 Train-f1: 0.21389547650417215 Test-f1: 0.1925925925925926 Train-acc: 0.5231788079470199 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2164/20000] Train-Loss: 1.1477986040076111 Test-Loss: 0.2901738209869986 Train-f1: 0.22801106361579082 Test-f1: 0.1925925925925926 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2165/20000] Train-Loss: 1.1471632873409225 Test-Loss: 0.2900461219583383 Train-f1: 0.22280912364945976 Test-f1: 0.1925925925925926 Train-acc: 0.5331125827814569 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2166/20000] Train-Loss: 1.1477508112846693 Test-Loss: 0.2900254567378464 Train-f1: 0.23164903704328038 Test-f1: 0.1925925925925926 Train-acc: 0.543046357615894 Test-acc: 0.5\n",
      "Epoch [2167/20000] Train-Loss: 1.1474651645971634 Test-Loss: 0.29016161419904635 Train-f1: 0.23336219336219335 Test-f1: 0.1925925925925926 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2168/20000] Train-Loss: 1.1461323484689705 Test-Loss: 0.2898761548123805 Train-f1: 0.213583587952859 Test-f1: 0.1925925925925926 Train-acc: 0.5231788079470199 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2169/20000] Train-Loss: 1.1460568396203052 Test-Loss: 0.2897772517956342 Train-f1: 0.22012234804112976 Test-f1: 0.1925925925925926 Train-acc: 0.5298013245033113 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2170/20000] Train-Loss: 1.1452990774389746 Test-Loss: 0.28956530597263236 Train-f1: 0.22579710144927537 Test-f1: 0.1925925925925926 Train-acc: 0.5364238410596026 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2171/20000] Train-Loss: 1.1447548553439737 Test-Loss: 0.2895643506052995 Train-f1: 0.22837034851869023 Test-f1: 0.1925925925925926 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2172/20000] Train-Loss: 1.1446575386547593 Test-Loss: 0.2893823927226979 Train-f1: 0.22874337783428694 Test-f1: 0.1925925925925926 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2173/20000] Train-Loss: 1.1438600111075916 Test-Loss: 0.2892728693785427 Train-f1: 0.21977832406089623 Test-f1: 0.19179632248939177 Train-acc: 0.5298013245033113 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2174/20000] Train-Loss: 1.1443572150222585 Test-Loss: 0.2891770336027834 Train-f1: 0.22801106361579082 Test-f1: 0.19179632248939177 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2175/20000] Train-Loss: 1.1432451964968657 Test-Loss: 0.2891181160330082 Train-f1: 0.22837034851869023 Test-f1: 0.1925925925925926 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2176/20000] Train-Loss: 1.1417730539356834 Test-Loss: 0.2889874994710813 Train-f1: 0.22543759271032 Test-f1: 0.19179632248939177 Train-acc: 0.5364238410596026 Test-acc: 0.5\n",
      "Epoch [2177/20000] Train-Loss: 1.1421047787217227 Test-Loss: 0.2890157673342576 Train-f1: 0.231263096136116 Test-f1: 0.1925925925925926 Train-acc: 0.543046357615894 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2178/20000] Train-Loss: 1.1411411116074903 Test-Loss: 0.28878367335196486 Train-f1: 0.22579710144927537 Test-f1: 0.19179632248939177 Train-acc: 0.5364238410596026 Test-acc: 0.5\n",
      "Epoch [2179/20000] Train-Loss: 1.1410677676661225 Test-Loss: 0.2889486495557696 Train-f1: 0.23891155352480417 Test-f1: 0.1925925925925926 Train-acc: 0.5529801324503312 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2180/20000] Train-Loss: 1.1406928947830361 Test-Loss: 0.2886186330665344 Train-f1: 0.22246376811594204 Test-f1: 0.1925925925925926 Train-acc: 0.5331125827814569 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2181/20000] Train-Loss: 1.1400479368470045 Test-Loss: 0.2885148573234376 Train-f1: 0.22874337783428694 Test-f1: 0.19179632248939177 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2182/20000] Train-Loss: 1.1392467237541943 Test-Loss: 0.2884035401818786 Train-f1: 0.22213180758635306 Test-f1: 0.2049655172413793 Train-acc: 0.5331125827814569 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2183/20000] Train-Loss: 1.1405024928590244 Test-Loss: 0.28831650279097965 Train-f1: 0.22801106361579082 Test-f1: 0.19179632248939177 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2184/20000] Train-Loss: 1.1392059776881152 Test-Loss: 0.288186940146618 Train-f1: 0.23653679653679652 Test-f1: 0.19179632248939177 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2185/20000] Train-Loss: 1.1381906024786794 Test-Loss: 0.28818655885431527 Train-f1: 0.22837034851869023 Test-f1: 0.1925925925925926 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2186/20000] Train-Loss: 1.1384595194302067 Test-Loss: 0.2880168761914327 Train-f1: 0.23411686254897054 Test-f1: 0.19179632248939177 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2187/20000] Train-Loss: 1.1385985088044015 Test-Loss: 0.28792426721775827 Train-f1: 0.231263096136116 Test-f1: 0.19179632248939177 Train-acc: 0.543046357615894 Test-acc: 0.5\n",
      "Epoch [2188/20000] Train-Loss: 1.137138618716496 Test-Loss: 0.2879464984016814 Train-f1: 0.22543759271032 Test-f1: 0.1925925925925926 Train-acc: 0.5364238410596026 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2189/20000] Train-Loss: 1.1368456797919506 Test-Loss: 0.287755271873271 Train-f1: 0.23336219336219335 Test-f1: 0.1925925925925926 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "Epoch [2190/20000] Train-Loss: 1.1359324379024707 Test-Loss: 0.2877779979912728 Train-f1: 0.23233085758350586 Test-f1: 0.1925925925925926 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "Epoch [2191/20000] Train-Loss: 1.1369064619841405 Test-Loss: 0.28779899316216717 Train-f1: 0.22837034851869023 Test-f1: 0.1925925925925926 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2192/20000] Train-Loss: 1.1346006570855027 Test-Loss: 0.28740648316035505 Train-f1: 0.21670354462232638 Test-f1: 0.19179632248939177 Train-acc: 0.5264900662251656 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2193/20000] Train-Loss: 1.1350790564995736 Test-Loss: 0.28729680468909835 Train-f1: 0.23653679653679652 Test-f1: 0.19179632248939177 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2194/20000] Train-Loss: 1.1350220634786006 Test-Loss: 0.28718076330928655 Train-f1: 0.231263096136116 Test-f1: 0.19179632248939177 Train-acc: 0.543046357615894 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2195/20000] Train-Loss: 1.1334267611471134 Test-Loss: 0.2871708787089484 Train-f1: 0.23373264248704664 Test-f1: 0.19179632248939177 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2196/20000] Train-Loss: 1.1333653079722974 Test-Loss: 0.2870663635476759 Train-f1: 0.23411686254897054 Test-f1: 0.19179632248939177 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2197/20000] Train-Loss: 1.1337375148749809 Test-Loss: 0.2869234511363112 Train-f1: 0.23336219336219335 Test-f1: 0.19179632248939177 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "Epoch [2198/20000] Train-Loss: 1.1320492074850992 Test-Loss: 0.28708099533260345 Train-f1: 0.24392125351546806 Test-f1: 0.1925925925925926 Train-acc: 0.5596026490066225 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2199/20000] Train-Loss: 1.1321192661405095 Test-Loss: 0.2869140850348104 Train-f1: 0.23089105609735766 Test-f1: 0.1925925925925926 Train-acc: 0.543046357615894 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2200/20000] Train-Loss: 1.131177359856171 Test-Loss: 0.28669214647250496 Train-f1: 0.23653679653679652 Test-f1: 0.19179632248939177 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2201/20000] Train-Loss: 1.1308995163883995 Test-Loss: 0.28652368913711496 Train-f1: 0.23411686254897054 Test-f1: 0.19179632248939177 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "Epoch [2202/20000] Train-Loss: 1.1308337893925058 Test-Loss: 0.28657741651884033 Train-f1: 0.23891155352480417 Test-f1: 0.1925925925925926 Train-acc: 0.5529801324503312 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2203/20000] Train-Loss: 1.1297771051898124 Test-Loss: 0.2863061515477729 Train-f1: 0.231263096136116 Test-f1: 0.19179632248939177 Train-acc: 0.543046357615894 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2204/20000] Train-Loss: 1.1293269304152214 Test-Loss: 0.2861689110089248 Train-f1: 0.23336219336219335 Test-f1: 0.19179632248939177 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2205/20000] Train-Loss: 1.1290190399331617 Test-Loss: 0.28608518610291966 Train-f1: 0.23853240797110273 Test-f1: 0.19179632248939177 Train-acc: 0.5529801324503312 Test-acc: 0.5\n",
      "Epoch [2206/20000] Train-Loss: 1.1285229869268212 Test-Loss: 0.28619837180265195 Train-f1: 0.23615485564304461 Test-f1: 0.1925925925925926 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2207/20000] Train-Loss: 1.1289963437000172 Test-Loss: 0.285902867611163 Train-f1: 0.23615485564304461 Test-f1: 0.19179632248939177 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "Epoch [2208/20000] Train-Loss: 1.1278430849845742 Test-Loss: 0.28595018389539945 Train-f1: 0.2412436906925096 Test-f1: 0.1925925925925926 Train-acc: 0.5562913907284768 Test-acc: 0.5\n",
      "Epoch [2209/20000] Train-Loss: 1.1268122876435738 Test-Loss: 0.28590903833874076 Train-f1: 0.23653679653679652 Test-f1: 0.1925925925925926 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2210/20000] Train-Loss: 1.1272245545214592 Test-Loss: 0.28555883403063725 Train-f1: 0.22837034851869023 Test-f1: 0.19179632248939177 Train-acc: 0.5397350993377483 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2211/20000] Train-Loss: 1.1263182853082268 Test-Loss: 0.28542686720785426 Train-f1: 0.23336219336219335 Test-f1: 0.2049655172413793 Train-acc: 0.5463576158940397 Test-acc: 0.5131578947368421\n",
      "Epoch [2212/20000] Train-Loss: 1.12591686369571 Test-Loss: 0.2854955043778556 Train-f1: 0.2405053170224674 Test-f1: 0.19179632248939177 Train-acc: 0.5562913907284768 Test-acc: 0.5\n",
      "Epoch [2213/20000] Train-Loss: 1.1249633904108738 Test-Loss: 0.285461214116798 Train-f1: 0.23734266900058348 Test-f1: 0.1925925925925926 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2214/20000] Train-Loss: 1.125094412857816 Test-Loss: 0.2851654771356618 Train-f1: 0.23734266900058348 Test-f1: 0.19179632248939177 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2215/20000] Train-Loss: 1.1243662130406613 Test-Loss: 0.2850840094174466 Train-f1: 0.23891155352480417 Test-f1: 0.19179632248939177 Train-acc: 0.5529801324503312 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2216/20000] Train-Loss: 1.12446400092057 Test-Loss: 0.28503801159752407 Train-f1: 0.231263096136116 Test-f1: 0.19179632248939177 Train-acc: 0.543046357615894 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2217/20000] Train-Loss: 1.1228082465661806 Test-Loss: 0.2848808175956187 Train-f1: 0.23578655352480418 Test-f1: 0.19179632248939177 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "Epoch [2218/20000] Train-Loss: 1.12254174601061 Test-Loss: 0.28511100028328323 Train-f1: 0.24506666666666668 Test-f1: 0.1925925925925926 Train-acc: 0.5629139072847682 Test-acc: 0.5\n",
      "Epoch [2219/20000] Train-Loss: 1.1219064646638062 Test-Loss: 0.28511075170235556 Train-f1: 0.2398194702878182 Test-f1: 0.1925925925925926 Train-acc: 0.5562913907284768 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2220/20000] Train-Loss: 1.1214275499538964 Test-Loss: 0.2846400767187048 Train-f1: 0.23451512865644122 Test-f1: 0.19179632248939177 Train-acc: 0.5463576158940397 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2221/20000] Train-Loss: 1.1208377855401506 Test-Loss: 0.28441842813555884 Train-f1: 0.23653679653679652 Test-f1: 0.19179632248939177 Train-acc: 0.5496688741721855 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2222/20000] Train-Loss: 1.1208819796155831 Test-Loss: 0.284324213625263 Train-f1: 0.2393044619422572 Test-f1: 0.19179632248939177 Train-acc: 0.5529801324503312 Test-acc: 0.5\n",
      "Epoch [2223/20000] Train-Loss: 1.119987196265149 Test-Loss: 0.284662877456674 Train-f1: 0.2405053170224674 Test-f1: 0.1925925925925926 Train-acc: 0.5562913907284768 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2224/20000] Train-Loss: 1.1194003427713797 Test-Loss: 0.284046949482298 Train-f1: 0.231263096136116 Test-f1: 0.2049655172413793 Train-acc: 0.543046357615894 Test-acc: 0.5131578947368421\n",
      "Epoch [2225/20000] Train-Loss: 1.1197269877875078 Test-Loss: 0.28407461209322166 Train-f1: 0.24764432647644327 Test-f1: 0.19179632248939177 Train-acc: 0.5662251655629139 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2226/20000] Train-Loss: 1.1185595560102697 Test-Loss: 0.28382164989569914 Train-f1: 0.23693264248704665 Test-f1: 0.2161948650427913 Train-acc: 0.5496688741721855 Test-acc: 0.5263157894736842\n",
      "Epoch [2227/20000] Train-Loss: 1.118013847383068 Test-Loss: 0.28383884147547767 Train-f1: 0.24506666666666668 Test-f1: 0.19179632248939177 Train-acc: 0.5629139072847682 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2228/20000] Train-Loss: 1.1179832899078794 Test-Loss: 0.2837396299443119 Train-f1: 0.2405053170224674 Test-f1: 0.19179632248939177 Train-acc: 0.5562913907284768 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2229/20000] Train-Loss: 1.117137059731495 Test-Loss: 0.2834955683254614 Train-f1: 0.23451512865644122 Test-f1: 0.20404040404040408 Train-acc: 0.5463576158940397 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2230/20000] Train-Loss: 1.1161076211163177 Test-Loss: 0.28338578632330425 Train-f1: 0.2378143832864604 Test-f1: 0.23863636363636362 Train-acc: 0.5529801324503312 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2231/20000] Train-Loss: 1.1161108496199137 Test-Loss: 0.2832974184452477 Train-f1: 0.23747501399216436 Test-f1: 0.2049655172413793 Train-acc: 0.5529801324503312 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2232/20000] Train-Loss: 1.1157419802695774 Test-Loss: 0.28316432843845085 Train-f1: 0.24578961954154951 Test-f1: 0.22770618556701033 Train-acc: 0.5629139072847682 Test-acc: 0.5394736842105263\n",
      "Epoch [2233/20000] Train-Loss: 1.1147366350079728 Test-Loss: 0.28320633418378566 Train-f1: 0.2398194702878182 Test-f1: 0.19179632248939177 Train-acc: 0.5562913907284768 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2234/20000] Train-Loss: 1.1138590575919405 Test-Loss: 0.28294316478921877 Train-f1: 0.23411686254897054 Test-f1: 0.22770618556701033 Train-acc: 0.5463576158940397 Test-acc: 0.5394736842105263\n",
      "best score_dx!!\n",
      "Epoch [2235/20000] Train-Loss: 1.1146973545353986 Test-Loss: 0.28284573307225175 Train-f1: 0.2447246184472462 Test-f1: 0.20404040404040408 Train-acc: 0.5629139072847682 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2236/20000] Train-Loss: 1.1134497368891936 Test-Loss: 0.28280783767497875 Train-f1: 0.2509490196078431 Test-f1: 0.2049655172413793 Train-acc: 0.5695364238410596 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2237/20000] Train-Loss: 1.1140889313425377 Test-Loss: 0.2826147270441574 Train-f1: 0.23615485564304461 Test-f1: 0.23863636363636362 Train-acc: 0.5496688741721855 Test-acc: 0.5526315789473685\n",
      "Epoch [2238/20000] Train-Loss: 1.1121754414339586 Test-Loss: 0.28293209783266476 Train-f1: 0.25019233010840425 Test-f1: 0.19179632248939177 Train-acc: 0.5695364238410596 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2239/20000] Train-Loss: 1.1119455291550802 Test-Loss: 0.2823995195986044 Train-f1: 0.23891155352480417 Test-f1: 0.24904024767801855 Train-acc: 0.5529801324503312 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2240/20000] Train-Loss: 1.1115123166470513 Test-Loss: 0.28235297283851873 Train-f1: 0.23714842662211083 Test-f1: 0.2049655172413793 Train-acc: 0.5529801324503312 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2241/20000] Train-Loss: 1.1111996878530659 Test-Loss: 0.28220359617518115 Train-f1: 0.24392125351546806 Test-f1: 0.20404040404040408 Train-acc: 0.5596026490066225 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2242/20000] Train-Loss: 1.11052272862838 Test-Loss: 0.2820780506357096 Train-f1: 0.24578961954154951 Test-f1: 0.22770618556701033 Train-acc: 0.5629139072847682 Test-acc: 0.5394736842105263\n",
      "Epoch [2243/20000] Train-Loss: 1.1109217601861425 Test-Loss: 0.2821688110860116 Train-f1: 0.24983368144194323 Test-f1: 0.19179632248939177 Train-acc: 0.5695364238410596 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2244/20000] Train-Loss: 1.1091340279275752 Test-Loss: 0.28183539911018296 Train-f1: 0.2412436906925096 Test-f1: 0.23863636363636362 Train-acc: 0.5562913907284768 Test-acc: 0.5526315789473685\n",
      "Epoch [2245/20000] Train-Loss: 1.108334806858556 Test-Loss: 0.28197036707084383 Train-f1: 0.25165082636741865 Test-f1: 0.19179632248939177 Train-acc: 0.5728476821192053 Test-acc: 0.5\n",
      "Epoch [2246/20000] Train-Loss: 1.1089747080963517 Test-Loss: 0.28211922944126205 Train-f1: 0.2428045449146839 Test-f1: 0.19179632248939177 Train-acc: 0.5596026490066225 Test-acc: 0.5\n",
      "Epoch [2247/20000] Train-Loss: 1.108707917117027 Test-Loss: 0.2818480125640994 Train-f1: 0.2405053170224674 Test-f1: 0.19179632248939177 Train-acc: 0.5562913907284768 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2248/20000] Train-Loss: 1.1082890700118768 Test-Loss: 0.28151711535983537 Train-f1: 0.23747501399216436 Test-f1: 0.2049655172413793 Train-acc: 0.5529801324503312 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2249/20000] Train-Loss: 1.1068501370347454 Test-Loss: 0.2813296789414716 Train-f1: 0.24764432647644327 Test-f1: 0.20404040404040408 Train-acc: 0.5662251655629139 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2250/20000] Train-Loss: 1.105992306461798 Test-Loss: 0.2813223682126145 Train-f1: 0.25019233010840425 Test-f1: 0.2049655172413793 Train-acc: 0.5695364238410596 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2251/20000] Train-Loss: 1.1060535513321743 Test-Loss: 0.28131872721124773 Train-f1: 0.24578961954154951 Test-f1: 0.19179632248939177 Train-acc: 0.5629139072847682 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2252/20000] Train-Loss: 1.1056666189330502 Test-Loss: 0.2809448660459997 Train-f1: 0.2454215918045705 Test-f1: 0.23863636363636362 Train-acc: 0.5629139072847682 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2253/20000] Train-Loss: 1.1054316768114625 Test-Loss: 0.28082745891941224 Train-f1: 0.24353562005277044 Test-f1: 0.23863636363636362 Train-acc: 0.5596026490066225 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2254/20000] Train-Loss: 1.1047550125821446 Test-Loss: 0.28073436907480964 Train-f1: 0.24729377938376654 Test-f1: 0.23863636363636362 Train-acc: 0.5662251655629139 Test-acc: 0.5526315789473685\n",
      "Epoch [2255/20000] Train-Loss: 1.1030933477927287 Test-Loss: 0.28087851780417183 Train-f1: 0.25019233010840425 Test-f1: 0.2049655172413793 Train-acc: 0.5695364238410596 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2256/20000] Train-Loss: 1.102657152524487 Test-Loss: 0.28048087438704017 Train-f1: 0.2378143832864604 Test-f1: 0.24904024767801855 Train-acc: 0.5529801324503312 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2257/20000] Train-Loss: 1.1029547751628037 Test-Loss: 0.28047859865192293 Train-f1: 0.2510066606115653 Test-f1: 0.2049655172413793 Train-acc: 0.5728476821192053 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2258/20000] Train-Loss: 1.1024812164024191 Test-Loss: 0.28038592473396445 Train-f1: 0.2428045449146839 Test-f1: 0.2049655172413793 Train-acc: 0.5596026490066225 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2259/20000] Train-Loss: 1.1014653113367405 Test-Loss: 0.2802978859772286 Train-f1: 0.24439522865912888 Test-f1: 0.2049655172413793 Train-acc: 0.5629139072847682 Test-acc: 0.5131578947368421\n",
      "Epoch [2260/20000] Train-Loss: 1.1004317240438073 Test-Loss: 0.2803019010620807 Train-f1: 0.24439522865912888 Test-f1: 0.2049655172413793 Train-acc: 0.5629139072847682 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2261/20000] Train-Loss: 1.101071308101861 Test-Loss: 0.28001761792517155 Train-f1: 0.24180491041804908 Test-f1: 0.2161948650427913 Train-acc: 0.5596026490066225 Test-acc: 0.5263157894736842\n",
      "best score_dx!!\n",
      "Epoch [2262/20000] Train-Loss: 1.1007614945979411 Test-Loss: 0.27996039530906136 Train-f1: 0.24506666666666668 Test-f1: 0.2049655172413793 Train-acc: 0.5629139072847682 Test-acc: 0.5131578947368421\n",
      "Epoch [2263/20000] Train-Loss: 1.0996749659024418 Test-Loss: 0.28046572763325284 Train-f1: 0.25165082636741865 Test-f1: 0.19179632248939177 Train-acc: 0.5728476821192053 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2264/20000] Train-Loss: 1.0979025382516518 Test-Loss: 0.27956994232276 Train-f1: 0.2412436906925096 Test-f1: 0.23863636363636362 Train-acc: 0.5562913907284768 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2265/20000] Train-Loss: 1.0993262846875624 Test-Loss: 0.2794495737275846 Train-f1: 0.2513225904530252 Test-f1: 0.23863636363636362 Train-acc: 0.5728476821192053 Test-acc: 0.5526315789473685\n",
      "Epoch [2266/20000] Train-Loss: 1.0986418595465606 Test-Loss: 0.27975111033328576 Train-f1: 0.25165082636741865 Test-f1: 0.19179632248939177 Train-acc: 0.5728476821192053 Test-acc: 0.5\n",
      "best score_dx!!\n",
      "Epoch [2267/20000] Train-Loss: 1.0973851586335266 Test-Loss: 0.2792940301775999 Train-f1: 0.2513225904530252 Test-f1: 0.23863636363636362 Train-acc: 0.5728476821192053 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2268/20000] Train-Loss: 1.0969209192248786 Test-Loss: 0.2791169471964088 Train-f1: 0.25019233010840425 Test-f1: 0.23863636363636362 Train-acc: 0.5695364238410596 Test-acc: 0.5526315789473685\n",
      "Epoch [2269/20000] Train-Loss: 1.095772220202999 Test-Loss: 0.27917591853614987 Train-f1: 0.24983368144194323 Test-f1: 0.2049655172413793 Train-acc: 0.5695364238410596 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2270/20000] Train-Loss: 1.0964808301685738 Test-Loss: 0.2789269661062857 Train-f1: 0.25019233010840425 Test-f1: 0.23863636363636362 Train-acc: 0.5695364238410596 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2271/20000] Train-Loss: 1.0954473732516008 Test-Loss: 0.2788356934930409 Train-f1: 0.24983368144194323 Test-f1: 0.23863636363636362 Train-acc: 0.5695364238410596 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2272/20000] Train-Loss: 1.0941928817342392 Test-Loss: 0.278655124769038 Train-f1: 0.24245862884160757 Test-f1: 0.24904024767801855 Train-acc: 0.5596026490066225 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2273/20000] Train-Loss: 1.0942198634383182 Test-Loss: 0.2785806097165517 Train-f1: 0.25165082636741865 Test-f1: 0.23863636363636362 Train-acc: 0.5728476821192053 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2274/20000] Train-Loss: 1.0936300421069327 Test-Loss: 0.2784581236613083 Train-f1: 0.2432001221420665 Test-f1: 0.23863636363636362 Train-acc: 0.5629139072847682 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2275/20000] Train-Loss: 1.0925738940288678 Test-Loss: 0.27828032819526016 Train-f1: 0.25165082636741865 Test-f1: 0.24904024767801855 Train-acc: 0.5728476821192053 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2276/20000] Train-Loss: 1.0920649103053255 Test-Loss: 0.27815861472418896 Train-f1: 0.24883392495896794 Test-f1: 0.24904024767801855 Train-acc: 0.5695364238410596 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2277/20000] Train-Loss: 1.092609579683194 Test-Loss: 0.27804286083522534 Train-f1: 0.2519915660341192 Test-f1: 0.24904024767801855 Train-acc: 0.5728476821192053 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2278/20000] Train-Loss: 1.0918986513247966 Test-Loss: 0.27792503510724875 Train-f1: 0.2548284454667433 Test-f1: 0.24904024767801855 Train-acc: 0.5761589403973509 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2279/20000] Train-Loss: 1.090076375857548 Test-Loss: 0.27792127770073266 Train-f1: 0.2510066606115653 Test-f1: 0.23863636363636362 Train-acc: 0.5728476821192053 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2280/20000] Train-Loss: 1.0900443844193461 Test-Loss: 0.27791197085777297 Train-f1: 0.2513225904530252 Test-f1: 0.22770618556701033 Train-acc: 0.5728476821192053 Test-acc: 0.5394736842105263\n",
      "best score_dx!!\n",
      "Epoch [2281/20000] Train-Loss: 1.0897553518504783 Test-Loss: 0.2778400140890852 Train-f1: 0.2534614659883173 Test-f1: 0.21737373737373739 Train-acc: 0.5761589403973509 Test-acc: 0.5263157894736842\n",
      "best score_dx!!\n",
      "Epoch [2282/20000] Train-Loss: 1.0887198300223357 Test-Loss: 0.2776171540582026 Train-f1: 0.2454215918045705 Test-f1: 0.23863636363636362 Train-acc: 0.5629139072847682 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2283/20000] Train-Loss: 1.0893932934254884 Test-Loss: 0.27733418662851417 Train-f1: 0.24883392495896794 Test-f1: 0.24904024767801855 Train-acc: 0.5695364238410596 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2284/20000] Train-Loss: 1.087962266761507 Test-Loss: 0.27725858680132837 Train-f1: 0.24948787061994607 Test-f1: 0.23863636363636362 Train-acc: 0.5695364238410596 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2285/20000] Train-Loss: 1.0871097629275819 Test-Loss: 0.27715866138545225 Train-f1: 0.2513225904530252 Test-f1: 0.23863636363636362 Train-acc: 0.5728476821192053 Test-acc: 0.5526315789473685\n",
      "Epoch [2286/20000] Train-Loss: 1.0861949585266821 Test-Loss: 0.2772488935099872 Train-f1: 0.2570713760117733 Test-f1: 0.22770618556701033 Train-acc: 0.5827814569536424 Test-acc: 0.5394736842105263\n",
      "Epoch [2287/20000] Train-Loss: 1.086083764000703 Test-Loss: 0.27752056074353726 Train-f1: 0.2544677277758693 Test-f1: 0.2049655172413793 Train-acc: 0.5761589403973509 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2288/20000] Train-Loss: 1.0861782266094389 Test-Loss: 0.2768506832887451 Train-f1: 0.24695598360021664 Test-f1: 0.23863636363636362 Train-acc: 0.5662251655629139 Test-acc: 0.5526315789473685\n",
      "Epoch [2289/20000] Train-Loss: 1.0855808736847725 Test-Loss: 0.27701604941807106 Train-f1: 0.2549742296711039 Test-f1: 0.21737373737373739 Train-acc: 0.5794701986754967 Test-acc: 0.5263157894736842\n",
      "best score_dx!!\n",
      "Epoch [2290/20000] Train-Loss: 1.0849092605217554 Test-Loss: 0.27661053612963804 Train-f1: 0.2519915660341192 Test-f1: 0.23863636363636362 Train-acc: 0.5728476821192053 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2291/20000] Train-Loss: 1.08477118972672 Test-Loss: 0.27645337848368606 Train-f1: 0.2544677277758693 Test-f1: 0.23863636363636362 Train-acc: 0.5761589403973509 Test-acc: 0.5526315789473685\n",
      "Epoch [2292/20000] Train-Loss: 1.081488125977558 Test-Loss: 0.2772669250142975 Train-f1: 0.2617592288311973 Test-f1: 0.19179632248939177 Train-acc: 0.5894039735099338 Test-acc: 0.5\n",
      "Epoch [2293/20000] Train-Loss: 1.082885178013515 Test-Loss: 0.2769395435919019 Train-f1: 0.25285190999476714 Test-f1: 0.2049655172413793 Train-acc: 0.5761589403973509 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2294/20000] Train-Loss: 1.0830995750817927 Test-Loss: 0.2760201098092631 Train-f1: 0.2491546866014951 Test-f1: 0.24904024767801855 Train-acc: 0.5695364238410596 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2295/20000] Train-Loss: 1.0818220947813346 Test-Loss: 0.2759384373466106 Train-f1: 0.253784438389343 Test-f1: 0.24904024767801855 Train-acc: 0.5761589403973509 Test-acc: 0.5657894736842105\n",
      "Epoch [2296/20000] Train-Loss: 1.08123416669907 Test-Loss: 0.276029569705056 Train-f1: 0.2620583492156575 Test-f1: 0.23863636363636362 Train-acc: 0.5894039735099338 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2297/20000] Train-Loss: 1.0804947110631207 Test-Loss: 0.2758301766862139 Train-f1: 0.25285190999476714 Test-f1: 0.23863636363636362 Train-acc: 0.5761589403973509 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2298/20000] Train-Loss: 1.0796768115177493 Test-Loss: 0.2756064801863855 Train-f1: 0.253784438389343 Test-f1: 0.23863636363636362 Train-acc: 0.5761589403973509 Test-acc: 0.5526315789473685\n",
      "Epoch [2299/20000] Train-Loss: 1.080705011567689 Test-Loss: 0.27563836115467033 Train-f1: 0.2491546866014951 Test-f1: 0.23863636363636362 Train-acc: 0.5695364238410596 Test-acc: 0.5526315789473685\n",
      "Epoch [2300/20000] Train-Loss: 1.0774804731660874 Test-Loss: 0.2759952470525125 Train-f1: 0.2617592288311973 Test-f1: 0.2049655172413793 Train-acc: 0.5894039735099338 Test-acc: 0.5131578947368421\n",
      "best score_dx!!\n",
      "Epoch [2301/20000] Train-Loss: 1.0790479017178727 Test-Loss: 0.27536182382636876 Train-f1: 0.24439522865912888 Test-f1: 0.23863636363636362 Train-acc: 0.5629139072847682 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2302/20000] Train-Loss: 1.0775880235508746 Test-Loss: 0.2751923598262069 Train-f1: 0.25315068493150683 Test-f1: 0.23863636363636362 Train-acc: 0.5761589403973509 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2303/20000] Train-Loss: 1.0769664559978043 Test-Loss: 0.27496350392078167 Train-f1: 0.2491546866014951 Test-f1: 0.24904024767801855 Train-acc: 0.5695364238410596 Test-acc: 0.5657894736842105\n",
      "Epoch [2304/20000] Train-Loss: 1.0762664304962946 Test-Loss: 0.27514632901318825 Train-f1: 0.26034335718788243 Test-f1: 0.23863636363636362 Train-acc: 0.5860927152317881 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2305/20000] Train-Loss: 1.07670531877437 Test-Loss: 0.2747686154422473 Train-f1: 0.25411979325022804 Test-f1: 0.23863636363636362 Train-acc: 0.5761589403973509 Test-acc: 0.5526315789473685\n",
      "Epoch [2306/20000] Train-Loss: 1.0762408120335847 Test-Loss: 0.2749280076883525 Train-f1: 0.2570713760117733 Test-f1: 0.23863636363636362 Train-acc: 0.5827814569536424 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2307/20000] Train-Loss: 1.0748072619672295 Test-Loss: 0.2746958004699042 Train-f1: 0.2576587934294931 Test-f1: 0.23863636363636362 Train-acc: 0.5827814569536424 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2308/20000] Train-Loss: 1.0744869814812352 Test-Loss: 0.2744383815963955 Train-f1: 0.2552676643585735 Test-f1: 0.23863636363636362 Train-acc: 0.5794701986754967 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2309/20000] Train-Loss: 1.0745574078871378 Test-Loss: 0.27431117500153895 Train-f1: 0.25315068493150683 Test-f1: 0.23863636363636362 Train-acc: 0.5761589403973509 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2310/20000] Train-Loss: 1.0728598471178086 Test-Loss: 0.27427891005050503 Train-f1: 0.2611960855382385 Test-f1: 0.23863636363636362 Train-acc: 0.5894039735099338 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2311/20000] Train-Loss: 1.0723789141313356 Test-Loss: 0.27396972088500343 Train-f1: 0.2513225904530252 Test-f1: 0.24753799392097262 Train-acc: 0.5728476821192053 Test-acc: 0.5657894736842105\n",
      "Epoch [2312/20000] Train-Loss: 1.072267530661316 Test-Loss: 0.27412463287093947 Train-f1: 0.2573591874422899 Test-f1: 0.23863636363636362 Train-acc: 0.5827814569536424 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2313/20000] Train-Loss: 1.0711252817262575 Test-Loss: 0.2737193784298204 Train-f1: 0.25285190999476714 Test-f1: 0.24623655913978496 Train-acc: 0.5761589403973509 Test-acc: 0.5657894736842105\n",
      "Epoch [2314/20000] Train-Loss: 1.0708979734802506 Test-Loss: 0.2737262878383964 Train-f1: 0.25914484974623 Test-f1: 0.23863636363636362 Train-acc: 0.5860927152317881 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2315/20000] Train-Loss: 1.070574621057774 Test-Loss: 0.27358277188137103 Train-f1: 0.25469252077562327 Test-f1: 0.23863636363636362 Train-acc: 0.5794701986754967 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2316/20000] Train-Loss: 1.0698345237505107 Test-Loss: 0.2734970635813388 Train-f1: 0.25557299843014125 Test-f1: 0.23863636363636362 Train-acc: 0.5794701986754967 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2317/20000] Train-Loss: 1.0689632882862787 Test-Loss: 0.2733156216329732 Train-f1: 0.2520258541089566 Test-f1: 0.24904024767801855 Train-acc: 0.5761589403973509 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2318/20000] Train-Loss: 1.0682352247705156 Test-Loss: 0.27312934069804407 Train-f1: 0.2594267702682891 Test-f1: 0.24753799392097262 Train-acc: 0.5860927152317881 Test-acc: 0.5657894736842105\n",
      "Epoch [2319/20000] Train-Loss: 1.068263501001592 Test-Loss: 0.2733079545196966 Train-f1: 0.2611960855382385 Test-f1: 0.23863636363636362 Train-acc: 0.5894039735099338 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2320/20000] Train-Loss: 1.0663222349748078 Test-Loss: 0.2728472899269128 Train-f1: 0.2582940868655154 Test-f1: 0.24512338425381905 Train-acc: 0.5827814569536424 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2321/20000] Train-Loss: 1.0674409677062207 Test-Loss: 0.27273806576367593 Train-f1: 0.2617592288311973 Test-f1: 0.24623655913978496 Train-acc: 0.5894039735099338 Test-acc: 0.5657894736842105\n",
      "Epoch [2322/20000] Train-Loss: 1.0670761441481937 Test-Loss: 0.2729629353729976 Train-f1: 0.2573591874422899 Test-f1: 0.23863636363636362 Train-acc: 0.5827814569536424 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2323/20000] Train-Loss: 1.0657564229442142 Test-Loss: 0.2724963806330651 Train-f1: 0.24986225895316805 Test-f1: 0.24623655913978496 Train-acc: 0.5728476821192053 Test-acc: 0.5657894736842105\n",
      "Epoch [2324/20000] Train-Loss: 1.0665233527470928 Test-Loss: 0.2728605993091666 Train-f1: 0.26635737586081915 Test-f1: 0.23863636363636362 Train-acc: 0.5960264900662252 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2325/20000] Train-Loss: 1.0639141277309083 Test-Loss: 0.2723206669944368 Train-f1: 0.2573591874422899 Test-f1: 0.24623655913978496 Train-acc: 0.5827814569536424 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2326/20000] Train-Loss: 1.064067337280709 Test-Loss: 0.2721673931352207 Train-f1: 0.2549742296711039 Test-f1: 0.24623655913978496 Train-acc: 0.5794701986754967 Test-acc: 0.5657894736842105\n",
      "Epoch [2327/20000] Train-Loss: 1.0633237382105452 Test-Loss: 0.2725755502705783 Train-f1: 0.2637767306995288 Test-f1: 0.23863636363636362 Train-acc: 0.5927152317880795 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2328/20000] Train-Loss: 1.0612063080754108 Test-Loss: 0.2719108298085471 Train-f1: 0.25411979325022804 Test-f1: 0.25471370734528626 Train-acc: 0.5761589403973509 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2329/20000] Train-Loss: 1.063025178408689 Test-Loss: 0.27184223638853705 Train-f1: 0.2657742272122062 Test-f1: 0.24623655913978496 Train-acc: 0.5960264900662252 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2330/20000] Train-Loss: 1.0626577793179055 Test-Loss: 0.27175369916717546 Train-f1: 0.2594267702682891 Test-f1: 0.24753799392097262 Train-acc: 0.5860927152317881 Test-acc: 0.5657894736842105\n",
      "Epoch [2331/20000] Train-Loss: 1.0605392633201907 Test-Loss: 0.2719777460440195 Train-f1: 0.2611960855382385 Test-f1: 0.23863636363636362 Train-acc: 0.5894039735099338 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2332/20000] Train-Loss: 1.0603315899946328 Test-Loss: 0.2713883443706585 Train-f1: 0.25972038263428987 Test-f1: 0.24623655913978496 Train-acc: 0.5860927152317881 Test-acc: 0.5657894736842105\n",
      "Epoch [2333/20000] Train-Loss: 1.0595140111537624 Test-Loss: 0.27160287521492804 Train-f1: 0.2637767306995288 Test-f1: 0.23863636363636362 Train-acc: 0.5927152317880795 Test-acc: 0.5526315789473685\n",
      "Epoch [2334/20000] Train-Loss: 1.0604946298189768 Test-Loss: 0.2716509206641599 Train-f1: 0.26147186147186147 Test-f1: 0.23863636363636362 Train-acc: 0.5894039735099338 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2335/20000] Train-Loss: 1.0586823786943724 Test-Loss: 0.27138025016145506 Train-f1: 0.260931744312026 Test-f1: 0.23863636363636362 Train-acc: 0.5894039735099338 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2336/20000] Train-Loss: 1.05685761450309 Test-Loss: 0.270894141476156 Train-f1: 0.2580592991913747 Test-f1: 0.25471370734528626 Train-acc: 0.5794701986754967 Test-acc: 0.5789473684210527\n",
      "Epoch [2337/20000] Train-Loss: 1.0580431761792373 Test-Loss: 0.2709389767667504 Train-f1: 0.2657742272122062 Test-f1: 0.2361094224924012 Train-acc: 0.5960264900662252 Test-acc: 0.5526315789473685\n",
      "Epoch [2338/20000] Train-Loss: 1.05473737681444 Test-Loss: 0.2719741218283593 Train-f1: 0.2604367626492631 Test-f1: 0.21737373737373739 Train-acc: 0.5894039735099338 Test-acc: 0.5263157894736842\n",
      "best score_dx!!\n",
      "Epoch [2339/20000] Train-Loss: 1.0567953711515303 Test-Loss: 0.270604842361409 Train-f1: 0.2544223693892568 Test-f1: 0.24623655913978496 Train-acc: 0.5794701986754967 Test-acc: 0.5657894736842105\n",
      "Epoch [2340/20000] Train-Loss: 1.054736480670309 Test-Loss: 0.2711571580640668 Train-f1: 0.2660599494402311 Test-f1: 0.23863636363636362 Train-acc: 0.5960264900662252 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2341/20000] Train-Loss: 1.0555304035185586 Test-Loss: 0.27036752777900297 Train-f1: 0.2544223693892568 Test-f1: 0.24623655913978496 Train-acc: 0.5794701986754967 Test-acc: 0.5657894736842105\n",
      "Epoch [2342/20000] Train-Loss: 1.054566600721921 Test-Loss: 0.2704244095980098 Train-f1: 0.2634958468761286 Test-f1: 0.23727554179566565 Train-acc: 0.5927152317880795 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2343/20000] Train-Loss: 1.0510377795117254 Test-Loss: 0.27019969468186344 Train-f1: 0.2582940868655154 Test-f1: 0.251559934318555 Train-acc: 0.5827814569536424 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2344/20000] Train-Loss: 1.0542209392829496 Test-Loss: 0.2700541700612735 Train-f1: 0.2677530017152659 Test-f1: 0.24623655913978496 Train-acc: 0.5993377483443708 Test-acc: 0.5657894736842105\n",
      "Epoch [2345/20000] Train-Loss: 1.0529109912093022 Test-Loss: 0.27014012034706403 Train-f1: 0.2657742272122062 Test-f1: 0.23727554179566565 Train-acc: 0.5960264900662252 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2346/20000] Train-Loss: 1.052219713058171 Test-Loss: 0.26960730204707 Train-f1: 0.25914484974623 Test-f1: 0.25471370734528626 Train-acc: 0.5860927152317881 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2347/20000] Train-Loss: 1.0512294976346404 Test-Loss: 0.26951120855916344 Train-f1: 0.2620583492156575 Test-f1: 0.25471370734528626 Train-acc: 0.5894039735099338 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2348/20000] Train-Loss: 1.0509362371413127 Test-Loss: 0.2693776157397759 Train-f1: 0.2632264565115693 Test-f1: 0.25471370734528626 Train-acc: 0.5927152317880795 Test-acc: 0.5789473684210527\n",
      "Epoch [2349/20000] Train-Loss: 1.0507676136165542 Test-Loss: 0.26947313756281865 Train-f1: 0.2634958468761286 Test-f1: 0.2361094224924012 Train-acc: 0.5927152317880795 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2350/20000] Train-Loss: 1.0496466457909015 Test-Loss: 0.26928444421200953 Train-f1: 0.2637767306995288 Test-f1: 0.24623655913978496 Train-acc: 0.5927152317880795 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2351/20000] Train-Loss: 1.0494907824786974 Test-Loss: 0.2691585791842819 Train-f1: 0.2634958468761286 Test-f1: 0.24623655913978496 Train-acc: 0.5927152317880795 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2352/20000] Train-Loss: 1.048193115965247 Test-Loss: 0.2688507804905383 Train-f1: 0.2594267702682891 Test-f1: 0.2536752136752137 Train-acc: 0.5860927152317881 Test-acc: 0.5789473684210527\n",
      "Epoch [2353/20000] Train-Loss: 1.0484526383406494 Test-Loss: 0.26974085110084156 Train-f1: 0.26451590081007464 Test-f1: 0.23863636363636362 Train-acc: 0.5960264900662252 Test-acc: 0.5526315789473685\n",
      "Epoch [2354/20000] Train-Loss: 1.048864614350665 Test-Loss: 0.26890905798415105 Train-f1: 0.26406926406926406 Test-f1: 0.2361094224924012 Train-acc: 0.5927152317880795 Test-acc: 0.5526315789473685\n",
      "Epoch [2355/20000] Train-Loss: 1.0460882946086427 Test-Loss: 0.2691062797105471 Train-f1: 0.2647453416149068 Test-f1: 0.23727554179566565 Train-acc: 0.5960264900662252 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2356/20000] Train-Loss: 1.0474979997787401 Test-Loss: 0.2683637544094936 Train-f1: 0.2552676643585735 Test-f1: 0.25471370734528626 Train-acc: 0.5794701986754967 Test-acc: 0.5789473684210527\n",
      "Epoch [2357/20000] Train-Loss: 1.045373842022246 Test-Loss: 0.26848480144743164 Train-f1: 0.262485754985755 Test-f1: 0.24623655913978496 Train-acc: 0.5927152317880795 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2358/20000] Train-Loss: 1.0459043197330733 Test-Loss: 0.2682278162573967 Train-f1: 0.2657742272122062 Test-f1: 0.24512338425381905 Train-acc: 0.5960264900662252 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2359/20000] Train-Loss: 1.0447922090019168 Test-Loss: 0.2680388554660837 Train-f1: 0.2623693892568065 Test-f1: 0.25471370734528626 Train-acc: 0.5894039735099338 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2360/20000] Train-Loss: 1.0437660097716501 Test-Loss: 0.2679453256983365 Train-f1: 0.2632264565115693 Test-f1: 0.24512338425381905 Train-acc: 0.5927152317880795 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2361/20000] Train-Loss: 1.042804544049628 Test-Loss: 0.26775767689592767 Train-f1: 0.2611960855382385 Test-f1: 0.25471370734528626 Train-acc: 0.5894039735099338 Test-acc: 0.5789473684210527\n",
      "Epoch [2362/20000] Train-Loss: 1.0424043006402075 Test-Loss: 0.2681023877167295 Train-f1: 0.2692052746632819 Test-f1: 0.2361094224924012 Train-acc: 0.6026490066225165 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2363/20000] Train-Loss: 1.041509476564576 Test-Loss: 0.26763964077026886 Train-f1: 0.2611960855382385 Test-f1: 0.24623655913978496 Train-acc: 0.5894039735099338 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2364/20000] Train-Loss: 1.040942137324253 Test-Loss: 0.2673325167036366 Train-f1: 0.2632264565115693 Test-f1: 0.2536752136752137 Train-acc: 0.5927152317880795 Test-acc: 0.5789473684210527\n",
      "Epoch [2365/20000] Train-Loss: 1.040793705591687 Test-Loss: 0.2676246756025584 Train-f1: 0.2604367626492631 Test-f1: 0.2361094224924012 Train-acc: 0.5894039735099338 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2366/20000] Train-Loss: 1.041596375367758 Test-Loss: 0.26727421512958577 Train-f1: 0.26147186147186147 Test-f1: 0.24512338425381905 Train-acc: 0.5894039735099338 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2367/20000] Train-Loss: 1.0391675354186944 Test-Loss: 0.2670550732301611 Train-f1: 0.2634958468761286 Test-f1: 0.24512338425381905 Train-acc: 0.5927152317880795 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2368/20000] Train-Loss: 1.0389950170440725 Test-Loss: 0.2669002107036784 Train-f1: 0.2632264565115693 Test-f1: 0.25471370734528626 Train-acc: 0.5927152317880795 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2369/20000] Train-Loss: 1.037915279559551 Test-Loss: 0.2668644759434517 Train-f1: 0.2655000537885036 Test-f1: 0.24512338425381905 Train-acc: 0.5960264900662252 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2370/20000] Train-Loss: 1.0385780707030798 Test-Loss: 0.2666907805356145 Train-f1: 0.2632264565115693 Test-f1: 0.24512338425381905 Train-acc: 0.5927152317880795 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2371/20000] Train-Loss: 1.036032694969537 Test-Loss: 0.2665100980456357 Train-f1: 0.2632264565115693 Test-f1: 0.2521064301552106 Train-acc: 0.5927152317880795 Test-acc: 0.5789473684210527\n",
      "Epoch [2372/20000] Train-Loss: 1.0367017013008688 Test-Loss: 0.2665470077790989 Train-f1: 0.27165926239334326 Test-f1: 0.24623655913978496 Train-acc: 0.6059602649006622 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2373/20000] Train-Loss: 1.0354495126865806 Test-Loss: 0.26619770357049755 Train-f1: 0.26067868581093234 Test-f1: 0.2536752136752137 Train-acc: 0.5894039735099338 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2374/20000] Train-Loss: 1.0350619629380127 Test-Loss: 0.26614745962658726 Train-f1: 0.26523727844482564 Test-f1: 0.2536752136752137 Train-acc: 0.5960264900662252 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2375/20000] Train-Loss: 1.0345777294420178 Test-Loss: 0.2659502729586573 Train-f1: 0.26523727844482564 Test-f1: 0.2536752136752137 Train-acc: 0.5960264900662252 Test-acc: 0.5789473684210527\n",
      "Epoch [2376/20000] Train-Loss: 1.03546980500728 Test-Loss: 0.26660568987185196 Train-f1: 0.2669850366125438 Test-f1: 0.2361094224924012 Train-acc: 0.5993377483443708 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2377/20000] Train-Loss: 1.0342643581053181 Test-Loss: 0.26576767746229785 Train-f1: 0.264689928163026 Test-f1: 0.2536752136752137 Train-acc: 0.5927152317880795 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2378/20000] Train-Loss: 1.0322345027667421 Test-Loss: 0.2655475005421021 Train-f1: 0.2588744588744589 Test-f1: 0.2536752136752137 Train-acc: 0.5860927152317881 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2379/20000] Train-Loss: 1.0329069702384979 Test-Loss: 0.2654756565611329 Train-f1: 0.2669850366125438 Test-f1: 0.2536752136752137 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2380/20000] Train-Loss: 1.0324023462807554 Test-Loss: 0.2652954456908937 Train-f1: 0.267485754985755 Test-f1: 0.2521064301552106 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2381/20000] Train-Loss: 1.0316988470624942 Test-Loss: 0.26525404937270847 Train-f1: 0.264985754985755 Test-f1: 0.2536752136752137 Train-acc: 0.5960264900662252 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2382/20000] Train-Loss: 1.0299442706388167 Test-Loss: 0.26503793345816684 Train-f1: 0.26998575498575506 Test-f1: 0.2536752136752137 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2383/20000] Train-Loss: 1.0298682869347286 Test-Loss: 0.2649046973469692 Train-f1: 0.2694541724150129 Test-f1: 0.25280898876404495 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2384/20000] Train-Loss: 1.0300725497726915 Test-Loss: 0.2648539197715173 Train-f1: 0.2669850366125438 Test-f1: 0.2536752136752137 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2385/20000] Train-Loss: 1.0283966727067684 Test-Loss: 0.2646391287352598 Train-f1: 0.26832199791284317 Test-f1: 0.2521064301552106 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2386/20000] Train-Loss: 1.0279828641803312 Test-Loss: 0.26451657126746525 Train-f1: 0.2669850366125438 Test-f1: 0.2536752136752137 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "Epoch [2387/20000] Train-Loss: 1.0282228366246158 Test-Loss: 0.264615182922808 Train-f1: 0.2694541724150129 Test-f1: 0.2441873915558126 Train-acc: 0.6026490066225165 Test-acc: 0.5657894736842105\n",
      "Epoch [2388/20000] Train-Loss: 1.025889155599188 Test-Loss: 0.26480374041502436 Train-f1: 0.2669850366125438 Test-f1: 0.2361094224924012 Train-acc: 0.5993377483443708 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2389/20000] Train-Loss: 1.0255765344467629 Test-Loss: 0.2641621649824246 Train-f1: 0.2669850366125438 Test-f1: 0.2536752136752137 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2390/20000] Train-Loss: 1.025481723526354 Test-Loss: 0.2640610618634789 Train-f1: 0.26722981366459625 Test-f1: 0.2536752136752137 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2391/20000] Train-Loss: 1.0254214047275492 Test-Loss: 0.2640546929916705 Train-f1: 0.2677530017152659 Test-f1: 0.2536752136752137 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2392/20000] Train-Loss: 1.0243759445106486 Test-Loss: 0.2637584699014656 Train-f1: 0.26523727844482564 Test-f1: 0.2536752136752137 Train-acc: 0.5960264900662252 Test-acc: 0.5789473684210527\n",
      "Epoch [2393/20000] Train-Loss: 1.0249800953360846 Test-Loss: 0.26455708560479646 Train-f1: 0.26652843185492375 Test-f1: 0.2361094224924012 Train-acc: 0.5993377483443708 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2394/20000] Train-Loss: 1.022345809561243 Test-Loss: 0.2634893590583081 Train-f1: 0.2588744588744589 Test-f1: 0.251559934318555 Train-acc: 0.5860927152317881 Test-acc: 0.5789473684210527\n",
      "Epoch [2395/20000] Train-Loss: 1.022435583556877 Test-Loss: 0.2635890385177237 Train-f1: 0.27116482746540554 Test-f1: 0.2441873915558126 Train-acc: 0.6059602649006622 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2396/20000] Train-Loss: 1.0216396940035015 Test-Loss: 0.26325730654842955 Train-f1: 0.26722981366459625 Test-f1: 0.2536752136752137 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "Epoch [2397/20000] Train-Loss: 1.022153083367059 Test-Loss: 0.26349652816114105 Train-f1: 0.26429729920315914 Test-f1: 0.2441873915558126 Train-acc: 0.5960264900662252 Test-acc: 0.5657894736842105\n",
      "Epoch [2398/20000] Train-Loss: 1.0201260060971158 Test-Loss: 0.26347262861194926 Train-f1: 0.26652843185492375 Test-f1: 0.24512338425381905 Train-acc: 0.5993377483443708 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2399/20000] Train-Loss: 1.0196245649566906 Test-Loss: 0.2628098660421995 Train-f1: 0.2694541724150129 Test-f1: 0.25280898876404495 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "Epoch [2400/20000] Train-Loss: 1.0192860619168282 Test-Loss: 0.26314263148134187 Train-f1: 0.2669850366125438 Test-f1: 0.2441873915558126 Train-acc: 0.5993377483443708 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2401/20000] Train-Loss: 1.0192103742918126 Test-Loss: 0.2627495286611377 Train-f1: 0.2694541724150129 Test-f1: 0.2536752136752137 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "Epoch [2402/20000] Train-Loss: 1.0182473001867764 Test-Loss: 0.2628579415954949 Train-f1: 0.26852453291426576 Test-f1: 0.2441873915558126 Train-acc: 0.6026490066225165 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2403/20000] Train-Loss: 1.0163813650076 Test-Loss: 0.26262016274200484 Train-f1: 0.267485754985755 Test-f1: 0.2596825396825397 Train-acc: 0.5993377483443708 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2404/20000] Train-Loss: 1.017568050047676 Test-Loss: 0.2621477092582207 Train-f1: 0.27030693103567593 Test-f1: 0.251559934318555 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2405/20000] Train-Loss: 1.016991893116338 Test-Loss: 0.262021768526261 Train-f1: 0.26429729920315914 Test-f1: 0.251559934318555 Train-acc: 0.5960264900662252 Test-acc: 0.5789473684210527\n",
      "Epoch [2406/20000] Train-Loss: 1.015172217676594 Test-Loss: 0.2623417361806914 Train-f1: 0.2694541724150129 Test-f1: 0.2441873915558126 Train-acc: 0.6026490066225165 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2407/20000] Train-Loss: 1.0130647817851706 Test-Loss: 0.261872426972472 Train-f1: 0.2686240520043337 Test-f1: 0.26046511627906976 Train-acc: 0.5993377483443708 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2408/20000] Train-Loss: 1.0152485080468083 Test-Loss: 0.2617113938019672 Train-f1: 0.2705053449951409 Test-f1: 0.2536752136752137 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "Epoch [2409/20000] Train-Loss: 1.012985534885109 Test-Loss: 0.26207284987548385 Train-f1: 0.26631634261692066 Test-f1: 0.2441873915558126 Train-acc: 0.5993377483443708 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2410/20000] Train-Loss: 1.0140435191667587 Test-Loss: 0.261685080418581 Train-f1: 0.2647453416149068 Test-f1: 0.2536752136752137 Train-acc: 0.5960264900662252 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2411/20000] Train-Loss: 1.0118830996793384 Test-Loss: 0.26123398450761015 Train-f1: 0.2669850366125438 Test-f1: 0.251559934318555 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2412/20000] Train-Loss: 1.0115196172520127 Test-Loss: 0.2612213655093618 Train-f1: 0.26852453291426576 Test-f1: 0.26 Train-acc: 0.6026490066225165 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2413/20000] Train-Loss: 1.0125656792196955 Test-Loss: 0.26099512954577836 Train-f1: 0.27093417146848264 Test-f1: 0.251559934318555 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "Epoch [2414/20000] Train-Loss: 1.0099253322109472 Test-Loss: 0.26120243998152315 Train-f1: 0.2683191756022838 Test-f1: 0.2536752136752137 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "Epoch [2415/20000] Train-Loss: 1.0098871784035965 Test-Loss: 0.26144527521076294 Train-f1: 0.2692052746632819 Test-f1: 0.2441873915558126 Train-acc: 0.6026490066225165 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2416/20000] Train-Loss: 1.0095700345035696 Test-Loss: 0.26064797902656056 Train-f1: 0.2694541724150129 Test-f1: 0.25280898876404495 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2417/20000] Train-Loss: 1.0087079471344225 Test-Loss: 0.2604562456301624 Train-f1: 0.27140648063541156 Test-f1: 0.251559934318555 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2418/20000] Train-Loss: 1.0074995013643715 Test-Loss: 0.2603289881766005 Train-f1: 0.27140648063541156 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "Epoch [2419/20000] Train-Loss: 1.0059279328771527 Test-Loss: 0.2614527912994628 Train-f1: 0.2722807017543859 Test-f1: 0.23512544802867383 Train-acc: 0.609271523178808 Test-acc: 0.5526315789473685\n",
      "best score_dx!!\n",
      "Epoch [2420/20000] Train-Loss: 1.0063919334002258 Test-Loss: 0.26014739734672726 Train-f1: 0.260931744312026 Test-f1: 0.26 Train-acc: 0.5894039735099338 Test-acc: 0.5921052631578947\n",
      "Epoch [2421/20000] Train-Loss: 1.0068866530065874 Test-Loss: 0.2602344796487026 Train-f1: 0.2722807017543859 Test-f1: 0.2536752136752137 Train-acc: 0.609271523178808 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2422/20000] Train-Loss: 1.0037472392929387 Test-Loss: 0.2600359463611818 Train-f1: 0.26451590081007464 Test-f1: 0.2596825396825397 Train-acc: 0.5960264900662252 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2423/20000] Train-Loss: 1.005560386640549 Test-Loss: 0.25970811273799105 Train-f1: 0.2683191756022838 Test-f1: 0.2521064301552106 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "Epoch [2424/20000] Train-Loss: 1.0050031336180543 Test-Loss: 0.25976458615223186 Train-f1: 0.2705053449951409 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2425/20000] Train-Loss: 1.003754949672324 Test-Loss: 0.2595998624622515 Train-f1: 0.2707143851831222 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2426/20000] Train-Loss: 1.0029225707449514 Test-Loss: 0.2593084906835554 Train-f1: 0.2687405850411631 Test-f1: 0.26046511627906976 Train-acc: 0.6026490066225165 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2427/20000] Train-Loss: 1.0020571683613049 Test-Loss: 0.25917092979292294 Train-f1: 0.2707143851831222 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2428/20000] Train-Loss: 1.0010886798675955 Test-Loss: 0.25907791178706363 Train-f1: 0.26896745624516766 Test-f1: 0.26 Train-acc: 0.6026490066225165 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2429/20000] Train-Loss: 1.001385475590969 Test-Loss: 0.25898558642954655 Train-f1: 0.2705053449951409 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "Epoch [2430/20000] Train-Loss: 1.0006321586086748 Test-Loss: 0.25907678956557695 Train-f1: 0.2705053449951409 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2431/20000] Train-Loss: 0.9997268305135313 Test-Loss: 0.2587632194142178 Train-f1: 0.27093417146848264 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2432/20000] Train-Loss: 0.9987675723438155 Test-Loss: 0.2586130807960434 Train-f1: 0.2681243926141885 Test-f1: 0.25280898876404495 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "Epoch [2433/20000] Train-Loss: 0.999652464668072 Test-Loss: 0.2586714426129014 Train-f1: 0.26675128693322053 Test-f1: 0.25280898876404495 Train-acc: 0.5993377483443708 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2434/20000] Train-Loss: 0.9976295225659082 Test-Loss: 0.2583996402431448 Train-f1: 0.2705053449951409 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "Epoch [2435/20000] Train-Loss: 0.9977461874713631 Test-Loss: 0.25853558412640704 Train-f1: 0.2705053449951409 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2436/20000] Train-Loss: 0.9971921905857338 Test-Loss: 0.2581542497695705 Train-f1: 0.2687405850411631 Test-f1: 0.25280898876404495 Train-acc: 0.6026490066225165 Test-acc: 0.5789473684210527\n",
      "Epoch [2437/20000] Train-Loss: 0.9964534508802171 Test-Loss: 0.25851750366805665 Train-f1: 0.27030693103567593 Test-f1: 0.2434188034188034 Train-acc: 0.6059602649006622 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2438/20000] Train-Loss: 0.9957903577476956 Test-Loss: 0.2577621034792733 Train-f1: 0.26896745624516766 Test-f1: 0.26046511627906976 Train-acc: 0.6026490066225165 Test-acc: 0.5921052631578947\n",
      "Epoch [2439/20000] Train-Loss: 0.9955310447309933 Test-Loss: 0.25783568521173356 Train-f1: 0.2687405850411631 Test-f1: 0.2595075955997905 Train-acc: 0.6026490066225165 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2440/20000] Train-Loss: 0.9954207947484143 Test-Loss: 0.25753472227879604 Train-f1: 0.270119027082974 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "Epoch [2441/20000] Train-Loss: 0.9947735904038171 Test-Loss: 0.25784061007070336 Train-f1: 0.27176915993042056 Test-f1: 0.25280898876404495 Train-acc: 0.609271523178808 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2442/20000] Train-Loss: 0.9948466319111885 Test-Loss: 0.25725884086860934 Train-f1: 0.2707143851831222 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "Epoch [2443/20000] Train-Loss: 0.992476346638072 Test-Loss: 0.25743230097435665 Train-f1: 0.26977430198257524 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2444/20000] Train-Loss: 0.9920790971681683 Test-Loss: 0.2569940440442615 Train-f1: 0.2683191756022838 Test-f1: 0.26 Train-acc: 0.6026490066225165 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2445/20000] Train-Loss: 0.9924783203002394 Test-Loss: 0.2569042053362979 Train-f1: 0.27116482746540554 Test-f1: 0.26 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2446/20000] Train-Loss: 0.991725651077795 Test-Loss: 0.25677677131341153 Train-f1: 0.26977430198257524 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2447/20000] Train-Loss: 0.9894332029006039 Test-Loss: 0.25661486952516305 Train-f1: 0.270119027082974 Test-f1: 0.26 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2448/20000] Train-Loss: 0.9891217496370776 Test-Loss: 0.2564911188979886 Train-f1: 0.2705053449951409 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2449/20000] Train-Loss: 0.9886208658413616 Test-Loss: 0.2563926027562861 Train-f1: 0.27247196825944453 Test-f1: 0.26 Train-acc: 0.609271523178808 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2450/20000] Train-Loss: 0.988662993715955 Test-Loss: 0.256244233730965 Train-f1: 0.27192940452166775 Test-f1: 0.26046511627906976 Train-acc: 0.609271523178808 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2451/20000] Train-Loss: 0.9873563173093496 Test-Loss: 0.25616424759583034 Train-f1: 0.2707143851831222 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2452/20000] Train-Loss: 0.9868522419965193 Test-Loss: 0.2559927792826715 Train-f1: 0.27030693103567593 Test-f1: 0.26 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "Epoch [2453/20000] Train-Loss: 0.9867626209432666 Test-Loss: 0.2560495763372448 Train-f1: 0.2705053449951409 Test-f1: 0.26186252771618623 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "Epoch [2454/20000] Train-Loss: 0.9859046135722689 Test-Loss: 0.2560592106315599 Train-f1: 0.27247196825944453 Test-f1: 0.26186252771618623 Train-acc: 0.609271523178808 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2455/20000] Train-Loss: 0.9849467767502693 Test-Loss: 0.25567513628262173 Train-f1: 0.2705053449951409 Test-f1: 0.26 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "Epoch [2456/20000] Train-Loss: 0.9834522419485587 Test-Loss: 0.25625652120236597 Train-f1: 0.2694703093557079 Test-f1: 0.25280898876404495 Train-acc: 0.6059602649006622 Test-acc: 0.5789473684210527\n",
      "best score_dx!!\n",
      "Epoch [2457/20000] Train-Loss: 0.9848931222207458 Test-Loss: 0.2553805349898106 Train-f1: 0.2707143851831222 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2458/20000] Train-Loss: 0.983840623157712 Test-Loss: 0.2552803868626717 Train-f1: 0.2769725348390556 Test-f1: 0.2841269841269841 Train-acc: 0.609271523178808 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2459/20000] Train-Loss: 0.9821716438083788 Test-Loss: 0.2551179629710519 Train-f1: 0.2787382779198636 Test-f1: 0.26 Train-acc: 0.6125827814569537 Test-acc: 0.5921052631578947\n",
      "Epoch [2460/20000] Train-Loss: 0.9823195489461148 Test-Loss: 0.25515077211692844 Train-f1: 0.27442546477327295 Test-f1: 0.26108374384236455 Train-acc: 0.6125827814569537 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2461/20000] Train-Loss: 0.9816190164766072 Test-Loss: 0.2549838795398314 Train-f1: 0.2705053449951409 Test-f1: 0.26046511627906976 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "Epoch [2462/20000] Train-Loss: 0.9794188354765676 Test-Loss: 0.2550959578054828 Train-f1: 0.27288629737609327 Test-f1: 0.28374047806671926 Train-acc: 0.609271523178808 Test-acc: 0.6052631578947368\n",
      "Epoch [2463/20000] Train-Loss: 0.9775430804252321 Test-Loss: 0.2557935797768619 Train-f1: 0.27375169606512895 Test-f1: 0.2434188034188034 Train-acc: 0.6125827814569537 Test-acc: 0.5657894736842105\n",
      "best score_dx!!\n",
      "Epoch [2464/20000] Train-Loss: 0.9794931845337966 Test-Loss: 0.254470479089192 Train-f1: 0.2694541724150129 Test-f1: 0.26 Train-acc: 0.6026490066225165 Test-acc: 0.5921052631578947\n",
      "Epoch [2465/20000] Train-Loss: 0.977923244316815 Test-Loss: 0.2545940642504565 Train-f1: 0.2681243926141885 Test-f1: 0.28374047806671926 Train-acc: 0.6026490066225165 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2466/20000] Train-Loss: 0.9793177072410854 Test-Loss: 0.2543712178224999 Train-f1: 0.28800889985247524 Test-f1: 0.26046511627906976 Train-acc: 0.6225165562913907 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2467/20000] Train-Loss: 0.9778913527118026 Test-Loss: 0.2542986299291037 Train-f1: 0.27030693103567593 Test-f1: 0.26108374384236455 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2468/20000] Train-Loss: 0.9773524850091896 Test-Loss: 0.25419164419038953 Train-f1: 0.26994152046783626 Test-f1: 0.26108374384236455 Train-acc: 0.6059602649006622 Test-acc: 0.5921052631578947\n",
      "Epoch [2469/20000] Train-Loss: 0.9763823898881624 Test-Loss: 0.25434753059722504 Train-f1: 0.2736087147738421 Test-f1: 0.26186252771618623 Train-acc: 0.6125827814569537 Test-acc: 0.5921052631578947\n",
      "Epoch [2470/20000] Train-Loss: 0.9776291007425768 Test-Loss: 0.25432440169939613 Train-f1: 0.26631634261692066 Test-f1: 0.26186252771618623 Train-acc: 0.5993377483443708 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2471/20000] Train-Loss: 0.975325495914432 Test-Loss: 0.25357461464193054 Train-f1: 0.27837175398814973 Test-f1: 0.2841269841269841 Train-acc: 0.6125827814569537 Test-acc: 0.6052631578947368\n",
      "Epoch [2472/20000] Train-Loss: 0.9753328182014935 Test-Loss: 0.2537620363137576 Train-f1: 0.27572291392516113 Test-f1: 0.26186252771618623 Train-acc: 0.6158940397350994 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2473/20000] Train-Loss: 0.9751202216534227 Test-Loss: 0.253325946044686 Train-f1: 0.2722807017543859 Test-f1: 0.2841269841269841 Train-acc: 0.609271523178808 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2474/20000] Train-Loss: 0.9742764022056525 Test-Loss: 0.2532425408580304 Train-f1: 0.270119027082974 Test-f1: 0.283855421686747 Train-acc: 0.6059602649006622 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2475/20000] Train-Loss: 0.9742480909260475 Test-Loss: 0.2531163146021 Train-f1: 0.27586860177949185 Test-f1: 0.2841269841269841 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2476/20000] Train-Loss: 0.9736812393056073 Test-Loss: 0.25296103195690284 Train-f1: 0.2720998833779241 Test-f1: 0.2841269841269841 Train-acc: 0.609271523178808 Test-acc: 0.6052631578947368\n",
      "Epoch [2477/20000] Train-Loss: 0.9723176164871837 Test-Loss: 0.2534766860827107 Train-f1: 0.2760244233378562 Test-f1: 0.26186252771618623 Train-acc: 0.6158940397350994 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2478/20000] Train-Loss: 0.9710966533884335 Test-Loss: 0.2527020088518815 Train-f1: 0.27192940452166775 Test-f1: 0.2841269841269841 Train-acc: 0.609271523178808 Test-acc: 0.6052631578947368\n",
      "Epoch [2479/20000] Train-Loss: 0.9707708853135782 Test-Loss: 0.2527655397091511 Train-f1: 0.27247196825944453 Test-f1: 0.28374047806671926 Train-acc: 0.609271523178808 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2480/20000] Train-Loss: 0.9707060239533399 Test-Loss: 0.25246035355371493 Train-f1: 0.28034554151495683 Test-f1: 0.2841269841269841 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2481/20000] Train-Loss: 0.9705171082423741 Test-Loss: 0.2523194656536942 Train-f1: 0.26961726579334405 Test-f1: 0.2841269841269841 Train-acc: 0.6059602649006622 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2482/20000] Train-Loss: 0.9703068295705689 Test-Loss: 0.2522170146303405 Train-f1: 0.27837175398814973 Test-f1: 0.2841269841269841 Train-acc: 0.6125827814569537 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2483/20000] Train-Loss: 0.9684538886430216 Test-Loss: 0.2520782841511434 Train-f1: 0.2787382779198636 Test-f1: 0.283855421686747 Train-acc: 0.6125827814569537 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2484/20000] Train-Loss: 0.9678388152381466 Test-Loss: 0.25196571897193804 Train-f1: 0.27586860177949185 Test-f1: 0.2841269841269841 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2485/20000] Train-Loss: 0.9677710659643166 Test-Loss: 0.2518285792871805 Train-f1: 0.27424154324999145 Test-f1: 0.2841269841269841 Train-acc: 0.6125827814569537 Test-acc: 0.6052631578947368\n",
      "Epoch [2486/20000] Train-Loss: 0.9665074579947911 Test-Loss: 0.2519151405066891 Train-f1: 0.2787382779198636 Test-f1: 0.28374047806671926 Train-acc: 0.6125827814569537 Test-acc: 0.6052631578947368\n",
      "Epoch [2487/20000] Train-Loss: 0.9677977429450734 Test-Loss: 0.2521012375654527 Train-f1: 0.2736087147738421 Test-f1: 0.26186252771618623 Train-acc: 0.6125827814569537 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2488/20000] Train-Loss: 0.9661132910449746 Test-Loss: 0.2516190815846483 Train-f1: 0.2740680105051332 Test-f1: 0.2841269841269841 Train-acc: 0.6125827814569537 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2489/20000] Train-Loss: 0.9652862651641336 Test-Loss: 0.25142730572107835 Train-f1: 0.27621059831829153 Test-f1: 0.2841269841269841 Train-acc: 0.609271523178808 Test-acc: 0.6052631578947368\n",
      "Epoch [2490/20000] Train-Loss: 0.9631667148417786 Test-Loss: 0.2521019217659592 Train-f1: 0.27176915993042056 Test-f1: 0.26186252771618623 Train-acc: 0.609271523178808 Test-acc: 0.5921052631578947\n",
      "Epoch [2491/20000] Train-Loss: 0.9634268630091442 Test-Loss: 0.2514650023172409 Train-f1: 0.27989492808947486 Test-f1: 0.28512366186784793 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2492/20000] Train-Loss: 0.9633584917007697 Test-Loss: 0.2514225007819139 Train-f1: 0.28034554151495683 Test-f1: 0.2858611344734137 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2493/20000] Train-Loss: 0.9630045513925579 Test-Loss: 0.25085422295828236 Train-f1: 0.27147896879240163 Test-f1: 0.3059606848446418 Train-acc: 0.609271523178808 Test-acc: 0.618421052631579\n",
      "Epoch [2494/20000] Train-Loss: 0.9607745555404552 Test-Loss: 0.2523989336071078 Train-f1: 0.282307716582645 Test-f1: 0.25280898876404495 Train-acc: 0.6192052980132451 Test-acc: 0.5789473684210527\n",
      "Epoch [2495/20000] Train-Loss: 0.9603075160523077 Test-Loss: 0.25085945691104894 Train-f1: 0.27192940452166775 Test-f1: 0.29222222222222227 Train-acc: 0.609271523178808 Test-acc: 0.618421052631579\n",
      "Epoch [2496/20000] Train-Loss: 0.9605476915281504 Test-Loss: 0.25126157551461764 Train-f1: 0.2857117453543167 Test-f1: 0.2858611344734137 Train-acc: 0.6258278145695364 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2497/20000] Train-Loss: 0.9603847979502241 Test-Loss: 0.2503876540260633 Train-f1: 0.2740680105051332 Test-f1: 0.3059606848446418 Train-acc: 0.6125827814569537 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2498/20000] Train-Loss: 0.9590407869856991 Test-Loss: 0.25032386158813735 Train-f1: 0.27586860177949185 Test-f1: 0.2841269841269841 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2499/20000] Train-Loss: 0.9582122918184891 Test-Loss: 0.25014056298892223 Train-f1: 0.2801851611507679 Test-f1: 0.283728054671851 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2500/20000] Train-Loss: 0.9591835058594621 Test-Loss: 0.2500050163430358 Train-f1: 0.27586860177949185 Test-f1: 0.3059606848446418 Train-acc: 0.6158940397350994 Test-acc: 0.618421052631579\n",
      "Epoch [2501/20000] Train-Loss: 0.9588364173856094 Test-Loss: 0.2502109240330994 Train-f1: 0.28069733538349856 Test-f1: 0.29222222222222227 Train-acc: 0.6158940397350994 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2502/20000] Train-Loss: 0.9572527764208433 Test-Loss: 0.24999488594256103 Train-f1: 0.27976488268658417 Test-f1: 0.2841269841269841 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "Epoch [2503/20000] Train-Loss: 0.9555173930546402 Test-Loss: 0.2508615762099182 Train-f1: 0.28813857516257463 Test-f1: 0.2858611344734137 Train-acc: 0.6225165562913907 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2504/20000] Train-Loss: 0.9566562892288563 Test-Loss: 0.2497599409122093 Train-f1: 0.28416145871627085 Test-f1: 0.2841269841269841 Train-acc: 0.6158940397350994 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2505/20000] Train-Loss: 0.954493316142287 Test-Loss: 0.24951452490290857 Train-f1: 0.28215481509512463 Test-f1: 0.283728054671851 Train-acc: 0.6192052980132451 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2506/20000] Train-Loss: 0.9567932444082488 Test-Loss: 0.24930423952209632 Train-f1: 0.2863028229255774 Test-f1: 0.283728054671851 Train-acc: 0.6192052980132451 Test-acc: 0.6052631578947368\n",
      "Epoch [2507/20000] Train-Loss: 0.954745763036113 Test-Loss: 0.24940821460358467 Train-f1: 0.2818793981968998 Test-f1: 0.29222222222222227 Train-acc: 0.6192052980132451 Test-acc: 0.618421052631579\n",
      "Epoch [2508/20000] Train-Loss: 0.9513283949137232 Test-Loss: 0.2504784861865006 Train-f1: 0.2818793981968998 Test-f1: 0.2858611344734137 Train-acc: 0.6192052980132451 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2509/20000] Train-Loss: 0.9542257221687922 Test-Loss: 0.2489121194108795 Train-f1: 0.27192940452166775 Test-f1: 0.283855421686747 Train-acc: 0.609271523178808 Test-acc: 0.6052631578947368\n",
      "Epoch [2510/20000] Train-Loss: 0.9534331608118934 Test-Loss: 0.24891744060898252 Train-f1: 0.27976488268658417 Test-f1: 0.3059606848446418 Train-acc: 0.6158940397350994 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2511/20000] Train-Loss: 0.9519576304232494 Test-Loss: 0.24881383698410037 Train-f1: 0.27375169606512895 Test-f1: 0.29225111636459156 Train-acc: 0.6125827814569537 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2512/20000] Train-Loss: 0.9516774836985552 Test-Loss: 0.24855942845834922 Train-f1: 0.28827834339780434 Test-f1: 0.283855421686747 Train-acc: 0.6225165562913907 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2513/20000] Train-Loss: 0.9510258305179561 Test-Loss: 0.2484681419104599 Train-f1: 0.29038576617381057 Test-f1: 0.3059606848446418 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2514/20000] Train-Loss: 0.9503750461341984 Test-Loss: 0.24833967223745515 Train-f1: 0.28385382784266566 Test-f1: 0.283855421686747 Train-acc: 0.6225165562913907 Test-acc: 0.6052631578947368\n",
      "Epoch [2515/20000] Train-Loss: 0.9478456406997312 Test-Loss: 0.24870837520882755 Train-f1: 0.29258426584005803 Test-f1: 0.2679591836734694 Train-acc: 0.6225165562913907 Test-acc: 0.6052631578947368\n",
      "Epoch [2516/20000] Train-Loss: 0.9503041503449046 Test-Loss: 0.2483550952587452 Train-f1: 0.2926329571850465 Test-f1: 0.2679591836734694 Train-acc: 0.6291390728476821 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2517/20000] Train-Loss: 0.9482670981345938 Test-Loss: 0.24801003128528196 Train-f1: 0.2817566974088713 Test-f1: 0.283855421686747 Train-acc: 0.6192052980132451 Test-acc: 0.6052631578947368\n",
      "Epoch [2518/20000] Train-Loss: 0.9480806837309463 Test-Loss: 0.24804565807471624 Train-f1: 0.28034554151495683 Test-f1: 0.29225111636459156 Train-acc: 0.6158940397350994 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2519/20000] Train-Loss: 0.9476205938266143 Test-Loss: 0.24772765317463294 Train-f1: 0.2860637725940469 Test-f1: 0.283728054671851 Train-acc: 0.6258278145695364 Test-acc: 0.6052631578947368\n",
      "Epoch [2520/20000] Train-Loss: 0.9453541287836544 Test-Loss: 0.2478258837757178 Train-f1: 0.28662813102119455 Test-f1: 0.29225111636459156 Train-acc: 0.6192052980132451 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2521/20000] Train-Loss: 0.9475340002008528 Test-Loss: 0.24757145719415918 Train-f1: 0.29196093925853556 Test-f1: 0.283855421686747 Train-acc: 0.6291390728476821 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2522/20000] Train-Loss: 0.9455038014668519 Test-Loss: 0.24739842670734577 Train-f1: 0.2820120736978201 Test-f1: 0.283855421686747 Train-acc: 0.6192052980132451 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2523/20000] Train-Loss: 0.9436819923959194 Test-Loss: 0.24727459461198464 Train-f1: 0.2942462119053955 Test-f1: 0.3057366994722579 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "Epoch [2524/20000] Train-Loss: 0.9447237472224731 Test-Loss: 0.2473229784389041 Train-f1: 0.2960884003206229 Test-f1: 0.3059606848446418 Train-acc: 0.6291390728476821 Test-acc: 0.618421052631579\n",
      "Epoch [2525/20000] Train-Loss: 0.9438941183526051 Test-Loss: 0.24734044568420668 Train-f1: 0.2817566974088713 Test-f1: 0.3059606848446418 Train-acc: 0.6192052980132451 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2526/20000] Train-Loss: 0.9428414394310076 Test-Loss: 0.24702111003561156 Train-f1: 0.2818793981968998 Test-f1: 0.3059606848446418 Train-acc: 0.6192052980132451 Test-acc: 0.618421052631579\n",
      "Epoch [2527/20000] Train-Loss: 0.9423536393733211 Test-Loss: 0.24704112436936496 Train-f1: 0.30006505735319294 Test-f1: 0.3059606848446418 Train-acc: 0.6291390728476821 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2528/20000] Train-Loss: 0.9408419080552433 Test-Loss: 0.24693849998621784 Train-f1: 0.28215481509512463 Test-f1: 0.2679591836734694 Train-acc: 0.6192052980132451 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2529/20000] Train-Loss: 0.9428852873922605 Test-Loss: 0.24657436199578558 Train-f1: 0.2988650773641109 Test-f1: 0.283728054671851 Train-acc: 0.6324503311258278 Test-acc: 0.6052631578947368\n",
      "Epoch [2530/20000] Train-Loss: 0.9398846857012435 Test-Loss: 0.24672250274214463 Train-f1: 0.29807764390896924 Test-f1: 0.2679591836734694 Train-acc: 0.6258278145695364 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2531/20000] Train-Loss: 0.9399127325004897 Test-Loss: 0.2464062642167719 Train-f1: 0.2941094438609868 Test-f1: 0.283855421686747 Train-acc: 0.6258278145695364 Test-acc: 0.6052631578947368\n",
      "Epoch [2532/20000] Train-Loss: 0.939763337648227 Test-Loss: 0.24652306162888393 Train-f1: 0.2860637725940469 Test-f1: 0.3059606848446418 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "Epoch [2533/20000] Train-Loss: 0.9385920709677636 Test-Loss: 0.24670953701496895 Train-f1: 0.2901114488348531 Test-f1: 0.3059606848446418 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2534/20000] Train-Loss: 0.9383243337556794 Test-Loss: 0.24612074344490503 Train-f1: 0.2753457224175456 Test-f1: 0.3059606848446418 Train-acc: 0.6158940397350994 Test-acc: 0.618421052631579\n",
      "Epoch [2535/20000] Train-Loss: 0.9376145104110007 Test-Loss: 0.24678454852143558 Train-f1: 0.2897751469088088 Test-f1: 0.3075618712123474 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2536/20000] Train-Loss: 0.9373215441385457 Test-Loss: 0.24578651751627442 Train-f1: 0.28800889985247524 Test-f1: 0.283728054671851 Train-acc: 0.6225165562913907 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2537/20000] Train-Loss: 0.9369445124958342 Test-Loss: 0.24565845828840283 Train-f1: 0.2998077243342266 Test-f1: 0.283728054671851 Train-acc: 0.6291390728476821 Test-acc: 0.6052631578947368\n",
      "Epoch [2538/20000] Train-Loss: 0.9371835983001585 Test-Loss: 0.24589290632289185 Train-f1: 0.3041386340468909 Test-f1: 0.3059606848446418 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2539/20000] Train-Loss: 0.9364895116059342 Test-Loss: 0.24545832251756122 Train-f1: 0.2818793981968998 Test-f1: 0.283728054671851 Train-acc: 0.6192052980132451 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2540/20000] Train-Loss: 0.9363280476117338 Test-Loss: 0.24538231438745345 Train-f1: 0.30217850786820427 Test-f1: 0.3059606848446418 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "Epoch [2541/20000] Train-Loss: 0.9346006045807144 Test-Loss: 0.24599766483130991 Train-f1: 0.2839789196310935 Test-f1: 0.3063356064580055 Train-acc: 0.6225165562913907 Test-acc: 0.618421052631579\n",
      "Epoch [2542/20000] Train-Loss: 0.933603607399582 Test-Loss: 0.24590246640540386 Train-f1: 0.2941094438609868 Test-f1: 0.3063356064580055 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2543/20000] Train-Loss: 0.9342852999774116 Test-Loss: 0.24532376356981575 Train-f1: 0.27768378650553877 Test-f1: 0.3059606848446418 Train-acc: 0.6192052980132451 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2544/20000] Train-Loss: 0.9342331536967736 Test-Loss: 0.24488239088313907 Train-f1: 0.3058821087588211 Test-f1: 0.283728054671851 Train-acc: 0.6324503311258278 Test-acc: 0.6052631578947368\n",
      "Epoch [2545/20000] Train-Loss: 0.9328346075139096 Test-Loss: 0.24496114374739056 Train-f1: 0.3079946248691739 Test-f1: 0.3059606848446418 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2546/20000] Train-Loss: 0.9314279114601325 Test-Loss: 0.24466277773214246 Train-f1: 0.2960884003206229 Test-f1: 0.283728054671851 Train-acc: 0.6291390728476821 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2547/20000] Train-Loss: 0.9320766000973414 Test-Loss: 0.2445275579989822 Train-f1: 0.30217850786820427 Test-f1: 0.29225111636459156 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "Epoch [2548/20000] Train-Loss: 0.93079660603285 Test-Loss: 0.24459585646555165 Train-f1: 0.2902435367239836 Test-f1: 0.3059606848446418 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "Epoch [2549/20000] Train-Loss: 0.9291015533645951 Test-Loss: 0.2451418112601422 Train-f1: 0.3004026567040266 Test-f1: 0.2681518987341772 Train-acc: 0.6291390728476821 Test-acc: 0.6052631578947368\n",
      "Epoch [2550/20000] Train-Loss: 0.9277990090575357 Test-Loss: 0.24549857978637582 Train-f1: 0.2894813119755911 Test-f1: 0.3075618712123474 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "Epoch [2551/20000] Train-Loss: 0.9309249340637041 Test-Loss: 0.24469243013257005 Train-f1: 0.2732393420610943 Test-f1: 0.3059606848446418 Train-acc: 0.6125827814569537 Test-acc: 0.618421052631579\n",
      "Epoch [2552/20000] Train-Loss: 0.9291226741335777 Test-Loss: 0.24454749619810526 Train-f1: 0.3040105149382172 Test-f1: 0.3059606848446418 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2553/20000] Train-Loss: 0.9287613623352748 Test-Loss: 0.2439097985434784 Train-f1: 0.2841140350684082 Test-f1: 0.3057366994722579 Train-acc: 0.6225165562913907 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2554/20000] Train-Loss: 0.9287086736804904 Test-Loss: 0.24376064082637525 Train-f1: 0.31010485139482763 Test-f1: 0.29225111636459156 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2555/20000] Train-Loss: 0.9285561554185888 Test-Loss: 0.24366206245235547 Train-f1: 0.30636085626911314 Test-f1: 0.3143543238127525 Train-acc: 0.6390728476821192 Test-acc: 0.631578947368421\n",
      "Epoch [2556/20000] Train-Loss: 0.9255183379304868 Test-Loss: 0.24394957312163873 Train-f1: 0.301598866626189 Test-f1: 0.3059606848446418 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "Epoch [2557/20000] Train-Loss: 0.9258807725725386 Test-Loss: 0.2437016749530185 Train-f1: 0.2902435367239836 Test-f1: 0.29222222222222227 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2558/20000] Train-Loss: 0.9250918580828763 Test-Loss: 0.24333807763006512 Train-f1: 0.3134000302745793 Test-f1: 0.29225111636459156 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "Epoch [2559/20000] Train-Loss: 0.9238763598432856 Test-Loss: 0.24337100403837136 Train-f1: 0.2897751469088088 Test-f1: 0.29222222222222227 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2560/20000] Train-Loss: 0.925164981089719 Test-Loss: 0.24313666307999304 Train-f1: 0.3023782083287972 Test-f1: 0.305679012345679 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "Epoch [2561/20000] Train-Loss: 0.9217056506395579 Test-Loss: 0.24396752760960777 Train-f1: 0.31059902209266427 Test-f1: 0.2681518987341772 Train-acc: 0.6291390728476821 Test-acc: 0.6052631578947368\n",
      "Epoch [2562/20000] Train-Loss: 0.9250825967004559 Test-Loss: 0.24318067775832983 Train-f1: 0.29974079739234716 Test-f1: 0.3059606848446418 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2563/20000] Train-Loss: 0.9230847516021078 Test-Loss: 0.2435760877192373 Train-f1: 0.30828566200906626 Test-f1: 0.3059606848446418 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2564/20000] Train-Loss: 0.9209767507831502 Test-Loss: 0.24299619964111593 Train-f1: 0.29439319377381484 Test-f1: 0.2681518987341772 Train-acc: 0.6258278145695364 Test-acc: 0.6052631578947368\n",
      "Epoch [2565/20000] Train-Loss: 0.9206606242128665 Test-Loss: 0.2431072939091482 Train-f1: 0.27921257237386266 Test-f1: 0.3059606848446418 Train-acc: 0.6225165562913907 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2566/20000] Train-Loss: 0.9208351042430731 Test-Loss: 0.24251861865971225 Train-f1: 0.2963566348722228 Test-f1: 0.29222222222222227 Train-acc: 0.6291390728476821 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2567/20000] Train-Loss: 0.9203790069866978 Test-Loss: 0.2424373287897942 Train-f1: 0.2962450146312804 Test-f1: 0.3057366994722579 Train-acc: 0.6291390728476821 Test-acc: 0.618421052631579\n",
      "Epoch [2568/20000] Train-Loss: 0.9187695057573532 Test-Loss: 0.2431556768166214 Train-f1: 0.29375950228887815 Test-f1: 0.3059606848446418 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2569/20000] Train-Loss: 0.9197824685017966 Test-Loss: 0.24232415037243096 Train-f1: 0.3209190660363844 Test-f1: 0.3057366994722579 Train-acc: 0.6456953642384106 Test-acc: 0.618421052631579\n",
      "Epoch [2570/20000] Train-Loss: 0.9166104942996987 Test-Loss: 0.24333806729939664 Train-f1: 0.3114183401417444 Test-f1: 0.26847662141779793 Train-acc: 0.6357615894039735 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2571/20000] Train-Loss: 0.9191088767062234 Test-Loss: 0.24230432413945427 Train-f1: 0.2898773397855967 Test-f1: 0.2923333046298688 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2572/20000] Train-Loss: 0.9188057053058626 Test-Loss: 0.2418764079494069 Train-f1: 0.3076354439153703 Test-f1: 0.305679012345679 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2573/20000] Train-Loss: 0.9176569511120681 Test-Loss: 0.2418079278497741 Train-f1: 0.3038925518925519 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2574/20000] Train-Loss: 0.917148501220085 Test-Loss: 0.24163967974202125 Train-f1: 0.3316028708133971 Test-f1: 0.29225111636459156 Train-acc: 0.652317880794702 Test-acc: 0.618421052631579\n",
      "Epoch [2575/20000] Train-Loss: 0.9163759972293757 Test-Loss: 0.24193451165207233 Train-f1: 0.29982400245324214 Test-f1: 0.3059606848446418 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2576/20000] Train-Loss: 0.9143187400543539 Test-Loss: 0.24349246978999514 Train-f1: 0.3038925518925519 Test-f1: 0.3075618712123474 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2577/20000] Train-Loss: 0.9169975488766086 Test-Loss: 0.2413283590573781 Train-f1: 0.29676938880328707 Test-f1: 0.29225111636459156 Train-acc: 0.6291390728476821 Test-acc: 0.618421052631579\n",
      "Epoch [2578/20000] Train-Loss: 0.9133567981846767 Test-Loss: 0.2420885429832943 Train-f1: 0.3098453886667515 Test-f1: 0.3059606848446418 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2579/20000] Train-Loss: 0.9145024960101377 Test-Loss: 0.2412037940650595 Train-f1: 0.3058821087588211 Test-f1: 0.29222222222222227 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2580/20000] Train-Loss: 0.914809306575498 Test-Loss: 0.24112376388868278 Train-f1: 0.2981311975591152 Test-f1: 0.305679012345679 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "Epoch [2581/20000] Train-Loss: 0.9134605256626797 Test-Loss: 0.24232740990563847 Train-f1: 0.29982400245324214 Test-f1: 0.34541231126596983 Train-acc: 0.6357615894039735 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2582/20000] Train-Loss: 0.9151587796169308 Test-Loss: 0.24087983788295053 Train-f1: 0.30772459699001314 Test-f1: 0.305679012345679 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2583/20000] Train-Loss: 0.9130889310771684 Test-Loss: 0.24118525662776968 Train-f1: 0.2898773397855967 Test-f1: 0.3059606848446418 Train-acc: 0.6258278145695364 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2584/20000] Train-Loss: 0.9113270965030368 Test-Loss: 0.24062183897858794 Train-f1: 0.31379702497384343 Test-f1: 0.3142329227323628 Train-acc: 0.6390728476821192 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2585/20000] Train-Loss: 0.9118015414582462 Test-Loss: 0.2405166098341587 Train-f1: 0.30818350448851284 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2586/20000] Train-Loss: 0.9109776592319969 Test-Loss: 0.2406761699663056 Train-f1: 0.3000200343229533 Test-f1: 0.3057366994722579 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2587/20000] Train-Loss: 0.9106469917662061 Test-Loss: 0.24094563012173062 Train-f1: 0.29712823495202095 Test-f1: 0.3059606848446418 Train-acc: 0.6291390728476821 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2588/20000] Train-Loss: 0.9106250005063021 Test-Loss: 0.2402229510177516 Train-f1: 0.30447978180672697 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2589/20000] Train-Loss: 0.9085606416920516 Test-Loss: 0.24031074394010166 Train-f1: 0.3040000630365494 Test-f1: 0.305679012345679 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2590/20000] Train-Loss: 0.9083128588862787 Test-Loss: 0.2412972791765622 Train-f1: 0.30607929890240787 Test-f1: 0.34541231126596983 Train-acc: 0.6390728476821192 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2591/20000] Train-Loss: 0.9080052707778118 Test-Loss: 0.23993144705637648 Train-f1: 0.30772459699001314 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2592/20000] Train-Loss: 0.907412503185155 Test-Loss: 0.23984674398128983 Train-f1: 0.30022463084121237 Test-f1: 0.32492063492063494 Train-acc: 0.6291390728476821 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2593/20000] Train-Loss: 0.9076955787235109 Test-Loss: 0.23982766160826488 Train-f1: 0.3043445294189054 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2594/20000] Train-Loss: 0.9071296092051772 Test-Loss: 0.2397070905676013 Train-f1: 0.3040000630365494 Test-f1: 0.32492063492063494 Train-acc: 0.6357615894039735 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2595/20000] Train-Loss: 0.9054684005712169 Test-Loss: 0.23955406727721193 Train-f1: 0.3158174953373061 Test-f1: 0.32492063492063494 Train-acc: 0.6423841059602649 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2596/20000] Train-Loss: 0.9063297606731711 Test-Loss: 0.23946446296455853 Train-f1: 0.3076354439153703 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2597/20000] Train-Loss: 0.9056314776144769 Test-Loss: 0.23966442916550615 Train-f1: 0.3076017592514777 Test-f1: 0.2679591836734694 Train-acc: 0.6357615894039735 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2598/20000] Train-Loss: 0.9050337798160039 Test-Loss: 0.23925268393219706 Train-f1: 0.30359875403353664 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2599/20000] Train-Loss: 0.9029373122918632 Test-Loss: 0.23921514844669697 Train-f1: 0.30612692218102394 Test-f1: 0.29222222222222227 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2600/20000] Train-Loss: 0.9055930740854378 Test-Loss: 0.23908651668616326 Train-f1: 0.2939244005910672 Test-f1: 0.3142329227323628 Train-acc: 0.6324503311258278 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2601/20000] Train-Loss: 0.9028109469779243 Test-Loss: 0.23897486600783524 Train-f1: 0.318833462220807 Test-f1: 0.29222222222222227 Train-acc: 0.6423841059602649 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2602/20000] Train-Loss: 0.9029877727118931 Test-Loss: 0.23887669618530666 Train-f1: 0.29422970062239473 Test-f1: 0.32492063492063494 Train-acc: 0.6258278145695364 Test-acc: 0.631578947368421\n",
      "Epoch [2603/20000] Train-Loss: 0.9020632773508196 Test-Loss: 0.23899697187343152 Train-f1: 0.3188716125045713 Test-f1: 0.305679012345679 Train-acc: 0.6423841059602649 Test-acc: 0.618421052631579\n",
      "Epoch [2604/20000] Train-Loss: 0.9018200867659661 Test-Loss: 0.2388826833156366 Train-f1: 0.3174607388235381 Test-f1: 0.2679591836734694 Train-acc: 0.6456953642384106 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2605/20000] Train-Loss: 0.9015915963784101 Test-Loss: 0.23862701595203475 Train-f1: 0.30989648033126294 Test-f1: 0.32492063492063494 Train-acc: 0.6390728476821192 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2606/20000] Train-Loss: 0.901224285940588 Test-Loss: 0.23851437536412393 Train-f1: 0.31544017381912626 Test-f1: 0.29222222222222227 Train-acc: 0.6423841059602649 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2607/20000] Train-Loss: 0.8996610152735088 Test-Loss: 0.23839926383878546 Train-f1: 0.29822910342036024 Test-f1: 0.32492063492063494 Train-acc: 0.6324503311258278 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2608/20000] Train-Loss: 0.8992403853909824 Test-Loss: 0.238377066707818 Train-f1: 0.3243301435406699 Test-f1: 0.32492063492063494 Train-acc: 0.6456953642384106 Test-acc: 0.631578947368421\n",
      "Epoch [2609/20000] Train-Loss: 0.8988384870442243 Test-Loss: 0.23845317595738416 Train-f1: 0.3209863094617603 Test-f1: 0.305679012345679 Train-acc: 0.6456953642384106 Test-acc: 0.618421052631579\n",
      "Epoch [2610/20000] Train-Loss: 0.9001581794448676 Test-Loss: 0.24010833624039116 Train-f1: 0.3156812992547581 Test-f1: 0.34541231126596983 Train-acc: 0.6423841059602649 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2611/20000] Train-Loss: 0.8977596285778071 Test-Loss: 0.23819696665064005 Train-f1: 0.290335644223358 Test-f1: 0.32492063492063494 Train-acc: 0.6258278145695364 Test-acc: 0.631578947368421\n",
      "Epoch [2612/20000] Train-Loss: 0.8951720888795164 Test-Loss: 0.24035704865683666 Train-f1: 0.3043892773892774 Test-f1: 0.26742857142857146 Train-acc: 0.6291390728476821 Test-acc: 0.5921052631578947\n",
      "Epoch [2613/20000] Train-Loss: 0.8977888148534292 Test-Loss: 0.23858393714319073 Train-f1: 0.3038206456360342 Test-f1: 0.3059606848446418 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2614/20000] Train-Loss: 0.8969278942420177 Test-Loss: 0.23775554281059824 Train-f1: 0.31925336345167576 Test-f1: 0.32492063492063494 Train-acc: 0.6423841059602649 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2615/20000] Train-Loss: 0.8964072175822095 Test-Loss: 0.237645854410024 Train-f1: 0.30070640333148163 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2616/20000] Train-Loss: 0.8959703242035925 Test-Loss: 0.23774390304729748 Train-f1: 0.32400556864693514 Test-f1: 0.32492063492063494 Train-acc: 0.6456953642384106 Test-acc: 0.631578947368421\n",
      "Epoch [2617/20000] Train-Loss: 0.8939800153466689 Test-Loss: 0.23767188293367378 Train-f1: 0.31344666238324964 Test-f1: 0.2679591836734694 Train-acc: 0.6390728476821192 Test-acc: 0.6052631578947368\n",
      "Epoch [2618/20000] Train-Loss: 0.8955830430960212 Test-Loss: 0.23770768996436126 Train-f1: 0.3064294954269979 Test-f1: 0.29222222222222227 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2619/20000] Train-Loss: 0.8938951031778003 Test-Loss: 0.23732950973401187 Train-f1: 0.30598280779819637 Test-f1: 0.29222222222222227 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "Epoch [2620/20000] Train-Loss: 0.8917630087099556 Test-Loss: 0.23910754858562017 Train-f1: 0.30828356924018774 Test-f1: 0.34541231126596983 Train-acc: 0.6423841059602649 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2621/20000] Train-Loss: 0.8941157487100371 Test-Loss: 0.2370813885537037 Train-f1: 0.3243301435406699 Test-f1: 0.29222222222222227 Train-acc: 0.6456953642384106 Test-acc: 0.618421052631579\n",
      "Epoch [2622/20000] Train-Loss: 0.8922624145142988 Test-Loss: 0.23728380297823745 Train-f1: 0.28743681860301123 Test-f1: 0.29222222222222227 Train-acc: 0.6291390728476821 Test-acc: 0.618421052631579\n",
      "Epoch [2623/20000] Train-Loss: 0.8914345856214079 Test-Loss: 0.237689386950311 Train-f1: 0.3371658708460895 Test-f1: 0.2652307692307692 Train-acc: 0.652317880794702 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2624/20000] Train-Loss: 0.8927345516878455 Test-Loss: 0.23693895499031983 Train-f1: 0.3260618846694796 Test-f1: 0.32492063492063494 Train-acc: 0.6490066225165563 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2625/20000] Train-Loss: 0.8912449691266191 Test-Loss: 0.23683175667956316 Train-f1: 0.3101973781116512 Test-f1: 0.32492063492063494 Train-acc: 0.6456953642384106 Test-acc: 0.631578947368421\n",
      "Epoch [2626/20000] Train-Loss: 0.8905829693823494 Test-Loss: 0.23697115134851462 Train-f1: 0.3263601054237758 Test-f1: 0.3592821031344793 Train-acc: 0.6490066225165563 Test-acc: 0.6578947368421053\n",
      "Epoch [2627/20000] Train-Loss: 0.8890481713117795 Test-Loss: 0.23742170562522788 Train-f1: 0.3412893975845058 Test-f1: 0.30585365853658536 Train-acc: 0.6556291390728477 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2628/20000] Train-Loss: 0.8915771839002481 Test-Loss: 0.236695213033869 Train-f1: 0.3135729690869878 Test-f1: 0.30565217391304345 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2629/20000] Train-Loss: 0.8902951283838858 Test-Loss: 0.23646241821415057 Train-f1: 0.3063094179103931 Test-f1: 0.32492063492063494 Train-acc: 0.6390728476821192 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2630/20000] Train-Loss: 0.8899361068495303 Test-Loss: 0.23631228309704047 Train-f1: 0.3341018619934283 Test-f1: 0.32492063492063494 Train-acc: 0.652317880794702 Test-acc: 0.631578947368421\n",
      "Epoch [2631/20000] Train-Loss: 0.8880107160737414 Test-Loss: 0.23640272484233069 Train-f1: 0.3161777787985947 Test-f1: 0.32492063492063494 Train-acc: 0.6423841059602649 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2632/20000] Train-Loss: 0.8878555570192055 Test-Loss: 0.23620845017766282 Train-f1: 0.3156863578410539 Test-f1: 0.29222222222222227 Train-acc: 0.6423841059602649 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2633/20000] Train-Loss: 0.8861937413898432 Test-Loss: 0.2361724772813865 Train-f1: 0.3156684861032687 Test-f1: 0.2679591836734694 Train-acc: 0.6423841059602649 Test-acc: 0.6052631578947368\n",
      "Epoch [2634/20000] Train-Loss: 0.8867724159149121 Test-Loss: 0.2364859610214442 Train-f1: 0.2899314116011031 Test-f1: 0.34272727272727266 Train-acc: 0.6324503311258278 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2635/20000] Train-Loss: 0.8867229102555538 Test-Loss: 0.23590462546872687 Train-f1: 0.3159144474665485 Test-f1: 0.32492063492063494 Train-acc: 0.6423841059602649 Test-acc: 0.631578947368421\n",
      "Epoch [2636/20000] Train-Loss: 0.8846176169225237 Test-Loss: 0.23617938841524477 Train-f1: 0.3175481244547834 Test-f1: 0.2932045303131246 Train-acc: 0.6456953642384106 Test-acc: 0.618421052631579\n",
      "Epoch [2637/20000] Train-Loss: 0.8849817667544295 Test-Loss: 0.23661958646238102 Train-f1: 0.31665905979005016 Test-f1: 0.3429540886905228 Train-acc: 0.6423841059602649 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2638/20000] Train-Loss: 0.8863000341152317 Test-Loss: 0.23556439562535406 Train-f1: 0.31952583735692924 Test-f1: 0.29222222222222227 Train-acc: 0.6423841059602649 Test-acc: 0.618421052631579\n",
      "Epoch [2639/20000] Train-Loss: 0.8854840003301468 Test-Loss: 0.23624023991699922 Train-f1: 0.30187983482550224 Test-f1: 0.37476190476190474 Train-acc: 0.6390728476821192 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2640/20000] Train-Loss: 0.884797200855281 Test-Loss: 0.2354267665514241 Train-f1: 0.3299355212634934 Test-f1: 0.32492063492063494 Train-acc: 0.6456953642384106 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2641/20000] Train-Loss: 0.884695030839356 Test-Loss: 0.23530674198048268 Train-f1: 0.3311784771233818 Test-f1: 0.30565217391304345 Train-acc: 0.652317880794702 Test-acc: 0.618421052631579\n",
      "Epoch [2642/20000] Train-Loss: 0.8829707792126249 Test-Loss: 0.2353452839681283 Train-f1: 0.31742594190394013 Test-f1: 0.32492063492063494 Train-acc: 0.6456953642384106 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2643/20000] Train-Loss: 0.8815723245040233 Test-Loss: 0.23522019148825746 Train-f1: 0.31892523010204854 Test-f1: 0.2679591836734694 Train-acc: 0.6423841059602649 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2644/20000] Train-Loss: 0.8841876843480649 Test-Loss: 0.2351062455228819 Train-f1: 0.30462902520031854 Test-f1: 0.29222222222222227 Train-acc: 0.6357615894039735 Test-acc: 0.618421052631579\n",
      "Epoch [2645/20000] Train-Loss: 0.8806380156401714 Test-Loss: 0.23572333206294485 Train-f1: 0.33394246729712035 Test-f1: 0.3592821031344793 Train-acc: 0.652317880794702 Test-acc: 0.6578947368421053\n",
      "Epoch [2646/20000] Train-Loss: 0.880190394564631 Test-Loss: 0.23552208390601523 Train-f1: 0.3174276158623985 Test-f1: 0.2652307692307692 Train-acc: 0.6456953642384106 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2647/20000] Train-Loss: 0.8816021098668746 Test-Loss: 0.2350774698774454 Train-f1: 0.3122563390335924 Test-f1: 0.2679591836734694 Train-acc: 0.6357615894039735 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2648/20000] Train-Loss: 0.8809557879465347 Test-Loss: 0.2347080592190864 Train-f1: 0.29913160717021603 Test-f1: 0.29222222222222227 Train-acc: 0.6324503311258278 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2649/20000] Train-Loss: 0.8807568440821724 Test-Loss: 0.23463264550024737 Train-f1: 0.31831729675741427 Test-f1: 0.32492063492063494 Train-acc: 0.6357615894039735 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2650/20000] Train-Loss: 0.8788550125560711 Test-Loss: 0.2345855185451286 Train-f1: 0.316731843575419 Test-f1: 0.29222222222222227 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "Epoch [2651/20000] Train-Loss: 0.8784030633141876 Test-Loss: 0.23518126737496936 Train-f1: 0.299625680824419 Test-f1: 0.3592821031344793 Train-acc: 0.6324503311258278 Test-acc: 0.6578947368421053\n",
      "Epoch [2652/20000] Train-Loss: 0.8800967107766218 Test-Loss: 0.23459890627685057 Train-f1: 0.3242308151120654 Test-f1: 0.3592821031344793 Train-acc: 0.6456953642384106 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2653/20000] Train-Loss: 0.8774030043725385 Test-Loss: 0.234543440691015 Train-f1: 0.32160287081339717 Test-f1: 0.29222222222222227 Train-acc: 0.6456953642384106 Test-acc: 0.618421052631579\n",
      "Epoch [2654/20000] Train-Loss: 0.8782974942914954 Test-Loss: 0.23488380514588292 Train-f1: 0.32382218796751916 Test-f1: 0.37476190476190474 Train-acc: 0.6456953642384106 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2655/20000] Train-Loss: 0.8771898114531063 Test-Loss: 0.2342458210701877 Train-f1: 0.30646084328842954 Test-f1: 0.32492063492063494 Train-acc: 0.6390728476821192 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2656/20000] Train-Loss: 0.8777260301636953 Test-Loss: 0.23404265354771803 Train-f1: 0.3103615580302729 Test-f1: 0.30565217391304345 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "Epoch [2657/20000] Train-Loss: 0.87504296223918 Test-Loss: 0.23485620279122582 Train-f1: 0.3091448103435485 Test-f1: 0.3733170731707317 Train-acc: 0.6423841059602649 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2658/20000] Train-Loss: 0.8764681244365309 Test-Loss: 0.2339795544669023 Train-f1: 0.34590289421885256 Test-f1: 0.34272727272727266 Train-acc: 0.6589403973509934 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2659/20000] Train-Loss: 0.8753606897018608 Test-Loss: 0.233820687272214 Train-f1: 0.3321700834141146 Test-f1: 0.28466965285554313 Train-acc: 0.6490066225165563 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2660/20000] Train-Loss: 0.8752297421859588 Test-Loss: 0.23380087701927235 Train-f1: 0.30625118951431907 Test-f1: 0.30565217391304345 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "Epoch [2661/20000] Train-Loss: 0.8736916710112799 Test-Loss: 0.23490642006442133 Train-f1: 0.3413872452333991 Test-f1: 0.2693877551020408 Train-acc: 0.652317880794702 Test-acc: 0.5921052631578947\n",
      "Epoch [2662/20000] Train-Loss: 0.8751029854920535 Test-Loss: 0.2341111563718887 Train-f1: 0.3144684705975028 Test-f1: 0.3733170731707317 Train-acc: 0.6456953642384106 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2663/20000] Train-Loss: 0.8744218782453829 Test-Loss: 0.23368804130230916 Train-f1: 0.32079793465033496 Test-f1: 0.32492063492063494 Train-acc: 0.6456953642384106 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2664/20000] Train-Loss: 0.8739550305461041 Test-Loss: 0.23367444754130445 Train-f1: 0.3432024255395042 Test-f1: 0.32492063492063494 Train-acc: 0.6589403973509934 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2665/20000] Train-Loss: 0.8741688279158341 Test-Loss: 0.23343253525568083 Train-f1: 0.3117941766217628 Test-f1: 0.32492063492063494 Train-acc: 0.6423841059602649 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2666/20000] Train-Loss: 0.8719536343200283 Test-Loss: 0.2332597698858429 Train-f1: 0.32071307296804685 Test-f1: 0.30565217391304345 Train-acc: 0.6456953642384106 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2667/20000] Train-Loss: 0.8719944949865173 Test-Loss: 0.23319368571904533 Train-f1: 0.3280923492910875 Test-f1: 0.32492063492063494 Train-acc: 0.652317880794702 Test-acc: 0.631578947368421\n",
      "Epoch [2668/20000] Train-Loss: 0.8713026561349501 Test-Loss: 0.23353763492569754 Train-f1: 0.32108847091605713 Test-f1: 0.3733170731707317 Train-acc: 0.6456953642384106 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2669/20000] Train-Loss: 0.8719885782924821 Test-Loss: 0.2330823797982648 Train-f1: 0.33461159030874177 Test-f1: 0.2679591836734694 Train-acc: 0.6456953642384106 Test-acc: 0.6052631578947368\n",
      "Epoch [2670/20000] Train-Loss: 0.8704196782413646 Test-Loss: 0.23344303073012335 Train-f1: 0.32799889326969633 Test-f1: 0.3592821031344793 Train-acc: 0.652317880794702 Test-acc: 0.6578947368421053\n",
      "Epoch [2671/20000] Train-Loss: 0.8693961170101153 Test-Loss: 0.23310148135947464 Train-f1: 0.29309941520467836 Test-f1: 0.3592821031344793 Train-acc: 0.6225165562913907 Test-acc: 0.6578947368421053\n",
      "Epoch [2672/20000] Train-Loss: 0.8707505425803436 Test-Loss: 0.23317612906187907 Train-f1: 0.3360793517218346 Test-f1: 0.3592821031344793 Train-acc: 0.6556291390728477 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2673/20000] Train-Loss: 0.8703594764507497 Test-Loss: 0.23268018724550907 Train-f1: 0.33448640373259464 Test-f1: 0.30565217391304345 Train-acc: 0.6490066225165563 Test-acc: 0.618421052631579\n",
      "Epoch [2674/20000] Train-Loss: 0.8689519644527168 Test-Loss: 0.23325409367274128 Train-f1: 0.3228471528471529 Test-f1: 0.2671899529042386 Train-acc: 0.6490066225165563 Test-acc: 0.5921052631578947\n",
      "Epoch [2675/20000] Train-Loss: 0.8686583568696801 Test-Loss: 0.23283340931311297 Train-f1: 0.3194497621610729 Test-f1: 0.3592821031344793 Train-acc: 0.6490066225165563 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2676/20000] Train-Loss: 0.8683409213657501 Test-Loss: 0.23243678775398194 Train-f1: 0.32108847091605713 Test-f1: 0.30565217391304345 Train-acc: 0.6456953642384106 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2677/20000] Train-Loss: 0.8677030889326407 Test-Loss: 0.2323795535754909 Train-f1: 0.3410669608863887 Test-f1: 0.32492063492063494 Train-acc: 0.6556291390728477 Test-acc: 0.631578947368421\n",
      "Epoch [2678/20000] Train-Loss: 0.8691409874380053 Test-Loss: 0.23253885814274813 Train-f1: 0.3403446698185146 Test-f1: 0.3592821031344793 Train-acc: 0.6622516556291391 Test-acc: 0.6578947368421053\n",
      "Epoch [2679/20000] Train-Loss: 0.8682947384723567 Test-Loss: 0.23297565796967612 Train-f1: 0.32613091078499884 Test-f1: 0.3876923076923077 Train-acc: 0.6490066225165563 Test-acc: 0.6842105263157895\n",
      "Epoch [2680/20000] Train-Loss: 0.8640645083688683 Test-Loss: 0.23304793898602094 Train-f1: 0.32485714285714284 Test-f1: 0.2693877551020408 Train-acc: 0.6423841059602649 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2681/20000] Train-Loss: 0.8690286912139557 Test-Loss: 0.23216142183209793 Train-f1: 0.33381678217842314 Test-f1: 0.28466965285554313 Train-acc: 0.652317880794702 Test-acc: 0.6052631578947368\n",
      "Epoch [2682/20000] Train-Loss: 0.8657894250761328 Test-Loss: 0.2329210676560379 Train-f1: 0.30805923464151314 Test-f1: 0.3876923076923077 Train-acc: 0.6423841059602649 Test-acc: 0.6842105263157895\n",
      "Epoch [2683/20000] Train-Loss: 0.8673992987319685 Test-Loss: 0.23286790017637457 Train-f1: 0.31683266017048844 Test-f1: 0.3733170731707317 Train-acc: 0.6390728476821192 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2684/20000] Train-Loss: 0.8655402254266453 Test-Loss: 0.23190343950859857 Train-f1: 0.3244186577733108 Test-f1: 0.3592821031344793 Train-acc: 0.6456953642384106 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2685/20000] Train-Loss: 0.8641930640336936 Test-Loss: 0.23183495297372095 Train-f1: 0.33646925973696346 Test-f1: 0.3592821031344793 Train-acc: 0.652317880794702 Test-acc: 0.6578947368421053\n",
      "Epoch [2686/20000] Train-Loss: 0.8632708247644341 Test-Loss: 0.23218596878739498 Train-f1: 0.33381678217842314 Test-f1: 0.3592821031344793 Train-acc: 0.652317880794702 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2687/20000] Train-Loss: 0.8645422188858691 Test-Loss: 0.23164490954841632 Train-f1: 0.3101681435203199 Test-f1: 0.30565217391304345 Train-acc: 0.6390728476821192 Test-acc: 0.618421052631579\n",
      "Epoch [2688/20000] Train-Loss: 0.8613226257944632 Test-Loss: 0.23204544661359436 Train-f1: 0.33895525503169305 Test-f1: 0.281895409554984 Train-acc: 0.652317880794702 Test-acc: 0.5921052631578947\n",
      "Epoch [2689/20000] Train-Loss: 0.8619853874830451 Test-Loss: 0.23195562876975845 Train-f1: 0.3318827250137154 Test-f1: 0.3733170731707317 Train-acc: 0.6490066225165563 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2690/20000] Train-Loss: 0.8622514877953368 Test-Loss: 0.23140232346029102 Train-f1: 0.3242301408304407 Test-f1: 0.30565217391304345 Train-acc: 0.6456953642384106 Test-acc: 0.618421052631579\n",
      "Epoch [2691/20000] Train-Loss: 0.8595941396689596 Test-Loss: 0.23310690456173058 Train-f1: 0.3209917366571916 Test-f1: 0.3876923076923077 Train-acc: 0.6456953642384106 Test-acc: 0.6842105263157895\n",
      "Epoch [2692/20000] Train-Loss: 0.8630659805808746 Test-Loss: 0.23177979788330877 Train-f1: 0.34329352090554766 Test-f1: 0.3592821031344793 Train-acc: 0.6556291390728477 Test-acc: 0.6578947368421053\n",
      "Epoch [2693/20000] Train-Loss: 0.8631507779218693 Test-Loss: 0.23160379824600988 Train-f1: 0.3311834441019103 Test-f1: 0.3592821031344793 Train-acc: 0.652317880794702 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2694/20000] Train-Loss: 0.8624202238759343 Test-Loss: 0.23109344442960555 Train-f1: 0.3390318582780491 Test-f1: 0.30565217391304345 Train-acc: 0.652317880794702 Test-acc: 0.618421052631579\n",
      "Epoch [2695/20000] Train-Loss: 0.8607050470766594 Test-Loss: 0.2321514892782012 Train-f1: 0.3237459836615955 Test-f1: 0.3876923076923077 Train-acc: 0.6456953642384106 Test-acc: 0.6842105263157895\n",
      "Epoch [2696/20000] Train-Loss: 0.8616699324254558 Test-Loss: 0.2312157725987337 Train-f1: 0.31914053477789084 Test-f1: 0.3592821031344793 Train-acc: 0.6423841059602649 Test-acc: 0.6578947368421053\n",
      "Epoch [2697/20000] Train-Loss: 0.8581503063547519 Test-Loss: 0.23112507794666348 Train-f1: 0.3311784771233818 Test-f1: 0.28814123917388407 Train-acc: 0.652317880794702 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2698/20000] Train-Loss: 0.8605079086588983 Test-Loss: 0.23094736788886677 Train-f1: 0.3260085172280294 Test-f1: 0.3592821031344793 Train-acc: 0.6490066225165563 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2699/20000] Train-Loss: 0.8593620999227282 Test-Loss: 0.2307898590414056 Train-f1: 0.3363229735268668 Test-f1: 0.3592821031344793 Train-acc: 0.652317880794702 Test-acc: 0.6578947368421053\n",
      "Epoch [2700/20000] Train-Loss: 0.8584638459761422 Test-Loss: 0.2312962875464314 Train-f1: 0.33166977447515145 Test-f1: 0.2693877551020408 Train-acc: 0.6490066225165563 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2701/20000] Train-Loss: 0.8582398181946446 Test-Loss: 0.2307255066840064 Train-f1: 0.3291805625352156 Test-f1: 0.3733170731707317 Train-acc: 0.6490066225165563 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2702/20000] Train-Loss: 0.8582873396897474 Test-Loss: 0.23067440136165088 Train-f1: 0.33106682738751675 Test-f1: 0.29222222222222227 Train-acc: 0.652317880794702 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2703/20000] Train-Loss: 0.8589461336644106 Test-Loss: 0.23064676462455205 Train-f1: 0.3296422329231318 Test-f1: 0.3592821031344793 Train-acc: 0.6456953642384106 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2704/20000] Train-Loss: 0.8574268298109847 Test-Loss: 0.23061719566925037 Train-f1: 0.3474109224109224 Test-f1: 0.30565217391304345 Train-acc: 0.6622516556291391 Test-acc: 0.618421052631579\n",
      "Epoch [2705/20000] Train-Loss: 0.8573080790159963 Test-Loss: 0.23064123192312963 Train-f1: 0.3069826481813863 Test-f1: 0.3733170731707317 Train-acc: 0.6390728476821192 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2706/20000] Train-Loss: 0.857133039734146 Test-Loss: 0.2304323982696701 Train-f1: 0.3360639731896591 Test-f1: 0.3592821031344793 Train-acc: 0.6556291390728477 Test-acc: 0.6578947368421053\n",
      "Epoch [2707/20000] Train-Loss: 0.8563007420993272 Test-Loss: 0.23099309912326862 Train-f1: 0.33121015890975586 Test-f1: 0.3876923076923077 Train-acc: 0.652317880794702 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2708/20000] Train-Loss: 0.8563871492704044 Test-Loss: 0.2301837802475667 Train-f1: 0.3388034016590401 Test-f1: 0.3592821031344793 Train-acc: 0.6556291390728477 Test-acc: 0.6578947368421053\n",
      "Epoch [2709/20000] Train-Loss: 0.854879415639535 Test-Loss: 0.2302092228286715 Train-f1: 0.3310226775722781 Test-f1: 0.3733170731707317 Train-acc: 0.652317880794702 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2710/20000] Train-Loss: 0.8572993623592093 Test-Loss: 0.2299954055531678 Train-f1: 0.33381678217842314 Test-f1: 0.3592821031344793 Train-acc: 0.652317880794702 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2711/20000] Train-Loss: 0.8546274378002665 Test-Loss: 0.2299384840102803 Train-f1: 0.32644825414785106 Test-f1: 0.3733170731707317 Train-acc: 0.6490066225165563 Test-acc: 0.6710526315789473\n",
      "Epoch [2712/20000] Train-Loss: 0.854477457635974 Test-Loss: 0.23077537757475594 Train-f1: 0.3212153672694151 Test-f1: 0.3762678062678063 Train-acc: 0.6456953642384106 Test-acc: 0.6710526315789473\n",
      "Epoch [2713/20000] Train-Loss: 0.8529905903282576 Test-Loss: 0.2303017360386068 Train-f1: 0.3410070536993891 Test-f1: 0.29047619047619044 Train-acc: 0.652317880794702 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2714/20000] Train-Loss: 0.8525913887214283 Test-Loss: 0.22988849931619126 Train-f1: 0.3406770528712599 Test-f1: 0.3592821031344793 Train-acc: 0.6589403973509934 Test-acc: 0.6578947368421053\n",
      "Epoch [2715/20000] Train-Loss: 0.851760293436995 Test-Loss: 0.23096377596572137 Train-f1: 0.3272457054176404 Test-f1: 0.3876923076923077 Train-acc: 0.6456953642384106 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2716/20000] Train-Loss: 0.8508133929196465 Test-Loss: 0.22978497029217712 Train-f1: 0.33159527794804805 Test-f1: 0.3592821031344793 Train-acc: 0.6490066225165563 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2717/20000] Train-Loss: 0.8510610139820127 Test-Loss: 0.22948536555699284 Train-f1: 0.32594378585939765 Test-f1: 0.3592821031344793 Train-acc: 0.6490066225165563 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2718/20000] Train-Loss: 0.851549092520391 Test-Loss: 0.22942325916214826 Train-f1: 0.3430349429470888 Test-f1: 0.32492063492063494 Train-acc: 0.6556291390728477 Test-acc: 0.631578947368421\n",
      "Epoch [2719/20000] Train-Loss: 0.8510390350222884 Test-Loss: 0.22946219052814612 Train-f1: 0.33801825560595444 Test-f1: 0.35274752232361883 Train-acc: 0.6589403973509934 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2720/20000] Train-Loss: 0.8527453396638537 Test-Loss: 0.2293792004334238 Train-f1: 0.32608833879649624 Test-f1: 0.3876923076923077 Train-acc: 0.6490066225165563 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2721/20000] Train-Loss: 0.8508622303830933 Test-Loss: 0.22924468182797306 Train-f1: 0.3519580173291665 Test-f1: 0.3592821031344793 Train-acc: 0.6622516556291391 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2722/20000] Train-Loss: 0.8498171631677586 Test-Loss: 0.229134107424144 Train-f1: 0.343153169548074 Test-f1: 0.30565217391304345 Train-acc: 0.6589403973509934 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2723/20000] Train-Loss: 0.8499997068796756 Test-Loss: 0.22910189161104874 Train-f1: 0.33509566030512367 Test-f1: 0.3592821031344793 Train-acc: 0.6589403973509934 Test-acc: 0.6578947368421053\n",
      "Epoch [2724/20000] Train-Loss: 0.8471353389376957 Test-Loss: 0.22952535975357524 Train-f1: 0.3407425907425908 Test-f1: 0.3876923076923077 Train-acc: 0.6556291390728477 Test-acc: 0.6842105263157895\n",
      "Epoch [2725/20000] Train-Loss: 0.8485080022059295 Test-Loss: 0.2296196047042093 Train-f1: 0.3477175697865353 Test-f1: 0.3876923076923077 Train-acc: 0.6622516556291391 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2726/20000] Train-Loss: 0.8497600138783896 Test-Loss: 0.22894158682066676 Train-f1: 0.33663678730920965 Test-f1: 0.3592821031344793 Train-acc: 0.6490066225165563 Test-acc: 0.6578947368421053\n",
      "Epoch [2727/20000] Train-Loss: 0.8475621830821425 Test-Loss: 0.22902962956144338 Train-f1: 0.35403725920484 Test-f1: 0.2892307692307692 Train-acc: 0.6655629139072847 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2728/20000] Train-Loss: 0.8474263549454564 Test-Loss: 0.22873057087717025 Train-f1: 0.32848954966602023 Test-f1: 0.34272727272727266 Train-acc: 0.652317880794702 Test-acc: 0.6447368421052632\n",
      "Epoch [2729/20000] Train-Loss: 0.849254199126117 Test-Loss: 0.22873702783855074 Train-f1: 0.33626747562457676 Test-f1: 0.28466965285554313 Train-acc: 0.652317880794702 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2730/20000] Train-Loss: 0.8481476429747569 Test-Loss: 0.22855897785148646 Train-f1: 0.3581156424231034 Test-f1: 0.30565217391304345 Train-acc: 0.6688741721854304 Test-acc: 0.618421052631579\n",
      "Epoch [2731/20000] Train-Loss: 0.8471424762784866 Test-Loss: 0.22857059284457432 Train-f1: 0.32291580400276054 Test-f1: 0.30565217391304345 Train-acc: 0.6490066225165563 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2732/20000] Train-Loss: 0.8469260074478504 Test-Loss: 0.2284827782981984 Train-f1: 0.35397219998015683 Test-f1: 0.3592821031344793 Train-acc: 0.6688741721854304 Test-acc: 0.6578947368421053\n",
      "Epoch [2733/20000] Train-Loss: 0.845732995257482 Test-Loss: 0.22850826163722562 Train-f1: 0.338595700799594 Test-f1: 0.3008360196032182 Train-acc: 0.6556291390728477 Test-acc: 0.6052631578947368\n",
      "Epoch [2734/20000] Train-Loss: 0.8457550949675853 Test-Loss: 0.228490740937508 Train-f1: 0.328051580073438 Test-f1: 0.28070562887153244 Train-acc: 0.652317880794702 Test-acc: 0.5921052631578947\n",
      "Epoch [2735/20000] Train-Loss: 0.8457458823265844 Test-Loss: 0.2286701429234712 Train-f1: 0.33832545480590176 Test-f1: 0.3876923076923077 Train-acc: 0.6556291390728477 Test-acc: 0.6842105263157895\n",
      "Epoch [2736/20000] Train-Loss: 0.8450548927651294 Test-Loss: 0.22861117220641056 Train-f1: 0.33798781228552355 Test-f1: 0.3876923076923077 Train-acc: 0.6589403973509934 Test-acc: 0.6842105263157895\n",
      "Epoch [2737/20000] Train-Loss: 0.8456998693243329 Test-Loss: 0.22911171176882494 Train-f1: 0.36024609984123346 Test-f1: 0.3876923076923077 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2738/20000] Train-Loss: 0.8442843422989856 Test-Loss: 0.22805854250368693 Train-f1: 0.33643761301989156 Test-f1: 0.3592821031344793 Train-acc: 0.652317880794702 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2739/20000] Train-Loss: 0.8440970036192517 Test-Loss: 0.2279767332207954 Train-f1: 0.32907535320651854 Test-f1: 0.32492063492063494 Train-acc: 0.6490066225165563 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2740/20000] Train-Loss: 0.8434112952882642 Test-Loss: 0.2279305564386945 Train-f1: 0.34759761399251843 Test-f1: 0.3592821031344793 Train-acc: 0.6622516556291391 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2741/20000] Train-Loss: 0.8425661976348914 Test-Loss: 0.22792044565023797 Train-f1: 0.32901783432181064 Test-f1: 0.32492063492063494 Train-acc: 0.6490066225165563 Test-acc: 0.631578947368421\n",
      "Epoch [2742/20000] Train-Loss: 0.843482057286792 Test-Loss: 0.22806758635190605 Train-f1: 0.3363109944486463 Test-f1: 0.2892307692307692 Train-acc: 0.6456953642384106 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2743/20000] Train-Loss: 0.8431600103650109 Test-Loss: 0.22772365772637493 Train-f1: 0.3452661847989743 Test-f1: 0.32492063492063494 Train-acc: 0.6589403973509934 Test-acc: 0.631578947368421\n",
      "Epoch [2744/20000] Train-Loss: 0.8412941211207802 Test-Loss: 0.22816169369947767 Train-f1: 0.3474981153216283 Test-f1: 0.3733170731707317 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2745/20000] Train-Loss: 0.8423736865537289 Test-Loss: 0.22761452853614209 Train-f1: 0.32691842837648954 Test-f1: 0.3592821031344793 Train-acc: 0.6456953642384106 Test-acc: 0.6578947368421053\n",
      "Epoch [2746/20000] Train-Loss: 0.8401080429238986 Test-Loss: 0.22776423442963215 Train-f1: 0.3304839431921006 Test-f1: 0.3876923076923077 Train-acc: 0.6556291390728477 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2747/20000] Train-Loss: 0.8408444186067863 Test-Loss: 0.2275125950530316 Train-f1: 0.3516076734732689 Test-f1: 0.3580952380952381 Train-acc: 0.6655629139072847 Test-acc: 0.6578947368421053\n",
      "Epoch [2748/20000] Train-Loss: 0.8401710894019556 Test-Loss: 0.2276391923497294 Train-f1: 0.34301531801531804 Test-f1: 0.3876923076923077 Train-acc: 0.6589403973509934 Test-acc: 0.6842105263157895\n",
      "Epoch [2749/20000] Train-Loss: 0.8403289726194592 Test-Loss: 0.22753976999734649 Train-f1: 0.343153169548074 Test-f1: 0.3876923076923077 Train-acc: 0.6589403973509934 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2750/20000] Train-Loss: 0.8404521980723201 Test-Loss: 0.22741244344408987 Train-f1: 0.3292666425033225 Test-f1: 0.3592821031344793 Train-acc: 0.6490066225165563 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2751/20000] Train-Loss: 0.8405053692182006 Test-Loss: 0.22735753359638536 Train-f1: 0.3363974791182514 Test-f1: 0.3876923076923077 Train-acc: 0.6490066225165563 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2752/20000] Train-Loss: 0.84033985500458 Test-Loss: 0.22723212732083442 Train-f1: 0.34753334950278925 Test-f1: 0.35274752232361883 Train-acc: 0.6589403973509934 Test-acc: 0.6447368421052632\n",
      "Epoch [2753/20000] Train-Loss: 0.8397292966876718 Test-Loss: 0.2275493663653834 Train-f1: 0.3382554065482663 Test-f1: 0.3876923076923077 Train-acc: 0.6556291390728477 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2754/20000] Train-Loss: 0.8394801773539489 Test-Loss: 0.22707263238382283 Train-f1: 0.351700420027213 Test-f1: 0.28466965285554313 Train-acc: 0.6622516556291391 Test-acc: 0.6052631578947368\n",
      "Epoch [2755/20000] Train-Loss: 0.838198111572281 Test-Loss: 0.22719078188431643 Train-f1: 0.3406177804398049 Test-f1: 0.2671899529042386 Train-acc: 0.652317880794702 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2756/20000] Train-Loss: 0.8385269364032796 Test-Loss: 0.2269022033818471 Train-f1: 0.3472199957180343 Test-f1: 0.32492063492063494 Train-acc: 0.6622516556291391 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2757/20000] Train-Loss: 0.8372837961881644 Test-Loss: 0.2268760156566253 Train-f1: 0.3515705769277563 Test-f1: 0.32492063492063494 Train-acc: 0.6655629139072847 Test-acc: 0.631578947368421\n",
      "Epoch [2758/20000] Train-Loss: 0.8362245436512882 Test-Loss: 0.22712377467799977 Train-f1: 0.3333333333333333 Test-f1: 0.3026697177726926 Train-acc: 0.6556291390728477 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2759/20000] Train-Loss: 0.8384361917018733 Test-Loss: 0.22673680292122972 Train-f1: 0.3513653483992467 Test-f1: 0.3592821031344793 Train-acc: 0.6655629139072847 Test-acc: 0.6578947368421053\n",
      "Epoch [2760/20000] Train-Loss: 0.8367864611081705 Test-Loss: 0.22691672978274072 Train-f1: 0.32238956740051217 Test-f1: 0.2892307692307692 Train-acc: 0.6423841059602649 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2761/20000] Train-Loss: 0.8359299187715763 Test-Loss: 0.22669040417743191 Train-f1: 0.33840391817919907 Test-f1: 0.319411839918169 Train-acc: 0.6556291390728477 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2762/20000] Train-Loss: 0.83593779925037 Test-Loss: 0.2265303066088217 Train-f1: 0.3518065268065268 Test-f1: 0.3592821031344793 Train-acc: 0.6655629139072847 Test-acc: 0.6578947368421053\n",
      "Epoch [2763/20000] Train-Loss: 0.8347518598045773 Test-Loss: 0.22667350253134347 Train-f1: 0.34528505971132956 Test-f1: 0.3876923076923077 Train-acc: 0.6589403973509934 Test-acc: 0.6842105263157895\n",
      "Epoch [2764/20000] Train-Loss: 0.8342073455907817 Test-Loss: 0.22782640297574538 Train-f1: 0.33790448103547144 Test-f1: 0.3876923076923077 Train-acc: 0.6589403973509934 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2765/20000] Train-Loss: 0.8353415980074852 Test-Loss: 0.2264425216497345 Train-f1: 0.3317400874401123 Test-f1: 0.3876923076923077 Train-acc: 0.6490066225165563 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2766/20000] Train-Loss: 0.8339424637421832 Test-Loss: 0.22636929516863707 Train-f1: 0.35199394961932207 Test-f1: 0.28070562887153244 Train-acc: 0.6688741721854304 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2767/20000] Train-Loss: 0.8339707514424995 Test-Loss: 0.226261701489117 Train-f1: 0.34300261810047916 Test-f1: 0.34272727272727266 Train-acc: 0.6589403973509934 Test-acc: 0.6447368421052632\n",
      "Epoch [2768/20000] Train-Loss: 0.834121802261101 Test-Loss: 0.22644296043733572 Train-f1: 0.35181680091364315 Test-f1: 0.3506772297469972 Train-acc: 0.6688741721854304 Test-acc: 0.6447368421052632\n",
      "Epoch [2769/20000] Train-Loss: 0.8330161977017507 Test-Loss: 0.22697664183957397 Train-f1: 0.3535908073839269 Test-f1: 0.28469253615073764 Train-acc: 0.6655629139072847 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2770/20000] Train-Loss: 0.8346763305333144 Test-Loss: 0.22620220160522356 Train-f1: 0.34291202129935583 Test-f1: 0.3592821031344793 Train-acc: 0.6589403973509934 Test-acc: 0.6578947368421053\n",
      "Epoch [2771/20000] Train-Loss: 0.8340692151810774 Test-Loss: 0.2266068715254624 Train-f1: 0.34943661096864853 Test-f1: 0.3876923076923077 Train-acc: 0.6622516556291391 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2772/20000] Train-Loss: 0.8334349536353982 Test-Loss: 0.2260339195274821 Train-f1: 0.3497430075188275 Test-f1: 0.3876923076923077 Train-acc: 0.6655629139072847 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2773/20000] Train-Loss: 0.8321156448674852 Test-Loss: 0.22594872227444285 Train-f1: 0.3624828934506354 Test-f1: 0.3876923076923077 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "Epoch [2774/20000] Train-Loss: 0.8316708497625922 Test-Loss: 0.22610066662665057 Train-f1: 0.37199570455384406 Test-f1: 0.3876923076923077 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [2775/20000] Train-Loss: 0.8324342425992617 Test-Loss: 0.22607529274934537 Train-f1: 0.3558850867732131 Test-f1: 0.3876923076923077 Train-acc: 0.6688741721854304 Test-acc: 0.6842105263157895\n",
      "Epoch [2776/20000] Train-Loss: 0.829696000137914 Test-Loss: 0.22747039542539138 Train-f1: 0.34703947737655605 Test-f1: 0.3762678062678063 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "Epoch [2777/20000] Train-Loss: 0.8308082977052307 Test-Loss: 0.22711650344881085 Train-f1: 0.34517415278168917 Test-f1: 0.3762678062678063 Train-acc: 0.6589403973509934 Test-acc: 0.6710526315789473\n",
      "Epoch [2778/20000] Train-Loss: 0.8310897369827235 Test-Loss: 0.22627328231418473 Train-f1: 0.35182231757951404 Test-f1: 0.28469253615073764 Train-acc: 0.6622516556291391 Test-acc: 0.5921052631578947\n",
      "best score_dx!!\n",
      "Epoch [2779/20000] Train-Loss: 0.8315868097038145 Test-Loss: 0.2256973649785973 Train-f1: 0.3552242967602175 Test-f1: 0.3733170731707317 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "Epoch [2780/20000] Train-Loss: 0.8296707366023184 Test-Loss: 0.22580014963481337 Train-f1: 0.33288741741607386 Test-f1: 0.3876923076923077 Train-acc: 0.6556291390728477 Test-acc: 0.6842105263157895\n",
      "Epoch [2781/20000] Train-Loss: 0.8291956620309696 Test-Loss: 0.22579426927450585 Train-f1: 0.34301531801531804 Test-f1: 0.28469253615073764 Train-acc: 0.6589403973509934 Test-acc: 0.5921052631578947\n",
      "Epoch [2782/20000] Train-Loss: 0.8285752773094404 Test-Loss: 0.2258607916940427 Train-f1: 0.3535533631813199 Test-f1: 0.3762678062678063 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2783/20000] Train-Loss: 0.8304581245319969 Test-Loss: 0.22548218303170017 Train-f1: 0.35563247344933213 Test-f1: 0.3876923076923077 Train-acc: 0.6655629139072847 Test-acc: 0.6842105263157895\n",
      "Epoch [2784/20000] Train-Loss: 0.8285026491008134 Test-Loss: 0.22560184363563962 Train-f1: 0.35361486815028054 Test-f1: 0.3876923076923077 Train-acc: 0.6688741721854304 Test-acc: 0.6842105263157895\n",
      "Epoch [2785/20000] Train-Loss: 0.8282557431601921 Test-Loss: 0.2258552528505179 Train-f1: 0.3581818181818182 Test-f1: 0.3876923076923077 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2786/20000] Train-Loss: 0.8284237878732236 Test-Loss: 0.22525445209250197 Train-f1: 0.3520448109897549 Test-f1: 0.37923076923076926 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [2787/20000] Train-Loss: 0.827828468276952 Test-Loss: 0.22569632319455254 Train-f1: 0.34500015000096357 Test-f1: 0.3762678062678063 Train-acc: 0.6622516556291391 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2788/20000] Train-Loss: 0.8288762594828009 Test-Loss: 0.22514123877762274 Train-f1: 0.3645945722910155 Test-f1: 0.3876923076923077 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2789/20000] Train-Loss: 0.8270792635615207 Test-Loss: 0.22522553458930825 Train-f1: 0.35935853166307946 Test-f1: 0.3876923076923077 Train-acc: 0.6688741721854304 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2790/20000] Train-Loss: 0.8272200049622277 Test-Loss: 0.2249810174037319 Train-f1: 0.3254113106637029 Test-f1: 0.3876923076923077 Train-acc: 0.652317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [2791/20000] Train-Loss: 0.8258537559404229 Test-Loss: 0.2253040907263321 Train-f1: 0.37756329203323047 Test-f1: 0.3026697177726926 Train-acc: 0.6854304635761589 Test-acc: 0.6052631578947368\n",
      "Epoch [2792/20000] Train-Loss: 0.8265012209963568 Test-Loss: 0.22503725530906415 Train-f1: 0.34488749786832784 Test-f1: 0.3762678062678063 Train-acc: 0.6622516556291391 Test-acc: 0.6710526315789473\n",
      "Epoch [2793/20000] Train-Loss: 0.8265878230938252 Test-Loss: 0.22504979139496176 Train-f1: 0.3403442978971344 Test-f1: 0.37701754385964914 Train-acc: 0.6589403973509934 Test-acc: 0.6710526315789473\n",
      "Epoch [2794/20000] Train-Loss: 0.8241901567358633 Test-Loss: 0.22590376845331692 Train-f1: 0.34686714924419837 Test-f1: 0.3762678062678063 Train-acc: 0.6589403973509934 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2795/20000] Train-Loss: 0.8245391462463648 Test-Loss: 0.22470086716514234 Train-f1: 0.37216875694864343 Test-f1: 0.3876923076923077 Train-acc: 0.6788079470198676 Test-acc: 0.6842105263157895\n",
      "Epoch [2796/20000] Train-Loss: 0.8250719497738463 Test-Loss: 0.22474124865486492 Train-f1: 0.35534589758233387 Test-f1: 0.3644599303135888 Train-acc: 0.6655629139072847 Test-acc: 0.6578947368421053\n",
      "Epoch [2797/20000] Train-Loss: 0.8250054123640144 Test-Loss: 0.2248282888752513 Train-f1: 0.3470155468852537 Test-f1: 0.3876923076923077 Train-acc: 0.6655629139072847 Test-acc: 0.6842105263157895\n",
      "Epoch [2798/20000] Train-Loss: 0.8254852194887161 Test-Loss: 0.22474315964941208 Train-f1: 0.34893854229572163 Test-f1: 0.3876923076923077 Train-acc: 0.6589403973509934 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2799/20000] Train-Loss: 0.8243354122127845 Test-Loss: 0.2244153551542135 Train-f1: 0.3545354521297099 Test-f1: 0.32492063492063494 Train-acc: 0.6622516556291391 Test-acc: 0.631578947368421\n",
      "Epoch [2800/20000] Train-Loss: 0.8246320669738014 Test-Loss: 0.22448361168114855 Train-f1: 0.3675456653824775 Test-f1: 0.3354285714285714 Train-acc: 0.6754966887417219 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2801/20000] Train-Loss: 0.8240209167040188 Test-Loss: 0.22438822914675574 Train-f1: 0.33642623754416034 Test-f1: 0.3733170731707317 Train-acc: 0.652317880794702 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2802/20000] Train-Loss: 0.8234872869964419 Test-Loss: 0.22433123557719611 Train-f1: 0.34735044418743566 Test-f1: 0.3733170731707317 Train-acc: 0.6622516556291391 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2803/20000] Train-Loss: 0.8234582439312359 Test-Loss: 0.22424581492444545 Train-f1: 0.35796308954203687 Test-f1: 0.3876923076923077 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2804/20000] Train-Loss: 0.825004813931707 Test-Loss: 0.22452902469503572 Train-f1: 0.35336571559995444 Test-f1: 0.3876923076923077 Train-acc: 0.6655629139072847 Test-acc: 0.6842105263157895\n",
      "Epoch [2805/20000] Train-Loss: 0.8224948018552427 Test-Loss: 0.22437063096004217 Train-f1: 0.335643866378499 Test-f1: 0.3876923076923077 Train-acc: 0.6556291390728477 Test-acc: 0.6842105263157895\n",
      "Epoch [2806/20000] Train-Loss: 0.8211794951989052 Test-Loss: 0.22458654835139474 Train-f1: 0.3491940868685055 Test-f1: 0.3762678062678063 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2807/20000] Train-Loss: 0.8207820215664807 Test-Loss: 0.22417633933085537 Train-f1: 0.3430386355725722 Test-f1: 0.3876923076923077 Train-acc: 0.6490066225165563 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2808/20000] Train-Loss: 0.8210865532306223 Test-Loss: 0.2239334436228939 Train-f1: 0.34979662376623455 Test-f1: 0.35781081346285937 Train-acc: 0.6589403973509934 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2809/20000] Train-Loss: 0.8212647294123476 Test-Loss: 0.2239166176396069 Train-f1: 0.36976219586203635 Test-f1: 0.3191824600520253 Train-acc: 0.6754966887417219 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2810/20000] Train-Loss: 0.8212017692281371 Test-Loss: 0.2238714640421914 Train-f1: 0.3602406292511403 Test-f1: 0.32492063492063494 Train-acc: 0.6754966887417219 Test-acc: 0.631578947368421\n",
      "Epoch [2811/20000] Train-Loss: 0.8208640885398552 Test-Loss: 0.22394859260741418 Train-f1: 0.3680118395229873 Test-f1: 0.3733170731707317 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2812/20000] Train-Loss: 0.8197969975693047 Test-Loss: 0.2237204234775615 Train-f1: 0.3660344686759781 Test-f1: 0.30589931074245874 Train-acc: 0.6721854304635762 Test-acc: 0.618421052631579\n",
      "Epoch [2813/20000] Train-Loss: 0.8191285394560989 Test-Loss: 0.22372158668631847 Train-f1: 0.3515282262243588 Test-f1: 0.37177666421260125 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [2814/20000] Train-Loss: 0.8184440324403005 Test-Loss: 0.2244854486621879 Train-f1: 0.3613847141939628 Test-f1: 0.3876923076923077 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2815/20000] Train-Loss: 0.8198355379116933 Test-Loss: 0.2239992532405516 Train-f1: 0.3449464525850197 Test-f1: 0.3762678062678063 Train-acc: 0.6622516556291391 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2816/20000] Train-Loss: 0.8200685728552174 Test-Loss: 0.22359908117681415 Train-f1: 0.36951796019938127 Test-f1: 0.3858556024378809 Train-acc: 0.6788079470198676 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2817/20000] Train-Loss: 0.818501787526953 Test-Loss: 0.22349159412506195 Train-f1: 0.3673458975823339 Test-f1: 0.3858556024378809 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "Epoch [2818/20000] Train-Loss: 0.8190101527724173 Test-Loss: 0.2236000415951322 Train-f1: 0.3755047878215961 Test-f1: 0.27164339419978517 Train-acc: 0.6821192052980133 Test-acc: 0.5921052631578947\n",
      "Epoch [2819/20000] Train-Loss: 0.8178766834648018 Test-Loss: 0.22350112803785915 Train-f1: 0.3425711024408093 Test-f1: 0.3858556024378809 Train-acc: 0.6622516556291391 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2820/20000] Train-Loss: 0.8178284223416197 Test-Loss: 0.22331457207982022 Train-f1: 0.3551399594027298 Test-f1: 0.35781081346285937 Train-acc: 0.6655629139072847 Test-acc: 0.6578947368421053\n",
      "Epoch [2821/20000] Train-Loss: 0.8177875221703031 Test-Loss: 0.2234554645948601 Train-f1: 0.3668583031918269 Test-f1: 0.3876923076923077 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "Epoch [2822/20000] Train-Loss: 0.8172642519434613 Test-Loss: 0.22339998216059517 Train-f1: 0.35379135879217233 Test-f1: 0.31946198830409356 Train-acc: 0.6688741721854304 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2823/20000] Train-Loss: 0.8165440466624722 Test-Loss: 0.2232030167989715 Train-f1: 0.35584604006357295 Test-f1: 0.3858556024378809 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2824/20000] Train-Loss: 0.816567396009282 Test-Loss: 0.2232797526528253 Train-f1: 0.363826750657671 Test-f1: 0.36375990438670264 Train-acc: 0.6721854304635762 Test-acc: 0.6578947368421053\n",
      "Epoch [2825/20000] Train-Loss: 0.8166715171943777 Test-Loss: 0.22328692521911891 Train-f1: 0.3645945722910155 Test-f1: 0.3747160012982798 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2826/20000] Train-Loss: 0.8160681862950582 Test-Loss: 0.22300031168611006 Train-f1: 0.3573696145124717 Test-f1: 0.34171711612548156 Train-acc: 0.6688741721854304 Test-acc: 0.6447368421052632\n",
      "Epoch [2827/20000] Train-Loss: 0.8152162765368282 Test-Loss: 0.2234500443674936 Train-f1: 0.3535790787831001 Test-f1: 0.3762678062678063 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [2828/20000] Train-Loss: 0.8157656294227236 Test-Loss: 0.22311089240005277 Train-f1: 0.3556108290933816 Test-f1: 0.3747160012982798 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [2829/20000] Train-Loss: 0.8131724086888602 Test-Loss: 0.22374304779778054 Train-f1: 0.3635137686755873 Test-f1: 0.30365217391304344 Train-acc: 0.6688741721854304 Test-acc: 0.6052631578947368\n",
      "Epoch [2830/20000] Train-Loss: 0.8146907179022767 Test-Loss: 0.223215887079818 Train-f1: 0.3548775483703415 Test-f1: 0.3876923076923077 Train-acc: 0.6655629139072847 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2831/20000] Train-Loss: 0.8137209568538268 Test-Loss: 0.2227768746096299 Train-f1: 0.3602406292511403 Test-f1: 0.33589743589743587 Train-acc: 0.6754966887417219 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2832/20000] Train-Loss: 0.8146301562521541 Test-Loss: 0.2227391207820488 Train-f1: 0.36147776683584576 Test-f1: 0.37177666421260125 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2833/20000] Train-Loss: 0.8149102390524315 Test-Loss: 0.22368565441156701 Train-f1: 0.35592869603071203 Test-f1: 0.30365217391304344 Train-acc: 0.6721854304635762 Test-acc: 0.6052631578947368\n",
      "Epoch [2834/20000] Train-Loss: 0.8144041535469735 Test-Loss: 0.2227677412110514 Train-f1: 0.36301839486274207 Test-f1: 0.31946198830409356 Train-acc: 0.6721854304635762 Test-acc: 0.618421052631579\n",
      "Epoch [2835/20000] Train-Loss: 0.813874195016294 Test-Loss: 0.22273973995642496 Train-f1: 0.3573031844733371 Test-f1: 0.33522267206477735 Train-acc: 0.6688741721854304 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2836/20000] Train-Loss: 0.812488762481327 Test-Loss: 0.22250892479831852 Train-f1: 0.35826707354576204 Test-f1: 0.37177666421260125 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [2837/20000] Train-Loss: 0.8116984876938312 Test-Loss: 0.2225138671368726 Train-f1: 0.3429473684210526 Test-f1: 0.3858556024378809 Train-acc: 0.6556291390728477 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2838/20000] Train-Loss: 0.8123164306718506 Test-Loss: 0.2224761636711832 Train-f1: 0.3574520981310535 Test-f1: 0.372913803496082 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [2839/20000] Train-Loss: 0.8119084085559168 Test-Loss: 0.22252824870469012 Train-f1: 0.36272926540774375 Test-f1: 0.3858556024378809 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2840/20000] Train-Loss: 0.8130433814428006 Test-Loss: 0.22289359996957772 Train-f1: 0.36540646406976796 Test-f1: 0.3762678062678063 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2841/20000] Train-Loss: 0.8115131418950752 Test-Loss: 0.2226529543752487 Train-f1: 0.3596662501301677 Test-f1: 0.3858556024378809 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2842/20000] Train-Loss: 0.8126080255030208 Test-Loss: 0.22222644803926922 Train-f1: 0.35549423644271416 Test-f1: 0.3858556024378809 Train-acc: 0.6688741721854304 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2843/20000] Train-Loss: 0.8123739733149378 Test-Loss: 0.22217019254949177 Train-f1: 0.3566552293811157 Test-f1: 0.34171711612548156 Train-acc: 0.6655629139072847 Test-acc: 0.6447368421052632\n",
      "Epoch [2844/20000] Train-Loss: 0.8106457653707427 Test-Loss: 0.22231484114260078 Train-f1: 0.3794570529852964 Test-f1: 0.31946198830409356 Train-acc: 0.6854304635761589 Test-acc: 0.618421052631579\n",
      "Epoch [2845/20000] Train-Loss: 0.8102946976678721 Test-Loss: 0.22231974540436575 Train-f1: 0.3595942744691395 Test-f1: 0.372913803496082 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2846/20000] Train-Loss: 0.8091198151292412 Test-Loss: 0.2221201203357933 Train-f1: 0.34712458757760783 Test-f1: 0.372913803496082 Train-acc: 0.6622516556291391 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2847/20000] Train-Loss: 0.8095893360533518 Test-Loss: 0.2220649021580403 Train-f1: 0.34050843577734746 Test-f1: 0.372913803496082 Train-acc: 0.6556291390728477 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2848/20000] Train-Loss: 0.8096722357954577 Test-Loss: 0.22192912371552412 Train-f1: 0.35328798185941046 Test-f1: 0.372913803496082 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2849/20000] Train-Loss: 0.8084042238152811 Test-Loss: 0.22189210829341108 Train-f1: 0.37162995369304375 Test-f1: 0.35155635062611806 Train-acc: 0.6821192052980133 Test-acc: 0.6447368421052632\n",
      "Epoch [2850/20000] Train-Loss: 0.8095447970842012 Test-Loss: 0.22210980361025945 Train-f1: 0.35500712038416954 Test-f1: 0.31946198830409356 Train-acc: 0.6655629139072847 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2851/20000] Train-Loss: 0.8099973008688149 Test-Loss: 0.22178786732688238 Train-f1: 0.3513935891804273 Test-f1: 0.3654367312903898 Train-acc: 0.6688741721854304 Test-acc: 0.6578947368421053\n",
      "Epoch [2852/20000] Train-Loss: 0.8080394913631569 Test-Loss: 0.2218055088293605 Train-f1: 0.35270965133090587 Test-f1: 0.3858556024378809 Train-acc: 0.6622516556291391 Test-acc: 0.6842105263157895\n",
      "Epoch [2853/20000] Train-Loss: 0.8089389485571983 Test-Loss: 0.22201393621974108 Train-f1: 0.36802853398988145 Test-f1: 0.3858556024378809 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2854/20000] Train-Loss: 0.8079170958598609 Test-Loss: 0.22165552814415598 Train-f1: 0.36193897740289493 Test-f1: 0.372913803496082 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2855/20000] Train-Loss: 0.8084396567323668 Test-Loss: 0.22160684176898773 Train-f1: 0.3637060257749913 Test-f1: 0.3654367312903898 Train-acc: 0.6754966887417219 Test-acc: 0.6578947368421053\n",
      "Epoch [2856/20000] Train-Loss: 0.8078606122849096 Test-Loss: 0.22220967386987706 Train-f1: 0.35356371354945837 Test-f1: 0.3876923076923077 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2857/20000] Train-Loss: 0.8085257451142355 Test-Loss: 0.22166284384820484 Train-f1: 0.364847298142558 Test-f1: 0.3026697177726926 Train-acc: 0.6721854304635762 Test-acc: 0.6052631578947368\n",
      "Epoch [2858/20000] Train-Loss: 0.8068315838177229 Test-Loss: 0.2216255387433293 Train-f1: 0.3490845758463957 Test-f1: 0.37701754385964914 Train-acc: 0.6622516556291391 Test-acc: 0.6710526315789473\n",
      "Epoch [2859/20000] Train-Loss: 0.806173926247966 Test-Loss: 0.22162913429965575 Train-f1: 0.36171845255711427 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2860/20000] Train-Loss: 0.806173372195779 Test-Loss: 0.2214656302896364 Train-f1: 0.3427780807411377 Test-f1: 0.3665708937355796 Train-acc: 0.6589403973509934 Test-acc: 0.6578947368421053\n",
      "Epoch [2861/20000] Train-Loss: 0.8048153050899198 Test-Loss: 0.22206088931711632 Train-f1: 0.3557092519883217 Test-f1: 0.3747160012982798 Train-acc: 0.6622516556291391 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2862/20000] Train-Loss: 0.8064437846074207 Test-Loss: 0.2213735726910227 Train-f1: 0.3667316964616868 Test-f1: 0.3858556024378809 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "Epoch [2863/20000] Train-Loss: 0.8039525229031059 Test-Loss: 0.22275548149598484 Train-f1: 0.37495264916079396 Test-f1: 0.3206006006006006 Train-acc: 0.6821192052980133 Test-acc: 0.618421052631579\n",
      "Epoch [2864/20000] Train-Loss: 0.8071418550672544 Test-Loss: 0.22153397302359015 Train-f1: 0.3704319622585876 Test-f1: 0.2933913043478261 Train-acc: 0.6788079470198676 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2865/20000] Train-Loss: 0.80485313990867 Test-Loss: 0.22136974108742666 Train-f1: 0.3612209715039042 Test-f1: 0.31946198830409356 Train-acc: 0.6721854304635762 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2866/20000] Train-Loss: 0.805542341905554 Test-Loss: 0.22112873317656945 Train-f1: 0.3627980206927576 Test-f1: 0.37923076923076926 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2867/20000] Train-Loss: 0.8049552920451978 Test-Loss: 0.22142130122719075 Train-f1: 0.35981573157315727 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2868/20000] Train-Loss: 0.8048802014649279 Test-Loss: 0.22108576938823563 Train-f1: 0.35549423644271416 Test-f1: 0.3858556024378809 Train-acc: 0.6688741721854304 Test-acc: 0.6842105263157895\n",
      "Epoch [2869/20000] Train-Loss: 0.8018967092282862 Test-Loss: 0.22254797740752638 Train-f1: 0.3770257547306682 Test-f1: 0.3762678062678063 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [2870/20000] Train-Loss: 0.8044557077210345 Test-Loss: 0.22132394012698597 Train-f1: 0.37282950967161493 Test-f1: 0.3747160012982798 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2871/20000] Train-Loss: 0.8025734115488592 Test-Loss: 0.2209785617106493 Train-f1: 0.37180108411789237 Test-f1: 0.37701754385964914 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2872/20000] Train-Loss: 0.8017571716394044 Test-Loss: 0.22219680312340287 Train-f1: 0.3595187969924812 Test-f1: 0.29504504504504503 Train-acc: 0.6721854304635762 Test-acc: 0.6052631578947368\n",
      "Epoch [2873/20000] Train-Loss: 0.8039347606318467 Test-Loss: 0.222130823263454 Train-f1: 0.37313363520260073 Test-f1: 0.3762678062678063 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [2874/20000] Train-Loss: 0.8036650178996001 Test-Loss: 0.22150485199538536 Train-f1: 0.3745062595062595 Test-f1: 0.30365217391304344 Train-acc: 0.6788079470198676 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2875/20000] Train-Loss: 0.8044399231424919 Test-Loss: 0.22073786729413417 Train-f1: 0.3380669889108257 Test-f1: 0.3663003663003663 Train-acc: 0.6589403973509934 Test-acc: 0.6578947368421053\n",
      "Epoch [2876/20000] Train-Loss: 0.8016741424981356 Test-Loss: 0.22203122422887547 Train-f1: 0.3513578476491717 Test-f1: 0.3747160012982798 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "Epoch [2877/20000] Train-Loss: 0.8028573882196498 Test-Loss: 0.2208438711693261 Train-f1: 0.3554673973107386 Test-f1: 0.3747160012982798 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [2878/20000] Train-Loss: 0.8008714781017033 Test-Loss: 0.22093813668078172 Train-f1: 0.36799150237536626 Test-f1: 0.37632258064516133 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2879/20000] Train-Loss: 0.8018782358606901 Test-Loss: 0.22070540706819755 Train-f1: 0.3632893772893773 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2880/20000] Train-Loss: 0.8004004539372804 Test-Loss: 0.2205522181074681 Train-f1: 0.374025373255155 Test-f1: 0.37923076923076926 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2881/20000] Train-Loss: 0.8021367350582214 Test-Loss: 0.22083172542205887 Train-f1: 0.35539825168957573 Test-f1: 0.3858556024378809 Train-acc: 0.6688741721854304 Test-acc: 0.6842105263157895\n",
      "Epoch [2882/20000] Train-Loss: 0.8010069911517511 Test-Loss: 0.22061837244381383 Train-f1: 0.37796891585298836 Test-f1: 0.36352052545156 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2883/20000] Train-Loss: 0.8000972501693324 Test-Loss: 0.2204643693131607 Train-f1: 0.36043714087716905 Test-f1: 0.36375990438670264 Train-acc: 0.6688741721854304 Test-acc: 0.6578947368421053\n",
      "Epoch [2884/20000] Train-Loss: 0.8004502371234279 Test-Loss: 0.22090648370317273 Train-f1: 0.3572255805702274 Test-f1: 0.2933913043478261 Train-acc: 0.6688741721854304 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2885/20000] Train-Loss: 0.8002614909124979 Test-Loss: 0.2203777702425545 Train-f1: 0.3616328475618601 Test-f1: 0.37701754385964914 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2886/20000] Train-Loss: 0.8005083018285911 Test-Loss: 0.22031837894559853 Train-f1: 0.3633217430571235 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2887/20000] Train-Loss: 0.7995116610780434 Test-Loss: 0.22023809640066935 Train-f1: 0.3628037694145049 Test-f1: 0.3779802955665025 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2888/20000] Train-Loss: 0.7994998079682479 Test-Loss: 0.22044294735330647 Train-f1: 0.3714661465750017 Test-f1: 0.31993162393162394 Train-acc: 0.6788079470198676 Test-acc: 0.618421052631579\n",
      "Epoch [2889/20000] Train-Loss: 0.7982963508513224 Test-Loss: 0.2220717943962835 Train-f1: 0.35754367598977416 Test-f1: 0.3762678062678063 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2890/20000] Train-Loss: 0.7969686746495896 Test-Loss: 0.2206349412955418 Train-f1: 0.3536415937419513 Test-f1: 0.36352052545156 Train-acc: 0.6622516556291391 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2891/20000] Train-Loss: 0.8003229675181811 Test-Loss: 0.22010919898697504 Train-f1: 0.36257654133159456 Test-f1: 0.3858556024378809 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2892/20000] Train-Loss: 0.7989300153374598 Test-Loss: 0.22048597514131693 Train-f1: 0.3691495294267154 Test-f1: 0.36352052545156 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "Epoch [2893/20000] Train-Loss: 0.7982967277447729 Test-Loss: 0.22082707374532842 Train-f1: 0.3594078778738801 Test-f1: 0.3747160012982798 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2894/20000] Train-Loss: 0.7982394766750369 Test-Loss: 0.22012945247031052 Train-f1: 0.36765935703640623 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2895/20000] Train-Loss: 0.7977796706722696 Test-Loss: 0.22013651759418507 Train-f1: 0.363653388376859 Test-f1: 0.3747160012982798 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2896/20000] Train-Loss: 0.7937438819902138 Test-Loss: 0.223100189418409 Train-f1: 0.3812466065883697 Test-f1: 0.31044595616024184 Train-acc: 0.6887417218543046 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2897/20000] Train-Loss: 0.7947757633742458 Test-Loss: 0.21993083944629077 Train-f1: 0.360210575940913 Test-f1: 0.3747160012982798 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2898/20000] Train-Loss: 0.7964037077736104 Test-Loss: 0.2210255056068468 Train-f1: 0.35982292035542607 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2899/20000] Train-Loss: 0.7994630076961231 Test-Loss: 0.21982735582787244 Train-f1: 0.37067718606180144 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2900/20000] Train-Loss: 0.796871094518047 Test-Loss: 0.21977953576132836 Train-f1: 0.3689312979785548 Test-f1: 0.3665708937355796 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "Epoch [2901/20000] Train-Loss: 0.7960557236276745 Test-Loss: 0.2201849836460313 Train-f1: 0.3715722338591053 Test-f1: 0.3631402831402831 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "Epoch [2902/20000] Train-Loss: 0.7941612592466887 Test-Loss: 0.2206853913502079 Train-f1: 0.3773613959345206 Test-f1: 0.3747160012982798 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [2903/20000] Train-Loss: 0.7955463830284858 Test-Loss: 0.22017733957251928 Train-f1: 0.36580451127819547 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2904/20000] Train-Loss: 0.7957382206641954 Test-Loss: 0.21968709887484808 Train-f1: 0.36039008412119405 Test-f1: 0.37632258064516133 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2905/20000] Train-Loss: 0.7946188181733246 Test-Loss: 0.21958042633470595 Train-f1: 0.36713553113553116 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2906/20000] Train-Loss: 0.7955852403574567 Test-Loss: 0.2194613004183823 Train-f1: 0.3654317076487318 Test-f1: 0.3858556024378809 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2907/20000] Train-Loss: 0.7953061104640123 Test-Loss: 0.2194856212633943 Train-f1: 0.3875375865758207 Test-f1: 0.372913803496082 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2908/20000] Train-Loss: 0.7953133087062402 Test-Loss: 0.2193475244632717 Train-f1: 0.3492162440696157 Test-f1: 0.37177666421260125 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "Epoch [2909/20000] Train-Loss: 0.7915975402237971 Test-Loss: 0.22531606498359075 Train-f1: 0.3729896229778117 Test-f1: 0.3020159788053152 Train-acc: 0.6821192052980133 Test-acc: 0.5789473684210527\n",
      "Epoch [2910/20000] Train-Loss: 0.7911342427505561 Test-Loss: 0.22117845862392943 Train-f1: 0.3574500034127363 Test-f1: 0.3747160012982798 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2911/20000] Train-Loss: 0.7947699291928286 Test-Loss: 0.21922837348183172 Train-f1: 0.3690709369941634 Test-f1: 0.3858556024378809 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2912/20000] Train-Loss: 0.7943508988225113 Test-Loss: 0.21918575674468946 Train-f1: 0.3658424725822533 Test-f1: 0.37177666421260125 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2913/20000] Train-Loss: 0.7941719342688733 Test-Loss: 0.21915889751319909 Train-f1: 0.3567501449586423 Test-f1: 0.36331707317073175 Train-acc: 0.6622516556291391 Test-acc: 0.6578947368421053\n",
      "Epoch [2914/20000] Train-Loss: 0.7928813660263345 Test-Loss: 0.2193047976814604 Train-f1: 0.37575243306187706 Test-f1: 0.36352052545156 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "Epoch [2915/20000] Train-Loss: 0.7938891593977635 Test-Loss: 0.2191957652059425 Train-f1: 0.3626089136839466 Test-f1: 0.3747160012982798 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2916/20000] Train-Loss: 0.7929509844781455 Test-Loss: 0.21905126397747687 Train-f1: 0.3797143371109912 Test-f1: 0.37923076923076926 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [2917/20000] Train-Loss: 0.7906534076711592 Test-Loss: 0.21990451844880438 Train-f1: 0.36118652400299267 Test-f1: 0.3747160012982798 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2918/20000] Train-Loss: 0.7929652576631149 Test-Loss: 0.21900706399870537 Train-f1: 0.3761557336112739 Test-f1: 0.36352052545156 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "Epoch [2919/20000] Train-Loss: 0.7922228826439655 Test-Loss: 0.21932182146806134 Train-f1: 0.35873891625615767 Test-f1: 0.3747160012982798 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2920/20000] Train-Loss: 0.7925764814626692 Test-Loss: 0.21888909653387617 Train-f1: 0.361745424251792 Test-f1: 0.37632258064516133 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2921/20000] Train-Loss: 0.7917154438317152 Test-Loss: 0.21882685495117546 Train-f1: 0.3848829702247333 Test-f1: 0.3500838659957387 Train-acc: 0.6920529801324503 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2922/20000] Train-Loss: 0.7918980757973715 Test-Loss: 0.21881281455170234 Train-f1: 0.36727674226282986 Test-f1: 0.3858556024378809 Train-acc: 0.6788079470198676 Test-acc: 0.6842105263157895\n",
      "Epoch [2923/20000] Train-Loss: 0.791035251337839 Test-Loss: 0.21932195543553965 Train-f1: 0.3768569205957142 Test-f1: 0.3858556024378809 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2924/20000] Train-Loss: 0.7905021067997695 Test-Loss: 0.21877134245837374 Train-f1: 0.3513161020870066 Test-f1: 0.3665708937355796 Train-acc: 0.6688741721854304 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2925/20000] Train-Loss: 0.7903623302358367 Test-Loss: 0.21876509431676316 Train-f1: 0.37611707599797145 Test-f1: 0.3660641025641026 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2926/20000] Train-Loss: 0.7889955799991998 Test-Loss: 0.2186707379562829 Train-f1: 0.36900657413520144 Test-f1: 0.37632258064516133 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "Epoch [2927/20000] Train-Loss: 0.7895596017953272 Test-Loss: 0.2186750340138375 Train-f1: 0.3570700719436866 Test-f1: 0.3858556024378809 Train-acc: 0.6688741721854304 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2928/20000] Train-Loss: 0.7907306270023552 Test-Loss: 0.21857867623889912 Train-f1: 0.3612166298299521 Test-f1: 0.37632258064516133 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "Epoch [2929/20000] Train-Loss: 0.7892880768419989 Test-Loss: 0.2189029622090045 Train-f1: 0.38159534349893426 Test-f1: 0.33552123552123553 Train-acc: 0.6887417218543046 Test-acc: 0.631578947368421\n",
      "Epoch [2930/20000] Train-Loss: 0.7894521729833169 Test-Loss: 0.21875787483919418 Train-f1: 0.3788207562448653 Test-f1: 0.3747160012982798 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [2931/20000] Train-Loss: 0.7904707552947512 Test-Loss: 0.2187526748205003 Train-f1: 0.3479045636923475 Test-f1: 0.3747160012982798 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2932/20000] Train-Loss: 0.7902843403729546 Test-Loss: 0.21855278831998318 Train-f1: 0.3679994378469538 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2933/20000] Train-Loss: 0.7878036297045241 Test-Loss: 0.21852134613513574 Train-f1: 0.3684778822709857 Test-f1: 0.37632258064516133 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2934/20000] Train-Loss: 0.7905906033989146 Test-Loss: 0.21858184092750488 Train-f1: 0.3712213609835839 Test-f1: 0.37632258064516133 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2935/20000] Train-Loss: 0.7885904681869728 Test-Loss: 0.2183253621147539 Train-f1: 0.3780374832791443 Test-f1: 0.37632258064516133 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2936/20000] Train-Loss: 0.7886387464653539 Test-Loss: 0.21828348433723294 Train-f1: 0.36498152575527054 Test-f1: 0.3858556024378809 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2937/20000] Train-Loss: 0.7860984094018777 Test-Loss: 0.219305067980888 Train-f1: 0.3711523529486641 Test-f1: 0.27633756205184773 Train-acc: 0.6754966887417219 Test-acc: 0.5921052631578947\n",
      "Epoch [2938/20000] Train-Loss: 0.7863126869653648 Test-Loss: 0.21946150825792382 Train-f1: 0.3677911782415323 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2939/20000] Train-Loss: 0.7881046761819087 Test-Loss: 0.21861294362893605 Train-f1: 0.3742726054232395 Test-f1: 0.34966947701510653 Train-acc: 0.6821192052980133 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2940/20000] Train-Loss: 0.7870365725140205 Test-Loss: 0.21812190952254784 Train-f1: 0.3642012356575463 Test-f1: 0.3858556024378809 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "Epoch [2941/20000] Train-Loss: 0.78725552581637 Test-Loss: 0.21814354924162688 Train-f1: 0.37580007433745 Test-f1: 0.36331707317073175 Train-acc: 0.6821192052980133 Test-acc: 0.6578947368421053\n",
      "Epoch [2942/20000] Train-Loss: 0.788080427198587 Test-Loss: 0.21898197486597756 Train-f1: 0.3616806051466074 Test-f1: 0.30479978306065264 Train-acc: 0.6754966887417219 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2943/20000] Train-Loss: 0.787626833031201 Test-Loss: 0.21811893951586886 Train-f1: 0.3734605418815945 Test-f1: 0.3858556024378809 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2944/20000] Train-Loss: 0.7863787449239125 Test-Loss: 0.21807780248779923 Train-f1: 0.3693523867436911 Test-f1: 0.3747160012982798 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [2945/20000] Train-Loss: 0.7867324274954368 Test-Loss: 0.21860136053328932 Train-f1: 0.3808215516934997 Test-f1: 0.3858556024378809 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [2946/20000] Train-Loss: 0.7872976259062113 Test-Loss: 0.21849944985831107 Train-f1: 0.37011713519108935 Test-f1: 0.30479978306065264 Train-acc: 0.6788079470198676 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2947/20000] Train-Loss: 0.7869834180684817 Test-Loss: 0.21787471011559156 Train-f1: 0.3700369374215528 Test-f1: 0.30365217391304344 Train-acc: 0.6788079470198676 Test-acc: 0.6052631578947368\n",
      "Epoch [2948/20000] Train-Loss: 0.7873881425990589 Test-Loss: 0.21792446835202317 Train-f1: 0.38543321845335266 Test-f1: 0.33525925925925926 Train-acc: 0.695364238410596 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2949/20000] Train-Loss: 0.7871950089210286 Test-Loss: 0.21785438679937014 Train-f1: 0.37260199628904384 Test-f1: 0.33525925925925926 Train-acc: 0.6821192052980133 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [2950/20000] Train-Loss: 0.7855985472743151 Test-Loss: 0.21771216772634222 Train-f1: 0.3782519790597239 Test-f1: 0.3063747846356542 Train-acc: 0.6854304635761589 Test-acc: 0.618421052631579\n",
      "Epoch [2951/20000] Train-Loss: 0.7844835252106255 Test-Loss: 0.21823299192052614 Train-f1: 0.3758076892960614 Test-f1: 0.3747160012982798 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [2952/20000] Train-Loss: 0.7839298674650442 Test-Loss: 0.2191252750068793 Train-f1: 0.3662838915470495 Test-f1: 0.2972925170068027 Train-acc: 0.6688741721854304 Test-acc: 0.6052631578947368\n",
      "Epoch [2953/20000] Train-Loss: 0.7856096385252748 Test-Loss: 0.21791811012688378 Train-f1: 0.35135182433248646 Test-f1: 0.3125060379528465 Train-acc: 0.6655629139072847 Test-acc: 0.618421052631579\n",
      "Epoch [2954/20000] Train-Loss: 0.7853042837303347 Test-Loss: 0.21773326046223596 Train-f1: 0.37380279243480335 Test-f1: 0.3497541528239203 Train-acc: 0.6821192052980133 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [2955/20000] Train-Loss: 0.7862011812253851 Test-Loss: 0.2176836082618362 Train-f1: 0.36540353113386825 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2956/20000] Train-Loss: 0.7835732641298387 Test-Loss: 0.21761341874658946 Train-f1: 0.37112844855255755 Test-f1: 0.36331707317073175 Train-acc: 0.6821192052980133 Test-acc: 0.6578947368421053\n",
      "Epoch [2957/20000] Train-Loss: 0.782234275411301 Test-Loss: 0.218226833474443 Train-f1: 0.3709505216853892 Test-f1: 0.3747160012982798 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2958/20000] Train-Loss: 0.7848795059559636 Test-Loss: 0.217476272421628 Train-f1: 0.37625524979627656 Test-f1: 0.37632258064516133 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [2959/20000] Train-Loss: 0.7822658667334658 Test-Loss: 0.21803249830699617 Train-f1: 0.3839774654490976 Test-f1: 0.3631402831402831 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2960/20000] Train-Loss: 0.7863989560406459 Test-Loss: 0.21738571584562355 Train-f1: 0.3684510831616178 Test-f1: 0.37701754385964914 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2961/20000] Train-Loss: 0.7820195444799174 Test-Loss: 0.2173549136258714 Train-f1: 0.3745307543823969 Test-f1: 0.3660641025641026 Train-acc: 0.6821192052980133 Test-acc: 0.6578947368421053\n",
      "Epoch [2962/20000] Train-Loss: 0.7809208574837205 Test-Loss: 0.217885310575222 Train-f1: 0.36480685636862265 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "Epoch [2963/20000] Train-Loss: 0.7820121748984249 Test-Loss: 0.21806083101415977 Train-f1: 0.3875791464662267 Test-f1: 0.3747160012982798 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [2964/20000] Train-Loss: 0.7839967092347312 Test-Loss: 0.2177620933175109 Train-f1: 0.36732866995308805 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2965/20000] Train-Loss: 0.7801816074373916 Test-Loss: 0.21802347331771113 Train-f1: 0.36321241881831146 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "Epoch [2966/20000] Train-Loss: 0.7816560136690525 Test-Loss: 0.21741470892599946 Train-f1: 0.3778934820840207 Test-f1: 0.3747160012982798 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [2967/20000] Train-Loss: 0.7811344900409034 Test-Loss: 0.21745332684321322 Train-f1: 0.36985693827799093 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2968/20000] Train-Loss: 0.7812219976458755 Test-Loss: 0.21768856686707588 Train-f1: 0.37501661129568103 Test-f1: 0.3206006006006006 Train-acc: 0.6821192052980133 Test-acc: 0.618421052631579\n",
      "Epoch [2969/20000] Train-Loss: 0.7810338015481542 Test-Loss: 0.21754773395932717 Train-f1: 0.35574436394661113 Test-f1: 0.3747160012982798 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2970/20000] Train-Loss: 0.7811177035342793 Test-Loss: 0.2173023525029651 Train-f1: 0.3673965110796794 Test-f1: 0.3747160012982798 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2971/20000] Train-Loss: 0.7807349476756776 Test-Loss: 0.2171245385787474 Train-f1: 0.3641298566806663 Test-f1: 0.3657909657909658 Train-acc: 0.6754966887417219 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2972/20000] Train-Loss: 0.7807462969150893 Test-Loss: 0.21705632773562664 Train-f1: 0.3700369374215528 Test-f1: 0.37632258064516133 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2973/20000] Train-Loss: 0.7801316420380366 Test-Loss: 0.21761326724830524 Train-f1: 0.36323301245129835 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "Epoch [2974/20000] Train-Loss: 0.7794302737597307 Test-Loss: 0.21794079978773226 Train-f1: 0.3771954099713297 Test-f1: 0.3698920797512347 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2975/20000] Train-Loss: 0.7809316107283691 Test-Loss: 0.21681605232812357 Train-f1: 0.3724624515762164 Test-f1: 0.36331707317073175 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "Epoch [2976/20000] Train-Loss: 0.7787036540933002 Test-Loss: 0.2170153823794279 Train-f1: 0.38773801349558923 Test-f1: 0.3631402831402831 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [2977/20000] Train-Loss: 0.7803070127186191 Test-Loss: 0.21686792147450654 Train-f1: 0.3818656114237326 Test-f1: 0.30365217391304344 Train-acc: 0.6887417218543046 Test-acc: 0.6052631578947368\n",
      "Epoch [2978/20000] Train-Loss: 0.7796687839341122 Test-Loss: 0.21691823023397586 Train-f1: 0.36541811955557224 Test-f1: 0.3206006006006006 Train-acc: 0.6788079470198676 Test-acc: 0.618421052631579\n",
      "Epoch [2979/20000] Train-Loss: 0.7788818257156745 Test-Loss: 0.21703006827746424 Train-f1: 0.3772413793103448 Test-f1: 0.30479978306065264 Train-acc: 0.6854304635761589 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [2980/20000] Train-Loss: 0.7780466607857566 Test-Loss: 0.2166427439838499 Train-f1: 0.36717580591558663 Test-f1: 0.36352052545156 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "Epoch [2981/20000] Train-Loss: 0.7795280756679771 Test-Loss: 0.21669065623017628 Train-f1: 0.383988603988604 Test-f1: 0.36331707317073175 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "Epoch [2982/20000] Train-Loss: 0.7781740792201007 Test-Loss: 0.2168314923651674 Train-f1: 0.3693035823237166 Test-f1: 0.3858556024378809 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [2983/20000] Train-Loss: 0.7780958575686641 Test-Loss: 0.21689430430332055 Train-f1: 0.36498703342693106 Test-f1: 0.3206006006006006 Train-acc: 0.6721854304635762 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2984/20000] Train-Loss: 0.7772318949726053 Test-Loss: 0.2165856326946033 Train-f1: 0.3672580271096696 Test-f1: 0.36331707317073175 Train-acc: 0.6754966887417219 Test-acc: 0.6578947368421053\n",
      "Epoch [2985/20000] Train-Loss: 0.7784579924690309 Test-Loss: 0.2172376729854107 Train-f1: 0.36871878580710415 Test-f1: 0.32145683844313977 Train-acc: 0.6788079470198676 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2986/20000] Train-Loss: 0.7780118990482625 Test-Loss: 0.21655872416598973 Train-f1: 0.35579679828001975 Test-f1: 0.3758783783783784 Train-acc: 0.6655629139072847 Test-acc: 0.6710526315789473\n",
      "Epoch [2987/20000] Train-Loss: 0.7767647659248739 Test-Loss: 0.2169231079504425 Train-f1: 0.37678824169842134 Test-f1: 0.3758783783783784 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2988/20000] Train-Loss: 0.7769858379914428 Test-Loss: 0.21641407258768205 Train-f1: 0.3655141242937853 Test-f1: 0.37632258064516133 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2989/20000] Train-Loss: 0.77598611825759 Test-Loss: 0.2166855719792071 Train-f1: 0.38680653391757724 Test-f1: 0.3206006006006006 Train-acc: 0.6920529801324503 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2990/20000] Train-Loss: 0.7780316334282238 Test-Loss: 0.21639206233225328 Train-f1: 0.36396010265364975 Test-f1: 0.3843589743589744 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "Epoch [2991/20000] Train-Loss: 0.7765507178875517 Test-Loss: 0.216824120593997 Train-f1: 0.3844128885086969 Test-f1: 0.3206006006006006 Train-acc: 0.6920529801324503 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [2992/20000] Train-Loss: 0.7766294358597802 Test-Loss: 0.21632395984526231 Train-f1: 0.3700272904483431 Test-f1: 0.3843589743589744 Train-acc: 0.6788079470198676 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [2993/20000] Train-Loss: 0.7755401599028865 Test-Loss: 0.21630952862670993 Train-f1: 0.38022627121063807 Test-f1: 0.3657909657909658 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [2994/20000] Train-Loss: 0.7772851359183567 Test-Loss: 0.2163302240435423 Train-f1: 0.3693985823849791 Test-f1: 0.3758783783783784 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [2995/20000] Train-Loss: 0.7747983814719109 Test-Loss: 0.21657395115935052 Train-f1: 0.36541811955557224 Test-f1: 0.3657909657909658 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [2996/20000] Train-Loss: 0.7772040762205186 Test-Loss: 0.21629551492355303 Train-f1: 0.38250688130206206 Test-f1: 0.37347480106100794 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [2997/20000] Train-Loss: 0.7735496437859734 Test-Loss: 0.2175260866896766 Train-f1: 0.3771909581142713 Test-f1: 0.3447978191731031 Train-acc: 0.6821192052980133 Test-acc: 0.631578947368421\n",
      "Epoch [2998/20000] Train-Loss: 0.7741064555039147 Test-Loss: 0.21681251794327547 Train-f1: 0.3804938306343824 Test-f1: 0.3747160012982798 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [2999/20000] Train-Loss: 0.7750842780988579 Test-Loss: 0.21614533270280126 Train-f1: 0.36875034882252206 Test-f1: 0.3758783783783784 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3000/20000] Train-Loss: 0.7773182122756177 Test-Loss: 0.21611652343384968 Train-f1: 0.3804938306343824 Test-f1: 0.3631402831402831 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3001/20000] Train-Loss: 0.7749842518638669 Test-Loss: 0.2160030434073212 Train-f1: 0.3722861099133851 Test-f1: 0.3758783783783784 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [3002/20000] Train-Loss: 0.7753726797513933 Test-Loss: 0.216019070995691 Train-f1: 0.3655141242937853 Test-f1: 0.3725208125208125 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [3003/20000] Train-Loss: 0.7737127764047371 Test-Loss: 0.21640818685978438 Train-f1: 0.3782210543080108 Test-f1: 0.3874509803921569 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "Epoch [3004/20000] Train-Loss: 0.7744301804289117 Test-Loss: 0.21618273884622066 Train-f1: 0.3767436542740187 Test-f1: 0.3758783783783784 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3005/20000] Train-Loss: 0.7751270702202802 Test-Loss: 0.21576127065723985 Train-f1: 0.3722109887135768 Test-f1: 0.3694235588972431 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [3006/20000] Train-Loss: 0.7741567071572535 Test-Loss: 0.2157693358500902 Train-f1: 0.37607017543859655 Test-f1: 0.36352052545156 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "Epoch [3007/20000] Train-Loss: 0.7744514489742368 Test-Loss: 0.2159981497901714 Train-f1: 0.3875791464662267 Test-f1: 0.3758783783783784 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3008/20000] Train-Loss: 0.7734276820493537 Test-Loss: 0.2158582802153275 Train-f1: 0.3782963877780109 Test-f1: 0.3843589743589744 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3009/20000] Train-Loss: 0.7753779444106431 Test-Loss: 0.21566084411273662 Train-f1: 0.37081975226399216 Test-f1: 0.37632258064516133 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [3010/20000] Train-Loss: 0.7736963666669757 Test-Loss: 0.21568947059364524 Train-f1: 0.3843335561097329 Test-f1: 0.3725208125208125 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3011/20000] Train-Loss: 0.7723483758813298 Test-Loss: 0.2155882465471347 Train-f1: 0.3696762344545311 Test-f1: 0.382280701754386 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "Epoch [3012/20000] Train-Loss: 0.7718340983256732 Test-Loss: 0.21560727031252616 Train-f1: 0.38158625445704797 Test-f1: 0.3657909657909658 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "Epoch [3013/20000] Train-Loss: 0.7721981830908425 Test-Loss: 0.2157449052697713 Train-f1: 0.38674523007856343 Test-f1: 0.33552123552123553 Train-acc: 0.6920529801324503 Test-acc: 0.631578947368421\n",
      "Epoch [3014/20000] Train-Loss: 0.7729008641464666 Test-Loss: 0.2167101468972029 Train-f1: 0.36163148841114945 Test-f1: 0.3313928012519562 Train-acc: 0.6754966887417219 Test-acc: 0.618421052631579\n",
      "Epoch [3015/20000] Train-Loss: 0.7733811247789232 Test-Loss: 0.2156203367286888 Train-f1: 0.3767151439405775 Test-f1: 0.3758783783783784 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3016/20000] Train-Loss: 0.7710742704551091 Test-Loss: 0.21615423206646037 Train-f1: 0.3555161172508015 Test-f1: 0.3747160012982798 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [3017/20000] Train-Loss: 0.7715947613754572 Test-Loss: 0.21596869089538026 Train-f1: 0.3778082603630351 Test-f1: 0.3755952380952381 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [3018/20000] Train-Loss: 0.7725901422254338 Test-Loss: 0.21587557793335038 Train-f1: 0.38036637562061293 Test-f1: 0.3747160012982798 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3019/20000] Train-Loss: 0.7734644616159986 Test-Loss: 0.21532250616877618 Train-f1: 0.3809460283306437 Test-f1: 0.382280701754386 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3020/20000] Train-Loss: 0.7711472238887879 Test-Loss: 0.21593624046124654 Train-f1: 0.3647502656000451 Test-f1: 0.3747160012982798 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3021/20000] Train-Loss: 0.7711577007489977 Test-Loss: 0.21526885130114437 Train-f1: 0.38930653391757725 Test-f1: 0.3758783783783784 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3022/20000] Train-Loss: 0.7729684259837245 Test-Loss: 0.2156041887966805 Train-f1: 0.37341409418557286 Test-f1: 0.3758783783783784 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [3023/20000] Train-Loss: 0.7717903829435058 Test-Loss: 0.21534354436687922 Train-f1: 0.36313487961532653 Test-f1: 0.37347480106100794 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3024/20000] Train-Loss: 0.7707583587767283 Test-Loss: 0.21520347885329938 Train-f1: 0.38707070707070707 Test-f1: 0.3758783783783784 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3025/20000] Train-Loss: 0.7702367577251898 Test-Loss: 0.21535067280805456 Train-f1: 0.3801520009994587 Test-f1: 0.37347480106100794 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3026/20000] Train-Loss: 0.7702000459889107 Test-Loss: 0.2152071339206093 Train-f1: 0.37607017543859655 Test-f1: 0.3758783783783784 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3027/20000] Train-Loss: 0.7703079444645887 Test-Loss: 0.21521955748719493 Train-f1: 0.3847038925479372 Test-f1: 0.3760180321942714 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3028/20000] Train-Loss: 0.7707302679974241 Test-Loss: 0.21589013866760684 Train-f1: 0.38453878137672604 Test-f1: 0.31410256410256415 Train-acc: 0.6887417218543046 Test-acc: 0.618421052631579\n",
      "best score_dx!!\n",
      "Epoch [3029/20000] Train-Loss: 0.7703196471193441 Test-Loss: 0.2151830167124096 Train-f1: 0.3659519676596485 Test-f1: 0.37347480106100794 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [3030/20000] Train-Loss: 0.7681378330868388 Test-Loss: 0.21553282633475587 Train-f1: 0.3860233521036922 Test-f1: 0.3874509803921569 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3031/20000] Train-Loss: 0.7696869127174146 Test-Loss: 0.21533639783316294 Train-f1: 0.3805952383527235 Test-f1: 0.3874509803921569 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3032/20000] Train-Loss: 0.7699882905104481 Test-Loss: 0.21493246131148153 Train-f1: 0.37896014557628804 Test-f1: 0.3758783783783784 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3033/20000] Train-Loss: 0.7702103336446887 Test-Loss: 0.21489273867990957 Train-f1: 0.38336973343554287 Test-f1: 0.3718340516303165 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3034/20000] Train-Loss: 0.7692357450480537 Test-Loss: 0.2148385974608198 Train-f1: 0.3662155036287015 Test-f1: 0.3758783783783784 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "Epoch [3035/20000] Train-Loss: 0.7692967877238511 Test-Loss: 0.21499651344322868 Train-f1: 0.3818656114237326 Test-f1: 0.3758783783783784 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3036/20000] Train-Loss: 0.7681596914820735 Test-Loss: 0.2158803677947985 Train-f1: 0.3873559497965857 Test-f1: 0.3698920797512347 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3037/20000] Train-Loss: 0.7688060749279954 Test-Loss: 0.21485094191931325 Train-f1: 0.36789090060934726 Test-f1: 0.3725208125208125 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [3038/20000] Train-Loss: 0.7667124863902691 Test-Loss: 0.2150793099805538 Train-f1: 0.3758033476186626 Test-f1: 0.3874509803921569 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3039/20000] Train-Loss: 0.7696362133318427 Test-Loss: 0.21475124943756152 Train-f1: 0.3640975001224442 Test-f1: 0.3657909657909658 Train-acc: 0.6754966887417219 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3040/20000] Train-Loss: 0.7674830169306103 Test-Loss: 0.21470507779137368 Train-f1: 0.3786933466594483 Test-f1: 0.3657909657909658 Train-acc: 0.6821192052980133 Test-acc: 0.6578947368421053\n",
      "Epoch [3041/20000] Train-Loss: 0.7671628724359018 Test-Loss: 0.21506906846311707 Train-f1: 0.3812110443053231 Test-f1: 0.37347480106100794 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3042/20000] Train-Loss: 0.7697372385779359 Test-Loss: 0.2147992171591026 Train-f1: 0.3791556591556592 Test-f1: 0.3725208125208125 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3043/20000] Train-Loss: 0.7671399444637177 Test-Loss: 0.2169225803499578 Train-f1: 0.3823217014784198 Test-f1: 0.35387163561076607 Train-acc: 0.6854304635761589 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3044/20000] Train-Loss: 0.7693115208714266 Test-Loss: 0.21454639487019186 Train-f1: 0.3858128881084405 Test-f1: 0.3718340516303165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3045/20000] Train-Loss: 0.767419349108547 Test-Loss: 0.21493657007947536 Train-f1: 0.3768461962387196 Test-f1: 0.3843589743589744 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3046/20000] Train-Loss: 0.7661415407780793 Test-Loss: 0.2144573777073612 Train-f1: 0.37081987734944555 Test-f1: 0.382280701754386 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [3047/20000] Train-Loss: 0.766072672015819 Test-Loss: 0.2154849563439411 Train-f1: 0.3814253624668164 Test-f1: 0.3747160012982798 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3048/20000] Train-Loss: 0.7662974604901812 Test-Loss: 0.21448434157004606 Train-f1: 0.3820259319753759 Test-f1: 0.3631402831402831 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [3049/20000] Train-Loss: 0.7650433957091576 Test-Loss: 0.21489913585732537 Train-f1: 0.38835056446302973 Test-f1: 0.3874509803921569 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3050/20000] Train-Loss: 0.7649424450695681 Test-Loss: 0.21565446119849174 Train-f1: 0.3811585532692424 Test-f1: 0.3747160012982798 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3051/20000] Train-Loss: 0.7656780423102582 Test-Loss: 0.2143822143039979 Train-f1: 0.387197788094138 Test-f1: 0.3758783783783784 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3052/20000] Train-Loss: 0.7656812479468909 Test-Loss: 0.21449109788799606 Train-f1: 0.39003710575139144 Test-f1: 0.3631402831402831 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3053/20000] Train-Loss: 0.765088334549701 Test-Loss: 0.21484005357635289 Train-f1: 0.3800955226361273 Test-f1: 0.37347480106100794 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3054/20000] Train-Loss: 0.7654081631331211 Test-Loss: 0.21452277481172488 Train-f1: 0.3670997318102664 Test-f1: 0.3725208125208125 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3055/20000] Train-Loss: 0.76492396522574 Test-Loss: 0.2142351536879739 Train-f1: 0.38484692501134193 Test-f1: 0.3758783783783784 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3056/20000] Train-Loss: 0.7652597898207703 Test-Loss: 0.21419872093731784 Train-f1: 0.38907856028312193 Test-f1: 0.3657909657909658 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3057/20000] Train-Loss: 0.764104744811218 Test-Loss: 0.2144973093329797 Train-f1: 0.38377858660663566 Test-f1: 0.3874509803921569 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3058/20000] Train-Loss: 0.7646411003789727 Test-Loss: 0.21413955089838327 Train-f1: 0.38032068979397515 Test-f1: 0.3758783783783784 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3059/20000] Train-Loss: 0.7611779250837863 Test-Loss: 0.21763893228887923 Train-f1: 0.3780565254249465 Test-f1: 0.3210894552723638 Train-acc: 0.6854304635761589 Test-acc: 0.6052631578947368\n",
      "best score_dx!!\n",
      "Epoch [3060/20000] Train-Loss: 0.7659026293487566 Test-Loss: 0.21407897490054778 Train-f1: 0.37068645640074216 Test-f1: 0.3713974358974359 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [3061/20000] Train-Loss: 0.7623583225087363 Test-Loss: 0.21525927407431292 Train-f1: 0.36344030205307554 Test-f1: 0.3313928012519562 Train-acc: 0.6721854304635762 Test-acc: 0.618421052631579\n",
      "Epoch [3062/20000] Train-Loss: 0.7634970442293985 Test-Loss: 0.21415734563018168 Train-f1: 0.38250543968813905 Test-f1: 0.3758783783783784 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3063/20000] Train-Loss: 0.7631205970813748 Test-Loss: 0.21398426636876236 Train-f1: 0.38324062846697954 Test-f1: 0.3758783783783784 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3064/20000] Train-Loss: 0.760505734573648 Test-Loss: 0.21487560409022616 Train-f1: 0.3664853759769014 Test-f1: 0.33980193002090503 Train-acc: 0.6721854304635762 Test-acc: 0.631578947368421\n",
      "Epoch [3065/20000] Train-Loss: 0.7602381746451935 Test-Loss: 0.2145077924945624 Train-f1: 0.3633538777606574 Test-f1: 0.3843589743589744 Train-acc: 0.6721854304635762 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3066/20000] Train-Loss: 0.7641883248635967 Test-Loss: 0.21387899378386202 Train-f1: 0.3916132570364995 Test-f1: 0.382280701754386 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3067/20000] Train-Loss: 0.7620065696437972 Test-Loss: 0.21396888629077634 Train-f1: 0.3935413950456324 Test-f1: 0.3631402831402831 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "Epoch [3068/20000] Train-Loss: 0.7624770364802944 Test-Loss: 0.21401826739117036 Train-f1: 0.38146571185823525 Test-f1: 0.344429003154435 Train-acc: 0.6920529801324503 Test-acc: 0.6447368421052632\n",
      "Epoch [3069/20000] Train-Loss: 0.761694553593325 Test-Loss: 0.21771064341126015 Train-f1: 0.37437543169863446 Test-f1: 0.3210894552723638 Train-acc: 0.6854304635761589 Test-acc: 0.6052631578947368\n",
      "Epoch [3070/20000] Train-Loss: 0.763686652424248 Test-Loss: 0.21417730337158294 Train-f1: 0.3768468791025182 Test-f1: 0.35 Train-acc: 0.6854304635761589 Test-acc: 0.6447368421052632\n",
      "Epoch [3071/20000] Train-Loss: 0.7629535973802293 Test-Loss: 0.21399393352445303 Train-f1: 0.37683733890630444 Test-f1: 0.36307826956739186 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3072/20000] Train-Loss: 0.7628762557149048 Test-Loss: 0.2136582301582663 Train-f1: 0.3845879213799842 Test-f1: 0.38165591397849463 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3073/20000] Train-Loss: 0.7614571847144412 Test-Loss: 0.21385868791067483 Train-f1: 0.3655948043719309 Test-f1: 0.38779161477791624 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "Epoch [3074/20000] Train-Loss: 0.7620396027806644 Test-Loss: 0.2138455751519722 Train-f1: 0.36983223825329087 Test-f1: 0.37732600732600735 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [3075/20000] Train-Loss: 0.7616023173361984 Test-Loss: 0.2137105174417644 Train-f1: 0.378241084633608 Test-f1: 0.3657909657909658 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "Epoch [3076/20000] Train-Loss: 0.7627770000000844 Test-Loss: 0.21368168920477784 Train-f1: 0.38745907780232625 Test-f1: 0.3718340516303165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3077/20000] Train-Loss: 0.7621208288832675 Test-Loss: 0.2137200129773332 Train-f1: 0.3744605070186465 Test-f1: 0.3725208125208125 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3078/20000] Train-Loss: 0.7602260943811612 Test-Loss: 0.2135513137640468 Train-f1: 0.3826138569873604 Test-f1: 0.3657909657909658 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3079/20000] Train-Loss: 0.7609393330051362 Test-Loss: 0.213525062084957 Train-f1: 0.37890909788572963 Test-f1: 0.3713974358974359 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3080/20000] Train-Loss: 0.7586647888669832 Test-Loss: 0.21460935368539408 Train-f1: 0.384815192975291 Test-f1: 0.3747160012982798 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3081/20000] Train-Loss: 0.75983526213697 Test-Loss: 0.21349421685493877 Train-f1: 0.36486257928118393 Test-f1: 0.3718340516303165 Train-acc: 0.6721854304635762 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3082/20000] Train-Loss: 0.7617901345946143 Test-Loss: 0.21345155865909124 Train-f1: 0.39069093688892825 Test-f1: 0.3657909657909658 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3083/20000] Train-Loss: 0.7615232916070782 Test-Loss: 0.21341141379218773 Train-f1: 0.3986195241676239 Test-f1: 0.3657909657909658 Train-acc: 0.7086092715231788 Test-acc: 0.6578947368421053\n",
      "Epoch [3084/20000] Train-Loss: 0.7613381906130312 Test-Loss: 0.21349397810084275 Train-f1: 0.39572191313082 Test-f1: 0.3775036674311453 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "Epoch [3085/20000] Train-Loss: 0.7572367778390788 Test-Loss: 0.214288275670957 Train-f1: 0.38586447510811234 Test-f1: 0.37347480106100794 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3086/20000] Train-Loss: 0.7602354929760672 Test-Loss: 0.2137228227797081 Train-f1: 0.37832578841488307 Test-f1: 0.3874509803921569 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "Epoch [3087/20000] Train-Loss: 0.7602331592450694 Test-Loss: 0.21369662818764817 Train-f1: 0.38221561057778813 Test-f1: 0.37347480106100794 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3088/20000] Train-Loss: 0.7593729854599792 Test-Loss: 0.2133533469465154 Train-f1: 0.37363754889178613 Test-f1: 0.3775036674311453 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3089/20000] Train-Loss: 0.7600034567252613 Test-Loss: 0.2138281516403883 Train-f1: 0.3872248315295079 Test-f1: 0.3817102615694165 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3090/20000] Train-Loss: 0.7589872572051245 Test-Loss: 0.2143855891585787 Train-f1: 0.38003671118425214 Test-f1: 0.3576072715361939 Train-acc: 0.6854304635761589 Test-acc: 0.6447368421052632\n",
      "Epoch [3091/20000] Train-Loss: 0.7591352467526125 Test-Loss: 0.21340245949478992 Train-f1: 0.38364676956135463 Test-f1: 0.3725208125208125 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3092/20000] Train-Loss: 0.7586081831692983 Test-Loss: 0.21309223589987933 Train-f1: 0.38491371763345994 Test-f1: 0.38165591397849463 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3093/20000] Train-Loss: 0.7590610958580221 Test-Loss: 0.21331256911341068 Train-f1: 0.3776769798083104 Test-f1: 0.3831751007613077 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3094/20000] Train-Loss: 0.7578583890211178 Test-Loss: 0.21298304393883977 Train-f1: 0.3814281215925385 Test-f1: 0.3405925925925926 Train-acc: 0.6887417218543046 Test-acc: 0.6447368421052632\n",
      "Epoch [3095/20000] Train-Loss: 0.760100752729507 Test-Loss: 0.21304042764150014 Train-f1: 0.37296227581941865 Test-f1: 0.38165591397849463 Train-acc: 0.6788079470198676 Test-acc: 0.6842105263157895\n",
      "Epoch [3096/20000] Train-Loss: 0.757414057461457 Test-Loss: 0.21312481109135778 Train-f1: 0.37536364936364935 Test-f1: 0.38779161477791624 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "Epoch [3097/20000] Train-Loss: 0.7578845424794876 Test-Loss: 0.2131397078959519 Train-f1: 0.38818605558264263 Test-f1: 0.3874509803921569 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3098/20000] Train-Loss: 0.7582779906602379 Test-Loss: 0.21297142075560738 Train-f1: 0.383735401675121 Test-f1: 0.3713974358974359 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3099/20000] Train-Loss: 0.7582387656990157 Test-Loss: 0.21292711755485608 Train-f1: 0.37077218728162126 Test-f1: 0.3657909657909658 Train-acc: 0.6821192052980133 Test-acc: 0.6578947368421053\n",
      "Epoch [3100/20000] Train-Loss: 0.757896628609621 Test-Loss: 0.2130821011362124 Train-f1: 0.38333388603327256 Test-f1: 0.3874509803921569 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3101/20000] Train-Loss: 0.7584462446893078 Test-Loss: 0.2131581027936307 Train-f1: 0.37233120366992195 Test-f1: 0.387620103473762 Train-acc: 0.6788079470198676 Test-acc: 0.6842105263157895\n",
      "Epoch [3102/20000] Train-Loss: 0.7579546988895627 Test-Loss: 0.21302984208703152 Train-f1: 0.3905012607452619 Test-f1: 0.3874509803921569 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3103/20000] Train-Loss: 0.7565186258795508 Test-Loss: 0.21299340976527306 Train-f1: 0.38616697588126153 Test-f1: 0.3725208125208125 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3104/20000] Train-Loss: 0.7582538901971777 Test-Loss: 0.212826077872433 Train-f1: 0.37464367990683783 Test-f1: 0.3881389909789509 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [3105/20000] Train-Loss: 0.7572275802765303 Test-Loss: 0.2132005375390465 Train-f1: 0.3718277672514789 Test-f1: 0.3874509803921569 Train-acc: 0.6754966887417219 Test-acc: 0.6842105263157895\n",
      "Epoch [3106/20000] Train-Loss: 0.7557507576575859 Test-Loss: 0.21621550097931966 Train-f1: 0.37518398783831175 Test-f1: 0.3485741000467508 Train-acc: 0.6788079470198676 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [3107/20000] Train-Loss: 0.7574576039228543 Test-Loss: 0.21268526028609244 Train-f1: 0.39137408388012734 Test-f1: 0.3758783783783784 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3108/20000] Train-Loss: 0.7576477487645654 Test-Loss: 0.21273281963388846 Train-f1: 0.37659254453226376 Test-f1: 0.3775036674311453 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3109/20000] Train-Loss: 0.7568690724777986 Test-Loss: 0.21320948356885408 Train-f1: 0.38032068979397515 Test-f1: 0.3817102615694165 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3110/20000] Train-Loss: 0.7567047440227075 Test-Loss: 0.21318530738426558 Train-f1: 0.3832314596810448 Test-f1: 0.3817102615694165 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3111/20000] Train-Loss: 0.7554769849376581 Test-Loss: 0.21270672799136855 Train-f1: 0.3706382859848206 Test-f1: 0.3775036674311453 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3112/20000] Train-Loss: 0.7559153142820247 Test-Loss: 0.21264976691100684 Train-f1: 0.3793400238366108 Test-f1: 0.38779161477791624 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3113/20000] Train-Loss: 0.7556298617557513 Test-Loss: 0.21253945123285065 Train-f1: 0.3768468791025182 Test-f1: 0.3657909657909658 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "Epoch [3114/20000] Train-Loss: 0.749900946217288 Test-Loss: 0.2153076774942856 Train-f1: 0.39681714860116485 Test-f1: 0.3578571428571428 Train-acc: 0.7052980132450332 Test-acc: 0.6447368421052632\n",
      "Epoch [3115/20000] Train-Loss: 0.7566460818401105 Test-Loss: 0.21299621297769636 Train-f1: 0.3751853971032053 Test-f1: 0.37347480106100794 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3116/20000] Train-Loss: 0.7554320470486471 Test-Loss: 0.21371382863610208 Train-f1: 0.3939993938370724 Test-f1: 0.3576072715361939 Train-acc: 0.6986754966887417 Test-acc: 0.6447368421052632\n",
      "Epoch [3117/20000] Train-Loss: 0.7558130602520269 Test-Loss: 0.21264505531653521 Train-f1: 0.3775063571487163 Test-f1: 0.3725208125208125 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [3118/20000] Train-Loss: 0.7548689773438503 Test-Loss: 0.21292996193917174 Train-f1: 0.3697724962606087 Test-f1: 0.37347480106100794 Train-acc: 0.6754966887417219 Test-acc: 0.6710526315789473\n",
      "Epoch [3119/20000] Train-Loss: 0.7510876020597792 Test-Loss: 0.21331630689476772 Train-f1: 0.3865940882334325 Test-f1: 0.3576072715361939 Train-acc: 0.6920529801324503 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3120/20000] Train-Loss: 0.7550430853271001 Test-Loss: 0.21233923474819458 Train-f1: 0.37543739914277785 Test-f1: 0.36314749920356804 Train-acc: 0.6821192052980133 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3121/20000] Train-Loss: 0.7542905336917032 Test-Loss: 0.21231160909197067 Train-f1: 0.3752094322060192 Test-f1: 0.3657909657909658 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "Epoch [3122/20000] Train-Loss: 0.7544498121869183 Test-Loss: 0.21286473074608075 Train-f1: 0.38338929197985905 Test-f1: 0.37347480106100794 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3123/20000] Train-Loss: 0.7538576866285046 Test-Loss: 0.21249461504879427 Train-f1: 0.3851414397921219 Test-f1: 0.36307826956739186 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3124/20000] Train-Loss: 0.7532205772767865 Test-Loss: 0.2121616837241495 Train-f1: 0.3835616438356164 Test-f1: 0.3688538587848933 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3125/20000] Train-Loss: 0.7558151086103576 Test-Loss: 0.21219528633030696 Train-f1: 0.36699815038609274 Test-f1: 0.3713974358974359 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [3126/20000] Train-Loss: 0.7496956431482816 Test-Loss: 0.21322199122423857 Train-f1: 0.38876405401912645 Test-f1: 0.37347480106100794 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3127/20000] Train-Loss: 0.7548525624567124 Test-Loss: 0.21217561106843386 Train-f1: 0.3854059979641375 Test-f1: 0.382280701754386 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3128/20000] Train-Loss: 0.7541722994961152 Test-Loss: 0.21209695014936242 Train-f1: 0.38914071652757487 Test-f1: 0.38165591397849463 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3129/20000] Train-Loss: 0.7525153606738225 Test-Loss: 0.2123580311526966 Train-f1: 0.38760291391870344 Test-f1: 0.36307826956739186 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3130/20000] Train-Loss: 0.7508409282908172 Test-Loss: 0.2120363496368535 Train-f1: 0.379722729977744 Test-f1: 0.38165591397849463 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3131/20000] Train-Loss: 0.7517772208216653 Test-Loss: 0.21248409561669654 Train-f1: 0.38323292821394145 Test-f1: 0.3817102615694165 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3132/20000] Train-Loss: 0.7526645349504246 Test-Loss: 0.21199776456403172 Train-f1: 0.3913976499690785 Test-f1: 0.38779161477791624 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3133/20000] Train-Loss: 0.7526785208796414 Test-Loss: 0.2125476963425206 Train-f1: 0.37782864534988725 Test-f1: 0.3576072715361939 Train-acc: 0.6854304635761589 Test-acc: 0.6447368421052632\n",
      "Epoch [3134/20000] Train-Loss: 0.7516208551155481 Test-Loss: 0.21199825839558706 Train-f1: 0.38482550651620107 Test-f1: 0.3713974358974359 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3135/20000] Train-Loss: 0.7526026529938205 Test-Loss: 0.21198394002691123 Train-f1: 0.3848040294700315 Test-f1: 0.3713974358974359 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3136/20000] Train-Loss: 0.7523701374463712 Test-Loss: 0.2121923381298905 Train-f1: 0.380751585070522 Test-f1: 0.35 Train-acc: 0.6821192052980133 Test-acc: 0.6447368421052632\n",
      "Epoch [3137/20000] Train-Loss: 0.7505304234401976 Test-Loss: 0.21245789799566664 Train-f1: 0.3811188741154612 Test-f1: 0.37347480106100794 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3138/20000] Train-Loss: 0.7512945558777362 Test-Loss: 0.21180406883855224 Train-f1: 0.38240842143835707 Test-f1: 0.38165591397849463 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3139/20000] Train-Loss: 0.7512066530404692 Test-Loss: 0.21181723689126766 Train-f1: 0.373648599293898 Test-f1: 0.3713974358974359 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3140/20000] Train-Loss: 0.7514656810133505 Test-Loss: 0.212075013875269 Train-f1: 0.38814860392710226 Test-f1: 0.3874509803921569 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3141/20000] Train-Loss: 0.7503130194691339 Test-Loss: 0.21183724010278754 Train-f1: 0.3679256940008587 Test-f1: 0.3713974358974359 Train-acc: 0.6788079470198676 Test-acc: 0.6710526315789473\n",
      "Epoch [3142/20000] Train-Loss: 0.7505678650975854 Test-Loss: 0.21204636828259357 Train-f1: 0.38046186953069544 Test-f1: 0.37732600732600735 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3143/20000] Train-Loss: 0.7514320059596784 Test-Loss: 0.21191659396305815 Train-f1: 0.3909270781186679 Test-f1: 0.3713974358974359 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3144/20000] Train-Loss: 0.7475243468349757 Test-Loss: 0.2154908655008898 Train-f1: 0.390204984019417 Test-f1: 0.3485741000467508 Train-acc: 0.695364238410596 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [3145/20000] Train-Loss: 0.7518473901285728 Test-Loss: 0.21169032444190683 Train-f1: 0.3650309852483664 Test-f1: 0.36307826956739186 Train-acc: 0.6688741721854304 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3146/20000] Train-Loss: 0.7515095790375169 Test-Loss: 0.2115966056097954 Train-f1: 0.3817028589282924 Test-f1: 0.38779161477791624 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3147/20000] Train-Loss: 0.7495881587061649 Test-Loss: 0.21179051892500553 Train-f1: 0.37375669188006666 Test-f1: 0.3874509803921569 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3148/20000] Train-Loss: 0.7506209429224592 Test-Loss: 0.21157452596467471 Train-f1: 0.39409708630047613 Test-f1: 0.3775036674311453 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3149/20000] Train-Loss: 0.7531767460578231 Test-Loss: 0.211774515859567 Train-f1: 0.3789195077530595 Test-f1: 0.37732600732600735 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3150/20000] Train-Loss: 0.7512192029334178 Test-Loss: 0.21148518554325138 Train-f1: 0.3855614973262032 Test-f1: 0.38779161477791624 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3151/20000] Train-Loss: 0.7496329872356449 Test-Loss: 0.21149293939315952 Train-f1: 0.3803106459097108 Test-f1: 0.38779161477791624 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3152/20000] Train-Loss: 0.7502858352368518 Test-Loss: 0.21274762885453424 Train-f1: 0.38095224382839205 Test-f1: 0.3576072715361939 Train-acc: 0.6854304635761589 Test-acc: 0.6447368421052632\n",
      "Epoch [3153/20000] Train-Loss: 0.7476315132448075 Test-Loss: 0.21279296185720972 Train-f1: 0.3907431100516356 Test-f1: 0.3576072715361939 Train-acc: 0.7019867549668874 Test-acc: 0.6447368421052632\n",
      "Epoch [3154/20000] Train-Loss: 0.7487761202190495 Test-Loss: 0.21300211513381834 Train-f1: 0.37880625099670495 Test-f1: 0.37347480106100794 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3155/20000] Train-Loss: 0.748843642437209 Test-Loss: 0.21140086246762196 Train-f1: 0.3866895733289176 Test-f1: 0.3713974358974359 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3156/20000] Train-Loss: 0.7500708366723738 Test-Loss: 0.21182007413675483 Train-f1: 0.38718224131757967 Test-f1: 0.37173468159383655 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3157/20000] Train-Loss: 0.7481471641361692 Test-Loss: 0.21237710216624694 Train-f1: 0.38903863330794763 Test-f1: 0.37347480106100794 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3158/20000] Train-Loss: 0.7509335128923154 Test-Loss: 0.21135642027541152 Train-f1: 0.38714214924900786 Test-f1: 0.3713974358974359 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3159/20000] Train-Loss: 0.7499273425030025 Test-Loss: 0.2114877536235307 Train-f1: 0.3799994614713845 Test-f1: 0.3713974358974359 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3160/20000] Train-Loss: 0.7489943364304706 Test-Loss: 0.2113270490407988 Train-f1: 0.3832623563277055 Test-f1: 0.3713974358974359 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3161/20000] Train-Loss: 0.7500825407941765 Test-Loss: 0.21121277558026985 Train-f1: 0.37551186971421907 Test-f1: 0.3833175833175833 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [3162/20000] Train-Loss: 0.7459848928398123 Test-Loss: 0.21212843632649542 Train-f1: 0.369734749762489 Test-f1: 0.3817102615694165 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3163/20000] Train-Loss: 0.7480080191065961 Test-Loss: 0.2114244154894036 Train-f1: 0.3831968740264421 Test-f1: 0.3718340516303165 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3164/20000] Train-Loss: 0.748034181064146 Test-Loss: 0.21157680745599564 Train-f1: 0.37881193856853085 Test-f1: 0.3725208125208125 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3165/20000] Train-Loss: 0.7467911490824644 Test-Loss: 0.21112793577188746 Train-f1: 0.38484692501134193 Test-f1: 0.3713974358974359 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3166/20000] Train-Loss: 0.7492637270739714 Test-Loss: 0.21156652251301353 Train-f1: 0.39224858002004676 Test-f1: 0.3576072715361939 Train-acc: 0.6986754966887417 Test-acc: 0.6447368421052632\n",
      "Epoch [3167/20000] Train-Loss: 0.7467101877081306 Test-Loss: 0.2112338181501174 Train-f1: 0.35451626787128243 Test-f1: 0.37732600732600735 Train-acc: 0.6688741721854304 Test-acc: 0.6710526315789473\n",
      "Epoch [3168/20000] Train-Loss: 0.746418299406491 Test-Loss: 0.21117710957734828 Train-f1: 0.39295842135222314 Test-f1: 0.37732600732600735 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3169/20000] Train-Loss: 0.747604983873909 Test-Loss: 0.21178052306117134 Train-f1: 0.3901479011160801 Test-f1: 0.3576072715361939 Train-acc: 0.695364238410596 Test-acc: 0.6447368421052632\n",
      "Epoch [3170/20000] Train-Loss: 0.7437735515410892 Test-Loss: 0.21338534789835076 Train-f1: 0.3801912489133584 Test-f1: 0.3747160012982798 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3171/20000] Train-Loss: 0.7489817601010368 Test-Loss: 0.21099281148784735 Train-f1: 0.3707767328456984 Test-f1: 0.3874509803921569 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [3172/20000] Train-Loss: 0.7433072261723862 Test-Loss: 0.2140998083067285 Train-f1: 0.38069113364254126 Test-f1: 0.35243741765480896 Train-acc: 0.6854304635761589 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [3173/20000] Train-Loss: 0.7500328318923862 Test-Loss: 0.2109835928687283 Train-f1: 0.38348200629965523 Test-f1: 0.3713974358974359 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3174/20000] Train-Loss: 0.7459396460417397 Test-Loss: 0.21100916416503904 Train-f1: 0.3809781058153926 Test-f1: 0.3713974358974359 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3175/20000] Train-Loss: 0.7461680958620924 Test-Loss: 0.2113091511343744 Train-f1: 0.3706890543323301 Test-f1: 0.37173468159383655 Train-acc: 0.6821192052980133 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3176/20000] Train-Loss: 0.7463377573503336 Test-Loss: 0.21081131244091786 Train-f1: 0.3961490208912889 Test-f1: 0.3713974358974359 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3177/20000] Train-Loss: 0.7479820859538654 Test-Loss: 0.21089220926515864 Train-f1: 0.38891572591231294 Test-f1: 0.3874509803921569 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3178/20000] Train-Loss: 0.7452711849735498 Test-Loss: 0.2112496820311838 Train-f1: 0.37784929251328303 Test-f1: 0.3725208125208125 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3179/20000] Train-Loss: 0.7478915563537958 Test-Loss: 0.21076426613937702 Train-f1: 0.39571803427784846 Test-f1: 0.3874509803921569 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3180/20000] Train-Loss: 0.7460128089898057 Test-Loss: 0.2107768358735675 Train-f1: 0.37655215727755786 Test-f1: 0.38165591397849463 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3181/20000] Train-Loss: 0.7458968720057891 Test-Loss: 0.21067981482103904 Train-f1: 0.38515428796666484 Test-f1: 0.3833175833175833 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3182/20000] Train-Loss: 0.7460890753928329 Test-Loss: 0.21095636161581424 Train-f1: 0.3864640643440542 Test-f1: 0.3718340516303165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3183/20000] Train-Loss: 0.7447301491812249 Test-Loss: 0.21061951072352353 Train-f1: 0.38232795470915715 Test-f1: 0.3833175833175833 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3184/20000] Train-Loss: 0.7444433604251993 Test-Loss: 0.21221187359967084 Train-f1: 0.3793164676741532 Test-f1: 0.3576072715361939 Train-acc: 0.6887417218543046 Test-acc: 0.6447368421052632\n",
      "Epoch [3185/20000] Train-Loss: 0.7461611456070109 Test-Loss: 0.21069443844305336 Train-f1: 0.39679729175899053 Test-f1: 0.3713974358974359 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3186/20000] Train-Loss: 0.7450875740268033 Test-Loss: 0.21068304823383593 Train-f1: 0.3823600782778865 Test-f1: 0.3874509803921569 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3187/20000] Train-Loss: 0.7461201818930752 Test-Loss: 0.2105158962732541 Train-f1: 0.3814954204595017 Test-f1: 0.387620103473762 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3188/20000] Train-Loss: 0.744539188411038 Test-Loss: 0.21328135257065445 Train-f1: 0.37684002383661086 Test-f1: 0.36694316436251917 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "Epoch [3189/20000] Train-Loss: 0.7446505941107434 Test-Loss: 0.2107492019701503 Train-f1: 0.3868512110726644 Test-f1: 0.3713974358974359 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3190/20000] Train-Loss: 0.744617178548697 Test-Loss: 0.21046721241323615 Train-f1: 0.3825257289879932 Test-f1: 0.3755952380952381 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3191/20000] Train-Loss: 0.741882115690263 Test-Loss: 0.21249120150251138 Train-f1: 0.38635124329595316 Test-f1: 0.3578571428571428 Train-acc: 0.6920529801324503 Test-acc: 0.6447368421052632\n",
      "Epoch [3192/20000] Train-Loss: 0.7466376489518777 Test-Loss: 0.2105657840331564 Train-f1: 0.37736992346894105 Test-f1: 0.37732600732600735 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3193/20000] Train-Loss: 0.7430265821504447 Test-Loss: 0.2108848488091518 Train-f1: 0.379258138787749 Test-f1: 0.3576072715361939 Train-acc: 0.6821192052980133 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3194/20000] Train-Loss: 0.7451704391991631 Test-Loss: 0.21030454080174366 Train-f1: 0.3808072172568024 Test-f1: 0.3943266634061284 Train-acc: 0.6854304635761589 Test-acc: 0.6973684210526315\n",
      "Epoch [3195/20000] Train-Loss: 0.744210559865598 Test-Loss: 0.21149553694571913 Train-f1: 0.3822688655672164 Test-f1: 0.3576072715361939 Train-acc: 0.6920529801324503 Test-acc: 0.6447368421052632\n",
      "Epoch [3196/20000] Train-Loss: 0.7449910077186604 Test-Loss: 0.21032499378978214 Train-f1: 0.38343631652381904 Test-f1: 0.3874509803921569 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3197/20000] Train-Loss: 0.7424759062806542 Test-Loss: 0.21033473247303072 Train-f1: 0.3787604363522619 Test-f1: 0.3713974358974359 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3198/20000] Train-Loss: 0.7429749598328749 Test-Loss: 0.21026564583053012 Train-f1: 0.38425679433998283 Test-f1: 0.36307826956739186 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3199/20000] Train-Loss: 0.7451598912701758 Test-Loss: 0.21016361917643286 Train-f1: 0.3923444184319146 Test-f1: 0.3688679858687878 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3200/20000] Train-Loss: 0.7438329524872 Test-Loss: 0.21018792943792694 Train-f1: 0.38593150684931504 Test-f1: 0.36862695125836253 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3201/20000] Train-Loss: 0.7423007611543295 Test-Loss: 0.21017493661873665 Train-f1: 0.3867709542508345 Test-f1: 0.38165591397849463 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3202/20000] Train-Loss: 0.7413403568425426 Test-Loss: 0.2104949516824688 Train-f1: 0.3835361580121058 Test-f1: 0.3718340516303165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3203/20000] Train-Loss: 0.7441072856656284 Test-Loss: 0.21037726464458373 Train-f1: 0.3865111487829212 Test-f1: 0.3718340516303165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3204/20000] Train-Loss: 0.7431748985605803 Test-Loss: 0.21015412666304198 Train-f1: 0.38472247771906476 Test-f1: 0.3833175833175833 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3205/20000] Train-Loss: 0.742796637899623 Test-Loss: 0.21013375456652542 Train-f1: 0.39288719648355785 Test-f1: 0.3874509803921569 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3206/20000] Train-Loss: 0.7417364952097714 Test-Loss: 0.2102409380860849 Train-f1: 0.3884268702331186 Test-f1: 0.3874509803921569 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3207/20000] Train-Loss: 0.7419362863858244 Test-Loss: 0.21017403881059632 Train-f1: 0.3880459770114943 Test-f1: 0.38165591397849463 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3208/20000] Train-Loss: 0.7431201438419038 Test-Loss: 0.21050499337070303 Train-f1: 0.3781232386043383 Test-f1: 0.3576072715361939 Train-acc: 0.6821192052980133 Test-acc: 0.6447368421052632\n",
      "Epoch [3209/20000] Train-Loss: 0.7406623695699293 Test-Loss: 0.21155691289352643 Train-f1: 0.3869380588558671 Test-f1: 0.37347480106100794 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3210/20000] Train-Loss: 0.7397164166566605 Test-Loss: 0.21220607671788433 Train-f1: 0.3707640511777266 Test-f1: 0.37435897435897436 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3211/20000] Train-Loss: 0.7427028032647506 Test-Loss: 0.20995465448709405 Train-f1: 0.3881821312502956 Test-f1: 0.3833175833175833 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3212/20000] Train-Loss: 0.7399402141873266 Test-Loss: 0.21045210703124692 Train-f1: 0.3946696329072658 Test-f1: 0.3817102615694165 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3213/20000] Train-Loss: 0.7411688318330764 Test-Loss: 0.20988634673754514 Train-f1: 0.38579822418211074 Test-f1: 0.3833175833175833 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3214/20000] Train-Loss: 0.7415945006304794 Test-Loss: 0.21044935055151298 Train-f1: 0.3872586055788933 Test-f1: 0.35361502347417845 Train-acc: 0.6920529801324503 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3215/20000] Train-Loss: 0.7421411353485567 Test-Loss: 0.20985785649004554 Train-f1: 0.39853685276342493 Test-f1: 0.3874509803921569 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3216/20000] Train-Loss: 0.7407039891867698 Test-Loss: 0.20980810651824724 Train-f1: 0.3918171776396529 Test-f1: 0.3833175833175833 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3217/20000] Train-Loss: 0.7416045033781 Test-Loss: 0.20988405883403366 Train-f1: 0.374243560898954 Test-f1: 0.37732600732600735 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [3218/20000] Train-Loss: 0.7390135243069113 Test-Loss: 0.21051017661590307 Train-f1: 0.3904961832061069 Test-f1: 0.37347480106100794 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3219/20000] Train-Loss: 0.7428502552086265 Test-Loss: 0.20972412634848564 Train-f1: 0.37561410859289684 Test-f1: 0.38165591397849463 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [3220/20000] Train-Loss: 0.7383995133981484 Test-Loss: 0.21195917819680513 Train-f1: 0.38881234129201214 Test-f1: 0.37003120910097653 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "Epoch [3221/20000] Train-Loss: 0.7422739801426842 Test-Loss: 0.2101153274124631 Train-f1: 0.38598342832484295 Test-f1: 0.37173468159383655 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3222/20000] Train-Loss: 0.7390439894493398 Test-Loss: 0.20983760842219593 Train-f1: 0.39117941864350153 Test-f1: 0.37732600732600735 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3223/20000] Train-Loss: 0.7434718061757396 Test-Loss: 0.20976014237169743 Train-f1: 0.3789951068146557 Test-f1: 0.37732600732600735 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3224/20000] Train-Loss: 0.7387536811042402 Test-Loss: 0.2098826295472643 Train-f1: 0.3836734693877551 Test-f1: 0.37173468159383655 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [3225/20000] Train-Loss: 0.7422245651331945 Test-Loss: 0.2097559656753538 Train-f1: 0.38418088956536445 Test-f1: 0.37732600732600735 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3226/20000] Train-Loss: 0.7397375360939189 Test-Loss: 0.20961991658255322 Train-f1: 0.37755335334512113 Test-f1: 0.3833175833175833 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3227/20000] Train-Loss: 0.7392210510253948 Test-Loss: 0.20951132331221767 Train-f1: 0.39121276136968497 Test-f1: 0.36862695125836253 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3228/20000] Train-Loss: 0.7394844635463133 Test-Loss: 0.20965363289037936 Train-f1: 0.3817569036747119 Test-f1: 0.36307826956739186 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3229/20000] Train-Loss: 0.7396723296412501 Test-Loss: 0.2095012661050672 Train-f1: 0.3837785716696317 Test-f1: 0.36307826956739186 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [3230/20000] Train-Loss: 0.7381410856658933 Test-Loss: 0.2102013854841994 Train-f1: 0.3904044848335321 Test-f1: 0.3817164103704004 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3231/20000] Train-Loss: 0.7394225576436604 Test-Loss: 0.21025911941260667 Train-f1: 0.38007436399217226 Test-f1: 0.3817164103704004 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3232/20000] Train-Loss: 0.7380611220940019 Test-Loss: 0.20997691946600577 Train-f1: 0.38503401360544215 Test-f1: 0.37173468159383655 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3233/20000] Train-Loss: 0.7389731109301559 Test-Loss: 0.20939376418985717 Train-f1: 0.3944919657575747 Test-f1: 0.3829831194859398 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3234/20000] Train-Loss: 0.7377202703466287 Test-Loss: 0.2093979538324478 Train-f1: 0.37555130367938805 Test-f1: 0.3829831194859398 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [3235/20000] Train-Loss: 0.7380170734715422 Test-Loss: 0.20954348477177936 Train-f1: 0.3864755625510322 Test-f1: 0.3713974358974359 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3236/20000] Train-Loss: 0.7375480665383332 Test-Loss: 0.21083125642312875 Train-f1: 0.39792892683050035 Test-f1: 0.37347480106100794 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3237/20000] Train-Loss: 0.7399435414040941 Test-Loss: 0.20951652764216638 Train-f1: 0.3789185410584317 Test-f1: 0.3713974358974359 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3238/20000] Train-Loss: 0.7403214671755666 Test-Loss: 0.20932515216241637 Train-f1: 0.37444414058291686 Test-f1: 0.3874509803921569 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3239/20000] Train-Loss: 0.7366810620092421 Test-Loss: 0.2092367299593339 Train-f1: 0.3848676108374384 Test-f1: 0.36862695125836253 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3240/20000] Train-Loss: 0.7379941060962053 Test-Loss: 0.20964319847852061 Train-f1: 0.36468650362558847 Test-f1: 0.382280701754386 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "Epoch [3241/20000] Train-Loss: 0.7389930057229263 Test-Loss: 0.21083416714743153 Train-f1: 0.3881653740078466 Test-f1: 0.3576072715361939 Train-acc: 0.695364238410596 Test-acc: 0.6447368421052632\n",
      "Epoch [3242/20000] Train-Loss: 0.7385092312911242 Test-Loss: 0.2092869635788049 Train-f1: 0.3798700514407088 Test-f1: 0.37732600732600735 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3243/20000] Train-Loss: 0.7363194696134225 Test-Loss: 0.21000823615493885 Train-f1: 0.38564842410511513 Test-f1: 0.3817102615694165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3244/20000] Train-Loss: 0.7415059573025937 Test-Loss: 0.20926588033282578 Train-f1: 0.37890662106963047 Test-f1: 0.36307826956739186 Train-acc: 0.6854304635761589 Test-acc: 0.6578947368421053\n",
      "Epoch [3245/20000] Train-Loss: 0.7364434619208479 Test-Loss: 0.21159477390165338 Train-f1: 0.39129483965946593 Test-f1: 0.37003120910097653 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3246/20000] Train-Loss: 0.7387021730005895 Test-Loss: 0.20922772169914083 Train-f1: 0.36998380175865103 Test-f1: 0.37732600732600735 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3247/20000] Train-Loss: 0.7377991807069334 Test-Loss: 0.209053270455081 Train-f1: 0.386246921182266 Test-f1: 0.3833175833175833 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3248/20000] Train-Loss: 0.7365468555167797 Test-Loss: 0.20903407127661164 Train-f1: 0.3855557978515776 Test-f1: 0.3833175833175833 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3249/20000] Train-Loss: 0.7361032417586245 Test-Loss: 0.20893807391086494 Train-f1: 0.38217967599410896 Test-f1: 0.36862695125836253 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3250/20000] Train-Loss: 0.7360808692444857 Test-Loss: 0.20909601551977405 Train-f1: 0.3822924494169381 Test-f1: 0.3833175833175833 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3251/20000] Train-Loss: 0.7365795904144737 Test-Loss: 0.20901992456476523 Train-f1: 0.38798460138129054 Test-f1: 0.3829831194859398 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3252/20000] Train-Loss: 0.7370046793380443 Test-Loss: 0.20990562612066152 Train-f1: 0.3806719054141735 Test-f1: 0.3817164103704004 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3253/20000] Train-Loss: 0.7349537238480254 Test-Loss: 0.2088895463519038 Train-f1: 0.38785806264810535 Test-f1: 0.3937837837837838 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3254/20000] Train-Loss: 0.7378375093810933 Test-Loss: 0.20886712224259152 Train-f1: 0.38606000632779314 Test-f1: 0.39327106683271074 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3255/20000] Train-Loss: 0.7354431296289838 Test-Loss: 0.2088434698794878 Train-f1: 0.3921196391100939 Test-f1: 0.39327106683271074 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3256/20000] Train-Loss: 0.7334737068602903 Test-Loss: 0.2093775158849502 Train-f1: 0.3892059831963677 Test-f1: 0.36091866028708136 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "Epoch [3257/20000] Train-Loss: 0.7352945629482136 Test-Loss: 0.20904731971586213 Train-f1: 0.3978408104666348 Test-f1: 0.37732600732600735 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "Epoch [3258/20000] Train-Loss: 0.7353513190685314 Test-Loss: 0.20895574394776031 Train-f1: 0.37995505452402006 Test-f1: 0.3833175833175833 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3259/20000] Train-Loss: 0.733555503789789 Test-Loss: 0.20949597376156143 Train-f1: 0.3968154579919286 Test-f1: 0.3817102615694165 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3260/20000] Train-Loss: 0.735762259787474 Test-Loss: 0.209938819316837 Train-f1: 0.38242813414958743 Test-f1: 0.3576072715361939 Train-acc: 0.6854304635761589 Test-acc: 0.6447368421052632\n",
      "Epoch [3261/20000] Train-Loss: 0.7357532303378318 Test-Loss: 0.20901645823708814 Train-f1: 0.38557695511560325 Test-f1: 0.3713974358974359 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3262/20000] Train-Loss: 0.7348935600823758 Test-Loss: 0.2087877618983268 Train-f1: 0.3977706542821649 Test-f1: 0.3833175833175833 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3263/20000] Train-Loss: 0.7351178069600226 Test-Loss: 0.20879961008495626 Train-f1: 0.381257744948044 Test-f1: 0.3874509803921569 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3264/20000] Train-Loss: 0.7363236588740947 Test-Loss: 0.20931456596305842 Train-f1: 0.39104961832061075 Test-f1: 0.3576072715361939 Train-acc: 0.695364238410596 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3265/20000] Train-Loss: 0.7352408319919016 Test-Loss: 0.20878139972815993 Train-f1: 0.3743167215180724 Test-f1: 0.3874509803921569 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3266/20000] Train-Loss: 0.7332396857248185 Test-Loss: 0.20865612504098593 Train-f1: 0.37890034364261166 Test-f1: 0.3833175833175833 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "Epoch [3267/20000] Train-Loss: 0.7358752729577186 Test-Loss: 0.20866153246556973 Train-f1: 0.38246487174209964 Test-f1: 0.3829831194859398 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3268/20000] Train-Loss: 0.7343609182527164 Test-Loss: 0.2097846000495498 Train-f1: 0.386874199851762 Test-f1: 0.35361502347417845 Train-acc: 0.6887417218543046 Test-acc: 0.6447368421052632\n",
      "Epoch [3269/20000] Train-Loss: 0.7311865020232509 Test-Loss: 0.20906697608972782 Train-f1: 0.3868464756271002 Test-f1: 0.3718340516303165 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3270/20000] Train-Loss: 0.7351641488319199 Test-Loss: 0.20864412116503958 Train-f1: 0.3776759019334263 Test-f1: 0.3833175833175833 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "Epoch [3271/20000] Train-Loss: 0.7335690570074689 Test-Loss: 0.20897227112399666 Train-f1: 0.39137365961322407 Test-f1: 0.37173468159383655 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3272/20000] Train-Loss: 0.7338865605405092 Test-Loss: 0.20842180537242208 Train-f1: 0.3886452129523047 Test-f1: 0.3937837837837838 Train-acc: 0.6920529801324503 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3273/20000] Train-Loss: 0.7350275289761105 Test-Loss: 0.20840424804686786 Train-f1: 0.38573478364044744 Test-f1: 0.39327106683271074 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "Epoch [3274/20000] Train-Loss: 0.7342299814774633 Test-Loss: 0.20853432114258447 Train-f1: 0.3913099781574358 Test-f1: 0.3833175833175833 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3275/20000] Train-Loss: 0.7345427047837123 Test-Loss: 0.2084805179273954 Train-f1: 0.38638482941700947 Test-f1: 0.3829831194859398 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3276/20000] Train-Loss: 0.7338694999108535 Test-Loss: 0.2084947240866053 Train-f1: 0.38678409712892475 Test-f1: 0.3755952380952381 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3277/20000] Train-Loss: 0.7334347005085848 Test-Loss: 0.20910599826941179 Train-f1: 0.38099772519211034 Test-f1: 0.3718340516303165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3278/20000] Train-Loss: 0.7332914720291704 Test-Loss: 0.20910823733299713 Train-f1: 0.39009640061969436 Test-f1: 0.3817164103704004 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3279/20000] Train-Loss: 0.732360785713172 Test-Loss: 0.2089521045860261 Train-f1: 0.37787903235919923 Test-f1: 0.3817102615694165 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3280/20000] Train-Loss: 0.7347501570154329 Test-Loss: 0.20918083071715968 Train-f1: 0.38551729354187225 Test-f1: 0.3817102615694165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3281/20000] Train-Loss: 0.7344311830631614 Test-Loss: 0.20821415342632346 Train-f1: 0.387791686922703 Test-f1: 0.39327106683271074 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3282/20000] Train-Loss: 0.7326344771672434 Test-Loss: 0.20820772825379386 Train-f1: 0.3849060210258044 Test-f1: 0.3833175833175833 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3283/20000] Train-Loss: 0.7347904647670228 Test-Loss: 0.20864487321431732 Train-f1: 0.3941973537382433 Test-f1: 0.3718340516303165 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3284/20000] Train-Loss: 0.7342268539091187 Test-Loss: 0.20828362385284752 Train-f1: 0.3729720613703165 Test-f1: 0.37732600732600735 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3285/20000] Train-Loss: 0.7331542418064836 Test-Loss: 0.2081952879751373 Train-f1: 0.3913895297007805 Test-f1: 0.3833175833175833 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3286/20000] Train-Loss: 0.7315631417435354 Test-Loss: 0.2085031358452333 Train-f1: 0.37983145381264505 Test-f1: 0.3713974358974359 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3287/20000] Train-Loss: 0.7285592643804628 Test-Loss: 0.21219719149371866 Train-f1: 0.3792392778372316 Test-f1: 0.3587961696306429 Train-acc: 0.6887417218543046 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [3288/20000] Train-Loss: 0.7328703194648919 Test-Loss: 0.20808391152287167 Train-f1: 0.39661802217633485 Test-f1: 0.39327106683271074 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3289/20000] Train-Loss: 0.732901680182393 Test-Loss: 0.20841053976728735 Train-f1: 0.3743435425144571 Test-f1: 0.37173468159383655 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "Epoch [3290/20000] Train-Loss: 0.7332578399704277 Test-Loss: 0.20819927846248112 Train-f1: 0.3849281341495875 Test-f1: 0.37732600732600735 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3291/20000] Train-Loss: 0.7310869511118571 Test-Loss: 0.208041873117767 Train-f1: 0.38493150684931504 Test-f1: 0.3829831194859398 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3292/20000] Train-Loss: 0.7308373258619814 Test-Loss: 0.2083376903755171 Train-f1: 0.3807567280394468 Test-f1: 0.3576072715361939 Train-acc: 0.6821192052980133 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3293/20000] Train-Loss: 0.7298919059731254 Test-Loss: 0.20793900518743202 Train-f1: 0.37371515845742653 Test-f1: 0.3833175833175833 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "Epoch [3294/20000] Train-Loss: 0.7299070348838753 Test-Loss: 0.20999218908946254 Train-f1: 0.3823961262105592 Test-f1: 0.36275724275724275 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3295/20000] Train-Loss: 0.7319363981355081 Test-Loss: 0.20898525371095206 Train-f1: 0.3936020828871279 Test-f1: 0.37173468159383655 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "Epoch [3296/20000] Train-Loss: 0.7324640477063294 Test-Loss: 0.2082022691996306 Train-f1: 0.38871893771410876 Test-f1: 0.3576072715361939 Train-acc: 0.6920529801324503 Test-acc: 0.6447368421052632\n",
      "Epoch [3297/20000] Train-Loss: 0.7313839032616457 Test-Loss: 0.20796200313774782 Train-f1: 0.37997976444120757 Test-f1: 0.3937837837837838 Train-acc: 0.6821192052980133 Test-acc: 0.6973684210526315\n",
      "Epoch [3298/20000] Train-Loss: 0.7316151112602958 Test-Loss: 0.20806259277675068 Train-f1: 0.3868801878936424 Test-f1: 0.3589655172413793 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3299/20000] Train-Loss: 0.7315138118180956 Test-Loss: 0.20777960070463083 Train-f1: 0.3729086079492898 Test-f1: 0.39327106683271074 Train-acc: 0.6854304635761589 Test-acc: 0.6973684210526315\n",
      "Epoch [3300/20000] Train-Loss: 0.7313943547053994 Test-Loss: 0.2086447497957779 Train-f1: 0.3822203532380152 Test-f1: 0.35361502347417845 Train-acc: 0.6887417218543046 Test-acc: 0.6447368421052632\n",
      "Epoch [3301/20000] Train-Loss: 0.7305679956489217 Test-Loss: 0.20800480136868837 Train-f1: 0.3896664110429448 Test-f1: 0.3819837398373984 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3302/20000] Train-Loss: 0.7302871393296853 Test-Loss: 0.208162069213131 Train-f1: 0.3800145450753972 Test-f1: 0.35361502347417845 Train-acc: 0.6887417218543046 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3303/20000] Train-Loss: 0.7324972980815398 Test-Loss: 0.20770093986757565 Train-f1: 0.39238503214162446 Test-f1: 0.39327106683271074 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3304/20000] Train-Loss: 0.7292156369071584 Test-Loss: 0.20768380354298993 Train-f1: 0.39364073538609823 Test-f1: 0.39327106683271074 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3305/20000] Train-Loss: 0.7298140141354248 Test-Loss: 0.2078739916440702 Train-f1: 0.3846298477241265 Test-f1: 0.3833175833175833 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3306/20000] Train-Loss: 0.7285203768232157 Test-Loss: 0.20777473794050788 Train-f1: 0.39839271169730867 Test-f1: 0.387620103473762 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3307/20000] Train-Loss: 0.7268543173956552 Test-Loss: 0.20888564705571874 Train-f1: 0.3864640643440542 Test-f1: 0.37347480106100794 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3308/20000] Train-Loss: 0.7302672896233671 Test-Loss: 0.20784095142708808 Train-f1: 0.390670527632332 Test-f1: 0.37242424242424244 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3309/20000] Train-Loss: 0.726390155178943 Test-Loss: 0.20865950930610372 Train-f1: 0.38788515263361817 Test-f1: 0.37347480106100794 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3310/20000] Train-Loss: 0.7319497568339499 Test-Loss: 0.20753629927143774 Train-f1: 0.3871552257266543 Test-f1: 0.3818790218790219 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3311/20000] Train-Loss: 0.7303159715756002 Test-Loss: 0.20751605805665352 Train-f1: 0.39025977155198455 Test-f1: 0.39327106683271074 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3312/20000] Train-Loss: 0.728563622501296 Test-Loss: 0.20800401726522247 Train-f1: 0.3713583722053864 Test-f1: 0.3817164103704004 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3313/20000] Train-Loss: 0.7302255490586491 Test-Loss: 0.20755824364334355 Train-f1: 0.37838297483255995 Test-f1: 0.3829831194859398 Train-acc: 0.6821192052980133 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3314/20000] Train-Loss: 0.727773932561455 Test-Loss: 0.2074380152163871 Train-f1: 0.393969396509001 Test-f1: 0.39327106683271074 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3315/20000] Train-Loss: 0.7287421758588016 Test-Loss: 0.20741082742824646 Train-f1: 0.3862454544850189 Test-f1: 0.39327106683271074 Train-acc: 0.6887417218543046 Test-acc: 0.6973684210526315\n",
      "Epoch [3316/20000] Train-Loss: 0.7295763058423805 Test-Loss: 0.20804738047011062 Train-f1: 0.3892784540433444 Test-f1: 0.36679195407046383 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "Epoch [3317/20000] Train-Loss: 0.7272786242848858 Test-Loss: 0.2075305528905027 Train-f1: 0.3845332493141432 Test-f1: 0.37815089964357795 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3318/20000] Train-Loss: 0.7282964285378212 Test-Loss: 0.20772392391338718 Train-f1: 0.3854963570127505 Test-f1: 0.3817102615694165 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3319/20000] Train-Loss: 0.7273614525487506 Test-Loss: 0.20737219702038603 Train-f1: 0.40144865673320956 Test-f1: 0.39327106683271074 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3320/20000] Train-Loss: 0.7260647120802031 Test-Loss: 0.20838580598769368 Train-f1: 0.38457000133619756 Test-f1: 0.3817102615694165 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3321/20000] Train-Loss: 0.7277067058844303 Test-Loss: 0.20878076785657307 Train-f1: 0.3845737389161707 Test-f1: 0.38470418470418466 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3322/20000] Train-Loss: 0.7290792757488185 Test-Loss: 0.2073551636661396 Train-f1: 0.39296276168614697 Test-f1: 0.3829831194859398 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3323/20000] Train-Loss: 0.7278593013003256 Test-Loss: 0.20875226419470852 Train-f1: 0.3943150598139925 Test-f1: 0.3817164103704004 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "Epoch [3324/20000] Train-Loss: 0.7239241374727177 Test-Loss: 0.20757640107838135 Train-f1: 0.39364073538609823 Test-f1: 0.37173468159383655 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "Epoch [3325/20000] Train-Loss: 0.7307646181758427 Test-Loss: 0.20769714127629704 Train-f1: 0.3999270880023906 Test-f1: 0.3817102615694165 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "Epoch [3326/20000] Train-Loss: 0.7284962281379636 Test-Loss: 0.20753077274821696 Train-f1: 0.37993349753694583 Test-f1: 0.3833175833175833 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3327/20000] Train-Loss: 0.7288539584667029 Test-Loss: 0.208864351944688 Train-f1: 0.38712643678160924 Test-f1: 0.37435897435897436 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3328/20000] Train-Loss: 0.7258186096433528 Test-Loss: 0.20993547908177917 Train-f1: 0.3969033408655649 Test-f1: 0.3748820781078846 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3329/20000] Train-Loss: 0.7284370635100048 Test-Loss: 0.20713519905868075 Train-f1: 0.3888119576087467 Test-f1: 0.3829831194859398 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3330/20000] Train-Loss: 0.7262832793892159 Test-Loss: 0.20879123041685585 Train-f1: 0.4019922059518186 Test-f1: 0.33980193002090503 Train-acc: 0.7052980132450332 Test-acc: 0.631578947368421\n",
      "Epoch [3331/20000] Train-Loss: 0.7257361601029587 Test-Loss: 0.20719470611152258 Train-f1: 0.38182361418679045 Test-f1: 0.36862695125836253 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3332/20000] Train-Loss: 0.7254471587037773 Test-Loss: 0.20937418447381703 Train-f1: 0.38971259330895464 Test-f1: 0.36694316436251917 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3333/20000] Train-Loss: 0.7279226360940615 Test-Loss: 0.20823144185142786 Train-f1: 0.37658246225319403 Test-f1: 0.36679195407046383 Train-acc: 0.6821192052980133 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3334/20000] Train-Loss: 0.7274444169358485 Test-Loss: 0.20699333929723326 Train-f1: 0.3800064368974154 Test-f1: 0.39327106683271074 Train-acc: 0.6854304635761589 Test-acc: 0.6973684210526315\n",
      "Epoch [3335/20000] Train-Loss: 0.7259740932169149 Test-Loss: 0.20805827685652142 Train-f1: 0.3920021645021645 Test-f1: 0.35361502347417845 Train-acc: 0.6986754966887417 Test-acc: 0.6447368421052632\n",
      "Epoch [3336/20000] Train-Loss: 0.7274963283278529 Test-Loss: 0.20859323017625606 Train-f1: 0.39346685082872934 Test-f1: 0.3576072715361939 Train-acc: 0.7086092715231788 Test-acc: 0.6447368421052632\n",
      "Epoch [3337/20000] Train-Loss: 0.7281607330742311 Test-Loss: 0.20702888434723543 Train-f1: 0.38790564033223873 Test-f1: 0.3829831194859398 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3338/20000] Train-Loss: 0.7247554117816236 Test-Loss: 0.20903774564578517 Train-f1: 0.3831968740264421 Test-f1: 0.3930313588850174 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3339/20000] Train-Loss: 0.7250370778325288 Test-Loss: 0.2078646689531902 Train-f1: 0.3944548937652386 Test-f1: 0.3576072715361939 Train-acc: 0.6986754966887417 Test-acc: 0.6447368421052632\n",
      "Epoch [3340/20000] Train-Loss: 0.7253777509719185 Test-Loss: 0.20708524565138195 Train-f1: 0.3844825443077128 Test-f1: 0.3937837837837838 Train-acc: 0.6920529801324503 Test-acc: 0.6973684210526315\n",
      "Epoch [3341/20000] Train-Loss: 0.7257425111934722 Test-Loss: 0.2074694415240314 Train-f1: 0.3951438425419339 Test-f1: 0.37173468159383655 Train-acc: 0.7052980132450332 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3342/20000] Train-Loss: 0.7260215036494181 Test-Loss: 0.20687122270286773 Train-f1: 0.38098387068527695 Test-f1: 0.3829831194859398 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3343/20000] Train-Loss: 0.7270190683950162 Test-Loss: 0.20705069890925792 Train-f1: 0.39347311476054403 Test-f1: 0.3817102615694165 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3344/20000] Train-Loss: 0.7254782054317818 Test-Loss: 0.20677824388601107 Train-f1: 0.3741384388807069 Test-f1: 0.3813858552700798 Train-acc: 0.6788079470198676 Test-acc: 0.6842105263157895\n",
      "Epoch [3345/20000] Train-Loss: 0.7249875905908212 Test-Loss: 0.2069092531958992 Train-f1: 0.3855906786262355 Test-f1: 0.3829831194859398 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3346/20000] Train-Loss: 0.7259705263801018 Test-Loss: 0.20673163321424942 Train-f1: 0.37833974585696284 Test-f1: 0.39327106683271074 Train-acc: 0.6821192052980133 Test-acc: 0.6973684210526315\n",
      "Epoch [3347/20000] Train-Loss: 0.7251821659296691 Test-Loss: 0.2067704386007158 Train-f1: 0.39042403248924995 Test-f1: 0.39327106683271074 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "Epoch [3348/20000] Train-Loss: 0.7246391343462609 Test-Loss: 0.20680783869054442 Train-f1: 0.39404028464434876 Test-f1: 0.37815089964357795 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3349/20000] Train-Loss: 0.723507382451339 Test-Loss: 0.20709481348787817 Train-f1: 0.36929807704626105 Test-f1: 0.37173468159383655 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "Epoch [3350/20000] Train-Loss: 0.7237082046512647 Test-Loss: 0.20710386888201443 Train-f1: 0.38800179495301446 Test-f1: 0.3833175833175833 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3351/20000] Train-Loss: 0.7235834258551781 Test-Loss: 0.20712071168703855 Train-f1: 0.3818044060258593 Test-f1: 0.38393822393822397 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3352/20000] Train-Loss: 0.7246515742109038 Test-Loss: 0.20681871993428358 Train-f1: 0.38685636856368566 Test-f1: 0.383598667160311 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3353/20000] Train-Loss: 0.7234941205848123 Test-Loss: 0.2066959994574214 Train-f1: 0.37745012719239524 Test-f1: 0.383598667160311 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3354/20000] Train-Loss: 0.7246318782442905 Test-Loss: 0.20661174753203723 Train-f1: 0.3967129056819263 Test-f1: 0.3829831194859398 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3355/20000] Train-Loss: 0.7236810529202344 Test-Loss: 0.20752137423403477 Train-f1: 0.3925490196078431 Test-f1: 0.37173468159383655 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3356/20000] Train-Loss: 0.7245720809471462 Test-Loss: 0.20650728058982293 Train-f1: 0.4010545632046192 Test-f1: 0.39327106683271074 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3357/20000] Train-Loss: 0.7235696040724624 Test-Loss: 0.20719696763413306 Train-f1: 0.3910974720466699 Test-f1: 0.3718340516303165 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3358/20000] Train-Loss: 0.7249823600286556 Test-Loss: 0.20664383500855873 Train-f1: 0.3865379901343515 Test-f1: 0.3829831194859398 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3359/20000] Train-Loss: 0.7251634112081112 Test-Loss: 0.20699959168389342 Train-f1: 0.3927511762454695 Test-f1: 0.3698920797512347 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3360/20000] Train-Loss: 0.7230821193580219 Test-Loss: 0.20642219942370604 Train-f1: 0.3845523364225878 Test-f1: 0.39361844303374544 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "Epoch [3361/20000] Train-Loss: 0.7214193727185353 Test-Loss: 0.20806193378375779 Train-f1: 0.3836931424224677 Test-f1: 0.3576072715361939 Train-acc: 0.6920529801324503 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3362/20000] Train-Loss: 0.7247405086560184 Test-Loss: 0.20635418216536935 Train-f1: 0.3877313936327219 Test-f1: 0.39361844303374544 Train-acc: 0.6920529801324503 Test-acc: 0.6973684210526315\n",
      "Epoch [3363/20000] Train-Loss: 0.7246222919496715 Test-Loss: 0.20723361024315134 Train-f1: 0.3767581175743098 Test-f1: 0.3732143626880469 Train-acc: 0.6854304635761589 Test-acc: 0.6710526315789473\n",
      "Epoch [3364/20000] Train-Loss: 0.726296247939282 Test-Loss: 0.20656529875881052 Train-f1: 0.39309620253517324 Test-f1: 0.383598667160311 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3365/20000] Train-Loss: 0.7232765665451788 Test-Loss: 0.2064064440601308 Train-f1: 0.37983449477351916 Test-f1: 0.3829831194859398 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3366/20000] Train-Loss: 0.7238173272399209 Test-Loss: 0.2062890064695187 Train-f1: 0.40452557981013265 Test-f1: 0.39327106683271074 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3367/20000] Train-Loss: 0.7230342539141839 Test-Loss: 0.2065519047604909 Train-f1: 0.3857050629288714 Test-f1: 0.3589655172413793 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3368/20000] Train-Loss: 0.7236748064214763 Test-Loss: 0.20620559286748222 Train-f1: 0.4043615820981771 Test-f1: 0.36862695125836253 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "Epoch [3369/20000] Train-Loss: 0.722386095245501 Test-Loss: 0.20629515994253614 Train-f1: 0.3800013720757635 Test-f1: 0.39327106683271074 Train-acc: 0.6887417218543046 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3370/20000] Train-Loss: 0.7197992519910122 Test-Loss: 0.20617232851342393 Train-f1: 0.383448275862069 Test-f1: 0.3813858552700798 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3371/20000] Train-Loss: 0.7213047340931552 Test-Loss: 0.20868276801653685 Train-f1: 0.3813759425224989 Test-f1: 0.37003120910097653 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [3372/20000] Train-Loss: 0.7237295858448072 Test-Loss: 0.20634857166119944 Train-f1: 0.38319952567940596 Test-f1: 0.383598667160311 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3373/20000] Train-Loss: 0.7223506638996191 Test-Loss: 0.2080043559392479 Train-f1: 0.39724630121459 Test-f1: 0.3928571428571429 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3374/20000] Train-Loss: 0.7206750357023687 Test-Loss: 0.20610857663755885 Train-f1: 0.401881541191886 Test-f1: 0.37815089964357795 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3375/20000] Train-Loss: 0.7220323070954152 Test-Loss: 0.20735416327342693 Train-f1: 0.3810215557638238 Test-f1: 0.3698920797512347 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [3376/20000] Train-Loss: 0.7193566183484676 Test-Loss: 0.2063013703611242 Train-f1: 0.40203946747238223 Test-f1: 0.3833175833175833 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3377/20000] Train-Loss: 0.72135085125428 Test-Loss: 0.20600648395280843 Train-f1: 0.38549618320610685 Test-f1: 0.39327106683271074 Train-acc: 0.6887417218543046 Test-acc: 0.6973684210526315\n",
      "Epoch [3378/20000] Train-Loss: 0.7203148861315627 Test-Loss: 0.20601923475856265 Train-f1: 0.3902033790972361 Test-f1: 0.3829831194859398 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3379/20000] Train-Loss: 0.7225035004353672 Test-Loss: 0.20620502206224095 Train-f1: 0.39014462809917355 Test-f1: 0.383598667160311 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3380/20000] Train-Loss: 0.7205202596907628 Test-Loss: 0.2061600750568309 Train-f1: 0.40026353276353277 Test-f1: 0.38393822393822397 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3381/20000] Train-Loss: 0.7203434938438719 Test-Loss: 0.2064204858385699 Train-f1: 0.3989506072874494 Test-f1: 0.3833175833175833 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3382/20000] Train-Loss: 0.7224347900918588 Test-Loss: 0.2064013978648949 Train-f1: 0.3903776107635005 Test-f1: 0.3833175833175833 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3383/20000] Train-Loss: 0.7215439181135372 Test-Loss: 0.2059051147938624 Train-f1: 0.3948904267589389 Test-f1: 0.3829831194859398 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3384/20000] Train-Loss: 0.7207709293385862 Test-Loss: 0.20689365357958198 Train-f1: 0.3887168457077669 Test-f1: 0.3698920797512347 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [3385/20000] Train-Loss: 0.7211100043075135 Test-Loss: 0.20674996503119783 Train-f1: 0.39152715349705774 Test-f1: 0.382280701754386 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3386/20000] Train-Loss: 0.7217522367816372 Test-Loss: 0.2062478803894544 Train-f1: 0.3765666476647837 Test-f1: 0.37173468159383655 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3387/20000] Train-Loss: 0.721474911593041 Test-Loss: 0.2058434891737117 Train-f1: 0.3910014296993364 Test-f1: 0.3724667833968483 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3388/20000] Train-Loss: 0.7191406394330315 Test-Loss: 0.2063780021077823 Train-f1: 0.38485055420550207 Test-f1: 0.38378378378378375 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3389/20000] Train-Loss: 0.7213990788680237 Test-Loss: 0.20658988406361742 Train-f1: 0.38446699375557547 Test-f1: 0.37173468159383655 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3390/20000] Train-Loss: 0.7191293210628648 Test-Loss: 0.20622138795242095 Train-f1: 0.4077548244961459 Test-f1: 0.3698920797512347 Train-acc: 0.7119205298013245 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3391/20000] Train-Loss: 0.7198591212933318 Test-Loss: 0.20579981615520052 Train-f1: 0.39752350979730994 Test-f1: 0.3829831194859398 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3392/20000] Train-Loss: 0.721038727878556 Test-Loss: 0.20571707312115425 Train-f1: 0.3832308868341414 Test-f1: 0.3829831194859398 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3393/20000] Train-Loss: 0.7179751945365259 Test-Loss: 0.2056675915195167 Train-f1: 0.39316694782978856 Test-f1: 0.39361844303374544 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3394/20000] Train-Loss: 0.7191565663328336 Test-Loss: 0.2057300141255534 Train-f1: 0.39247346439575426 Test-f1: 0.3818790218790219 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3395/20000] Train-Loss: 0.7214627786594816 Test-Loss: 0.20579919770409288 Train-f1: 0.38942528735632187 Test-f1: 0.3829831194859398 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3396/20000] Train-Loss: 0.7189354570450807 Test-Loss: 0.20590694225250555 Train-f1: 0.3831233699847692 Test-f1: 0.38393822393822397 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3397/20000] Train-Loss: 0.7189478934227981 Test-Loss: 0.20762184848741158 Train-f1: 0.3892784540433444 Test-f1: 0.38173669467787114 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3398/20000] Train-Loss: 0.7197610929250053 Test-Loss: 0.20562592335609467 Train-f1: 0.38853209798331745 Test-f1: 0.383598667160311 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3399/20000] Train-Loss: 0.7192013606254446 Test-Loss: 0.20554492644072048 Train-f1: 0.3916198203419299 Test-f1: 0.39327106683271074 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3400/20000] Train-Loss: 0.7200913328369666 Test-Loss: 0.2055470017693291 Train-f1: 0.398723053065245 Test-f1: 0.3813858552700798 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3401/20000] Train-Loss: 0.7195798460332341 Test-Loss: 0.20548889610259824 Train-f1: 0.3799308300395257 Test-f1: 0.39327106683271074 Train-acc: 0.6887417218543046 Test-acc: 0.6973684210526315\n",
      "Epoch [3402/20000] Train-Loss: 0.7212809614009165 Test-Loss: 0.20549532617996866 Train-f1: 0.38733067873961347 Test-f1: 0.3813858552700798 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3403/20000] Train-Loss: 0.715623255870788 Test-Loss: 0.20545339204001928 Train-f1: 0.40522441395082903 Test-f1: 0.3813858552700798 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3404/20000] Train-Loss: 0.7169468723285883 Test-Loss: 0.2066145637935934 Train-f1: 0.3841695377548091 Test-f1: 0.382280701754386 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3405/20000] Train-Loss: 0.7213063962416403 Test-Loss: 0.2059161805060295 Train-f1: 0.3824954877116478 Test-f1: 0.35361502347417845 Train-acc: 0.6854304635761589 Test-acc: 0.6447368421052632\n",
      "Epoch [3406/20000] Train-Loss: 0.7195551952489518 Test-Loss: 0.20672257349714002 Train-f1: 0.39058055364310307 Test-f1: 0.35361502347417845 Train-acc: 0.7052980132450332 Test-acc: 0.6447368421052632\n",
      "Epoch [3407/20000] Train-Loss: 0.71794055101398 Test-Loss: 0.20633815164886787 Train-f1: 0.3798700514407088 Test-f1: 0.382280701754386 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3408/20000] Train-Loss: 0.71879131400612 Test-Loss: 0.20537575270691585 Train-f1: 0.38456624454188776 Test-f1: 0.3829831194859398 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3409/20000] Train-Loss: 0.7189143974606779 Test-Loss: 0.205363009155795 Train-f1: 0.38790195233993685 Test-f1: 0.3829831194859398 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3410/20000] Train-Loss: 0.7175191543625701 Test-Loss: 0.205323440846888 Train-f1: 0.3927621471096601 Test-f1: 0.39327106683271074 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3411/20000] Train-Loss: 0.7190003271249505 Test-Loss: 0.2053461245589827 Train-f1: 0.39835137085137085 Test-f1: 0.3944216706148934 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3412/20000] Train-Loss: 0.7181383739402978 Test-Loss: 0.2061722554657563 Train-f1: 0.3855393791275561 Test-f1: 0.3698920797512347 Train-acc: 0.6920529801324503 Test-acc: 0.6578947368421053\n",
      "Epoch [3413/20000] Train-Loss: 0.7171214040615628 Test-Loss: 0.20594551280932125 Train-f1: 0.3943524970963995 Test-f1: 0.3698920797512347 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "Epoch [3414/20000] Train-Loss: 0.7168977995657618 Test-Loss: 0.2053453294557232 Train-f1: 0.3867173816895179 Test-f1: 0.383598667160311 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3415/20000] Train-Loss: 0.716610712828031 Test-Loss: 0.20537564062808608 Train-f1: 0.3897197096815831 Test-f1: 0.36862695125836253 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3416/20000] Train-Loss: 0.7180182776631036 Test-Loss: 0.20553452689945387 Train-f1: 0.38956265060240963 Test-f1: 0.3589655172413793 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "Epoch [3417/20000] Train-Loss: 0.7172947243714666 Test-Loss: 0.20535954571763154 Train-f1: 0.37308149265898305 Test-f1: 0.36307826956739186 Train-acc: 0.6788079470198676 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3418/20000] Train-Loss: 0.7202436834855785 Test-Loss: 0.20525398919783588 Train-f1: 0.3863385598141696 Test-f1: 0.383598667160311 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3419/20000] Train-Loss: 0.7175582795582651 Test-Loss: 0.20511631903092353 Train-f1: 0.3869001132194686 Test-f1: 0.39327106683271074 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "Epoch [3420/20000] Train-Loss: 0.7188222208578344 Test-Loss: 0.20519472506326594 Train-f1: 0.3921832279126709 Test-f1: 0.39327106683271074 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3421/20000] Train-Loss: 0.7152975206061783 Test-Loss: 0.20560385936122733 Train-f1: 0.3855185795656075 Test-f1: 0.39474903474903467 Train-acc: 0.6920529801324503 Test-acc: 0.6973684210526315\n",
      "Epoch [3422/20000] Train-Loss: 0.7172601128341864 Test-Loss: 0.20515969878197948 Train-f1: 0.4023467152054471 Test-f1: 0.383598667160311 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3423/20000] Train-Loss: 0.7164400634709525 Test-Loss: 0.20512133934780002 Train-f1: 0.3801564763836569 Test-f1: 0.383598667160311 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3424/20000] Train-Loss: 0.7158488362002525 Test-Loss: 0.20504293153269926 Train-f1: 0.40465648854961833 Test-f1: 0.3944216706148934 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3425/20000] Train-Loss: 0.7171514751436475 Test-Loss: 0.20501984776216098 Train-f1: 0.39828265121039247 Test-f1: 0.3813858552700798 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3426/20000] Train-Loss: 0.7175599579855494 Test-Loss: 0.20497110766640747 Train-f1: 0.38549618320610685 Test-f1: 0.36862695125836253 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3427/20000] Train-Loss: 0.7181575323161619 Test-Loss: 0.20500589145726436 Train-f1: 0.3878017030274166 Test-f1: 0.3829831194859398 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3428/20000] Train-Loss: 0.7155256367618097 Test-Loss: 0.20495310760945765 Train-f1: 0.38342581720627467 Test-f1: 0.3813858552700798 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3429/20000] Train-Loss: 0.7169737546536303 Test-Loss: 0.2050492777755804 Train-f1: 0.3854786620530565 Test-f1: 0.3643966079582518 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3430/20000] Train-Loss: 0.7156832823192512 Test-Loss: 0.20500944145014066 Train-f1: 0.3891025014657026 Test-f1: 0.3813858552700798 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3431/20000] Train-Loss: 0.7160509984345397 Test-Loss: 0.20529394379392343 Train-f1: 0.3902143601655797 Test-f1: 0.36307826956739186 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "Epoch [3432/20000] Train-Loss: 0.7151832445820964 Test-Loss: 0.20510607137717835 Train-f1: 0.3948505104521147 Test-f1: 0.40337454059472105 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [3433/20000] Train-Loss: 0.7154437512782904 Test-Loss: 0.20520920983658003 Train-f1: 0.384502516453736 Test-f1: 0.3976190476190476 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3434/20000] Train-Loss: 0.717021907224927 Test-Loss: 0.204943577600097 Train-f1: 0.3972817460087306 Test-f1: 0.3944216706148934 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3435/20000] Train-Loss: 0.7136141636422602 Test-Loss: 0.2048917961134967 Train-f1: 0.38928368121442125 Test-f1: 0.383598667160311 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3436/20000] Train-Loss: 0.715377726170332 Test-Loss: 0.20499510277541422 Train-f1: 0.3956574092574092 Test-f1: 0.3937837837837838 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3437/20000] Train-Loss: 0.7156183891884117 Test-Loss: 0.20478635436230821 Train-f1: 0.3893955461293743 Test-f1: 0.37815089964357795 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3438/20000] Train-Loss: 0.7150723819975284 Test-Loss: 0.2046955159665166 Train-f1: 0.3913992848045459 Test-f1: 0.36862695125836253 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3439/20000] Train-Loss: 0.7147600818160714 Test-Loss: 0.20505610872957486 Train-f1: 0.3841205837173579 Test-f1: 0.38393822393822397 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3440/20000] Train-Loss: 0.7130823695149713 Test-Loss: 0.20499957464459495 Train-f1: 0.3938165807560138 Test-f1: 0.3755952380952381 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3441/20000] Train-Loss: 0.7149789291703567 Test-Loss: 0.20490615827015382 Train-f1: 0.3904760349651111 Test-f1: 0.383598667160311 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3442/20000] Train-Loss: 0.7065729243315336 Test-Loss: 0.2097126515149641 Train-f1: 0.39465728738534117 Test-f1: 0.3298039215686274 Train-acc: 0.6986754966887417 Test-acc: 0.6052631578947368\n",
      "Epoch [3443/20000] Train-Loss: 0.7157064776787035 Test-Loss: 0.20564775288399542 Train-f1: 0.3866625842008573 Test-f1: 0.39499368769317844 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3444/20000] Train-Loss: 0.7154366302159246 Test-Loss: 0.20472624039802498 Train-f1: 0.3928697295911829 Test-f1: 0.36862695125836253 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3445/20000] Train-Loss: 0.7144413232203779 Test-Loss: 0.20524257592205566 Train-f1: 0.38107303811110027 Test-f1: 0.3698920797512347 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3446/20000] Train-Loss: 0.7151876518661564 Test-Loss: 0.2046444188880501 Train-f1: 0.3844513386750049 Test-f1: 0.36862695125836253 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3447/20000] Train-Loss: 0.7143706735806171 Test-Loss: 0.20510490012706836 Train-f1: 0.3933509325435151 Test-f1: 0.3817102615694165 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3448/20000] Train-Loss: 0.7144270980781728 Test-Loss: 0.20502982067560402 Train-f1: 0.37894336865180855 Test-f1: 0.38393822393822397 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3449/20000] Train-Loss: 0.7129936226211929 Test-Loss: 0.20462558893770344 Train-f1: 0.3895030753388439 Test-f1: 0.383598667160311 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3450/20000] Train-Loss: 0.7156638912185252 Test-Loss: 0.2046278170311285 Train-f1: 0.4045088731220118 Test-f1: 0.383598667160311 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3451/20000] Train-Loss: 0.7137887853237015 Test-Loss: 0.2046132947324769 Train-f1: 0.3948448496998308 Test-f1: 0.39327106683271074 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3452/20000] Train-Loss: 0.7145745232768195 Test-Loss: 0.20502035384747477 Train-f1: 0.40536316509128445 Test-f1: 0.38393822393822397 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3453/20000] Train-Loss: 0.7136260026107711 Test-Loss: 0.20465023016984946 Train-f1: 0.4008243865030675 Test-f1: 0.37242424242424244 Train-acc: 0.7086092715231788 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3454/20000] Train-Loss: 0.7133459800779242 Test-Loss: 0.2045040735867197 Train-f1: 0.39247346439575426 Test-f1: 0.383598667160311 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3455/20000] Train-Loss: 0.7136322117210552 Test-Loss: 0.20449135167392574 Train-f1: 0.391831462145655 Test-f1: 0.40500423562470306 Train-acc: 0.6986754966887417 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3456/20000] Train-Loss: 0.7133751509905508 Test-Loss: 0.20440374764880534 Train-f1: 0.3891805758115918 Test-f1: 0.383598667160311 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3457/20000] Train-Loss: 0.7134091454592082 Test-Loss: 0.20524217168958597 Train-f1: 0.39577772325006955 Test-f1: 0.36679195407046383 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "Epoch [3458/20000] Train-Loss: 0.7136383151479111 Test-Loss: 0.20871123987856027 Train-f1: 0.3924181374227401 Test-f1: 0.3736495388669302 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3459/20000] Train-Loss: 0.7131513282422959 Test-Loss: 0.2043340195617489 Train-f1: 0.39891029216182594 Test-f1: 0.3813858552700798 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3460/20000] Train-Loss: 0.7111454587077886 Test-Loss: 0.20501782837799745 Train-f1: 0.3898965068696754 Test-f1: 0.38378378378378375 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3461/20000] Train-Loss: 0.713475700478808 Test-Loss: 0.20442564728590795 Train-f1: 0.40428241888256566 Test-f1: 0.383598667160311 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3462/20000] Train-Loss: 0.7127902073847586 Test-Loss: 0.2042167840170982 Train-f1: 0.40335214339561876 Test-f1: 0.3813858552700798 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3463/20000] Train-Loss: 0.7134419245986973 Test-Loss: 0.20515645001764002 Train-f1: 0.3870750434614642 Test-f1: 0.35361502347417845 Train-acc: 0.695364238410596 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3464/20000] Train-Loss: 0.7163072031450228 Test-Loss: 0.20421195819405483 Train-f1: 0.39359156626506026 Test-f1: 0.37815089964357795 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3465/20000] Train-Loss: 0.7153539029817593 Test-Loss: 0.20465179332882544 Train-f1: 0.3797113815501776 Test-f1: 0.38393822393822397 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3466/20000] Train-Loss: 0.7138197398890804 Test-Loss: 0.20505881021859504 Train-f1: 0.3865045145816558 Test-f1: 0.38393822393822397 Train-acc: 0.6920529801324503 Test-acc: 0.6842105263157895\n",
      "Epoch [3467/20000] Train-Loss: 0.7129640912269795 Test-Loss: 0.20539852908561326 Train-f1: 0.3878501329977676 Test-f1: 0.3698920797512347 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3468/20000] Train-Loss: 0.7127130160926262 Test-Loss: 0.20413020308809543 Train-f1: 0.3914328063241107 Test-f1: 0.40500423562470306 Train-acc: 0.7019867549668874 Test-acc: 0.7105263157894737\n",
      "Epoch [3469/20000] Train-Loss: 0.7116754466027098 Test-Loss: 0.20673412971343297 Train-f1: 0.3890721104338872 Test-f1: 0.3870074422248336 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3470/20000] Train-Loss: 0.7118340191315143 Test-Loss: 0.2044326025354391 Train-f1: 0.38947343513377086 Test-f1: 0.3990109890109891 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "Epoch [3471/20000] Train-Loss: 0.7120259595792493 Test-Loss: 0.2054415024056676 Train-f1: 0.4026495640747692 Test-f1: 0.3576072715361939 Train-acc: 0.7086092715231788 Test-acc: 0.6447368421052632\n",
      "Epoch [3472/20000] Train-Loss: 0.7140515228907743 Test-Loss: 0.20413466761386995 Train-f1: 0.3901872084096144 Test-f1: 0.3944216706148934 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3473/20000] Train-Loss: 0.7103526187047258 Test-Loss: 0.2050375115277176 Train-f1: 0.39284022394487517 Test-f1: 0.3931022029613579 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3474/20000] Train-Loss: 0.711769196957282 Test-Loss: 0.20514608039980525 Train-f1: 0.384023910120991 Test-f1: 0.3698920797512347 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "Epoch [3475/20000] Train-Loss: 0.7109053805244259 Test-Loss: 0.2046706022669493 Train-f1: 0.39563909774436085 Test-f1: 0.40569520569520573 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [3476/20000] Train-Loss: 0.7124826791425747 Test-Loss: 0.2042825230089927 Train-f1: 0.3934101898478489 Test-f1: 0.37242424242424244 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3477/20000] Train-Loss: 0.7130758641886916 Test-Loss: 0.20401903178490605 Train-f1: 0.4007119992351285 Test-f1: 0.383598667160311 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3478/20000] Train-Loss: 0.7123258939536767 Test-Loss: 0.20400899632332029 Train-f1: 0.3819411888924084 Test-f1: 0.40500423562470306 Train-acc: 0.6854304635761589 Test-acc: 0.7105263157894737\n",
      "Epoch [3479/20000] Train-Loss: 0.7129844933293377 Test-Loss: 0.2042664195641219 Train-f1: 0.3925272063426683 Test-f1: 0.37792792792792795 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3480/20000] Train-Loss: 0.7120056240312387 Test-Loss: 0.20399627485082125 Train-f1: 0.3901120018193189 Test-f1: 0.3944216706148934 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3481/20000] Train-Loss: 0.7098718345023097 Test-Loss: 0.20464091926755423 Train-f1: 0.3910253518193965 Test-f1: 0.36679195407046383 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "Epoch [3482/20000] Train-Loss: 0.7077562902109193 Test-Loss: 0.20480827246409344 Train-f1: 0.41869387465734 Test-f1: 0.36679195407046383 Train-acc: 0.7251655629139073 Test-acc: 0.6578947368421053\n",
      "Epoch [3483/20000] Train-Loss: 0.7137148165294951 Test-Loss: 0.204310780336356 Train-f1: 0.3840527791942886 Test-f1: 0.3755952380952381 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3484/20000] Train-Loss: 0.7128137246423405 Test-Loss: 0.20495852580946378 Train-f1: 0.4007898021893605 Test-f1: 0.36679195407046383 Train-acc: 0.7086092715231788 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3485/20000] Train-Loss: 0.7118496723580138 Test-Loss: 0.20386517845282134 Train-f1: 0.38901391409558383 Test-f1: 0.3944216706148934 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3486/20000] Train-Loss: 0.7113604170207302 Test-Loss: 0.20408151406723535 Train-f1: 0.37651930352119506 Test-f1: 0.37792792792792795 Train-acc: 0.6821192052980133 Test-acc: 0.6710526315789473\n",
      "Epoch [3487/20000] Train-Loss: 0.7110971907972683 Test-Loss: 0.20401456123893133 Train-f1: 0.3974458204334365 Test-f1: 0.38875457875457875 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3488/20000] Train-Loss: 0.712619426349538 Test-Loss: 0.2038975973567094 Train-f1: 0.40398004593741027 Test-f1: 0.3932710668327107 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3489/20000] Train-Loss: 0.7095342149864591 Test-Loss: 0.20406631859644137 Train-f1: 0.3973011198961256 Test-f1: 0.38875457875457875 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3490/20000] Train-Loss: 0.7101225401615047 Test-Loss: 0.20372484605667196 Train-f1: 0.40379113471175787 Test-f1: 0.383598667160311 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3491/20000] Train-Loss: 0.7092295257291887 Test-Loss: 0.20433666256525954 Train-f1: 0.39284022394487517 Test-f1: 0.3931022029613579 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3492/20000] Train-Loss: 0.7094339842122638 Test-Loss: 0.20442213028703418 Train-f1: 0.3941414141414141 Test-f1: 0.36679195407046383 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "Epoch [3493/20000] Train-Loss: 0.710520834638163 Test-Loss: 0.2044879102498454 Train-f1: 0.39442640692640696 Test-f1: 0.39492929461969706 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3494/20000] Train-Loss: 0.7092054959911488 Test-Loss: 0.20428124285330065 Train-f1: 0.39886572740582416 Test-f1: 0.37792792792792795 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "Epoch [3495/20000] Train-Loss: 0.712511771656431 Test-Loss: 0.20404118889373668 Train-f1: 0.3841695377548091 Test-f1: 0.3755952380952381 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "Epoch [3496/20000] Train-Loss: 0.708521208014913 Test-Loss: 0.20525219710483822 Train-f1: 0.3918907439865524 Test-f1: 0.3794480519480519 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3497/20000] Train-Loss: 0.708376741435796 Test-Loss: 0.20535984304582336 Train-f1: 0.3891327923058035 Test-f1: 0.40378576378576375 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3498/20000] Train-Loss: 0.7094432926704426 Test-Loss: 0.2044025667408087 Train-f1: 0.39153351698806244 Test-f1: 0.3935739935739936 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3499/20000] Train-Loss: 0.7098967760759407 Test-Loss: 0.20368162869978854 Train-f1: 0.3974889020488172 Test-f1: 0.3944216706148934 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3500/20000] Train-Loss: 0.7106152865408081 Test-Loss: 0.20357386469191924 Train-f1: 0.4008524833209785 Test-f1: 0.37815089964357795 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3501/20000] Train-Loss: 0.7070992078533996 Test-Loss: 0.2058575724313489 Train-f1: 0.3831590229653966 Test-f1: 0.36694316436251917 Train-acc: 0.6887417218543046 Test-acc: 0.6578947368421053\n",
      "Epoch [3502/20000] Train-Loss: 0.7128787302987336 Test-Loss: 0.20358107333376366 Train-f1: 0.3854406263859794 Test-f1: 0.383598667160311 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3503/20000] Train-Loss: 0.7078394065206861 Test-Loss: 0.20532289773343312 Train-f1: 0.40945118989800877 Test-f1: 0.3739206739206739 Train-acc: 0.7152317880794702 Test-acc: 0.6710526315789473\n",
      "Epoch [3504/20000] Train-Loss: 0.709245549235772 Test-Loss: 0.20385355608422126 Train-f1: 0.39468388271633714 Test-f1: 0.3990109890109891 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3505/20000] Train-Loss: 0.7104603448712241 Test-Loss: 0.203457439523594 Train-f1: 0.4062778627172712 Test-f1: 0.3944216706148934 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3506/20000] Train-Loss: 0.7096252922066544 Test-Loss: 0.20342664062697446 Train-f1: 0.39014786242365795 Test-f1: 0.40500423562470306 Train-acc: 0.6986754966887417 Test-acc: 0.7105263157894737\n",
      "Epoch [3507/20000] Train-Loss: 0.7070309060852438 Test-Loss: 0.2050899912235722 Train-f1: 0.39009618381773825 Test-f1: 0.37306501547987614 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3508/20000] Train-Loss: 0.7068404037126702 Test-Loss: 0.2056809354583948 Train-f1: 0.3983577371850084 Test-f1: 0.3870074422248336 Train-acc: 0.7086092715231788 Test-acc: 0.6710526315789473\n",
      "Epoch [3509/20000] Train-Loss: 0.7101545426323079 Test-Loss: 0.20473721178518636 Train-f1: 0.3971851116287235 Test-f1: 0.3576072715361939 Train-acc: 0.7019867549668874 Test-acc: 0.6447368421052632\n",
      "Epoch [3510/20000] Train-Loss: 0.7094904534363213 Test-Loss: 0.2034276627726702 Train-f1: 0.38789821289821297 Test-f1: 0.3944216706148934 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3511/20000] Train-Loss: 0.7075881209767303 Test-Loss: 0.2034150890824509 Train-f1: 0.4014141414141414 Test-f1: 0.383598667160311 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3512/20000] Train-Loss: 0.707381236536086 Test-Loss: 0.20469610225923734 Train-f1: 0.3798385911629274 Test-f1: 0.3931022029613579 Train-acc: 0.6854304635761589 Test-acc: 0.6842105263157895\n",
      "Epoch [3513/20000] Train-Loss: 0.7091697361961906 Test-Loss: 0.2046556188817769 Train-f1: 0.3969971988795518 Test-f1: 0.3698920797512347 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3514/20000] Train-Loss: 0.7107015752706455 Test-Loss: 0.20332108645901084 Train-f1: 0.38788017414244 Test-f1: 0.3944216706148934 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3515/20000] Train-Loss: 0.707086440000663 Test-Loss: 0.20411405531055432 Train-f1: 0.39924455125154656 Test-f1: 0.39492929461969706 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3516/20000] Train-Loss: 0.7122797310729904 Test-Loss: 0.2044765740540468 Train-f1: 0.39158465588238506 Test-f1: 0.3817164103704004 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "Epoch [3517/20000] Train-Loss: 0.707777719466076 Test-Loss: 0.20380568171374644 Train-f1: 0.4013671581568047 Test-f1: 0.3874509803921569 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3518/20000] Train-Loss: 0.7091513241768193 Test-Loss: 0.2037413728338215 Train-f1: 0.38674901185770755 Test-f1: 0.38859649122807016 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3519/20000] Train-Loss: 0.7075883257465365 Test-Loss: 0.20423563621153615 Train-f1: 0.3821873820704914 Test-f1: 0.38313883299798795 Train-acc: 0.6920529801324503 Test-acc: 0.6710526315789473\n",
      "Epoch [3520/20000] Train-Loss: 0.7088096653167841 Test-Loss: 0.20363216288930794 Train-f1: 0.41637296037296034 Test-f1: 0.37242424242424244 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [3521/20000] Train-Loss: 0.7007453066182635 Test-Loss: 0.20728080875611016 Train-f1: 0.39328358654092777 Test-f1: 0.3863070263070263 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3522/20000] Train-Loss: 0.707584305654677 Test-Loss: 0.20395235624436303 Train-f1: 0.3963036957208452 Test-f1: 0.3589655172413793 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "Epoch [3523/20000] Train-Loss: 0.7083516458371438 Test-Loss: 0.2041912206350407 Train-f1: 0.4013123871622577 Test-f1: 0.35361502347417845 Train-acc: 0.7119205298013245 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3524/20000] Train-Loss: 0.7099556199714745 Test-Loss: 0.20308010227656795 Train-f1: 0.39679156626506024 Test-f1: 0.37815089964357795 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3525/20000] Train-Loss: 0.7075738038923097 Test-Loss: 0.20347623628693584 Train-f1: 0.385645473113934 Test-f1: 0.3589655172413793 Train-acc: 0.695364238410596 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3526/20000] Train-Loss: 0.7083164402753653 Test-Loss: 0.20304984416789226 Train-f1: 0.37959367420618895 Test-f1: 0.3944216706148934 Train-acc: 0.6920529801324503 Test-acc: 0.6973684210526315\n",
      "Epoch [3527/20000] Train-Loss: 0.7061414603601028 Test-Loss: 0.2032421998838679 Train-f1: 0.3970256166982923 Test-f1: 0.37242424242424244 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3528/20000] Train-Loss: 0.7068391959080065 Test-Loss: 0.20301163624412463 Train-f1: 0.3927250078700711 Test-f1: 0.39327106683271074 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3529/20000] Train-Loss: 0.7075530381752421 Test-Loss: 0.20523835932957543 Train-f1: 0.3934019689417052 Test-f1: 0.38536287242169587 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3530/20000] Train-Loss: 0.7076998700884861 Test-Loss: 0.2034527422844361 Train-f1: 0.3916198203419299 Test-f1: 0.387620103473762 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3531/20000] Train-Loss: 0.706173863780144 Test-Loss: 0.20455512492534475 Train-f1: 0.3912569060773481 Test-f1: 0.3930313588850174 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3532/20000] Train-Loss: 0.7080493381944271 Test-Loss: 0.20428046074534426 Train-f1: 0.39737593869431176 Test-f1: 0.3828421453328792 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3533/20000] Train-Loss: 0.7047524560365953 Test-Loss: 0.20373399719191151 Train-f1: 0.3965624010136205 Test-f1: 0.387620103473762 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3534/20000] Train-Loss: 0.7072562497864004 Test-Loss: 0.20293531819013164 Train-f1: 0.3907892743523708 Test-f1: 0.3643966079582518 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3535/20000] Train-Loss: 0.7055367230441242 Test-Loss: 0.20306092407172102 Train-f1: 0.3862849305063838 Test-f1: 0.38859649122807016 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3536/20000] Train-Loss: 0.7051054102380097 Test-Loss: 0.2028996231893843 Train-f1: 0.4005526967791118 Test-f1: 0.3829831194859398 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3537/20000] Train-Loss: 0.7056579548061334 Test-Loss: 0.20420835439059484 Train-f1: 0.40865223602951517 Test-f1: 0.3819837398373984 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3538/20000] Train-Loss: 0.7053762910949563 Test-Loss: 0.20286588757363863 Train-f1: 0.39238503214162446 Test-f1: 0.3829831194859398 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3539/20000] Train-Loss: 0.7067588444462178 Test-Loss: 0.2028232584522105 Train-f1: 0.39345284927503654 Test-f1: 0.3829831194859398 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3540/20000] Train-Loss: 0.7039944045607193 Test-Loss: 0.20278512054211847 Train-f1: 0.39924455125154656 Test-f1: 0.39361844303374544 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3541/20000] Train-Loss: 0.705702615607562 Test-Loss: 0.2028209357532755 Train-f1: 0.3881379007465964 Test-f1: 0.39327106683271074 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3542/20000] Train-Loss: 0.7053538391521297 Test-Loss: 0.20280295298919174 Train-f1: 0.4015251225897593 Test-f1: 0.3829831194859398 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3543/20000] Train-Loss: 0.7047160705509264 Test-Loss: 0.20295099172212464 Train-f1: 0.410957634633667 Test-f1: 0.37242424242424244 Train-acc: 0.7218543046357616 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3544/20000] Train-Loss: 0.704618953892032 Test-Loss: 0.20272402520413293 Train-f1: 0.4004015063949137 Test-f1: 0.37815089964357795 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3545/20000] Train-Loss: 0.7052813569814442 Test-Loss: 0.20341217378446957 Train-f1: 0.3982709766722306 Test-f1: 0.3589655172413793 Train-acc: 0.7086092715231788 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3546/20000] Train-Loss: 0.7061975456963091 Test-Loss: 0.2026834960718345 Train-f1: 0.38855618466898956 Test-f1: 0.3643966079582518 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3547/20000] Train-Loss: 0.7045331621064728 Test-Loss: 0.2036034759432542 Train-f1: 0.3818335993615324 Test-f1: 0.37242424242424244 Train-acc: 0.695364238410596 Test-acc: 0.6710526315789473\n",
      "Epoch [3548/20000] Train-Loss: 0.7057252252128499 Test-Loss: 0.202755504294843 Train-f1: 0.3959267430599392 Test-f1: 0.3944216706148934 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3549/20000] Train-Loss: 0.7007813399447387 Test-Loss: 0.20627847816225744 Train-f1: 0.41240019858657007 Test-f1: 0.3736495388669302 Train-acc: 0.7218543046357616 Test-acc: 0.6578947368421053\n",
      "Epoch [3550/20000] Train-Loss: 0.7079794152491841 Test-Loss: 0.20317536656144378 Train-f1: 0.39702045195002944 Test-f1: 0.3780701754385965 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "Epoch [3551/20000] Train-Loss: 0.7058817223090333 Test-Loss: 0.2029292263487839 Train-f1: 0.4082937920974731 Test-f1: 0.38859649122807016 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3552/20000] Train-Loss: 0.7052228168173026 Test-Loss: 0.20271913181791357 Train-f1: 0.38890195890147883 Test-f1: 0.38859649122807016 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3553/20000] Train-Loss: 0.7057382566670107 Test-Loss: 0.20262763433764924 Train-f1: 0.40394388220475175 Test-f1: 0.3912626101730912 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3554/20000] Train-Loss: 0.7046097437103324 Test-Loss: 0.2028521007115859 Train-f1: 0.4000133378601222 Test-f1: 0.37242424242424244 Train-acc: 0.7086092715231788 Test-acc: 0.6710526315789473\n",
      "Epoch [3555/20000] Train-Loss: 0.7042632237624751 Test-Loss: 0.20300553417285455 Train-f1: 0.3982113854680226 Test-f1: 0.3932710668327107 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3556/20000] Train-Loss: 0.7039804342520347 Test-Loss: 0.2026684052494392 Train-f1: 0.39475501011307745 Test-f1: 0.3932710668327107 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3557/20000] Train-Loss: 0.7041981221224631 Test-Loss: 0.20266038386388513 Train-f1: 0.4071522476675149 Test-f1: 0.3944216706148934 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3558/20000] Train-Loss: 0.6978409417866613 Test-Loss: 0.20423538613980358 Train-f1: 0.4004919423240034 Test-f1: 0.3835661881977671 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3559/20000] Train-Loss: 0.7068898714583063 Test-Loss: 0.2028216320534509 Train-f1: 0.4055629377240268 Test-f1: 0.37242424242424244 Train-acc: 0.7152317880794702 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3560/20000] Train-Loss: 0.7048270445800793 Test-Loss: 0.2024728482990445 Train-f1: 0.3878484320557491 Test-f1: 0.39361844303374544 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "Epoch [3561/20000] Train-Loss: 0.7028290798660128 Test-Loss: 0.2025908269099604 Train-f1: 0.3824490029316515 Test-f1: 0.3932710668327107 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3562/20000] Train-Loss: 0.6993174255425586 Test-Loss: 0.20638562772917418 Train-f1: 0.3934735938170889 Test-f1: 0.3736495388669302 Train-acc: 0.6986754966887417 Test-acc: 0.6578947368421053\n",
      "Epoch [3563/20000] Train-Loss: 0.7058842654542968 Test-Loss: 0.2025742738338923 Train-f1: 0.3997583928769467 Test-f1: 0.3944216706148934 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3564/20000] Train-Loss: 0.7046139566063914 Test-Loss: 0.20324193350936556 Train-f1: 0.39366546927917956 Test-f1: 0.3935739935739936 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3565/20000] Train-Loss: 0.7045926001727335 Test-Loss: 0.20243315250068636 Train-f1: 0.38799618320610685 Test-f1: 0.3944216706148934 Train-acc: 0.6920529801324503 Test-acc: 0.6973684210526315\n",
      "Epoch [3566/20000] Train-Loss: 0.7039860290835055 Test-Loss: 0.204272989649186 Train-f1: 0.3945581160639193 Test-f1: 0.39338345864661656 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3567/20000] Train-Loss: 0.7016453725267703 Test-Loss: 0.20682340790454717 Train-f1: 0.405384806206997 Test-f1: 0.36137681159420293 Train-acc: 0.7119205298013245 Test-acc: 0.6447368421052632\n",
      "Epoch [3568/20000] Train-Loss: 0.7042668641923447 Test-Loss: 0.20299688001912822 Train-f1: 0.40442684748459384 Test-f1: 0.387620103473762 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3569/20000] Train-Loss: 0.7026598235875716 Test-Loss: 0.2025472225206609 Train-f1: 0.38588269967580313 Test-f1: 0.3912626101730912 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "Epoch [3570/20000] Train-Loss: 0.7036857525406567 Test-Loss: 0.2025289822888091 Train-f1: 0.3924493402369509 Test-f1: 0.3944216706148934 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3571/20000] Train-Loss: 0.7019153993990136 Test-Loss: 0.20236941034983874 Train-f1: 0.39438521551905964 Test-f1: 0.3944216706148934 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3572/20000] Train-Loss: 0.7022834771462363 Test-Loss: 0.20252358988570762 Train-f1: 0.40568258063756507 Test-f1: 0.37242424242424244 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "Epoch [3573/20000] Train-Loss: 0.7046163063313047 Test-Loss: 0.20341608568013175 Train-f1: 0.3949605343047966 Test-f1: 0.3828421453328792 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "Epoch [3574/20000] Train-Loss: 0.7018732362043175 Test-Loss: 0.20285543812087317 Train-f1: 0.399276086380228 Test-f1: 0.3932710668327107 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3575/20000] Train-Loss: 0.7022755451285384 Test-Loss: 0.20249243930492372 Train-f1: 0.3999667931688805 Test-f1: 0.3944216706148934 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3576/20000] Train-Loss: 0.7018797926168804 Test-Loss: 0.20335560478714873 Train-f1: 0.4005263157894737 Test-f1: 0.37173468159383655 Train-acc: 0.7119205298013245 Test-acc: 0.6578947368421053\n",
      "Epoch [3577/20000] Train-Loss: 0.70371981415847 Test-Loss: 0.20246443404040854 Train-f1: 0.41143578008566567 Test-f1: 0.38528382095523883 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3578/20000] Train-Loss: 0.7013175260197231 Test-Loss: 0.202877961549625 Train-f1: 0.3982458645532951 Test-f1: 0.3943266634061284 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3579/20000] Train-Loss: 0.701678652676624 Test-Loss: 0.2033751002839171 Train-f1: 0.4089265416311452 Test-f1: 0.3535950334691809 Train-acc: 0.7152317880794702 Test-acc: 0.6447368421052632\n",
      "Epoch [3580/20000] Train-Loss: 0.7023569238107911 Test-Loss: 0.20472878411663936 Train-f1: 0.38858590016010464 Test-f1: 0.34320939077972323 Train-acc: 0.6920529801324503 Test-acc: 0.631578947368421\n",
      "best score_dx!!\n",
      "Epoch [3581/20000] Train-Loss: 0.7032075265473469 Test-Loss: 0.20218746773331678 Train-f1: 0.3905418475766882 Test-f1: 0.3912626101730912 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3582/20000] Train-Loss: 0.7060145590941941 Test-Loss: 0.20222815931202034 Train-f1: 0.3942973934278282 Test-f1: 0.39361844303374544 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3583/20000] Train-Loss: 0.7021632227981911 Test-Loss: 0.2021735046207765 Train-f1: 0.3809253474803145 Test-f1: 0.3944094779711218 Train-acc: 0.6920529801324503 Test-acc: 0.6973684210526315\n",
      "Epoch [3584/20000] Train-Loss: 0.7038068929006307 Test-Loss: 0.2026741923021764 Train-f1: 0.39492783532257214 Test-f1: 0.37242424242424244 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3585/20000] Train-Loss: 0.701660439486714 Test-Loss: 0.20257621408862025 Train-f1: 0.4032133524873881 Test-f1: 0.3589655172413793 Train-acc: 0.7086092715231788 Test-acc: 0.6578947368421053\n",
      "Epoch [3586/20000] Train-Loss: 0.7025733895572669 Test-Loss: 0.2030238683933322 Train-f1: 0.3890899949723479 Test-f1: 0.39761904761904765 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3587/20000] Train-Loss: 0.6996635388571768 Test-Loss: 0.2042600318114293 Train-f1: 0.40277984486176804 Test-f1: 0.3930313588850174 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3588/20000] Train-Loss: 0.6984678481985648 Test-Loss: 0.2028729388568139 Train-f1: 0.3824153768855199 Test-f1: 0.3935739935739936 Train-acc: 0.695364238410596 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3589/20000] Train-Loss: 0.7014537428758981 Test-Loss: 0.20214777216100602 Train-f1: 0.4061397155303627 Test-f1: 0.3912626101730912 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3590/20000] Train-Loss: 0.7009345574445535 Test-Loss: 0.2021147573701412 Train-f1: 0.3982224159930432 Test-f1: 0.3944216706148934 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3591/20000] Train-Loss: 0.7007408381904373 Test-Loss: 0.2021762942789699 Train-f1: 0.3936037078059288 Test-f1: 0.40500423562470306 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3592/20000] Train-Loss: 0.7004142202428413 Test-Loss: 0.20203780486340214 Train-f1: 0.3914837381501835 Test-f1: 0.3829831194859398 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3593/20000] Train-Loss: 0.7014114845073952 Test-Loss: 0.2023511050525934 Train-f1: 0.3961344211344211 Test-f1: 0.38859649122807016 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3594/20000] Train-Loss: 0.6987084467853838 Test-Loss: 0.20553086999657008 Train-f1: 0.395201171499904 Test-f1: 0.377011089596902 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "Epoch [3595/20000] Train-Loss: 0.7026835423467428 Test-Loss: 0.20338250398397878 Train-f1: 0.3989827151483419 Test-f1: 0.3794480519480519 Train-acc: 0.7086092715231788 Test-acc: 0.6710526315789473\n",
      "Epoch [3596/20000] Train-Loss: 0.7003982741811476 Test-Loss: 0.204459312923662 Train-f1: 0.3966024213278508 Test-f1: 0.36261904761904756 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3597/20000] Train-Loss: 0.7004911407200803 Test-Loss: 0.20198938805393904 Train-f1: 0.41414837089490303 Test-f1: 0.3912626101730912 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [3598/20000] Train-Loss: 0.6985278237687943 Test-Loss: 0.20530799725118418 Train-f1: 0.4076708567506113 Test-f1: 0.3862887989203779 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3599/20000] Train-Loss: 0.7052010248314954 Test-Loss: 0.2023134309597091 Train-f1: 0.392166581039016 Test-f1: 0.38528382095523883 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3600/20000] Train-Loss: 0.6987224639067888 Test-Loss: 0.20265067660725558 Train-f1: 0.39718396365969466 Test-f1: 0.3997076023391813 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3601/20000] Train-Loss: 0.7019301731351504 Test-Loss: 0.20232710016746952 Train-f1: 0.40370906123660105 Test-f1: 0.38873873873873876 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3602/20000] Train-Loss: 0.7013128012352137 Test-Loss: 0.2026860086988591 Train-f1: 0.4050289078103647 Test-f1: 0.3589655172413793 Train-acc: 0.7119205298013245 Test-acc: 0.6578947368421053\n",
      "Epoch [3603/20000] Train-Loss: 0.6982892441688746 Test-Loss: 0.20418573279772909 Train-f1: 0.3958152958152958 Test-f1: 0.35395031967299023 Train-acc: 0.7052980132450332 Test-acc: 0.6447368421052632\n",
      "Epoch [3604/20000] Train-Loss: 0.7024433588219494 Test-Loss: 0.20333105274985092 Train-f1: 0.3900727428326915 Test-f1: 0.391454746803584 Train-acc: 0.6986754966887417 Test-acc: 0.6842105263157895\n",
      "Epoch [3605/20000] Train-Loss: 0.7000999100948947 Test-Loss: 0.20346438856399465 Train-f1: 0.40577400348902276 Test-f1: 0.36261904761904756 Train-acc: 0.7152317880794702 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3606/20000] Train-Loss: 0.7002909454554251 Test-Loss: 0.20189263910221275 Train-f1: 0.400297213622291 Test-f1: 0.3944094779711218 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3607/20000] Train-Loss: 0.700946749087721 Test-Loss: 0.2021584010485684 Train-f1: 0.3981850561362757 Test-f1: 0.37732600732600735 Train-acc: 0.7086092715231788 Test-acc: 0.6710526315789473\n",
      "Epoch [3608/20000] Train-Loss: 0.7008091791847361 Test-Loss: 0.2019011535833173 Train-f1: 0.4077263310756235 Test-f1: 0.37745098039215685 Train-acc: 0.7152317880794702 Test-acc: 0.6710526315789473\n",
      "Epoch [3609/20000] Train-Loss: 0.702535948849397 Test-Loss: 0.202074680220103 Train-f1: 0.38549618320610685 Test-f1: 0.387620103473762 Train-acc: 0.6887417218543046 Test-acc: 0.6842105263157895\n",
      "Epoch [3610/20000] Train-Loss: 0.6996523763553316 Test-Loss: 0.20280632219595096 Train-f1: 0.38611287934692523 Test-f1: 0.37242424242424244 Train-acc: 0.6887417218543046 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3611/20000] Train-Loss: 0.7011572709688658 Test-Loss: 0.20176466594481518 Train-f1: 0.3926828655987012 Test-f1: 0.3829831194859398 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3612/20000] Train-Loss: 0.7002430513425485 Test-Loss: 0.20192667182603533 Train-f1: 0.4067806267806267 Test-f1: 0.3832710668327107 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3613/20000] Train-Loss: 0.6990605796605186 Test-Loss: 0.20172155257753804 Train-f1: 0.39359569244315007 Test-f1: 0.403812890600341 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [3614/20000] Train-Loss: 0.7004752981626468 Test-Loss: 0.2025004043815321 Train-f1: 0.4031089987272841 Test-f1: 0.38528382095523883 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3615/20000] Train-Loss: 0.6987550913114735 Test-Loss: 0.20255241219846074 Train-f1: 0.398580121703854 Test-f1: 0.3992156862745099 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3616/20000] Train-Loss: 0.6987420790961003 Test-Loss: 0.20191738756967714 Train-f1: 0.4021411097924286 Test-f1: 0.3992156862745099 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3617/20000] Train-Loss: 0.6995345621154204 Test-Loss: 0.2028768250538464 Train-f1: 0.40033655015813796 Test-f1: 0.3820132918724468 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3618/20000] Train-Loss: 0.7000998318093954 Test-Loss: 0.20164199488482154 Train-f1: 0.3961896796986134 Test-f1: 0.3832710668327107 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3619/20000] Train-Loss: 0.6962087565526184 Test-Loss: 0.2044833709219484 Train-f1: 0.4103132161955692 Test-f1: 0.3736495388669302 Train-acc: 0.7185430463576159 Test-acc: 0.6578947368421053\n",
      "Epoch [3620/20000] Train-Loss: 0.6987121203512678 Test-Loss: 0.20172010667725687 Train-f1: 0.38788515263361817 Test-f1: 0.37732600732600735 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3621/20000] Train-Loss: 0.6975464224965142 Test-Loss: 0.2036699050252015 Train-f1: 0.40915460053198016 Test-f1: 0.35395031967299023 Train-acc: 0.7152317880794702 Test-acc: 0.6447368421052632\n",
      "best score_dx!!\n",
      "Epoch [3622/20000] Train-Loss: 0.7004001440846833 Test-Loss: 0.20158222011654403 Train-f1: 0.39985576632283226 Test-f1: 0.3829831194859398 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3623/20000] Train-Loss: 0.6969627771700933 Test-Loss: 0.20297297750969126 Train-f1: 0.3913308355030588 Test-f1: 0.3827185427185428 Train-acc: 0.7086092715231788 Test-acc: 0.6710526315789473\n",
      "Epoch [3624/20000] Train-Loss: 0.6987343412221706 Test-Loss: 0.20170777687867655 Train-f1: 0.3997990238300316 Test-f1: 0.3997076023391813 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3625/20000] Train-Loss: 0.6987681636684736 Test-Loss: 0.20154006267527366 Train-f1: 0.4086926101619019 Test-f1: 0.3832710668327107 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3626/20000] Train-Loss: 0.6987682791340304 Test-Loss: 0.20166923456266092 Train-f1: 0.39237573601735454 Test-f1: 0.37745098039215685 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3627/20000] Train-Loss: 0.700512399614869 Test-Loss: 0.2020642303401535 Train-f1: 0.3980757907246847 Test-f1: 0.38873873873873876 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3628/20000] Train-Loss: 0.699579725778449 Test-Loss: 0.20167668974470446 Train-f1: 0.40249748583993783 Test-f1: 0.37745098039215685 Train-acc: 0.7086092715231788 Test-acc: 0.6710526315789473\n",
      "Epoch [3629/20000] Train-Loss: 0.6982793316253479 Test-Loss: 0.2032753222300305 Train-f1: 0.3900604680731531 Test-f1: 0.38285714285714284 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3630/20000] Train-Loss: 0.6983253983239331 Test-Loss: 0.20145573308768056 Train-f1: 0.40547855180607806 Test-f1: 0.3912626101730912 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3631/20000] Train-Loss: 0.701312568716336 Test-Loss: 0.20190944184841836 Train-f1: 0.40659639877031173 Test-f1: 0.37745098039215685 Train-acc: 0.7218543046357616 Test-acc: 0.6710526315789473\n",
      "Epoch [3632/20000] Train-Loss: 0.6986151908251786 Test-Loss: 0.2038239325472567 Train-f1: 0.3974588078588079 Test-f1: 0.38702721809826796 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "Epoch [3633/20000] Train-Loss: 0.6996045944189379 Test-Loss: 0.20168992177479247 Train-f1: 0.39618682728775706 Test-f1: 0.387620103473762 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3634/20000] Train-Loss: 0.6957549931667096 Test-Loss: 0.20305966419772514 Train-f1: 0.40098667453146375 Test-f1: 0.3930313588850174 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3635/20000] Train-Loss: 0.6978796225659705 Test-Loss: 0.2022255843487048 Train-f1: 0.39168788339794863 Test-f1: 0.38528382095523883 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3636/20000] Train-Loss: 0.699064064500902 Test-Loss: 0.20153664343665267 Train-f1: 0.39521526618963937 Test-f1: 0.3589655172413793 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "Epoch [3637/20000] Train-Loss: 0.6989526600777622 Test-Loss: 0.20161678973189084 Train-f1: 0.4006698386359403 Test-f1: 0.3912626101730912 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3638/20000] Train-Loss: 0.6987082975159806 Test-Loss: 0.20148715993444788 Train-f1: 0.3936037078059288 Test-f1: 0.38528382095523883 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3639/20000] Train-Loss: 0.6971653439221714 Test-Loss: 0.20144305150252587 Train-f1: 0.39977319977319975 Test-f1: 0.387620103473762 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3640/20000] Train-Loss: 0.6985497884966303 Test-Loss: 0.20156576172595775 Train-f1: 0.4005526967791118 Test-f1: 0.37748538011695904 Train-acc: 0.7086092715231788 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3641/20000] Train-Loss: 0.6972929545170329 Test-Loss: 0.20141330140250382 Train-f1: 0.39627757136609165 Test-f1: 0.40500423562470306 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "Epoch [3642/20000] Train-Loss: 0.6936490414485507 Test-Loss: 0.20639613249611438 Train-f1: 0.39517228915662655 Test-f1: 0.3801069518716577 Train-acc: 0.7019867549668874 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3643/20000] Train-Loss: 0.7009002712077981 Test-Loss: 0.20139638985970815 Train-f1: 0.40245565988523735 Test-f1: 0.3997076023391813 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3644/20000] Train-Loss: 0.6973249330443787 Test-Loss: 0.20155072061476814 Train-f1: 0.395173181601368 Test-f1: 0.387620103473762 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3645/20000] Train-Loss: 0.695988381381887 Test-Loss: 0.20131436817248 Train-f1: 0.40073696145124715 Test-f1: 0.4055205890822329 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3646/20000] Train-Loss: 0.6976683249775985 Test-Loss: 0.20125912268235988 Train-f1: 0.40132320111178466 Test-f1: 0.382993099186322 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3647/20000] Train-Loss: 0.6982433972821763 Test-Loss: 0.20169229421001825 Train-f1: 0.39805389845916545 Test-f1: 0.37748538011695904 Train-acc: 0.7152317880794702 Test-acc: 0.6710526315789473\n",
      "Epoch [3648/20000] Train-Loss: 0.6971245089629906 Test-Loss: 0.20125993866646524 Train-f1: 0.39967838765008573 Test-f1: 0.4055205890822329 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [3649/20000] Train-Loss: 0.6965116708043233 Test-Loss: 0.20151971230020424 Train-f1: 0.39364717334240223 Test-f1: 0.40500423562470306 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [3650/20000] Train-Loss: 0.6973602945800319 Test-Loss: 0.2016094113767768 Train-f1: 0.3875858918575534 Test-f1: 0.387620103473762 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3651/20000] Train-Loss: 0.697136265752068 Test-Loss: 0.20119309219891435 Train-f1: 0.4004015063949137 Test-f1: 0.3944094779711218 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3652/20000] Train-Loss: 0.6973805321978916 Test-Loss: 0.20116223764010444 Train-f1: 0.41327843129846825 Test-f1: 0.3912626101730912 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [3653/20000] Train-Loss: 0.6964757766058088 Test-Loss: 0.20143117122709103 Train-f1: 0.39977319977319975 Test-f1: 0.3992156862745099 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3654/20000] Train-Loss: 0.6984557031829198 Test-Loss: 0.20148378987582066 Train-f1: 0.39433248660077236 Test-f1: 0.3992156862745099 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3655/20000] Train-Loss: 0.6967160563650652 Test-Loss: 0.2015087846045431 Train-f1: 0.39465260800162305 Test-f1: 0.40381289060034087 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "Epoch [3656/20000] Train-Loss: 0.6971916943794617 Test-Loss: 0.2016535144448459 Train-f1: 0.4033787767900048 Test-f1: 0.387620103473762 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3657/20000] Train-Loss: 0.6943875794732398 Test-Loss: 0.20113567632412008 Train-f1: 0.38672125772928634 Test-f1: 0.40500423562470306 Train-acc: 0.6986754966887417 Test-acc: 0.7105263157894737\n",
      "Epoch [3658/20000] Train-Loss: 0.6938840554056979 Test-Loss: 0.20140803593331905 Train-f1: 0.39957448213908264 Test-f1: 0.3832710668327107 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3659/20000] Train-Loss: 0.6974699055650467 Test-Loss: 0.2011096203846099 Train-f1: 0.3978868190632897 Test-f1: 0.38528382095523883 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "Epoch [3660/20000] Train-Loss: 0.694389134923648 Test-Loss: 0.20124117760754315 Train-f1: 0.40770662287903664 Test-f1: 0.3755952380952381 Train-acc: 0.7218543046357616 Test-acc: 0.6710526315789473\n",
      "Epoch [3661/20000] Train-Loss: 0.6951531431076401 Test-Loss: 0.20135861237511385 Train-f1: 0.3951333135097631 Test-f1: 0.40500423562470306 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "Epoch [3662/20000] Train-Loss: 0.6947042333961947 Test-Loss: 0.20238355807478922 Train-f1: 0.3886596768336964 Test-f1: 0.38847594814978065 Train-acc: 0.695364238410596 Test-acc: 0.6842105263157895\n",
      "Epoch [3663/20000] Train-Loss: 0.6955880299275407 Test-Loss: 0.20151558241683543 Train-f1: 0.3972774746230124 Test-f1: 0.3755952380952381 Train-acc: 0.7019867549668874 Test-acc: 0.6710526315789473\n",
      "Epoch [3664/20000] Train-Loss: 0.6970003150501919 Test-Loss: 0.20416062735238066 Train-f1: 0.39162571941333 Test-f1: 0.3643886674993556 Train-acc: 0.7052980132450332 Test-acc: 0.6447368421052632\n",
      "Epoch [3665/20000] Train-Loss: 0.6982689806007892 Test-Loss: 0.20145909776532192 Train-f1: 0.4037325960304138 Test-f1: 0.37745098039215685 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "Epoch [3666/20000] Train-Loss: 0.6953253674936417 Test-Loss: 0.20162972576244095 Train-f1: 0.3892231174830928 Test-f1: 0.4104395604395604 Train-acc: 0.6986754966887417 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3667/20000] Train-Loss: 0.6958540983473838 Test-Loss: 0.2009343585607393 Train-f1: 0.4053917378917379 Test-f1: 0.3813858552700798 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3668/20000] Train-Loss: 0.6964558299340194 Test-Loss: 0.2012861077198667 Train-f1: 0.4108227272907663 Test-f1: 0.40500423562470306 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3669/20000] Train-Loss: 0.694310764231553 Test-Loss: 0.20100251237751585 Train-f1: 0.3959017695544642 Test-f1: 0.3932710668327107 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3670/20000] Train-Loss: 0.6955181910565672 Test-Loss: 0.2030537416837451 Train-f1: 0.400497936531119 Test-f1: 0.4075187969924812 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3671/20000] Train-Loss: 0.6928711867633746 Test-Loss: 0.2045404552452007 Train-f1: 0.4003617051844245 Test-f1: 0.3736495388669302 Train-acc: 0.7086092715231788 Test-acc: 0.6578947368421053\n",
      "Epoch [3672/20000] Train-Loss: 0.6978433234824931 Test-Loss: 0.20103376996388056 Train-f1: 0.38791231240669444 Test-f1: 0.3932710668327107 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3673/20000] Train-Loss: 0.6953966436616744 Test-Loss: 0.20122332127106557 Train-f1: 0.41715852774676304 Test-f1: 0.38873873873873876 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "Epoch [3674/20000] Train-Loss: 0.6941682184826858 Test-Loss: 0.2011135363107609 Train-f1: 0.40255884377688095 Test-f1: 0.3813858552700798 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3675/20000] Train-Loss: 0.6947578021818713 Test-Loss: 0.20082072179544763 Train-f1: 0.39514255741635756 Test-f1: 0.39361844303374544 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3676/20000] Train-Loss: 0.6956113554682093 Test-Loss: 0.20091777562967178 Train-f1: 0.4014757281011769 Test-f1: 0.3992156862745099 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3677/20000] Train-Loss: 0.6952739662555576 Test-Loss: 0.20106911187903984 Train-f1: 0.4014141414141414 Test-f1: 0.387620103473762 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3678/20000] Train-Loss: 0.6974025882793722 Test-Loss: 0.20111529832448716 Train-f1: 0.39020162494731947 Test-f1: 0.3992156862745099 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3679/20000] Train-Loss: 0.6959593455633578 Test-Loss: 0.20074714930670334 Train-f1: 0.41289320185871914 Test-f1: 0.3912626101730912 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3680/20000] Train-Loss: 0.6936180434158712 Test-Loss: 0.20208547012188283 Train-f1: 0.40064831002331003 Test-f1: 0.391963414898459 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3681/20000] Train-Loss: 0.6964626840999694 Test-Loss: 0.2008876036251735 Train-f1: 0.38329239974834217 Test-f1: 0.3755952380952381 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3682/20000] Train-Loss: 0.6952874701105282 Test-Loss: 0.20207334784518988 Train-f1: 0.41077185221351764 Test-f1: 0.37574107435309534 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "Epoch [3683/20000] Train-Loss: 0.694794277603668 Test-Loss: 0.20091104911869842 Train-f1: 0.3980031379261161 Test-f1: 0.3829831194859398 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3684/20000] Train-Loss: 0.6950071373497148 Test-Loss: 0.2009987273018869 Train-f1: 0.406924467484264 Test-f1: 0.387620103473762 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3685/20000] Train-Loss: 0.6950614606970874 Test-Loss: 0.2008475254768766 Train-f1: 0.40064831002331003 Test-f1: 0.3832710668327107 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3686/20000] Train-Loss: 0.6951740446033436 Test-Loss: 0.2006362146485601 Train-f1: 0.4016730908175508 Test-f1: 0.3912626101730912 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3687/20000] Train-Loss: 0.6949252828452098 Test-Loss: 0.2006478622916832 Train-f1: 0.40083411638836514 Test-f1: 0.3813858552700798 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3688/20000] Train-Loss: 0.6961144712757633 Test-Loss: 0.20094471989681853 Train-f1: 0.39128871024573586 Test-f1: 0.3997076023391813 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3689/20000] Train-Loss: 0.6984008836155435 Test-Loss: 0.200968005850055 Train-f1: 0.3900455484880711 Test-f1: 0.3932710668327107 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3690/20000] Train-Loss: 0.6963459222707629 Test-Loss: 0.20153788759787883 Train-f1: 0.4064115231758997 Test-f1: 0.38859649122807016 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3691/20000] Train-Loss: 0.6960128076818762 Test-Loss: 0.20162639707059216 Train-f1: 0.4028231060917829 Test-f1: 0.3853289826938169 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3692/20000] Train-Loss: 0.695607372323828 Test-Loss: 0.20055814296677074 Train-f1: 0.40684638425331626 Test-f1: 0.39361844303374544 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3693/20000] Train-Loss: 0.6940825303309035 Test-Loss: 0.20144501777117144 Train-f1: 0.39776396210223985 Test-f1: 0.37732600732600735 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3694/20000] Train-Loss: 0.6930222738988691 Test-Loss: 0.20052077922893644 Train-f1: 0.41023688395951174 Test-f1: 0.3832710668327107 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3695/20000] Train-Loss: 0.6944160421022965 Test-Loss: 0.20137270859629913 Train-f1: 0.4019328168883515 Test-f1: 0.3932710668327107 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3696/20000] Train-Loss: 0.6875695904724966 Test-Loss: 0.20318092365601384 Train-f1: 0.4075783475783476 Test-f1: 0.3614773258532024 Train-acc: 0.7152317880794702 Test-acc: 0.6447368421052632\n",
      "Epoch [3697/20000] Train-Loss: 0.6928607248075299 Test-Loss: 0.20057723255434928 Train-f1: 0.3910931986274463 Test-f1: 0.3829831194859398 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3698/20000] Train-Loss: 0.6929767370289824 Test-Loss: 0.2006288822206288 Train-f1: 0.39587325783972127 Test-f1: 0.3932710668327107 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3699/20000] Train-Loss: 0.6933292523606487 Test-Loss: 0.20132761071083158 Train-f1: 0.40801883766056946 Test-f1: 0.3932710668327107 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3700/20000] Train-Loss: 0.6938599523916724 Test-Loss: 0.20070014047466514 Train-f1: 0.4089493000467245 Test-f1: 0.387620103473762 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3701/20000] Train-Loss: 0.6887441771438989 Test-Loss: 0.20490061025354345 Train-f1: 0.3921879863056334 Test-f1: 0.3972204048454569 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3702/20000] Train-Loss: 0.695766340300603 Test-Loss: 0.20245025708291065 Train-f1: 0.40588831458396674 Test-f1: 0.39338345864661656 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3703/20000] Train-Loss: 0.6908647245861225 Test-Loss: 0.20076239715861038 Train-f1: 0.4027452411456949 Test-f1: 0.40500423562470306 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3704/20000] Train-Loss: 0.6922508328228796 Test-Loss: 0.20125498098948869 Train-f1: 0.40411941315706057 Test-f1: 0.3997076023391813 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3705/20000] Train-Loss: 0.6894101625519651 Test-Loss: 0.20346802157779828 Train-f1: 0.405084743550589 Test-f1: 0.37080209895052474 Train-acc: 0.7119205298013245 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3706/20000] Train-Loss: 0.69504401368616 Test-Loss: 0.20046115759190444 Train-f1: 0.40599910793933985 Test-f1: 0.4163765578329385 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "Epoch [3707/20000] Train-Loss: 0.6928437639150566 Test-Loss: 0.20131103547979465 Train-f1: 0.3982113854680226 Test-f1: 0.3997076023391813 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3708/20000] Train-Loss: 0.6945191248334184 Test-Loss: 0.20045412424060893 Train-f1: 0.40631474209538343 Test-f1: 0.387620103473762 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3709/20000] Train-Loss: 0.693470497575583 Test-Loss: 0.2034063046261121 Train-f1: 0.4052218295005856 Test-f1: 0.37080209895052474 Train-acc: 0.7119205298013245 Test-acc: 0.6578947368421053\n",
      "Epoch [3710/20000] Train-Loss: 0.6927717646872092 Test-Loss: 0.2008631814776077 Train-f1: 0.4059107117386963 Test-f1: 0.3816475095785441 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3711/20000] Train-Loss: 0.6901328746543254 Test-Loss: 0.20101354569872693 Train-f1: 0.40059306453389365 Test-f1: 0.38528382095523883 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3712/20000] Train-Loss: 0.6919865018554955 Test-Loss: 0.20063521617284572 Train-f1: 0.4005403750351698 Test-f1: 0.3755952380952381 Train-acc: 0.7152317880794702 Test-acc: 0.6710526315789473\n",
      "Epoch [3713/20000] Train-Loss: 0.6895563183967455 Test-Loss: 0.20124006062716568 Train-f1: 0.4019294834685919 Test-f1: 0.4054556104091708 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3714/20000] Train-Loss: 0.6954135277686253 Test-Loss: 0.20033392734272618 Train-f1: 0.39577772325006955 Test-f1: 0.3912626101730912 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3715/20000] Train-Loss: 0.6937188200098607 Test-Loss: 0.20027806860736813 Train-f1: 0.40689831002331 Test-f1: 0.387571211132855 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3716/20000] Train-Loss: 0.6920968413459969 Test-Loss: 0.20025104248013778 Train-f1: 0.4012837022132797 Test-f1: 0.39361844303374544 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3717/20000] Train-Loss: 0.6926314884033765 Test-Loss: 0.2011685699278894 Train-f1: 0.4057298371182128 Test-f1: 0.38171759190827126 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3718/20000] Train-Loss: 0.6923750155576909 Test-Loss: 0.20034746033758102 Train-f1: 0.4004410040826226 Test-f1: 0.38528382095523883 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3719/20000] Train-Loss: 0.6922387134314438 Test-Loss: 0.200594146362418 Train-f1: 0.40397117198018345 Test-f1: 0.3813858552700798 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3720/20000] Train-Loss: 0.6899483703623955 Test-Loss: 0.20078738779063396 Train-f1: 0.40403412377096587 Test-f1: 0.3755952380952381 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "Epoch [3721/20000] Train-Loss: 0.6917650337722516 Test-Loss: 0.20062645523581238 Train-f1: 0.40824921028020505 Test-f1: 0.3932710668327107 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3722/20000] Train-Loss: 0.6929516179203595 Test-Loss: 0.2005231121884356 Train-f1: 0.4051333670845866 Test-f1: 0.4104395604395604 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3723/20000] Train-Loss: 0.6884984817081786 Test-Loss: 0.20046152469150963 Train-f1: 0.39118539996588775 Test-f1: 0.3932710668327107 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3724/20000] Train-Loss: 0.6927634908202333 Test-Loss: 0.20059781278108024 Train-f1: 0.4083305955789177 Test-f1: 0.3932710668327107 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3725/20000] Train-Loss: 0.6931277492282543 Test-Loss: 0.20028561191615588 Train-f1: 0.4111546699552151 Test-f1: 0.3755952380952381 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "Epoch [3726/20000] Train-Loss: 0.6910658909274502 Test-Loss: 0.20067029611829013 Train-f1: 0.4052889515571117 Test-f1: 0.38528382095523883 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3727/20000] Train-Loss: 0.692727793582098 Test-Loss: 0.20161336554296744 Train-f1: 0.40661780513217816 Test-f1: 0.4098471168893704 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3728/20000] Train-Loss: 0.691438454026708 Test-Loss: 0.2012004157240656 Train-f1: 0.3926661316825252 Test-f1: 0.41698841698841693 Train-acc: 0.7052980132450332 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [3729/20000] Train-Loss: 0.6915186501556654 Test-Loss: 0.20014443525083442 Train-f1: 0.4076502028547278 Test-f1: 0.38859649122807016 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3730/20000] Train-Loss: 0.6884927243582727 Test-Loss: 0.20043895055923286 Train-f1: 0.39278074096185217 Test-f1: 0.4055205890822329 Train-acc: 0.6986754966887417 Test-acc: 0.7105263157894737\n",
      "Epoch [3731/20000] Train-Loss: 0.6894320738284945 Test-Loss: 0.2017634745442293 Train-f1: 0.39979227646114623 Test-f1: 0.391454746803584 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3732/20000] Train-Loss: 0.6897606655438451 Test-Loss: 0.20029683052056427 Train-f1: 0.4073227632287457 Test-f1: 0.3932710668327107 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3733/20000] Train-Loss: 0.6916224046401752 Test-Loss: 0.20244510149811676 Train-f1: 0.4085256896388284 Test-f1: 0.3645586297760211 Train-acc: 0.7152317880794702 Test-acc: 0.6447368421052632\n",
      "Epoch [3734/20000] Train-Loss: 0.6922214196234588 Test-Loss: 0.20018342990813603 Train-f1: 0.399140152346953 Test-f1: 0.3932710668327107 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3735/20000] Train-Loss: 0.6891668642599136 Test-Loss: 0.20007287025801226 Train-f1: 0.40821052631578947 Test-f1: 0.3932710668327107 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3736/20000] Train-Loss: 0.6896906144460945 Test-Loss: 0.2021435048511419 Train-f1: 0.41718333889692527 Test-f1: 0.398138329209379 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3737/20000] Train-Loss: 0.690476684938532 Test-Loss: 0.1999333816840497 Train-f1: 0.41077855477855474 Test-f1: 0.3813858552700798 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3738/20000] Train-Loss: 0.6913169065053675 Test-Loss: 0.20391606606298396 Train-f1: 0.4010867471244829 Test-f1: 0.3645586297760211 Train-acc: 0.7185430463576159 Test-acc: 0.6447368421052632\n",
      "Epoch [3739/20000] Train-Loss: 0.6879143887577527 Test-Loss: 0.20080465558095978 Train-f1: 0.40251748251748254 Test-f1: 0.3818790218790219 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3740/20000] Train-Loss: 0.6877850188101603 Test-Loss: 0.20206412933816306 Train-f1: 0.39764019589014793 Test-f1: 0.37080209895052474 Train-acc: 0.7052980132450332 Test-acc: 0.6578947368421053\n",
      "Epoch [3741/20000] Train-Loss: 0.6902913099925465 Test-Loss: 0.20039124033630484 Train-f1: 0.3942538529570153 Test-f1: 0.387620103473762 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3742/20000] Train-Loss: 0.693046778671102 Test-Loss: 0.20175035911952538 Train-f1: 0.4276101013905892 Test-f1: 0.3821524212221886 Train-acc: 0.7384105960264901 Test-acc: 0.6710526315789473\n",
      "Epoch [3743/20000] Train-Loss: 0.6896699118039138 Test-Loss: 0.20188260875557804 Train-f1: 0.4060752652601619 Test-f1: 0.3762540614191384 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "Epoch [3744/20000] Train-Loss: 0.6913665867552988 Test-Loss: 0.20095649137106195 Train-f1: 0.3949912524596849 Test-f1: 0.3876470946893482 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3745/20000] Train-Loss: 0.6845166306223016 Test-Loss: 0.20294979533409935 Train-f1: 0.4209423974590315 Test-f1: 0.37080209895052474 Train-acc: 0.7284768211920529 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3746/20000] Train-Loss: 0.691534247555698 Test-Loss: 0.19988489485436836 Train-f1: 0.40249540409659623 Test-f1: 0.3813858552700798 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3747/20000] Train-Loss: 0.6901799996277211 Test-Loss: 0.1998382319184758 Train-f1: 0.39854087198672306 Test-f1: 0.4055205890822329 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [3748/20000] Train-Loss: 0.6894696627289348 Test-Loss: 0.20191658734366127 Train-f1: 0.40745565988523735 Test-f1: 0.3761533733817086 Train-acc: 0.7152317880794702 Test-acc: 0.6578947368421053\n",
      "Epoch [3749/20000] Train-Loss: 0.6904484331929552 Test-Loss: 0.20017738293859824 Train-f1: 0.40276251400541 Test-f1: 0.3912626101730912 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3750/20000] Train-Loss: 0.6902616006684323 Test-Loss: 0.19997438075240148 Train-f1: 0.39666824732490774 Test-f1: 0.3755952380952381 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "Epoch [3751/20000] Train-Loss: 0.6921348730270317 Test-Loss: 0.20111000286203898 Train-f1: 0.40583306529216046 Test-f1: 0.38171759190827126 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3752/20000] Train-Loss: 0.6894444260764798 Test-Loss: 0.20037129642064846 Train-f1: 0.3977864955512258 Test-f1: 0.39361844303374544 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3753/20000] Train-Loss: 0.6924337422840267 Test-Loss: 0.20044865800459952 Train-f1: 0.39618682728775706 Test-f1: 0.3932710668327107 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3754/20000] Train-Loss: 0.6897415946970334 Test-Loss: 0.1998637486598661 Train-f1: 0.4009756293943704 Test-f1: 0.3990109890109891 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3755/20000] Train-Loss: 0.6888412883009465 Test-Loss: 0.19971461854926428 Train-f1: 0.40145362716972144 Test-f1: 0.3813858552700798 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3756/20000] Train-Loss: 0.6886699749397913 Test-Loss: 0.19980564192102762 Train-f1: 0.41347254554977714 Test-f1: 0.3990109890109891 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3757/20000] Train-Loss: 0.689606038765523 Test-Loss: 0.20004466506687082 Train-f1: 0.39329834176166845 Test-f1: 0.39361844303374544 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3758/20000] Train-Loss: 0.685926017563856 Test-Loss: 0.20341587511240275 Train-f1: 0.41627039627039625 Test-f1: 0.3645586297760211 Train-acc: 0.7284768211920529 Test-acc: 0.6447368421052632\n",
      "Epoch [3759/20000] Train-Loss: 0.6897739934788915 Test-Loss: 0.2016218880289208 Train-f1: 0.40223700865902695 Test-f1: 0.39338345864661656 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3760/20000] Train-Loss: 0.688859000529913 Test-Loss: 0.20008412815543183 Train-f1: 0.4043615820981771 Test-f1: 0.40500423562470306 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3761/20000] Train-Loss: 0.6897488653316991 Test-Loss: 0.19984430189265956 Train-f1: 0.4054858839817806 Test-f1: 0.3932710668327107 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3762/20000] Train-Loss: 0.687177521270433 Test-Loss: 0.20113327224508096 Train-f1: 0.4123369821077773 Test-f1: 0.406006006006006 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3763/20000] Train-Loss: 0.6910462054382189 Test-Loss: 0.19970039578440602 Train-f1: 0.40601263470828697 Test-f1: 0.4102339181286549 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3764/20000] Train-Loss: 0.6893306135643316 Test-Loss: 0.19964781327632167 Train-f1: 0.42791715274473896 Test-f1: 0.3813858552700798 Train-acc: 0.7384105960264901 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3765/20000] Train-Loss: 0.6890937374254054 Test-Loss: 0.19957183044389645 Train-f1: 0.39786023786023794 Test-f1: 0.40500423562470306 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [3766/20000] Train-Loss: 0.6895019210973687 Test-Loss: 0.19964313692259925 Train-f1: 0.4097435897435897 Test-f1: 0.4055205890822329 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [3767/20000] Train-Loss: 0.688624110190579 Test-Loss: 0.20359933010986314 Train-f1: 0.40519733477540615 Test-f1: 0.37080209895052474 Train-acc: 0.7119205298013245 Test-acc: 0.6578947368421053\n",
      "Epoch [3768/20000] Train-Loss: 0.686656326038235 Test-Loss: 0.19997225834970045 Train-f1: 0.4027763526463217 Test-f1: 0.39361844303374544 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3769/20000] Train-Loss: 0.6857190048245804 Test-Loss: 0.20243218723475406 Train-f1: 0.4170114257919136 Test-f1: 0.364987922705314 Train-acc: 0.7284768211920529 Test-acc: 0.6447368421052632\n",
      "Epoch [3770/20000] Train-Loss: 0.6891501170172931 Test-Loss: 0.20019635417202716 Train-f1: 0.39469346443030656 Test-f1: 0.4055205890822329 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "Epoch [3771/20000] Train-Loss: 0.6895398404469305 Test-Loss: 0.19988342923460448 Train-f1: 0.4065707149093026 Test-f1: 0.387620103473762 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3772/20000] Train-Loss: 0.6892232521857917 Test-Loss: 0.19960815867347673 Train-f1: 0.41373531511650546 Test-f1: 0.3755952380952381 Train-acc: 0.7218543046357616 Test-acc: 0.6710526315789473\n",
      "Epoch [3773/20000] Train-Loss: 0.6893392407648826 Test-Loss: 0.19996725891576517 Train-f1: 0.3901872084096144 Test-f1: 0.4104395604395604 Train-acc: 0.7019867549668874 Test-acc: 0.7105263157894737\n",
      "Epoch [3774/20000] Train-Loss: 0.6805643479623112 Test-Loss: 0.20440353147059545 Train-f1: 0.4018979399362417 Test-f1: 0.38473148473148466 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3775/20000] Train-Loss: 0.6929650830812305 Test-Loss: 0.19950660059895875 Train-f1: 0.41107091689370173 Test-f1: 0.3813858552700798 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3776/20000] Train-Loss: 0.6887183806858018 Test-Loss: 0.19953070477193963 Train-f1: 0.39946851809118666 Test-f1: 0.4055205890822329 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3777/20000] Train-Loss: 0.6870394199784937 Test-Loss: 0.19942299398309993 Train-f1: 0.4179143510951292 Test-f1: 0.3813858552700798 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "Epoch [3778/20000] Train-Loss: 0.687472854232104 Test-Loss: 0.1997609085928297 Train-f1: 0.3918783237503443 Test-f1: 0.4055205890822329 Train-acc: 0.6986754966887417 Test-acc: 0.7105263157894737\n",
      "Epoch [3779/20000] Train-Loss: 0.6881123820567426 Test-Loss: 0.19982177730220255 Train-f1: 0.40583578842494034 Test-f1: 0.387620103473762 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3780/20000] Train-Loss: 0.6876537282683585 Test-Loss: 0.19947618126776057 Train-f1: 0.4068486256291134 Test-f1: 0.3932710668327107 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3781/20000] Train-Loss: 0.6876264945063725 Test-Loss: 0.19943777420774983 Train-f1: 0.4044897131853654 Test-f1: 0.4055205890822329 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3782/20000] Train-Loss: 0.6870657154154641 Test-Loss: 0.2016041071923603 Train-f1: 0.4134772608230284 Test-f1: 0.38702721809826796 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [3783/20000] Train-Loss: 0.6879003128095039 Test-Loss: 0.19971457467737452 Train-f1: 0.400190010225346 Test-f1: 0.38528382095523883 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3784/20000] Train-Loss: 0.6873599911887435 Test-Loss: 0.19935926975527887 Train-f1: 0.3982648930017351 Test-f1: 0.3813858552700798 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3785/20000] Train-Loss: 0.6884304154142428 Test-Loss: 0.19964999386108556 Train-f1: 0.4169711538461538 Test-f1: 0.4055205890822329 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [3786/20000] Train-Loss: 0.6880943336801838 Test-Loss: 0.2013735875697029 Train-f1: 0.4082858073121803 Test-f1: 0.37984639016897076 Train-acc: 0.7152317880794702 Test-acc: 0.6710526315789473\n",
      "Epoch [3787/20000] Train-Loss: 0.6865710029932883 Test-Loss: 0.20060268855298172 Train-f1: 0.40183220908884626 Test-f1: 0.4054556104091708 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [3788/20000] Train-Loss: 0.6871547283178293 Test-Loss: 0.19943887560204662 Train-f1: 0.41952259317528773 Test-f1: 0.3932710668327107 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3789/20000] Train-Loss: 0.6871707209795797 Test-Loss: 0.19931006545908792 Train-f1: 0.4084505422634919 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3790/20000] Train-Loss: 0.6871532857732661 Test-Loss: 0.19993272800043657 Train-f1: 0.40378566413733746 Test-f1: 0.3995495495495495 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3791/20000] Train-Loss: 0.6859648183409576 Test-Loss: 0.2007097008023887 Train-f1: 0.40479101069760903 Test-f1: 0.41698841698841693 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "Epoch [3792/20000] Train-Loss: 0.6866999711927055 Test-Loss: 0.19994957188106177 Train-f1: 0.4095017218356876 Test-f1: 0.4102339181286549 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [3793/20000] Train-Loss: 0.6865339377830737 Test-Loss: 0.19944302487626014 Train-f1: 0.41255678627941395 Test-f1: 0.40500423562470306 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3794/20000] Train-Loss: 0.6885533613821252 Test-Loss: 0.19980685550888172 Train-f1: 0.40269215291750504 Test-f1: 0.3813858552700798 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3795/20000] Train-Loss: 0.686162400720294 Test-Loss: 0.1992492602503261 Train-f1: 0.39834654450419216 Test-f1: 0.3813858552700798 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3796/20000] Train-Loss: 0.6877382162900688 Test-Loss: 0.19921652144320812 Train-f1: 0.40775923548709175 Test-f1: 0.3912626101730912 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3797/20000] Train-Loss: 0.6879867273630645 Test-Loss: 0.19925536955896886 Train-f1: 0.3999666482226147 Test-f1: 0.4163765578329386 Train-acc: 0.7052980132450332 Test-acc: 0.7236842105263158\n",
      "Epoch [3798/20000] Train-Loss: 0.6839368855177912 Test-Loss: 0.2033564005969061 Train-f1: 0.4040025224498717 Test-f1: 0.3645586297760211 Train-acc: 0.7152317880794702 Test-acc: 0.6447368421052632\n",
      "Epoch [3799/20000] Train-Loss: 0.6877684772423069 Test-Loss: 0.1993764328166486 Train-f1: 0.40361161524500916 Test-f1: 0.387620103473762 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3800/20000] Train-Loss: 0.6871473309214593 Test-Loss: 0.20040768124662747 Train-f1: 0.3937836335402258 Test-f1: 0.3971709042131577 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3801/20000] Train-Loss: 0.6860159433375677 Test-Loss: 0.19928363735239027 Train-f1: 0.4087042083081224 Test-f1: 0.387620103473762 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3802/20000] Train-Loss: 0.6876307409773809 Test-Loss: 0.19963018202395752 Train-f1: 0.40930561698363555 Test-f1: 0.4104395604395604 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3803/20000] Train-Loss: 0.6855542891929443 Test-Loss: 0.2000709059259821 Train-f1: 0.4116066207811972 Test-f1: 0.3987726358148894 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3804/20000] Train-Loss: 0.6876869135645244 Test-Loss: 0.2000224936674472 Train-f1: 0.4044383616383616 Test-f1: 0.4163765578329385 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3805/20000] Train-Loss: 0.6872483752731221 Test-Loss: 0.20031817916554898 Train-f1: 0.3883087267470747 Test-f1: 0.3932710668327107 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3806/20000] Train-Loss: 0.6877391823593791 Test-Loss: 0.1991636821112996 Train-f1: 0.4232343877513733 Test-f1: 0.3755952380952381 Train-acc: 0.7317880794701986 Test-acc: 0.6710526315789473\n",
      "Epoch [3807/20000] Train-Loss: 0.6831349821023106 Test-Loss: 0.20193385184488505 Train-f1: 0.408027287499425 Test-f1: 0.364987922705314 Train-acc: 0.7218543046357616 Test-acc: 0.6447368421052632\n",
      "Epoch [3808/20000] Train-Loss: 0.6881956522511977 Test-Loss: 0.20013443525299612 Train-f1: 0.4044897131853654 Test-f1: 0.4098471168893704 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3809/20000] Train-Loss: 0.6878567195863895 Test-Loss: 0.19920938400334193 Train-f1: 0.41150954729018857 Test-f1: 0.3755952380952381 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "Epoch [3810/20000] Train-Loss: 0.6847980384067937 Test-Loss: 0.2001011470977553 Train-f1: 0.4026586792183712 Test-f1: 0.3853289826938169 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3811/20000] Train-Loss: 0.6852783834084034 Test-Loss: 0.19964835306100423 Train-f1: 0.4129492493335728 Test-f1: 0.3813858552700798 Train-acc: 0.7218543046357616 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3812/20000] Train-Loss: 0.68344374013195 Test-Loss: 0.19913995182064173 Train-f1: 0.40439638910709197 Test-f1: 0.3995495495495495 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3813/20000] Train-Loss: 0.6856018356779364 Test-Loss: 0.19909071103107354 Train-f1: 0.41388311406331457 Test-f1: 0.3995495495495495 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3814/20000] Train-Loss: 0.6856845746543792 Test-Loss: 0.1991550239441259 Train-f1: 0.4068295592632121 Test-f1: 0.3755952380952381 Train-acc: 0.7152317880794702 Test-acc: 0.6710526315789473\n",
      "Epoch [3815/20000] Train-Loss: 0.6844563940367654 Test-Loss: 0.20004203336079168 Train-f1: 0.4028908900909661 Test-f1: 0.41722031629152373 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "Epoch [3816/20000] Train-Loss: 0.683745584989308 Test-Loss: 0.19928717765424364 Train-f1: 0.4063441329929103 Test-f1: 0.4055205890822329 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3817/20000] Train-Loss: 0.6831823255535892 Test-Loss: 0.20034928711738922 Train-f1: 0.39333361319843835 Test-f1: 0.3935739935739936 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [3818/20000] Train-Loss: 0.6856374692935213 Test-Loss: 0.19969023370042532 Train-f1: 0.3990835426626692 Test-f1: 0.387620103473762 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3819/20000] Train-Loss: 0.6856815350706171 Test-Loss: 0.19901937729870878 Train-f1: 0.404053072846699 Test-f1: 0.3932710668327107 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3820/20000] Train-Loss: 0.685714862487291 Test-Loss: 0.19916857400048588 Train-f1: 0.404601665001665 Test-f1: 0.3932710668327107 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3821/20000] Train-Loss: 0.6851821663189323 Test-Loss: 0.19949746854937173 Train-f1: 0.4153240243465808 Test-f1: 0.4055205890822329 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3822/20000] Train-Loss: 0.6847999530877897 Test-Loss: 0.19906867431031192 Train-f1: 0.4012981904012588 Test-f1: 0.39361844303374544 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3823/20000] Train-Loss: 0.6845135151148494 Test-Loss: 0.20171380289739674 Train-f1: 0.4051361048023436 Test-f1: 0.39781825303564433 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3824/20000] Train-Loss: 0.6870182398730461 Test-Loss: 0.19902683746189723 Train-f1: 0.406748553132236 Test-f1: 0.40500423562470306 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [3825/20000] Train-Loss: 0.6859291456263275 Test-Loss: 0.1991317421207098 Train-f1: 0.40140189656729097 Test-f1: 0.40500423562470306 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3826/20000] Train-Loss: 0.6845141854774157 Test-Loss: 0.1988884318161339 Train-f1: 0.4107477798275344 Test-f1: 0.3932710668327107 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3827/20000] Train-Loss: 0.6849048128265458 Test-Loss: 0.19949069251184512 Train-f1: 0.406134142407976 Test-f1: 0.37574107435309534 Train-acc: 0.7152317880794702 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3828/20000] Train-Loss: 0.6851460823027652 Test-Loss: 0.19888574778668996 Train-f1: 0.39734866856047485 Test-f1: 0.3997076023391813 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3829/20000] Train-Loss: 0.6849350575222022 Test-Loss: 0.199301852747658 Train-f1: 0.413157361025336 Test-f1: 0.3995495495495495 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3830/20000] Train-Loss: 0.68779217214903 Test-Loss: 0.19875954842566665 Train-f1: 0.40482983682983686 Test-f1: 0.3813858552700798 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3831/20000] Train-Loss: 0.6847599807066544 Test-Loss: 0.19872049447066345 Train-f1: 0.39742249004306923 Test-f1: 0.4163765578329386 Train-acc: 0.7052980132450332 Test-acc: 0.7236842105263158\n",
      "Epoch [3832/20000] Train-Loss: 0.6824662775698909 Test-Loss: 0.20069141440400767 Train-f1: 0.4006801852643638 Test-f1: 0.39378378378378376 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3833/20000] Train-Loss: 0.6832591738370827 Test-Loss: 0.1998694522700262 Train-f1: 0.4051449909677758 Test-f1: 0.4088710166306303 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3834/20000] Train-Loss: 0.6848053240111067 Test-Loss: 0.20148930785019517 Train-f1: 0.41568128551862804 Test-f1: 0.364987922705314 Train-acc: 0.7251655629139073 Test-acc: 0.6447368421052632\n",
      "Epoch [3835/20000] Train-Loss: 0.6846444138772075 Test-Loss: 0.19915190276793326 Train-f1: 0.3920021645021645 Test-f1: 0.3755952380952381 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3836/20000] Train-Loss: 0.6820510324673229 Test-Loss: 0.19908257570469237 Train-f1: 0.416357162435692 Test-f1: 0.4163765578329386 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [3837/20000] Train-Loss: 0.6845157174519542 Test-Loss: 0.2005519037334364 Train-f1: 0.3919740675990676 Test-f1: 0.37984639016897076 Train-acc: 0.6986754966887417 Test-acc: 0.6710526315789473\n",
      "Epoch [3838/20000] Train-Loss: 0.6829163505576222 Test-Loss: 0.19899506743818768 Train-f1: 0.40250146414198423 Test-f1: 0.40500423562470306 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [3839/20000] Train-Loss: 0.6860772455922003 Test-Loss: 0.19901870463150695 Train-f1: 0.39906235069278545 Test-f1: 0.40500423562470306 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [3840/20000] Train-Loss: 0.6838961529075532 Test-Loss: 0.20098322108046898 Train-f1: 0.41199365052125786 Test-f1: 0.41722031629152373 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [3841/20000] Train-Loss: 0.6841299800876202 Test-Loss: 0.1990308583249405 Train-f1: 0.40138611663877155 Test-f1: 0.40500423562470306 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3842/20000] Train-Loss: 0.6854319777160363 Test-Loss: 0.19867500924430137 Train-f1: 0.39330582185427543 Test-f1: 0.40500423562470306 Train-acc: 0.7019867549668874 Test-acc: 0.7105263157894737\n",
      "Epoch [3843/20000] Train-Loss: 0.681943587522094 Test-Loss: 0.1989838299882555 Train-f1: 0.41878675562144796 Test-f1: 0.3944216706148934 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3844/20000] Train-Loss: 0.6849220557646772 Test-Loss: 0.19859490413295944 Train-f1: 0.42036257309941527 Test-f1: 0.3912626101730912 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [3845/20000] Train-Loss: 0.6831972118776105 Test-Loss: 0.19866862316191003 Train-f1: 0.39461988304093565 Test-f1: 0.40500423562470306 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "Epoch [3846/20000] Train-Loss: 0.6843703135175855 Test-Loss: 0.19864984756200177 Train-f1: 0.4012981904012588 Test-f1: 0.387620103473762 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3847/20000] Train-Loss: 0.6842554124620434 Test-Loss: 0.19912850051159592 Train-f1: 0.40917461558891877 Test-f1: 0.40500423562470306 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3848/20000] Train-Loss: 0.6859717153941648 Test-Loss: 0.1991040412468025 Train-f1: 0.42094544277157186 Test-f1: 0.4055205890822329 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3849/20000] Train-Loss: 0.6839695671137709 Test-Loss: 0.19858992973039497 Train-f1: 0.4044129840183183 Test-f1: 0.3995495495495495 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3850/20000] Train-Loss: 0.683428523694923 Test-Loss: 0.19853221159396883 Train-f1: 0.41290083568564573 Test-f1: 0.4055205890822329 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3851/20000] Train-Loss: 0.6794102274250733 Test-Loss: 0.20029932559543526 Train-f1: 0.4137809835521475 Test-f1: 0.4061776061776062 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3852/20000] Train-Loss: 0.6843829993618129 Test-Loss: 0.19932163167519665 Train-f1: 0.402921533693177 Test-f1: 0.39361844303374544 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3853/20000] Train-Loss: 0.6827492865909115 Test-Loss: 0.19976159266489724 Train-f1: 0.4257655421660516 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [3854/20000] Train-Loss: 0.6835985970229579 Test-Loss: 0.19883977141730222 Train-f1: 0.4113938235705457 Test-f1: 0.3995495495495495 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3855/20000] Train-Loss: 0.6808766303987663 Test-Loss: 0.20006838145297934 Train-f1: 0.406819314019314 Test-f1: 0.3987726358148894 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3856/20000] Train-Loss: 0.6821425028880949 Test-Loss: 0.19856491248860564 Train-f1: 0.4073477812177503 Test-f1: 0.39361844303374544 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3857/20000] Train-Loss: 0.6842961321847602 Test-Loss: 0.19889971192862974 Train-f1: 0.4057891027780869 Test-f1: 0.3981870467616905 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3858/20000] Train-Loss: 0.6813751193733439 Test-Loss: 0.19977559252435695 Train-f1: 0.41827877610490527 Test-f1: 0.3853289826938169 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "Epoch [3859/20000] Train-Loss: 0.6818373425029609 Test-Loss: 0.19865036677432993 Train-f1: 0.4069826173826174 Test-f1: 0.3995495495495495 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3860/20000] Train-Loss: 0.6815704336442145 Test-Loss: 0.19988487903783425 Train-f1: 0.4073685048141959 Test-f1: 0.4054556104091708 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3861/20000] Train-Loss: 0.6827494744462038 Test-Loss: 0.2005456215530098 Train-f1: 0.40202893676048035 Test-f1: 0.374380551659654 Train-acc: 0.7052980132450332 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [3862/20000] Train-Loss: 0.6830853291539764 Test-Loss: 0.19835711958578425 Train-f1: 0.39740132411233864 Test-f1: 0.3912626101730912 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3863/20000] Train-Loss: 0.6823224901530195 Test-Loss: 0.19841688049072873 Train-f1: 0.4087021054825932 Test-f1: 0.4102339181286549 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3864/20000] Train-Loss: 0.6817549301239794 Test-Loss: 0.19836074624654823 Train-f1: 0.42167421053444054 Test-f1: 0.3912626101730912 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [3865/20000] Train-Loss: 0.6809696518823529 Test-Loss: 0.1983981434029837 Train-f1: 0.4040320197044335 Test-f1: 0.3912626101730912 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3866/20000] Train-Loss: 0.6808788786581211 Test-Loss: 0.19853799445266102 Train-f1: 0.4017187873000717 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3867/20000] Train-Loss: 0.6780054887475899 Test-Loss: 0.2051292050182064 Train-f1: 0.4029801777170198 Test-f1: 0.3560176026484719 Train-acc: 0.7086092715231788 Test-acc: 0.631578947368421\n",
      "Epoch [3868/20000] Train-Loss: 0.6847160242950842 Test-Loss: 0.1985255894540323 Train-f1: 0.4028762129115487 Test-f1: 0.4055205890822329 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3869/20000] Train-Loss: 0.6830318286174826 Test-Loss: 0.19927520186633768 Train-f1: 0.409393603286733 Test-f1: 0.3853289826938169 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3870/20000] Train-Loss: 0.6820691322151595 Test-Loss: 0.19848266309567184 Train-f1: 0.40611756180965874 Test-f1: 0.4055205890822329 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [3871/20000] Train-Loss: 0.6818547405048688 Test-Loss: 0.19896401563244037 Train-f1: 0.4106156624788099 Test-f1: 0.3975433999344907 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3872/20000] Train-Loss: 0.6825578752295833 Test-Loss: 0.2008490074902507 Train-f1: 0.41236973029588897 Test-f1: 0.374380551659654 Train-acc: 0.7218543046357616 Test-acc: 0.6578947368421053\n",
      "Epoch [3873/20000] Train-Loss: 0.6831199835308165 Test-Loss: 0.19840149183763642 Train-f1: 0.4022854145592147 Test-f1: 0.3932710668327107 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3874/20000] Train-Loss: 0.6808435383759561 Test-Loss: 0.1982039406880387 Train-f1: 0.4153996101364522 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [3875/20000] Train-Loss: 0.6792470899504901 Test-Loss: 0.20168694768519047 Train-f1: 0.4028842876618951 Test-f1: 0.3624687656171914 Train-acc: 0.7119205298013245 Test-acc: 0.6447368421052632\n",
      "Epoch [3876/20000] Train-Loss: 0.6806105889465057 Test-Loss: 0.1983517075536154 Train-f1: 0.3867538826530927 Test-f1: 0.39361844303374544 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3877/20000] Train-Loss: 0.6798852996191639 Test-Loss: 0.20033780506054757 Train-f1: 0.4020109976433622 Test-f1: 0.404082760967104 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "Epoch [3878/20000] Train-Loss: 0.6823966327045842 Test-Loss: 0.20018389401641534 Train-f1: 0.41420457257410154 Test-f1: 0.37984639016897076 Train-acc: 0.7218543046357616 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3879/20000] Train-Loss: 0.6808796213115388 Test-Loss: 0.19812080002614135 Train-f1: 0.41621237241543235 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [3880/20000] Train-Loss: 0.6807094114534861 Test-Loss: 0.19826410195078253 Train-f1: 0.4043755715331508 Test-f1: 0.4102339181286549 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [3881/20000] Train-Loss: 0.67926340564182 Test-Loss: 0.19902917495931033 Train-f1: 0.40620443825944647 Test-f1: 0.39361844303374544 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3882/20000] Train-Loss: 0.6812759454499454 Test-Loss: 0.1984554149440387 Train-f1: 0.4067806267806267 Test-f1: 0.3995495495495495 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3883/20000] Train-Loss: 0.6809148204738996 Test-Loss: 0.19896842522359476 Train-f1: 0.39797539980335894 Test-f1: 0.3992867589605914 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3884/20000] Train-Loss: 0.6776928286335745 Test-Loss: 0.19807290110245726 Train-f1: 0.40750056960583275 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3885/20000] Train-Loss: 0.6807564425051409 Test-Loss: 0.19937578784967958 Train-f1: 0.4004894892320164 Test-f1: 0.3853289826938169 Train-acc: 0.7052980132450332 Test-acc: 0.6842105263157895\n",
      "Epoch [3886/20000] Train-Loss: 0.6825165616034855 Test-Loss: 0.1996338067304003 Train-f1: 0.41374392722891395 Test-f1: 0.4098471168893704 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3887/20000] Train-Loss: 0.6810157591428233 Test-Loss: 0.19806285893042258 Train-f1: 0.4100811185617898 Test-f1: 0.37815089964357795 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3888/20000] Train-Loss: 0.6784292952209023 Test-Loss: 0.19897508049885154 Train-f1: 0.40488200470359265 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [3889/20000] Train-Loss: 0.6788613277242895 Test-Loss: 0.19900540589911336 Train-f1: 0.41732155909324187 Test-f1: 0.40500423562470306 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3890/20000] Train-Loss: 0.6797435941575797 Test-Loss: 0.19799982741259364 Train-f1: 0.4060388792864306 Test-f1: 0.3912626101730912 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3891/20000] Train-Loss: 0.6800525216524553 Test-Loss: 0.19827736010793884 Train-f1: 0.4065440542704204 Test-f1: 0.3880952380952381 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "Epoch [3892/20000] Train-Loss: 0.6797584586773929 Test-Loss: 0.19824860087784668 Train-f1: 0.41703949185307926 Test-f1: 0.38875457875457875 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3893/20000] Train-Loss: 0.6800785666375048 Test-Loss: 0.1979553854270913 Train-f1: 0.4075320363090147 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3894/20000] Train-Loss: 0.6788231241635383 Test-Loss: 0.19949322161691138 Train-f1: 0.4115881641555369 Test-f1: 0.41722031629152373 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [3895/20000] Train-Loss: 0.6828380382194318 Test-Loss: 0.1981349752987632 Train-f1: 0.39471778098004684 Test-f1: 0.3932710668327107 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3896/20000] Train-Loss: 0.6805745971520097 Test-Loss: 0.19887852467102038 Train-f1: 0.4088719790847451 Test-f1: 0.3853289826938169 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3897/20000] Train-Loss: 0.6803744215859657 Test-Loss: 0.19797599519411224 Train-f1: 0.4140595275840203 Test-f1: 0.4102339181286549 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [3898/20000] Train-Loss: 0.6801586248920464 Test-Loss: 0.19804868952209584 Train-f1: 0.39641579169624636 Test-f1: 0.4102339181286549 Train-acc: 0.7019867549668874 Test-acc: 0.7105263157894737\n",
      "Epoch [3899/20000] Train-Loss: 0.6792480972948008 Test-Loss: 0.2005957329862359 Train-f1: 0.4134848652028394 Test-f1: 0.38753740538637566 Train-acc: 0.7218543046357616 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3900/20000] Train-Loss: 0.6795507414589821 Test-Loss: 0.19792464473283913 Train-f1: 0.40752893238081833 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3901/20000] Train-Loss: 0.6779789900320764 Test-Loss: 0.19803353437811608 Train-f1: 0.41236608533789243 Test-f1: 0.3995495495495495 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [3902/20000] Train-Loss: 0.6786172409252067 Test-Loss: 0.19809865695390433 Train-f1: 0.40774980330448474 Test-f1: 0.3995495495495495 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3903/20000] Train-Loss: 0.6779683760224243 Test-Loss: 0.1979895928213469 Train-f1: 0.3915705674046441 Test-f1: 0.39361844303374544 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3904/20000] Train-Loss: 0.6794246293900719 Test-Loss: 0.19787335803605435 Train-f1: 0.4062873870269481 Test-f1: 0.4055205890822329 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3905/20000] Train-Loss: 0.6799172954015441 Test-Loss: 0.1978621683341682 Train-f1: 0.4079178027926923 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3906/20000] Train-Loss: 0.6783566211604863 Test-Loss: 0.2007753469519428 Train-f1: 0.3985374583717413 Test-f1: 0.38753740538637566 Train-acc: 0.7119205298013245 Test-acc: 0.6710526315789473\n",
      "Epoch [3907/20000] Train-Loss: 0.6813462577656958 Test-Loss: 0.19981765770772295 Train-f1: 0.41596985421299726 Test-f1: 0.39378378378378376 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [3908/20000] Train-Loss: 0.6805642379150749 Test-Loss: 0.19789236020975284 Train-f1: 0.3969541746907697 Test-f1: 0.4163765578329386 Train-acc: 0.7019867549668874 Test-acc: 0.7236842105263158\n",
      "Epoch [3909/20000] Train-Loss: 0.6782842077699612 Test-Loss: 0.19834876773313875 Train-f1: 0.4111301010697389 Test-f1: 0.403812890600341 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [3910/20000] Train-Loss: 0.6781842077573256 Test-Loss: 0.19792483121388899 Train-f1: 0.4061665177283838 Test-f1: 0.37242424242424244 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [3911/20000] Train-Loss: 0.6803904074041351 Test-Loss: 0.19776994977611062 Train-f1: 0.3971929824561404 Test-f1: 0.41539227895392283 Train-acc: 0.7086092715231788 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [3912/20000] Train-Loss: 0.6784010021226904 Test-Loss: 0.19773834819190186 Train-f1: 0.40519028066620455 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3913/20000] Train-Loss: 0.679964636224151 Test-Loss: 0.1977882139344002 Train-f1: 0.41386867761777635 Test-f1: 0.3912626101730912 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3914/20000] Train-Loss: 0.6783499937282501 Test-Loss: 0.1994746733541423 Train-f1: 0.40500988326576365 Test-f1: 0.3677434231212661 Train-acc: 0.7152317880794702 Test-acc: 0.6578947368421053\n",
      "Epoch [3915/20000] Train-Loss: 0.6781636713745463 Test-Loss: 0.19823486349563973 Train-f1: 0.40724229434578485 Test-f1: 0.39361844303374544 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3916/20000] Train-Loss: 0.6780198345622257 Test-Loss: 0.19807802263961105 Train-f1: 0.4092835469932735 Test-f1: 0.38875457875457875 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3917/20000] Train-Loss: 0.6790638116391352 Test-Loss: 0.19804998251288553 Train-f1: 0.4053266590677815 Test-f1: 0.4102339181286549 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3918/20000] Train-Loss: 0.6772650866804993 Test-Loss: 0.19762815376828938 Train-f1: 0.39858180778134944 Test-f1: 0.41539227895392283 Train-acc: 0.7019867549668874 Test-acc: 0.7236842105263158\n",
      "Epoch [3919/20000] Train-Loss: 0.6788995969082543 Test-Loss: 0.19786131394731346 Train-f1: 0.4084703785057142 Test-f1: 0.4102339181286549 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [3920/20000] Train-Loss: 0.6782761563423071 Test-Loss: 0.20123280310314423 Train-f1: 0.4241289844329133 Test-f1: 0.374380551659654 Train-acc: 0.7317880794701986 Test-acc: 0.6578947368421053\n",
      "Epoch [3921/20000] Train-Loss: 0.6796763102016478 Test-Loss: 0.19785137530540925 Train-f1: 0.4143668740331128 Test-f1: 0.41539227895392283 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [3922/20000] Train-Loss: 0.6759945192417394 Test-Loss: 0.19774557758007663 Train-f1: 0.41735643456467253 Test-f1: 0.3727586206896552 Train-acc: 0.7284768211920529 Test-acc: 0.6710526315789473\n",
      "Epoch [3923/20000] Train-Loss: 0.6792878962523478 Test-Loss: 0.19771571280145878 Train-f1: 0.4147521147874505 Test-f1: 0.39761904761904765 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [3924/20000] Train-Loss: 0.6796532539476489 Test-Loss: 0.1980238989190994 Train-f1: 0.4049556598852373 Test-f1: 0.4163765578329386 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [3925/20000] Train-Loss: 0.6773433838867177 Test-Loss: 0.1975672254068417 Train-f1: 0.4140768588137009 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [3926/20000] Train-Loss: 0.6784057547772511 Test-Loss: 0.1982363578176946 Train-f1: 0.4029904712127654 Test-f1: 0.3853289826938169 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "Epoch [3927/20000] Train-Loss: 0.6788371086432262 Test-Loss: 0.1998637101291256 Train-f1: 0.4044535695930166 Test-f1: 0.3672049689440994 Train-acc: 0.7152317880794702 Test-acc: 0.6578947368421053\n",
      "Epoch [3928/20000] Train-Loss: 0.6776976763136394 Test-Loss: 0.19807026040798162 Train-f1: 0.4108920554362679 Test-f1: 0.3992156862745099 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [3929/20000] Train-Loss: 0.6826891264420679 Test-Loss: 0.1977180552759658 Train-f1: 0.4029408450704225 Test-f1: 0.3880952380952381 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [3930/20000] Train-Loss: 0.675678853293507 Test-Loss: 0.1983940352049646 Train-f1: 0.40881317840925535 Test-f1: 0.39361844303374544 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3931/20000] Train-Loss: 0.6766503379477427 Test-Loss: 0.19806931398514874 Train-f1: 0.4135668279089332 Test-f1: 0.3995495495495495 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3932/20000] Train-Loss: 0.6771456443266171 Test-Loss: 0.1975515894442305 Train-f1: 0.4021393452578991 Test-f1: 0.3944216706148934 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3933/20000] Train-Loss: 0.676695337866198 Test-Loss: 0.19810176998715384 Train-f1: 0.40255723042747443 Test-f1: 0.4055205890822329 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "Epoch [3934/20000] Train-Loss: 0.6803127674095965 Test-Loss: 0.19779828133280866 Train-f1: 0.3994853489118755 Test-f1: 0.3995495495495495 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [3935/20000] Train-Loss: 0.6797067885662169 Test-Loss: 0.1979781667723327 Train-f1: 0.4076336206506995 Test-f1: 0.3995495495495495 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [3936/20000] Train-Loss: 0.6804716527460364 Test-Loss: 0.19833829786636234 Train-f1: 0.4165012951718798 Test-f1: 0.4055205890822329 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3937/20000] Train-Loss: 0.6790652575687588 Test-Loss: 0.1975363673221897 Train-f1: 0.4082424242424242 Test-f1: 0.3995495495495495 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3938/20000] Train-Loss: 0.6757487096054945 Test-Loss: 0.19781827265327778 Train-f1: 0.41794369925356645 Test-f1: 0.3944216706148934 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [3939/20000] Train-Loss: 0.6784075822103656 Test-Loss: 0.19825651069412434 Train-f1: 0.394224535313022 Test-f1: 0.3884795955218491 Train-acc: 0.7019867549668874 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3940/20000] Train-Loss: 0.6790592206040356 Test-Loss: 0.19747163749267702 Train-f1: 0.40537501248377106 Test-f1: 0.38875457875457875 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3941/20000] Train-Loss: 0.680004688611774 Test-Loss: 0.19792470524655437 Train-f1: 0.41186484730344375 Test-f1: 0.3880952380952381 Train-acc: 0.7218543046357616 Test-acc: 0.6842105263157895\n",
      "Epoch [3942/20000] Train-Loss: 0.6764748036846433 Test-Loss: 0.19758324720661088 Train-f1: 0.423518060886482 Test-f1: 0.403812890600341 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3943/20000] Train-Loss: 0.6775845016382976 Test-Loss: 0.197326496972776 Train-f1: 0.4073685048141959 Test-f1: 0.3912626101730912 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3944/20000] Train-Loss: 0.680787438692037 Test-Loss: 0.1974071644622521 Train-f1: 0.3962487321888886 Test-f1: 0.4102339181286549 Train-acc: 0.7086092715231788 Test-acc: 0.7105263157894737\n",
      "Epoch [3945/20000] Train-Loss: 0.6787351511095359 Test-Loss: 0.19739271238045034 Train-f1: 0.3936279982922074 Test-f1: 0.3944216706148934 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [3946/20000] Train-Loss: 0.6766155459435096 Test-Loss: 0.19757045150405977 Train-f1: 0.4140795948191559 Test-f1: 0.3944216706148934 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3947/20000] Train-Loss: 0.6753019509674271 Test-Loss: 0.19961776691533015 Train-f1: 0.4123383561091208 Test-f1: 0.41722031629152373 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [3948/20000] Train-Loss: 0.6780406191827175 Test-Loss: 0.19849786609206133 Train-f1: 0.40742900664765463 Test-f1: 0.4202012072434608 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3949/20000] Train-Loss: 0.6764473801361198 Test-Loss: 0.19776383151820554 Train-f1: 0.42145129615082483 Test-f1: 0.3975433999344907 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [3950/20000] Train-Loss: 0.6754916374982227 Test-Loss: 0.19760146922526134 Train-f1: 0.4159662215239591 Test-f1: 0.4055205890822329 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [3951/20000] Train-Loss: 0.6787742394387747 Test-Loss: 0.1978365192579348 Train-f1: 0.40276014375020175 Test-f1: 0.4055205890822329 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [3952/20000] Train-Loss: 0.6775364975671937 Test-Loss: 0.19747487894435228 Train-f1: 0.40457755643859866 Test-f1: 0.3944216706148934 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3953/20000] Train-Loss: 0.678219526693215 Test-Loss: 0.19793164887861986 Train-f1: 0.41287191024146175 Test-f1: 0.3992867589605914 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3954/20000] Train-Loss: 0.673726727912572 Test-Loss: 0.2015245271961633 Train-f1: 0.41925060748142506 Test-f1: 0.398138329209379 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "Epoch [3955/20000] Train-Loss: 0.6759518315691275 Test-Loss: 0.1994444650644643 Train-f1: 0.42302923976608187 Test-f1: 0.36305366591080873 Train-acc: 0.7317880794701986 Test-acc: 0.6578947368421053\n",
      "Epoch [3956/20000] Train-Loss: 0.6797140787740974 Test-Loss: 0.1975679919408059 Train-f1: 0.40161303153244116 Test-f1: 0.4206959706959708 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3957/20000] Train-Loss: 0.6743784193404297 Test-Loss: 0.19820612974797674 Train-f1: 0.4066635753104788 Test-f1: 0.4055205890822329 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3958/20000] Train-Loss: 0.674582958864401 Test-Loss: 0.19715943568677916 Train-f1: 0.4124611245335591 Test-f1: 0.4163765578329386 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "Epoch [3959/20000] Train-Loss: 0.6772940332717245 Test-Loss: 0.20037682948789404 Train-f1: 0.4147748890298034 Test-f1: 0.398138329209379 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3960/20000] Train-Loss: 0.6770016066967173 Test-Loss: 0.19713179670136807 Train-f1: 0.41466298959256703 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [3961/20000] Train-Loss: 0.6758006621279011 Test-Loss: 0.19957319107121277 Train-f1: 0.40920329773940417 Test-f1: 0.403968253968254 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [3962/20000] Train-Loss: 0.6761313424867755 Test-Loss: 0.19761719967402372 Train-f1: 0.4124147767793467 Test-f1: 0.4102339181286549 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [3963/20000] Train-Loss: 0.6749696590570455 Test-Loss: 0.20005231325480852 Train-f1: 0.41204703832752615 Test-f1: 0.39338345864661656 Train-acc: 0.7218543046357616 Test-acc: 0.6842105263157895\n",
      "Epoch [3964/20000] Train-Loss: 0.6764616770301835 Test-Loss: 0.19718647912378173 Train-f1: 0.4140795948191559 Test-f1: 0.4206959706959708 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [3965/20000] Train-Loss: 0.6797556262232096 Test-Loss: 0.1990073174042765 Train-f1: 0.39954427151795574 Test-f1: 0.37984639016897076 Train-acc: 0.7052980132450332 Test-acc: 0.6710526315789473\n",
      "Epoch [3966/20000] Train-Loss: 0.6756440927851859 Test-Loss: 0.1994605752576769 Train-f1: 0.4025536858086142 Test-f1: 0.39194805194805193 Train-acc: 0.7086092715231788 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3967/20000] Train-Loss: 0.6769283212163159 Test-Loss: 0.19707414261165956 Train-f1: 0.40145316433119993 Test-f1: 0.4055205890822329 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [3968/20000] Train-Loss: 0.6761531976511304 Test-Loss: 0.19728581992597174 Train-f1: 0.41214035087719303 Test-f1: 0.4102339181286549 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [3969/20000] Train-Loss: 0.6789138528592871 Test-Loss: 0.19750734525252464 Train-f1: 0.4208160750575714 Test-f1: 0.40539227895392277 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3970/20000] Train-Loss: 0.6761487948507966 Test-Loss: 0.19701049265320267 Train-f1: 0.40428183186803873 Test-f1: 0.3912626101730912 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3971/20000] Train-Loss: 0.6775845455984622 Test-Loss: 0.1971798925786921 Train-f1: 0.4125518009195307 Test-f1: 0.3912626101730912 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [3972/20000] Train-Loss: 0.6776897112382088 Test-Loss: 0.19739130563562207 Train-f1: 0.40263108706886125 Test-f1: 0.4102339181286549 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [3973/20000] Train-Loss: 0.6754450713184972 Test-Loss: 0.1969624315401676 Train-f1: 0.40356467477648106 Test-f1: 0.4163765578329386 Train-acc: 0.7086092715231788 Test-acc: 0.7236842105263158\n",
      "Epoch [3974/20000] Train-Loss: 0.6760735950283906 Test-Loss: 0.19705705883012375 Train-f1: 0.41533735679481654 Test-f1: 0.4102339181286549 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [3975/20000] Train-Loss: 0.673065353976239 Test-Loss: 0.19862959829905782 Train-f1: 0.4213956716970643 Test-f1: 0.3769047619047619 Train-acc: 0.7317880794701986 Test-acc: 0.6710526315789473\n",
      "Epoch [3976/20000] Train-Loss: 0.6769279220626865 Test-Loss: 0.19734993365111572 Train-f1: 0.40803986599758435 Test-f1: 0.3995495495495495 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [3977/20000] Train-Loss: 0.6745155837990418 Test-Loss: 0.19768185432353413 Train-f1: 0.3993939723699074 Test-f1: 0.4202012072434608 Train-acc: 0.7019867549668874 Test-acc: 0.7236842105263158\n",
      "Epoch [3978/20000] Train-Loss: 0.6769960073438216 Test-Loss: 0.1976743746537821 Train-f1: 0.4107610293647649 Test-f1: 0.4202012072434608 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [3979/20000] Train-Loss: 0.6772493260986598 Test-Loss: 0.19785005694872945 Train-f1: 0.4247176201874188 Test-f1: 0.4098471168893704 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [3980/20000] Train-Loss: 0.6743713200201902 Test-Loss: 0.19736996419697314 Train-f1: 0.42984091763148 Test-f1: 0.40500423562470306 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [3981/20000] Train-Loss: 0.6749691326226002 Test-Loss: 0.19750890414277508 Train-f1: 0.40945118989800877 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [3982/20000] Train-Loss: 0.6735519680084895 Test-Loss: 0.19774914934775314 Train-f1: 0.406085629985735 Test-f1: 0.3884795955218491 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [3983/20000] Train-Loss: 0.6741185195125076 Test-Loss: 0.19683621234850318 Train-f1: 0.4058296710742842 Test-f1: 0.4163765578329386 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "Epoch [3984/20000] Train-Loss: 0.674055386825433 Test-Loss: 0.1971935000130192 Train-f1: 0.4119254110118062 Test-f1: 0.4102339181286549 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [3985/20000] Train-Loss: 0.6737424713577914 Test-Loss: 0.19869837244392313 Train-f1: 0.40697490975971984 Test-f1: 0.41421602787456446 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [3986/20000] Train-Loss: 0.6753016703691094 Test-Loss: 0.1969928263117106 Train-f1: 0.396543690758098 Test-f1: 0.42702535176346623 Train-acc: 0.7052980132450332 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [3987/20000] Train-Loss: 0.6733501177854415 Test-Loss: 0.19680210786879937 Train-f1: 0.4044383616383616 Test-f1: 0.42702535176346623 Train-acc: 0.7152317880794702 Test-acc: 0.7368421052631579\n",
      "Epoch [3988/20000] Train-Loss: 0.6748330917391531 Test-Loss: 0.19732935608117913 Train-f1: 0.40995427335947204 Test-f1: 0.3853289826938169 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [3989/20000] Train-Loss: 0.6730875941051303 Test-Loss: 0.19969775675352752 Train-f1: 0.4060576054753719 Test-f1: 0.37154687962141375 Train-acc: 0.7152317880794702 Test-acc: 0.6578947368421053\n",
      "Epoch [3990/20000] Train-Loss: 0.6769993475072478 Test-Loss: 0.19771506646850523 Train-f1: 0.41104485340928354 Test-f1: 0.3853289826938169 Train-acc: 0.7218543046357616 Test-acc: 0.6842105263157895\n",
      "Epoch [3991/20000] Train-Loss: 0.6757595228148775 Test-Loss: 0.19730924073669864 Train-f1: 0.3991106844750445 Test-f1: 0.3981870467616905 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [3992/20000] Train-Loss: 0.6747218182122964 Test-Loss: 0.19728082472208797 Train-f1: 0.42504122066158595 Test-f1: 0.39974131559497417 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3993/20000] Train-Loss: 0.6726331988562362 Test-Loss: 0.19677903597748672 Train-f1: 0.39404801496570363 Test-f1: 0.3912626101730912 Train-acc: 0.6986754966887417 Test-acc: 0.6973684210526315\n",
      "Epoch [3994/20000] Train-Loss: 0.6739004008305416 Test-Loss: 0.19680629533068875 Train-f1: 0.4040127862027951 Test-f1: 0.40539227895392277 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [3995/20000] Train-Loss: 0.6747449276948995 Test-Loss: 0.19710511977147005 Train-f1: 0.4030745760722878 Test-f1: 0.3944094779711218 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [3996/20000] Train-Loss: 0.6767489321643613 Test-Loss: 0.19769511548471097 Train-f1: 0.41090909090909084 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [3997/20000] Train-Loss: 0.6723517911907859 Test-Loss: 0.2010522839503842 Train-f1: 0.4076513865308432 Test-f1: 0.398138329209379 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [3998/20000] Train-Loss: 0.6750567511729064 Test-Loss: 0.1984226755056956 Train-f1: 0.40123612387730007 Test-f1: 0.4030252100840336 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [3999/20000] Train-Loss: 0.6735607316369875 Test-Loss: 0.19660463276450135 Train-f1: 0.3997962648556876 Test-f1: 0.4163765578329386 Train-acc: 0.7086092715231788 Test-acc: 0.7236842105263158\n",
      "Epoch [4000/20000] Train-Loss: 0.6747894408552113 Test-Loss: 0.19920867867748676 Train-f1: 0.41497922367487583 Test-f1: 0.41421602787456446 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4001/20000] Train-Loss: 0.6722224038484145 Test-Loss: 0.1968266744416291 Train-f1: 0.4171833061554368 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4002/20000] Train-Loss: 0.6744039269085433 Test-Loss: 0.196651373266362 Train-f1: 0.39813561484720406 Test-f1: 0.3944216706148934 Train-acc: 0.7052980132450332 Test-acc: 0.6973684210526315\n",
      "Epoch [4003/20000] Train-Loss: 0.6732900517446828 Test-Loss: 0.19669083736400053 Train-f1: 0.4136933540455786 Test-f1: 0.4102339181286549 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4004/20000] Train-Loss: 0.6752644000170753 Test-Loss: 0.1966104370849493 Train-f1: 0.42030952758402035 Test-f1: 0.4055205890822329 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4005/20000] Train-Loss: 0.6735139876863289 Test-Loss: 0.1966118910180354 Train-f1: 0.4031256414818058 Test-f1: 0.3882337370127722 Train-acc: 0.7086092715231788 Test-acc: 0.6973684210526315\n",
      "Epoch [4006/20000] Train-Loss: 0.6744295341807772 Test-Loss: 0.1985185772397285 Train-f1: 0.4134920149917365 Test-f1: 0.41421602787456446 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4007/20000] Train-Loss: 0.6734470517289723 Test-Loss: 0.19662458284434092 Train-f1: 0.4068488349451548 Test-f1: 0.41587454059472106 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [4008/20000] Train-Loss: 0.6726883606799144 Test-Loss: 0.19876307410296418 Train-f1: 0.42514727487394843 Test-f1: 0.41421602787456446 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4009/20000] Train-Loss: 0.6735383950779763 Test-Loss: 0.19650847714041172 Train-f1: 0.4064868952506237 Test-f1: 0.42702535176346623 Train-acc: 0.7152317880794702 Test-acc: 0.7368421052631579\n",
      "Epoch [4010/20000] Train-Loss: 0.6712630566545751 Test-Loss: 0.1979324899453585 Train-f1: 0.40916057529524474 Test-f1: 0.38847594814978065 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [4011/20000] Train-Loss: 0.6737824853543888 Test-Loss: 0.19790436958796453 Train-f1: 0.41879106464357196 Test-f1: 0.40539227895392277 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4012/20000] Train-Loss: 0.6735020865411934 Test-Loss: 0.1995417875401722 Train-f1: 0.4170114257919136 Test-f1: 0.40828157349896477 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4013/20000] Train-Loss: 0.6726675906419283 Test-Loss: 0.19705716386252956 Train-f1: 0.4218015954123501 Test-f1: 0.39361844303374544 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4014/20000] Train-Loss: 0.6749771845053942 Test-Loss: 0.1965519892203515 Train-f1: 0.4268752712169116 Test-f1: 0.3944216706148934 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4015/20000] Train-Loss: 0.6729864015581786 Test-Loss: 0.19961229631405547 Train-f1: 0.41237265070432894 Test-f1: 0.3863768115942029 Train-acc: 0.7218543046357616 Test-acc: 0.6710526315789473\n",
      "Epoch [4016/20000] Train-Loss: 0.673308885203993 Test-Loss: 0.1966637600978239 Train-f1: 0.40845014774864136 Test-f1: 0.4102339181286549 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4017/20000] Train-Loss: 0.6732442656249438 Test-Loss: 0.1976492573322115 Train-f1: 0.4069303694017653 Test-f1: 0.4098471168893704 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [4018/20000] Train-Loss: 0.6722567011671993 Test-Loss: 0.19998744577688735 Train-f1: 0.4111546699552151 Test-f1: 0.3978919631093544 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4019/20000] Train-Loss: 0.6730190488892882 Test-Loss: 0.19637962128289363 Train-f1: 0.42313894883693515 Test-f1: 0.42702535176346623 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4020/20000] Train-Loss: 0.6725222015487103 Test-Loss: 0.19656238046370889 Train-f1: 0.40818084120608356 Test-f1: 0.3944094779711218 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [4021/20000] Train-Loss: 0.6733005989721383 Test-Loss: 0.19655525620997424 Train-f1: 0.41373531511650546 Test-f1: 0.4163765578329386 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [4022/20000] Train-Loss: 0.6739605774198989 Test-Loss: 0.19642728752289698 Train-f1: 0.41150954729018857 Test-f1: 0.41539227895392283 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "Epoch [4023/20000] Train-Loss: 0.6732166634004934 Test-Loss: 0.197471690736217 Train-f1: 0.4028051731905687 Test-f1: 0.4202012072434608 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [4024/20000] Train-Loss: 0.6744724734317034 Test-Loss: 0.19703239393056649 Train-f1: 0.42307345260198864 Test-f1: 0.4088710166306303 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4025/20000] Train-Loss: 0.6710917007315381 Test-Loss: 0.19740921427969385 Train-f1: 0.4099771552669672 Test-f1: 0.4098471168893704 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4026/20000] Train-Loss: 0.6725758772742023 Test-Loss: 0.196532442414462 Train-f1: 0.40957635118424063 Test-f1: 0.4102339181286549 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [4027/20000] Train-Loss: 0.6721694113418084 Test-Loss: 0.19837274284105538 Train-f1: 0.41230576441102756 Test-f1: 0.403968253968254 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4028/20000] Train-Loss: 0.6716122519984479 Test-Loss: 0.19633011532139827 Train-f1: 0.40553824191340226 Test-f1: 0.42702535176346623 Train-acc: 0.7119205298013245 Test-acc: 0.7368421052631579\n",
      "Epoch [4029/20000] Train-Loss: 0.6738167080132589 Test-Loss: 0.19645194033961264 Train-f1: 0.40585513078470825 Test-f1: 0.39361844303374544 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [4030/20000] Train-Loss: 0.6734722057932082 Test-Loss: 0.19671335311147395 Train-f1: 0.4074960722702278 Test-f1: 0.3947478253682927 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [4031/20000] Train-Loss: 0.6697751156177463 Test-Loss: 0.19857897773267033 Train-f1: 0.4098569274338887 Test-f1: 0.40569520569520573 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4032/20000] Train-Loss: 0.6752724033904146 Test-Loss: 0.19677913599562633 Train-f1: 0.4134452214452214 Test-f1: 0.4098471168893704 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4033/20000] Train-Loss: 0.6723707142064556 Test-Loss: 0.19646302554436101 Train-f1: 0.40975618702043837 Test-f1: 0.4102339181286549 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4034/20000] Train-Loss: 0.6724722076257308 Test-Loss: 0.19651935692248856 Train-f1: 0.42573353550827714 Test-f1: 0.4102339181286549 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4035/20000] Train-Loss: 0.6713594193077878 Test-Loss: 0.1978092042864762 Train-f1: 0.40605081608615184 Test-f1: 0.3944094779711218 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [4036/20000] Train-Loss: 0.6696516739185615 Test-Loss: 0.19792921446710324 Train-f1: 0.4068070175438597 Test-f1: 0.4030252100840336 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [4037/20000] Train-Loss: 0.6716254817401405 Test-Loss: 0.19716785647254167 Train-f1: 0.40808601261403926 Test-f1: 0.3822680765443726 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4038/20000] Train-Loss: 0.6703822755725801 Test-Loss: 0.196256149428127 Train-f1: 0.4078948374030341 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4039/20000] Train-Loss: 0.6748756364140722 Test-Loss: 0.19662283094119137 Train-f1: 0.41297453160167696 Test-f1: 0.4098471168893704 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4040/20000] Train-Loss: 0.6725953154648469 Test-Loss: 0.20093660419014547 Train-f1: 0.4020834903421403 Test-f1: 0.398138329209379 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [4041/20000] Train-Loss: 0.671760040037556 Test-Loss: 0.19646157088229912 Train-f1: 0.42118563322508695 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4042/20000] Train-Loss: 0.6712181656656309 Test-Loss: 0.19616692037213918 Train-f1: 0.41091747119277483 Test-f1: 0.403812890600341 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4043/20000] Train-Loss: 0.6714476573052911 Test-Loss: 0.19621600813179765 Train-f1: 0.4149778672032193 Test-f1: 0.4163765578329386 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4044/20000] Train-Loss: 0.6707331954041109 Test-Loss: 0.1964825797236476 Train-f1: 0.40445507999635133 Test-f1: 0.4102339181286549 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [4045/20000] Train-Loss: 0.6656894717234768 Test-Loss: 0.1987782526997099 Train-f1: 0.40904196413845106 Test-f1: 0.39492929461969706 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4046/20000] Train-Loss: 0.6698837104299731 Test-Loss: 0.1961584311709188 Train-f1: 0.4095065553293401 Test-f1: 0.3944094779711218 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [4047/20000] Train-Loss: 0.6695168468911546 Test-Loss: 0.19642547988973022 Train-f1: 0.419840906614362 Test-f1: 0.3995495495495495 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4048/20000] Train-Loss: 0.6712249356807288 Test-Loss: 0.1962591082922761 Train-f1: 0.4102751862934929 Test-f1: 0.42702535176346623 Train-acc: 0.7185430463576159 Test-acc: 0.7368421052631579\n",
      "Epoch [4049/20000] Train-Loss: 0.6709660108724638 Test-Loss: 0.19868892953811879 Train-f1: 0.424181648859615 Test-f1: 0.3827185427185428 Train-acc: 0.7384105960264901 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [4050/20000] Train-Loss: 0.6719455117618353 Test-Loss: 0.19608549445912282 Train-f1: 0.4253359230475155 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4051/20000] Train-Loss: 0.6702505518211586 Test-Loss: 0.19739770153350375 Train-f1: 0.4249073986878865 Test-f1: 0.36305366591080873 Train-acc: 0.7350993377483444 Test-acc: 0.6578947368421053\n",
      "Epoch [4052/20000] Train-Loss: 0.6697592212157373 Test-Loss: 0.19735653515601728 Train-f1: 0.4043243272760534 Test-f1: 0.4202012072434608 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "Epoch [4053/20000] Train-Loss: 0.6716029481388908 Test-Loss: 0.1964868090971592 Train-f1: 0.4075438596491228 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4054/20000] Train-Loss: 0.6707003557119973 Test-Loss: 0.19707060273412966 Train-f1: 0.4091423586898896 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4055/20000] Train-Loss: 0.6714706541213126 Test-Loss: 0.19642220037873562 Train-f1: 0.41051552767249244 Test-f1: 0.3944216706148934 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [4056/20000] Train-Loss: 0.6707716931080595 Test-Loss: 0.19783923113911267 Train-f1: 0.4159529257927428 Test-f1: 0.4030252100840336 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [4057/20000] Train-Loss: 0.6729013374280719 Test-Loss: 0.19610791188816623 Train-f1: 0.4228735212080409 Test-f1: 0.4094974131559498 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4058/20000] Train-Loss: 0.672968374358535 Test-Loss: 0.19608540310918288 Train-f1: 0.402644866870219 Test-f1: 0.41539227895392283 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "Epoch [4059/20000] Train-Loss: 0.6729325030697508 Test-Loss: 0.19789011280617955 Train-f1: 0.3965103851282713 Test-f1: 0.4030252100840336 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4060/20000] Train-Loss: 0.6716669129033154 Test-Loss: 0.19596744105480957 Train-f1: 0.41397446873183597 Test-f1: 0.41587454059472106 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [4061/20000] Train-Loss: 0.6706903730306244 Test-Loss: 0.1961430118606518 Train-f1: 0.4130533978017333 Test-f1: 0.41587454059472106 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4062/20000] Train-Loss: 0.6683690129208214 Test-Loss: 0.19841160059397164 Train-f1: 0.4167631182827119 Test-f1: 0.403968253968254 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4063/20000] Train-Loss: 0.6712780078047034 Test-Loss: 0.19653290619592376 Train-f1: 0.4204870636636885 Test-f1: 0.4098471168893704 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4064/20000] Train-Loss: 0.6706887821014639 Test-Loss: 0.19589192261555857 Train-f1: 0.406097060833903 Test-f1: 0.4163765578329386 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "Epoch [4065/20000] Train-Loss: 0.6696796336900434 Test-Loss: 0.19638727587528065 Train-f1: 0.41462224465758035 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4066/20000] Train-Loss: 0.6695258115397013 Test-Loss: 0.1975770987086187 Train-f1: 0.4187231074010735 Test-f1: 0.41421602787456446 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4067/20000] Train-Loss: 0.6704469058969884 Test-Loss: 0.19677195127951758 Train-f1: 0.41249366267364945 Test-f1: 0.38847594814978065 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "Epoch [4068/20000] Train-Loss: 0.6706867598264665 Test-Loss: 0.1969473573429451 Train-f1: 0.4134112853219948 Test-f1: 0.4098471168893704 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4069/20000] Train-Loss: 0.6696196884622438 Test-Loss: 0.1963410598967589 Train-f1: 0.4123992422883112 Test-f1: 0.3880952380952381 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "Epoch [4070/20000] Train-Loss: 0.6700462214128603 Test-Loss: 0.19614341835034665 Train-f1: 0.41294938815275417 Test-f1: 0.3981870467616905 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [4071/20000] Train-Loss: 0.6663491209471648 Test-Loss: 0.196186049787337 Train-f1: 0.4255662216337489 Test-f1: 0.41539227895392283 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4072/20000] Train-Loss: 0.6718902793136524 Test-Loss: 0.1964015234249461 Train-f1: 0.4164210383071375 Test-f1: 0.4163765578329386 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4073/20000] Train-Loss: 0.6706613311918247 Test-Loss: 0.19619214418878916 Train-f1: 0.4207094177371352 Test-f1: 0.4088710166306303 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4074/20000] Train-Loss: 0.6704543919445313 Test-Loss: 0.1971830626670435 Train-f1: 0.4193650990600647 Test-f1: 0.36305366591080873 Train-acc: 0.7317880794701986 Test-acc: 0.6578947368421053\n",
      "Epoch [4075/20000] Train-Loss: 0.6692177044970965 Test-Loss: 0.19594020768990214 Train-f1: 0.4081698748641582 Test-f1: 0.4102339181286549 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4076/20000] Train-Loss: 0.6696046887164001 Test-Loss: 0.1960830523541779 Train-f1: 0.3957259755323036 Test-f1: 0.3947478253682927 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [4077/20000] Train-Loss: 0.670065984358987 Test-Loss: 0.19653363244797695 Train-f1: 0.4153356136672919 Test-f1: 0.38877263581488936 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4078/20000] Train-Loss: 0.6722459716082152 Test-Loss: 0.1958030012537919 Train-f1: 0.4174016472483613 Test-f1: 0.3944094779711218 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4079/20000] Train-Loss: 0.6680210478929546 Test-Loss: 0.19770161328235542 Train-f1: 0.4162888223650848 Test-f1: 0.38285714285714284 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4080/20000] Train-Loss: 0.6706874710478195 Test-Loss: 0.19718447348337578 Train-f1: 0.4207378861928146 Test-f1: 0.3769047619047619 Train-acc: 0.7317880794701986 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [4081/20000] Train-Loss: 0.6697814105109323 Test-Loss: 0.1957299612457697 Train-f1: 0.42169691706469825 Test-f1: 0.41539227895392283 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4082/20000] Train-Loss: 0.6693415268168739 Test-Loss: 0.19735009121744276 Train-f1: 0.40994250329366794 Test-f1: 0.4030252100840336 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [4083/20000] Train-Loss: 0.6717384207264073 Test-Loss: 0.19641397203476027 Train-f1: 0.40811065845956207 Test-f1: 0.3882337370127722 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [4084/20000] Train-Loss: 0.6675821047754751 Test-Loss: 0.19719654952627363 Train-f1: 0.40765440877692 Test-f1: 0.4098471168893704 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4085/20000] Train-Loss: 0.6682220950503939 Test-Loss: 0.19729009384429969 Train-f1: 0.42373158756137475 Test-f1: 0.41421602787456446 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4086/20000] Train-Loss: 0.671370124439046 Test-Loss: 0.19723484920947415 Train-f1: 0.4092246156521623 Test-f1: 0.38877263581488936 Train-acc: 0.7218543046357616 Test-acc: 0.6842105263157895\n",
      "Epoch [4087/20000] Train-Loss: 0.6677778756204559 Test-Loss: 0.19582604662998937 Train-f1: 0.42911718678081556 Test-f1: 0.37386274465843106 Train-acc: 0.7384105960264901 Test-acc: 0.6842105263157895\n",
      "Epoch [4088/20000] Train-Loss: 0.6688077421674743 Test-Loss: 0.19585758700923506 Train-f1: 0.4173242235017436 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4089/20000] Train-Loss: 0.6670683130020134 Test-Loss: 0.19576591298122567 Train-f1: 0.41387136265185037 Test-f1: 0.37386274465843106 Train-acc: 0.7218543046357616 Test-acc: 0.6842105263157895\n",
      "Epoch [4090/20000] Train-Loss: 0.672204027731726 Test-Loss: 0.19625788514113787 Train-f1: 0.40167084377610696 Test-f1: 0.4098471168893704 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4091/20000] Train-Loss: 0.6696528670255047 Test-Loss: 0.1956909756565581 Train-f1: 0.4234503093385795 Test-f1: 0.4196428571428571 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4092/20000] Train-Loss: 0.6680810834300701 Test-Loss: 0.19559121355637127 Train-f1: 0.40870804050154225 Test-f1: 0.42702535176346623 Train-acc: 0.7185430463576159 Test-acc: 0.7368421052631579\n",
      "Epoch [4093/20000] Train-Loss: 0.6662381195916027 Test-Loss: 0.19676510351347162 Train-f1: 0.4284817362736898 Test-f1: 0.3944094779711218 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4094/20000] Train-Loss: 0.6691681458087713 Test-Loss: 0.19605280897437002 Train-f1: 0.4174636957245652 Test-f1: 0.38847594814978065 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "Epoch [4095/20000] Train-Loss: 0.6682792822471985 Test-Loss: 0.19675739652818938 Train-f1: 0.4266060882303476 Test-f1: 0.419345725515886 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4096/20000] Train-Loss: 0.6684429572079237 Test-Loss: 0.1975051667016618 Train-f1: 0.41374049087970716 Test-f1: 0.40357595892479614 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [4097/20000] Train-Loss: 0.6657907550211908 Test-Loss: 0.19924613422856344 Train-f1: 0.4111083733566955 Test-f1: 0.35778421900161034 Train-acc: 0.7185430463576159 Test-acc: 0.6447368421052632\n",
      "Epoch [4098/20000] Train-Loss: 0.673520871646706 Test-Loss: 0.19701325475328743 Train-f1: 0.40929851839165865 Test-f1: 0.41421602787456446 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4099/20000] Train-Loss: 0.6682420893923118 Test-Loss: 0.19565259658445378 Train-f1: 0.400190010225346 Test-f1: 0.40921568627450977 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [4100/20000] Train-Loss: 0.6676007813511088 Test-Loss: 0.19576991166234722 Train-f1: 0.411594473981153 Test-f1: 0.39361844303374544 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4101/20000] Train-Loss: 0.6658793407018732 Test-Loss: 0.19623359860568906 Train-f1: 0.40611756180965874 Test-f1: 0.3876470946893482 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [4102/20000] Train-Loss: 0.6654827341232531 Test-Loss: 0.19710036635485012 Train-f1: 0.42212956825597037 Test-f1: 0.3769047619047619 Train-acc: 0.7317880794701986 Test-acc: 0.6710526315789473\n",
      "Epoch [4103/20000] Train-Loss: 0.6687630571925275 Test-Loss: 0.19562016317678882 Train-f1: 0.4082414984542645 Test-f1: 0.4206959706959708 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [4104/20000] Train-Loss: 0.6700923747898777 Test-Loss: 0.19798295069089006 Train-f1: 0.3994347590353121 Test-f1: 0.41421602787456446 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [4105/20000] Train-Loss: 0.6688007610550761 Test-Loss: 0.19641787390917922 Train-f1: 0.4099771552669672 Test-f1: 0.4098471168893704 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4106/20000] Train-Loss: 0.667786595337962 Test-Loss: 0.1963418810291233 Train-f1: 0.414524356296039 Test-f1: 0.4163765578329386 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [4107/20000] Train-Loss: 0.6693549311012134 Test-Loss: 0.1963753964429928 Train-f1: 0.410075595492797 Test-f1: 0.4202012072434608 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4108/20000] Train-Loss: 0.6658048102474 Test-Loss: 0.19551547470390881 Train-f1: 0.40960628997242265 Test-f1: 0.42702535176346623 Train-acc: 0.7185430463576159 Test-acc: 0.7368421052631579\n",
      "Epoch [4109/20000] Train-Loss: 0.667884226048822 Test-Loss: 0.19607038912926864 Train-f1: 0.39988688906721687 Test-f1: 0.3876470946893482 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [4110/20000] Train-Loss: 0.6668147070252891 Test-Loss: 0.19638137349839702 Train-f1: 0.41971152852685306 Test-f1: 0.4055205890822329 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4111/20000] Train-Loss: 0.6713311441337628 Test-Loss: 0.1955035229463862 Train-f1: 0.40795036887994635 Test-f1: 0.3947478253682927 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [4112/20000] Train-Loss: 0.6688902702091905 Test-Loss: 0.19578970455356104 Train-f1: 0.40455089917878945 Test-f1: 0.4098471168893704 Train-acc: 0.7119205298013245 Test-acc: 0.7105263157894737\n",
      "Epoch [4113/20000] Train-Loss: 0.664400657340304 Test-Loss: 0.19654563842383438 Train-f1: 0.4219960690785641 Test-f1: 0.419345725515886 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4114/20000] Train-Loss: 0.6655938027648663 Test-Loss: 0.19580620417765568 Train-f1: 0.41424808151691195 Test-f1: 0.4055205890822329 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4115/20000] Train-Loss: 0.6665955492204314 Test-Loss: 0.1954765708440215 Train-f1: 0.4269386280864042 Test-f1: 0.39974131559497417 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4116/20000] Train-Loss: 0.6667049769592313 Test-Loss: 0.19762980755128143 Train-f1: 0.4257652526839487 Test-f1: 0.3769047619047619 Train-acc: 0.7350993377483444 Test-acc: 0.6710526315789473\n",
      "Epoch [4117/20000] Train-Loss: 0.6665646383144611 Test-Loss: 0.19759025234820107 Train-f1: 0.41379999294009673 Test-f1: 0.38327526132404177 Train-acc: 0.7317880794701986 Test-acc: 0.6710526315789473\n",
      "Epoch [4118/20000] Train-Loss: 0.6652835788281035 Test-Loss: 0.19560618303666616 Train-f1: 0.42358145976520534 Test-f1: 0.4206959706959708 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4119/20000] Train-Loss: 0.6640439649543184 Test-Loss: 0.19755608440511985 Train-f1: 0.4113843163970701 Test-f1: 0.39361844303374544 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4120/20000] Train-Loss: 0.6695519043135094 Test-Loss: 0.19534514569323938 Train-f1: 0.4047569424040011 Test-f1: 0.4163765578329386 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "Epoch [4121/20000] Train-Loss: 0.668458933237346 Test-Loss: 0.1983689196719336 Train-f1: 0.41445791051054204 Test-f1: 0.38285714285714284 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4122/20000] Train-Loss: 0.666611799215334 Test-Loss: 0.19604486645762798 Train-f1: 0.4160216603105741 Test-f1: 0.4088710166306303 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4123/20000] Train-Loss: 0.6671209499055127 Test-Loss: 0.19597001510903236 Train-f1: 0.4210845969985503 Test-f1: 0.3991149190696547 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4124/20000] Train-Loss: 0.668062841086838 Test-Loss: 0.1952922493459927 Train-f1: 0.4136182612918892 Test-f1: 0.4102339181286549 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4125/20000] Train-Loss: 0.6684839245545982 Test-Loss: 0.19546482386293235 Train-f1: 0.4171811740107058 Test-f1: 0.3944094779711218 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4126/20000] Train-Loss: 0.6661755290410458 Test-Loss: 0.19627042889156018 Train-f1: 0.40578678452336436 Test-f1: 0.4163765578329386 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "Epoch [4127/20000] Train-Loss: 0.667270508038749 Test-Loss: 0.19544465446356193 Train-f1: 0.4111450673288129 Test-f1: 0.419345725515886 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "Epoch [4128/20000] Train-Loss: 0.6672076083626103 Test-Loss: 0.19535569242682874 Train-f1: 0.41521421538193726 Test-f1: 0.37386274465843106 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "Epoch [4129/20000] Train-Loss: 0.6661754532653267 Test-Loss: 0.19684304998678512 Train-f1: 0.4167241209494731 Test-f1: 0.36305366591080873 Train-acc: 0.7284768211920529 Test-acc: 0.6578947368421053\n",
      "Epoch [4130/20000] Train-Loss: 0.665160500720009 Test-Loss: 0.19541357230058637 Train-f1: 0.41619507011579665 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4131/20000] Train-Loss: 0.6664023753389139 Test-Loss: 0.19554602442008795 Train-f1: 0.40288927298721805 Test-f1: 0.42702535176346623 Train-acc: 0.7119205298013245 Test-acc: 0.7368421052631579\n",
      "Epoch [4132/20000] Train-Loss: 0.6678240192983402 Test-Loss: 0.19584856123959288 Train-f1: 0.42468649573728917 Test-f1: 0.3991149190696547 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4133/20000] Train-Loss: 0.6657530956300484 Test-Loss: 0.1952268435775325 Train-f1: 0.4092656797535906 Test-f1: 0.4206959706959708 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4134/20000] Train-Loss: 0.6685117826955125 Test-Loss: 0.19513117974030814 Train-f1: 0.4050132091103781 Test-f1: 0.41539227895392283 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "Epoch [4135/20000] Train-Loss: 0.6662985129033104 Test-Loss: 0.1956235582728101 Train-f1: 0.4180405968727136 Test-f1: 0.4202012072434608 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4136/20000] Train-Loss: 0.6651426843203687 Test-Loss: 0.19514528923999133 Train-f1: 0.4249832281264803 Test-f1: 0.4148280156310388 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4137/20000] Train-Loss: 0.6646923512107483 Test-Loss: 0.1976158900398725 Train-f1: 0.412241292276628 Test-f1: 0.41421602787456446 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4138/20000] Train-Loss: 0.6680444253917922 Test-Loss: 0.19686693389941506 Train-f1: 0.41534472917553356 Test-f1: 0.403968253968254 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4139/20000] Train-Loss: 0.6670677657577538 Test-Loss: 0.19639733104306933 Train-f1: 0.42187720618922614 Test-f1: 0.4202012072434608 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4140/20000] Train-Loss: 0.6651210067179077 Test-Loss: 0.19532183413314663 Train-f1: 0.40754929577464793 Test-f1: 0.4102339181286549 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4141/20000] Train-Loss: 0.6684492276712202 Test-Loss: 0.19728384883444713 Train-f1: 0.4093461430836675 Test-f1: 0.41421602787456446 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [4142/20000] Train-Loss: 0.6668474253420449 Test-Loss: 0.19516266101114477 Train-f1: 0.4066030529375725 Test-f1: 0.4206959706959708 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4143/20000] Train-Loss: 0.6679504298907178 Test-Loss: 0.19508778074292776 Train-f1: 0.4159662215239591 Test-f1: 0.4206959706959708 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4144/20000] Train-Loss: 0.6648840410736888 Test-Loss: 0.19582998920258982 Train-f1: 0.400456129236617 Test-f1: 0.4202012072434608 Train-acc: 0.7086092715231788 Test-acc: 0.7236842105263158\n",
      "Epoch [4145/20000] Train-Loss: 0.6663794657366738 Test-Loss: 0.19605469314863236 Train-f1: 0.40182595182595177 Test-f1: 0.3876470946893482 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "Epoch [4146/20000] Train-Loss: 0.6648415616414252 Test-Loss: 0.1981310266164772 Train-f1: 0.41366668014066865 Test-f1: 0.3821524212221886 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4147/20000] Train-Loss: 0.6683922414371012 Test-Loss: 0.1955123238726036 Train-f1: 0.40678897113350987 Test-f1: 0.38877263581488936 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [4148/20000] Train-Loss: 0.6667242112575773 Test-Loss: 0.19574609035892115 Train-f1: 0.4143769994211965 Test-f1: 0.4055205890822329 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4149/20000] Train-Loss: 0.6670468246413087 Test-Loss: 0.19503959982304356 Train-f1: 0.4158569848683274 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4150/20000] Train-Loss: 0.6654765955278685 Test-Loss: 0.1952179342500501 Train-f1: 0.41554132651561615 Test-f1: 0.40381289060034087 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4151/20000] Train-Loss: 0.6647301245593654 Test-Loss: 0.19657104816982587 Train-f1: 0.41789464786945507 Test-f1: 0.3991149190696547 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4152/20000] Train-Loss: 0.6673965015635648 Test-Loss: 0.19522562365320112 Train-f1: 0.40645599570185775 Test-f1: 0.38875457875457875 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [4153/20000] Train-Loss: 0.6655102707461045 Test-Loss: 0.19520999765899522 Train-f1: 0.41793834129396445 Test-f1: 0.4202012072434608 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4154/20000] Train-Loss: 0.6660838803410952 Test-Loss: 0.19735831476457222 Train-f1: 0.42500682500682496 Test-f1: 0.36305366591080873 Train-acc: 0.7350993377483444 Test-acc: 0.6578947368421053\n",
      "Epoch [4155/20000] Train-Loss: 0.6622562146072778 Test-Loss: 0.1960043870884935 Train-f1: 0.41901255639794127 Test-f1: 0.42702535176346623 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4156/20000] Train-Loss: 0.6642916196566248 Test-Loss: 0.19585324857894973 Train-f1: 0.41875389731864476 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4157/20000] Train-Loss: 0.6663815205159008 Test-Loss: 0.19554811924788834 Train-f1: 0.40832178755144916 Test-f1: 0.4163765578329386 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [4158/20000] Train-Loss: 0.6675732423222476 Test-Loss: 0.19956586285337 Train-f1: 0.42372522573152 Test-f1: 0.3878182530356443 Train-acc: 0.7384105960264901 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [4159/20000] Train-Loss: 0.6659051147488003 Test-Loss: 0.1949365287707123 Train-f1: 0.4188778967339748 Test-f1: 0.3944094779711218 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4160/20000] Train-Loss: 0.6636006278049924 Test-Loss: 0.19779892176683264 Train-f1: 0.40868541216053833 Test-f1: 0.413538961038961 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [4161/20000] Train-Loss: 0.6680718892052632 Test-Loss: 0.1952823728221338 Train-f1: 0.42036395336166504 Test-f1: 0.3822680765443726 Train-acc: 0.7317880794701986 Test-acc: 0.6842105263157895\n",
      "Epoch [4162/20000] Train-Loss: 0.666296717093662 Test-Loss: 0.19544124738844615 Train-f1: 0.41835033273150735 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4163/20000] Train-Loss: 0.664805865349913 Test-Loss: 0.19713112968333535 Train-f1: 0.40917153369317705 Test-f1: 0.38285714285714284 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "Epoch [4164/20000] Train-Loss: 0.6654349391952178 Test-Loss: 0.19495249602351647 Train-f1: 0.4136549707602339 Test-f1: 0.4206959706959708 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4165/20000] Train-Loss: 0.666522767003572 Test-Loss: 0.19489622072302343 Train-f1: 0.4221436818171108 Test-f1: 0.38875457875457875 Train-acc: 0.7350993377483444 Test-acc: 0.6842105263157895\n",
      "Epoch [4166/20000] Train-Loss: 0.6622821658006804 Test-Loss: 0.1966324743555042 Train-f1: 0.41114706489178604 Test-f1: 0.3944216706148934 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4167/20000] Train-Loss: 0.6683612395947723 Test-Loss: 0.19489119131570679 Train-f1: 0.415784531751915 Test-f1: 0.4206959706959708 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4168/20000] Train-Loss: 0.6643144419796728 Test-Loss: 0.19523244564737013 Train-f1: 0.40602548625083834 Test-f1: 0.38847594814978065 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [4169/20000] Train-Loss: 0.664639059196357 Test-Loss: 0.19604465815118616 Train-f1: 0.41041639897099963 Test-f1: 0.3944094779711218 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "Epoch [4170/20000] Train-Loss: 0.6644010823582871 Test-Loss: 0.19518449480162586 Train-f1: 0.42488652451842635 Test-f1: 0.39974131559497417 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4171/20000] Train-Loss: 0.6654825649145205 Test-Loss: 0.19502763696948455 Train-f1: 0.41269079580026186 Test-f1: 0.3944094779711218 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [4172/20000] Train-Loss: 0.6643241065116378 Test-Loss: 0.19676881876817312 Train-f1: 0.4218792758336479 Test-f1: 0.41287828450619146 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4173/20000] Train-Loss: 0.6658137492706753 Test-Loss: 0.1951186234103358 Train-f1: 0.42357433560561636 Test-f1: 0.42702535176346623 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4174/20000] Train-Loss: 0.6651845358938598 Test-Loss: 0.19484246593192375 Train-f1: 0.41460811930405966 Test-f1: 0.38875457875457875 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "Epoch [4175/20000] Train-Loss: 0.6646246204850422 Test-Loss: 0.19496330472194714 Train-f1: 0.41395187873983785 Test-f1: 0.4202012072434608 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "Epoch [4176/20000] Train-Loss: 0.6616331545581171 Test-Loss: 0.1948475327125191 Train-f1: 0.42849654561295747 Test-f1: 0.4256306381556967 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4177/20000] Train-Loss: 0.6641391752703625 Test-Loss: 0.1977453864714875 Train-f1: 0.40917153369317705 Test-f1: 0.4030252100840336 Train-acc: 0.7185430463576159 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4178/20000] Train-Loss: 0.6648613589056805 Test-Loss: 0.19479676553588815 Train-f1: 0.41501523943496865 Test-f1: 0.3944094779711218 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4179/20000] Train-Loss: 0.6664860172682588 Test-Loss: 0.1946568489681252 Train-f1: 0.42212476920726416 Test-f1: 0.4272316024211532 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4180/20000] Train-Loss: 0.6657196678629178 Test-Loss: 0.195518395757944 Train-f1: 0.4051305862790153 Test-f1: 0.4202012072434608 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [4181/20000] Train-Loss: 0.6659916819661164 Test-Loss: 0.1949333166938274 Train-f1: 0.4149605022230327 Test-f1: 0.3981870467616905 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4182/20000] Train-Loss: 0.6663328519539184 Test-Loss: 0.1946690017135385 Train-f1: 0.4188312336948837 Test-f1: 0.3944094779711218 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4183/20000] Train-Loss: 0.6643644291819866 Test-Loss: 0.19499050335808082 Train-f1: 0.4131683097452809 Test-f1: 0.40539227895392277 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4184/20000] Train-Loss: 0.6677303347188531 Test-Loss: 0.19500050666333035 Train-f1: 0.41274379406690304 Test-f1: 0.3992867589605914 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [4185/20000] Train-Loss: 0.6626182686937655 Test-Loss: 0.19481139983987342 Train-f1: 0.41143963421291596 Test-f1: 0.39361844303374544 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [4186/20000] Train-Loss: 0.6621089557289567 Test-Loss: 0.19488596763496255 Train-f1: 0.4182891446236642 Test-f1: 0.4202012072434608 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4187/20000] Train-Loss: 0.663087459742754 Test-Loss: 0.19455037057504782 Train-f1: 0.41441489558992617 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4188/20000] Train-Loss: 0.6635314186434198 Test-Loss: 0.1957595470741821 Train-f1: 0.4099824754981502 Test-f1: 0.3884795955218491 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [4189/20000] Train-Loss: 0.6595586925281884 Test-Loss: 0.19720064066175702 Train-f1: 0.4178075437587312 Test-f1: 0.39492929461969706 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4190/20000] Train-Loss: 0.6639094665732932 Test-Loss: 0.19487129092687916 Train-f1: 0.4168058984233567 Test-f1: 0.39361844303374544 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4191/20000] Train-Loss: 0.6642871027354738 Test-Loss: 0.19485041695611202 Train-f1: 0.41947075705096487 Test-f1: 0.39361844303374544 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4192/20000] Train-Loss: 0.6603859987960281 Test-Loss: 0.19519149346416997 Train-f1: 0.398153092894311 Test-f1: 0.3991149190696547 Train-acc: 0.7019867549668874 Test-acc: 0.6973684210526315\n",
      "Epoch [4193/20000] Train-Loss: 0.6623613771866659 Test-Loss: 0.1945966703064515 Train-f1: 0.41564413056500715 Test-f1: 0.4206959706959708 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4194/20000] Train-Loss: 0.6620288310460565 Test-Loss: 0.19476379431902127 Train-f1: 0.417498565498175 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4195/20000] Train-Loss: 0.662267043175882 Test-Loss: 0.1976464121645518 Train-f1: 0.4189780517585396 Test-f1: 0.41421602787456446 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4196/20000] Train-Loss: 0.663002467316445 Test-Loss: 0.19470828404333912 Train-f1: 0.4248954635793759 Test-f1: 0.4256306381556967 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4197/20000] Train-Loss: 0.661434396577335 Test-Loss: 0.19581858708743544 Train-f1: 0.4265107212475634 Test-f1: 0.38877263581488936 Train-acc: 0.7450331125827815 Test-acc: 0.6842105263157895\n",
      "Epoch [4198/20000] Train-Loss: 0.6629263445608622 Test-Loss: 0.19470902098376874 Train-f1: 0.43453309848046684 Test-f1: 0.37386274465843106 Train-acc: 0.7450331125827815 Test-acc: 0.6842105263157895\n",
      "Epoch [4199/20000] Train-Loss: 0.6667325297443436 Test-Loss: 0.19457888339000703 Train-f1: 0.42074879791808567 Test-f1: 0.3944216706148934 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4200/20000] Train-Loss: 0.6631987044960282 Test-Loss: 0.19447232438403422 Train-f1: 0.39754414978749075 Test-f1: 0.4256306381556967 Train-acc: 0.7019867549668874 Test-acc: 0.7368421052631579\n",
      "Epoch [4201/20000] Train-Loss: 0.6630960412253427 Test-Loss: 0.19568475375302516 Train-f1: 0.4182754829172688 Test-f1: 0.4030252100840336 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4202/20000] Train-Loss: 0.6636749386529839 Test-Loss: 0.19448101806409224 Train-f1: 0.40418250160135855 Test-f1: 0.4206959706959708 Train-acc: 0.7119205298013245 Test-acc: 0.7236842105263158\n",
      "Epoch [4203/20000] Train-Loss: 0.6626609115452845 Test-Loss: 0.19451169741223412 Train-f1: 0.4277251042393132 Test-f1: 0.3944216706148934 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4204/20000] Train-Loss: 0.6640540971193579 Test-Loss: 0.19813845733408494 Train-f1: 0.41628424279016674 Test-f1: 0.41421602787456446 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4205/20000] Train-Loss: 0.6641346600435054 Test-Loss: 0.19506134267238978 Train-f1: 0.4124498136795288 Test-f1: 0.3975433999344907 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4206/20000] Train-Loss: 0.6652426132021548 Test-Loss: 0.19442776185281216 Train-f1: 0.42886776502171536 Test-f1: 0.4055205890822329 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4207/20000] Train-Loss: 0.66073024928643 Test-Loss: 0.19590110742875366 Train-f1: 0.4158806695503882 Test-f1: 0.41421602787456446 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4208/20000] Train-Loss: 0.6592336307762049 Test-Loss: 0.19543772438042234 Train-f1: 0.4191174418400177 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4209/20000] Train-Loss: 0.6610325421001598 Test-Loss: 0.19591077888031058 Train-f1: 0.40497155325353285 Test-f1: 0.41421602787456446 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4210/20000] Train-Loss: 0.6617467546854555 Test-Loss: 0.19442564069565915 Train-f1: 0.4072918308379302 Test-f1: 0.42702535176346623 Train-acc: 0.7152317880794702 Test-acc: 0.7368421052631579\n",
      "Epoch [4211/20000] Train-Loss: 0.6625189736237069 Test-Loss: 0.1948819144470671 Train-f1: 0.4178690023644368 Test-f1: 0.4202012072434608 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4212/20000] Train-Loss: 0.6649113114068609 Test-Loss: 0.1947141450566394 Train-f1: 0.4182615361229588 Test-f1: 0.4202012072434608 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4213/20000] Train-Loss: 0.6637050672831751 Test-Loss: 0.19436728348552604 Train-f1: 0.40915644234323467 Test-f1: 0.4256306381556967 Train-acc: 0.7185430463576159 Test-acc: 0.7368421052631579\n",
      "Epoch [4214/20000] Train-Loss: 0.6640425472394142 Test-Loss: 0.19542092808507003 Train-f1: 0.4166769974165584 Test-f1: 0.4188159258581794 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4215/20000] Train-Loss: 0.6608408592848285 Test-Loss: 0.1965837749487551 Train-f1: 0.4164080528367948 Test-f1: 0.3947478253682927 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4216/20000] Train-Loss: 0.665115939331326 Test-Loss: 0.19496259435976768 Train-f1: 0.41650349650349644 Test-f1: 0.4202012072434608 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4217/20000] Train-Loss: 0.6613220554922689 Test-Loss: 0.19450289782227553 Train-f1: 0.40284041375085416 Test-f1: 0.38875457875457875 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [4218/20000] Train-Loss: 0.6625657537782266 Test-Loss: 0.19591744139500353 Train-f1: 0.4075210705815886 Test-f1: 0.38285714285714284 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "Epoch [4219/20000] Train-Loss: 0.660627606055126 Test-Loss: 0.19463398046516714 Train-f1: 0.40975640409092373 Test-f1: 0.42702535176346623 Train-acc: 0.7152317880794702 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4220/20000] Train-Loss: 0.6641447473962658 Test-Loss: 0.19419915185474534 Train-f1: 0.4207785100639157 Test-f1: 0.41539227895392283 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4221/20000] Train-Loss: 0.6618083915472911 Test-Loss: 0.1941902307955083 Train-f1: 0.4221035483636298 Test-f1: 0.42702535176346623 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4222/20000] Train-Loss: 0.664546459782089 Test-Loss: 0.19535404594239014 Train-f1: 0.39787294322763433 Test-f1: 0.4088710166306303 Train-acc: 0.7052980132450332 Test-acc: 0.7105263157894737\n",
      "Epoch [4223/20000] Train-Loss: 0.6622494497599887 Test-Loss: 0.19772017675904235 Train-f1: 0.41675816638194335 Test-f1: 0.4025672877846791 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4224/20000] Train-Loss: 0.6632830564914193 Test-Loss: 0.19418742146851256 Train-f1: 0.41693314868437625 Test-f1: 0.4163765578329386 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4225/20000] Train-Loss: 0.6613981216089607 Test-Loss: 0.19571428709276242 Train-f1: 0.41552097996470466 Test-f1: 0.41421602787456446 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4226/20000] Train-Loss: 0.6630929954931594 Test-Loss: 0.19463293136484813 Train-f1: 0.4098960979655029 Test-f1: 0.3981870467616905 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [4227/20000] Train-Loss: 0.6582671779549335 Test-Loss: 0.2032814306670091 Train-f1: 0.4173993326934503 Test-f1: 0.36573251310093413 Train-acc: 0.7251655629139073 Test-acc: 0.631578947368421\n",
      "Epoch [4228/20000] Train-Loss: 0.6647768061020033 Test-Loss: 0.19462014527654803 Train-f1: 0.4129119686137865 Test-f1: 0.40539227895392277 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4229/20000] Train-Loss: 0.6596576951713031 Test-Loss: 0.19444136082681127 Train-f1: 0.41745947146346885 Test-f1: 0.3947478253682927 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4230/20000] Train-Loss: 0.6623324815630472 Test-Loss: 0.19486482138015798 Train-f1: 0.4061120556595865 Test-f1: 0.3975433999344907 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [4231/20000] Train-Loss: 0.6585645885176017 Test-Loss: 0.19455355486750586 Train-f1: 0.4133569921376073 Test-f1: 0.40381289060034087 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4232/20000] Train-Loss: 0.6627094057618012 Test-Loss: 0.1943094545255074 Train-f1: 0.4119161540214171 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4233/20000] Train-Loss: 0.6626865380901943 Test-Loss: 0.1943439265048208 Train-f1: 0.42449003924170714 Test-f1: 0.42022634339762915 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4234/20000] Train-Loss: 0.6576740438891818 Test-Loss: 0.2025677854093702 Train-f1: 0.4162159624413146 Test-f1: 0.4078964194373401 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4235/20000] Train-Loss: 0.663127626812582 Test-Loss: 0.19490117792398087 Train-f1: 0.40543866231052605 Test-f1: 0.38847594814978065 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [4236/20000] Train-Loss: 0.6613282557000993 Test-Loss: 0.19470143474138407 Train-f1: 0.41843739784243217 Test-f1: 0.4202012072434608 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4237/20000] Train-Loss: 0.6591320335910158 Test-Loss: 0.1945629756804973 Train-f1: 0.4215541043295089 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4238/20000] Train-Loss: 0.6601303892551948 Test-Loss: 0.1964698217808633 Train-f1: 0.42724767491016696 Test-f1: 0.41421602787456446 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4239/20000] Train-Loss: 0.6637612851722213 Test-Loss: 0.19416794628515122 Train-f1: 0.4244229529335913 Test-f1: 0.4202012072434608 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4240/20000] Train-Loss: 0.6615048509043966 Test-Loss: 0.19404223301535745 Train-f1: 0.4082330680351419 Test-f1: 0.4163765578329386 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "Epoch [4241/20000] Train-Loss: 0.6612823156930427 Test-Loss: 0.19455993100567018 Train-f1: 0.4186764064877823 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4242/20000] Train-Loss: 0.6587679892916602 Test-Loss: 0.1940117595667683 Train-f1: 0.4143270735524256 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4243/20000] Train-Loss: 0.6604468452338439 Test-Loss: 0.19421453204935016 Train-f1: 0.40877755159939494 Test-f1: 0.3884795955218491 Train-acc: 0.7185430463576159 Test-acc: 0.6842105263157895\n",
      "Epoch [4244/20000] Train-Loss: 0.6589947641923085 Test-Loss: 0.195304007111512 Train-f1: 0.4155584288588747 Test-f1: 0.3884795955218491 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "Epoch [4245/20000] Train-Loss: 0.660444312490302 Test-Loss: 0.19851568085274027 Train-f1: 0.4054904556989959 Test-f1: 0.403968253968254 Train-acc: 0.7119205298013245 Test-acc: 0.6973684210526315\n",
      "Epoch [4246/20000] Train-Loss: 0.6613387974524171 Test-Loss: 0.1942102106657098 Train-f1: 0.41551016720551887 Test-f1: 0.4202012072434608 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4247/20000] Train-Loss: 0.6591139574299778 Test-Loss: 0.19398073980703093 Train-f1: 0.42195587813962376 Test-f1: 0.4148280156310388 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4248/20000] Train-Loss: 0.657753006855184 Test-Loss: 0.2021113711331566 Train-f1: 0.41634383524002916 Test-f1: 0.40699940582293526 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4249/20000] Train-Loss: 0.6616362929349848 Test-Loss: 0.19574257678352708 Train-f1: 0.4337946876942159 Test-f1: 0.41287828450619146 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4250/20000] Train-Loss: 0.6602656246798966 Test-Loss: 0.19555430700431478 Train-f1: 0.42748887379956724 Test-f1: 0.3483956043956044 Train-acc: 0.7417218543046358 Test-acc: 0.6447368421052632\n",
      "Epoch [4251/20000] Train-Loss: 0.6608390881677861 Test-Loss: 0.19498238801520965 Train-f1: 0.4191210598758251 Test-f1: 0.4202012072434608 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4252/20000] Train-Loss: 0.6599519336219271 Test-Loss: 0.19382051893042063 Train-f1: 0.41499743353234714 Test-f1: 0.42702535176346623 Train-acc: 0.7218543046357616 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4253/20000] Train-Loss: 0.6628636099455877 Test-Loss: 0.19381186773291523 Train-f1: 0.4189723650163428 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4254/20000] Train-Loss: 0.6583300059507707 Test-Loss: 0.19581115751190767 Train-f1: 0.41067097336715275 Test-f1: 0.41287828450619146 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4255/20000] Train-Loss: 0.6583592262535651 Test-Loss: 0.19585316640334424 Train-f1: 0.41038624384342864 Test-f1: 0.41421602787456446 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4256/20000] Train-Loss: 0.6577586864246523 Test-Loss: 0.1940958022657387 Train-f1: 0.4050998867454491 Test-f1: 0.40381289060034087 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4257/20000] Train-Loss: 0.6564255756011731 Test-Loss: 0.19614930424814112 Train-f1: 0.4324479095307284 Test-f1: 0.41421602787456446 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4258/20000] Train-Loss: 0.6615175272326509 Test-Loss: 0.19402241337544754 Train-f1: 0.41564413056500715 Test-f1: 0.38875457875457875 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "Epoch [4259/20000] Train-Loss: 0.6594781403977442 Test-Loss: 0.19566710711514032 Train-f1: 0.41289561978359063 Test-f1: 0.403968253968254 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4260/20000] Train-Loss: 0.6596542018631986 Test-Loss: 0.19379550315490673 Train-f1: 0.4143668740331128 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4261/20000] Train-Loss: 0.6595666422172936 Test-Loss: 0.19375715706542931 Train-f1: 0.41627075705096495 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4262/20000] Train-Loss: 0.6605455744772627 Test-Loss: 0.19395346405725103 Train-f1: 0.4092337734796355 Test-f1: 0.3944094779711218 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "Epoch [4263/20000] Train-Loss: 0.6622843205920128 Test-Loss: 0.19394406219640134 Train-f1: 0.4104637206764867 Test-f1: 0.4055205890822329 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4264/20000] Train-Loss: 0.6571842012597995 Test-Loss: 0.19722811444268276 Train-f1: 0.4263707301278938 Test-f1: 0.41454173067076294 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4265/20000] Train-Loss: 0.6612312396444399 Test-Loss: 0.19540244629742798 Train-f1: 0.4072918308379302 Test-f1: 0.4030252100840336 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4266/20000] Train-Loss: 0.6602010320535097 Test-Loss: 0.19372001788271187 Train-f1: 0.42562030045667765 Test-f1: 0.42702535176346623 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4267/20000] Train-Loss: 0.6577950133254074 Test-Loss: 0.19552425363864526 Train-f1: 0.403509303320394 Test-f1: 0.403968253968254 Train-acc: 0.7152317880794702 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4268/20000] Train-Loss: 0.6594479919482437 Test-Loss: 0.19368891349997347 Train-f1: 0.4124217034398385 Test-f1: 0.42702535176346623 Train-acc: 0.7185430463576159 Test-acc: 0.7368421052631579\n",
      "Epoch [4269/20000] Train-Loss: 0.6595229496157268 Test-Loss: 0.19369034980091254 Train-f1: 0.4145236917132019 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4270/20000] Train-Loss: 0.6622243602013372 Test-Loss: 0.1938490167437589 Train-f1: 0.4227948070681572 Test-f1: 0.39974131559497417 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4271/20000] Train-Loss: 0.6562845820024716 Test-Loss: 0.1937698336011082 Train-f1: 0.42259619702011825 Test-f1: 0.4163765578329386 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4272/20000] Train-Loss: 0.6595022542635363 Test-Loss: 0.19382530031930717 Train-f1: 0.4086008189353841 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4273/20000] Train-Loss: 0.6569786669937691 Test-Loss: 0.19545362169950622 Train-f1: 0.41468003702722755 Test-f1: 0.3944216706148934 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [4274/20000] Train-Loss: 0.6584145072835563 Test-Loss: 0.19424113029093387 Train-f1: 0.41845714811746293 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4275/20000] Train-Loss: 0.6580841618666919 Test-Loss: 0.19835258808562936 Train-f1: 0.422014814757405 Test-f1: 0.4077657004830918 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4276/20000] Train-Loss: 0.6639531630717579 Test-Loss: 0.19380809167492813 Train-f1: 0.4163010146440138 Test-f1: 0.4098471168893704 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4277/20000] Train-Loss: 0.6581756719197214 Test-Loss: 0.194954064653866 Train-f1: 0.4070261450644468 Test-f1: 0.41421602787456446 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4278/20000] Train-Loss: 0.6569397644490476 Test-Loss: 0.1936484754390067 Train-f1: 0.42361057382333983 Test-f1: 0.4102339181286549 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4279/20000] Train-Loss: 0.6578834983652293 Test-Loss: 0.19372624674655656 Train-f1: 0.41021729635573756 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4280/20000] Train-Loss: 0.6619531451226074 Test-Loss: 0.19418424725332248 Train-f1: 0.41937899301360637 Test-f1: 0.4098471168893704 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4281/20000] Train-Loss: 0.6592707044566845 Test-Loss: 0.19377479385713747 Train-f1: 0.41964461164925276 Test-f1: 0.4098471168893704 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4282/20000] Train-Loss: 0.6590340813403898 Test-Loss: 0.19375448625816988 Train-f1: 0.41139947565355695 Test-f1: 0.358634703196347 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "Epoch [4283/20000] Train-Loss: 0.6586502205572227 Test-Loss: 0.19391634564775917 Train-f1: 0.41389430310589564 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4284/20000] Train-Loss: 0.657657683469433 Test-Loss: 0.19575956851852624 Train-f1: 0.4163658510947461 Test-f1: 0.38285714285714284 Train-acc: 0.7350993377483444 Test-acc: 0.6710526315789473\n",
      "Epoch [4285/20000] Train-Loss: 0.6585819081798249 Test-Loss: 0.19446532596371438 Train-f1: 0.4193494253095789 Test-f1: 0.4202012072434608 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4286/20000] Train-Loss: 0.6585102405583628 Test-Loss: 0.19442482886449008 Train-f1: 0.4201886928076982 Test-f1: 0.4202012072434608 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4287/20000] Train-Loss: 0.6568177269215766 Test-Loss: 0.19887167654754478 Train-f1: 0.4262830008752335 Test-f1: 0.3515837104072398 Train-acc: 0.7384105960264901 Test-acc: 0.6447368421052632\n",
      "Epoch [4288/20000] Train-Loss: 0.6574391977970602 Test-Loss: 0.19425440262054766 Train-f1: 0.4142454097626511 Test-f1: 0.3535764034757999 Train-acc: 0.7218543046357616 Test-acc: 0.6578947368421053\n",
      "best score_dx!!\n",
      "Epoch [4289/20000] Train-Loss: 0.6594740865193173 Test-Loss: 0.1934362871490975 Train-f1: 0.42480239745270654 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4290/20000] Train-Loss: 0.6596593367052872 Test-Loss: 0.19341712672610453 Train-f1: 0.41300713674814393 Test-f1: 0.42702535176346623 Train-acc: 0.7218543046357616 Test-acc: 0.7368421052631579\n",
      "Epoch [4291/20000] Train-Loss: 0.6546730230139558 Test-Loss: 0.19728926343652822 Train-f1: 0.4085524655193944 Test-f1: 0.3827185427185428 Train-acc: 0.7185430463576159 Test-acc: 0.6710526315789473\n",
      "Epoch [4292/20000] Train-Loss: 0.6596941283031517 Test-Loss: 0.1935618760364097 Train-f1: 0.4331696672122204 Test-f1: 0.4202012072434608 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4293/20000] Train-Loss: 0.6595364853735116 Test-Loss: 0.19395787444693122 Train-f1: 0.4079074838896929 Test-f1: 0.4098471168893704 Train-acc: 0.7152317880794702 Test-acc: 0.7105263157894737\n",
      "Epoch [4294/20000] Train-Loss: 0.658446775172279 Test-Loss: 0.1935800867118834 Train-f1: 0.426730958410195 Test-f1: 0.4202012072434608 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4295/20000] Train-Loss: 0.6565864186245816 Test-Loss: 0.19388534917884048 Train-f1: 0.4105594817432273 Test-f1: 0.4202012072434608 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4296/20000] Train-Loss: 0.659886186059046 Test-Loss: 0.19359102560641508 Train-f1: 0.42266840882307016 Test-f1: 0.4202012072434608 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4297/20000] Train-Loss: 0.6582500893343195 Test-Loss: 0.19398607334916557 Train-f1: 0.42301729046953457 Test-f1: 0.3947478253682927 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4298/20000] Train-Loss: 0.6604009605178529 Test-Loss: 0.19575671907634673 Train-f1: 0.41266730660077044 Test-f1: 0.41454173067076294 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4299/20000] Train-Loss: 0.6557058767271033 Test-Loss: 0.19359406320203648 Train-f1: 0.4306323569668765 Test-f1: 0.3947478253682927 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [4300/20000] Train-Loss: 0.6559946972791778 Test-Loss: 0.1945355115972286 Train-f1: 0.4208906131399173 Test-f1: 0.40381289060034087 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4301/20000] Train-Loss: 0.6589451881198525 Test-Loss: 0.19562478856797502 Train-f1: 0.42158094723620076 Test-f1: 0.3935014005602241 Train-acc: 0.7350993377483444 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4302/20000] Train-Loss: 0.6581821368763584 Test-Loss: 0.1933271513489867 Train-f1: 0.4197499076222423 Test-f1: 0.3944216706148934 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4303/20000] Train-Loss: 0.6557590830846134 Test-Loss: 0.19333991370464887 Train-f1: 0.4246987717812668 Test-f1: 0.42702535176346623 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4304/20000] Train-Loss: 0.653815037620744 Test-Loss: 0.19552213036185315 Train-f1: 0.4270001537726083 Test-f1: 0.42702535176346623 Train-acc: 0.7384105960264901 Test-acc: 0.7368421052631579\n",
      "Epoch [4305/20000] Train-Loss: 0.6580868701793798 Test-Loss: 0.1938320231210876 Train-f1: 0.4251683433936956 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4306/20000] Train-Loss: 0.6561390983705713 Test-Loss: 0.19432628296413296 Train-f1: 0.4168496250246303 Test-f1: 0.3947478253682927 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4307/20000] Train-Loss: 0.6583387512366415 Test-Loss: 0.19513925298208812 Train-f1: 0.4237440839368193 Test-f1: 0.41287828450619146 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4308/20000] Train-Loss: 0.658317373631324 Test-Loss: 0.19434582363970385 Train-f1: 0.4206757219089911 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4309/20000] Train-Loss: 0.6597902486486639 Test-Loss: 0.1932142526679512 Train-f1: 0.40417162145527924 Test-f1: 0.4256306381556967 Train-acc: 0.7086092715231788 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4310/20000] Train-Loss: 0.6577326855610971 Test-Loss: 0.1931769628665312 Train-f1: 0.4194996752166542 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4311/20000] Train-Loss: 0.6570454590714665 Test-Loss: 0.19325871143762796 Train-f1: 0.4185193323501368 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4312/20000] Train-Loss: 0.6525495894366405 Test-Loss: 0.19382639327097387 Train-f1: 0.4175231860338243 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4313/20000] Train-Loss: 0.6576121279595007 Test-Loss: 0.19354400275128752 Train-f1: 0.4122639162732483 Test-f1: 0.3884795955218491 Train-acc: 0.7218543046357616 Test-acc: 0.6842105263157895\n",
      "Epoch [4314/20000] Train-Loss: 0.6587943118880849 Test-Loss: 0.19374185723275836 Train-f1: 0.41660547134605813 Test-f1: 0.4202012072434608 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4315/20000] Train-Loss: 0.6571952432179894 Test-Loss: 0.19352420187414204 Train-f1: 0.415303071486817 Test-f1: 0.4098471168893704 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4316/20000] Train-Loss: 0.6567090372986987 Test-Loss: 0.19329621093887686 Train-f1: 0.41811238406983087 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4317/20000] Train-Loss: 0.6560568030403573 Test-Loss: 0.19756277948474082 Train-f1: 0.41373836282037013 Test-f1: 0.3828045436741089 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4318/20000] Train-Loss: 0.6621179411920376 Test-Loss: 0.19389114694928447 Train-f1: 0.42130438866240716 Test-f1: 0.4188159258581794 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4319/20000] Train-Loss: 0.658265387740598 Test-Loss: 0.1933353303515052 Train-f1: 0.41880026640026635 Test-f1: 0.3884795955218491 Train-acc: 0.7317880794701986 Test-acc: 0.6842105263157895\n",
      "Epoch [4320/20000] Train-Loss: 0.6538931254786916 Test-Loss: 0.19356121376464078 Train-f1: 0.41663569257490235 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4321/20000] Train-Loss: 0.657281970694904 Test-Loss: 0.19308784318825195 Train-f1: 0.4161055004006641 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4322/20000] Train-Loss: 0.6563901718778354 Test-Loss: 0.19461725634570665 Train-f1: 0.4158532742916237 Test-f1: 0.41421602787456446 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4323/20000] Train-Loss: 0.6548490484520463 Test-Loss: 0.1933912918261173 Train-f1: 0.42523856016446426 Test-f1: 0.4272316024211532 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4324/20000] Train-Loss: 0.6566757092927372 Test-Loss: 0.19707354068035451 Train-f1: 0.40823412069329945 Test-f1: 0.41287828450619146 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4325/20000] Train-Loss: 0.6575779758251392 Test-Loss: 0.19508696446060875 Train-f1: 0.4152312902876316 Test-f1: 0.3827185427185428 Train-acc: 0.7317880794701986 Test-acc: 0.6710526315789473\n",
      "Epoch [4326/20000] Train-Loss: 0.6594732262566948 Test-Loss: 0.19336959882786875 Train-f1: 0.42170837342116396 Test-f1: 0.40539227895392277 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4327/20000] Train-Loss: 0.6551473483956881 Test-Loss: 0.19313908407101096 Train-f1: 0.4252555954683615 Test-f1: 0.4202012072434608 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4328/20000] Train-Loss: 0.6573146651271647 Test-Loss: 0.19374574010355972 Train-f1: 0.4208815858111633 Test-f1: 0.3884795955218491 Train-acc: 0.7317880794701986 Test-acc: 0.6842105263157895\n",
      "Epoch [4329/20000] Train-Loss: 0.653636198937437 Test-Loss: 0.1956342317720697 Train-f1: 0.411554995404172 Test-f1: 0.3827185427185428 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [4330/20000] Train-Loss: 0.6567307016506667 Test-Loss: 0.19299780981436168 Train-f1: 0.41817901061803087 Test-f1: 0.4265359477124183 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4331/20000] Train-Loss: 0.6560069793871356 Test-Loss: 0.19297481384651002 Train-f1: 0.4098818260655716 Test-f1: 0.4256306381556967 Train-acc: 0.7185430463576159 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4332/20000] Train-Loss: 0.6571672527368269 Test-Loss: 0.19291906986626323 Train-f1: 0.41478096352286337 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4333/20000] Train-Loss: 0.659306959097398 Test-Loss: 0.1929011913545782 Train-f1: 0.4078451570109275 Test-f1: 0.42702535176346623 Train-acc: 0.7152317880794702 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4334/20000] Train-Loss: 0.6554403982233543 Test-Loss: 0.19289318576798678 Train-f1: 0.4180059905862216 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4335/20000] Train-Loss: 0.6557142111257805 Test-Loss: 0.19282073875710729 Train-f1: 0.41448391705602966 Test-f1: 0.42702535176346623 Train-acc: 0.7218543046357616 Test-acc: 0.7368421052631579\n",
      "Epoch [4336/20000] Train-Loss: 0.6553979607521522 Test-Loss: 0.19372995908666416 Train-f1: 0.42142385900426477 Test-f1: 0.42702535176346623 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4337/20000] Train-Loss: 0.6551340180393228 Test-Loss: 0.19290402638464205 Train-f1: 0.41070669631826046 Test-f1: 0.4272316024211532 Train-acc: 0.7185430463576159 Test-acc: 0.7368421052631579\n",
      "Epoch [4338/20000] Train-Loss: 0.6554601888553092 Test-Loss: 0.19312336860173945 Train-f1: 0.4245633131608518 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4339/20000] Train-Loss: 0.6569394486255853 Test-Loss: 0.19437827604129512 Train-f1: 0.41311904657999465 Test-f1: 0.38285714285714284 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4340/20000] Train-Loss: 0.655673297277041 Test-Loss: 0.19288024701374643 Train-f1: 0.42100412127305126 Test-f1: 0.3944216706148934 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4341/20000] Train-Loss: 0.6571763564180573 Test-Loss: 0.19276729914160465 Train-f1: 0.41763509794398573 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4342/20000] Train-Loss: 0.6563229905827792 Test-Loss: 0.19369706247972235 Train-f1: 0.4096224431858785 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4343/20000] Train-Loss: 0.6543795821762294 Test-Loss: 0.19419931143224128 Train-f1: 0.4227642668015312 Test-f1: 0.403968253968254 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4344/20000] Train-Loss: 0.6552483350147026 Test-Loss: 0.1976195774749704 Train-f1: 0.4197133976360573 Test-f1: 0.40361535448491975 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4345/20000] Train-Loss: 0.6544107992494743 Test-Loss: 0.19542722578820845 Train-f1: 0.41249475162518634 Test-f1: 0.41421602787456446 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4346/20000] Train-Loss: 0.653840642960514 Test-Loss: 0.19328648619800176 Train-f1: 0.42310850135107037 Test-f1: 0.4202012072434608 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4347/20000] Train-Loss: 0.6540560883125766 Test-Loss: 0.19309056100533617 Train-f1: 0.4187659307637809 Test-f1: 0.40381289060034087 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4348/20000] Train-Loss: 0.653322351801843 Test-Loss: 0.19414714419133794 Train-f1: 0.4102873156663221 Test-f1: 0.40381289060034087 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4349/20000] Train-Loss: 0.6578052763228864 Test-Loss: 0.19320522947874644 Train-f1: 0.4139619528548536 Test-f1: 0.40381289060034087 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4350/20000] Train-Loss: 0.6554896324829494 Test-Loss: 0.19455809190060958 Train-f1: 0.4305650413659567 Test-f1: 0.403968253968254 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4351/20000] Train-Loss: 0.6549126668473997 Test-Loss: 0.19276159976465207 Train-f1: 0.4254933151444332 Test-f1: 0.4102339181286549 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4352/20000] Train-Loss: 0.6534555015395904 Test-Loss: 0.19267382689219342 Train-f1: 0.43305406373075545 Test-f1: 0.4256306381556967 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4353/20000] Train-Loss: 0.6545761297652856 Test-Loss: 0.1940681503585348 Train-f1: 0.41698274287043036 Test-f1: 0.3827185427185428 Train-acc: 0.7317880794701986 Test-acc: 0.6710526315789473\n",
      "Epoch [4354/20000] Train-Loss: 0.6514752387354922 Test-Loss: 0.1933704042188552 Train-f1: 0.418996810522094 Test-f1: 0.3947478253682927 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4355/20000] Train-Loss: 0.656062641255569 Test-Loss: 0.19771038451520392 Train-f1: 0.41479258919337997 Test-f1: 0.3828045436741089 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4356/20000] Train-Loss: 0.6557752486651596 Test-Loss: 0.19548729067015014 Train-f1: 0.42279262493574776 Test-f1: 0.41287828450619146 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4357/20000] Train-Loss: 0.6541396740762114 Test-Loss: 0.193474021273606 Train-f1: 0.4168496250246303 Test-f1: 0.3884795955218491 Train-acc: 0.7317880794701986 Test-acc: 0.6842105263157895\n",
      "Epoch [4358/20000] Train-Loss: 0.6534696095507747 Test-Loss: 0.19335583188920621 Train-f1: 0.4092337734796355 Test-f1: 0.42022634339762915 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [4359/20000] Train-Loss: 0.656025142242685 Test-Loss: 0.1932747204762817 Train-f1: 0.415307730667733 Test-f1: 0.4163765578329386 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4360/20000] Train-Loss: 0.6536044077312633 Test-Loss: 0.19438962186519762 Train-f1: 0.42212032143066625 Test-f1: 0.41287828450619146 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4361/20000] Train-Loss: 0.6526844990802642 Test-Loss: 0.19514056634590746 Train-f1: 0.41511533611771273 Test-f1: 0.3944216706148934 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4362/20000] Train-Loss: 0.6507690778315631 Test-Loss: 0.19347895891764585 Train-f1: 0.4212229808682765 Test-f1: 0.4202012072434608 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4363/20000] Train-Loss: 0.6526464354854156 Test-Loss: 0.19257144362964043 Train-f1: 0.41225517812758905 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4364/20000] Train-Loss: 0.6534962479577404 Test-Loss: 0.19338196189797108 Train-f1: 0.4216767798391398 Test-f1: 0.3991149190696547 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4365/20000] Train-Loss: 0.6523547026322477 Test-Loss: 0.19963485915466578 Train-f1: 0.4315438081631077 Test-f1: 0.40641711229946526 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4366/20000] Train-Loss: 0.6567977889404826 Test-Loss: 0.19354181884400262 Train-f1: 0.41608848773145296 Test-f1: 0.4202012072434608 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4367/20000] Train-Loss: 0.6526060849355528 Test-Loss: 0.19325787435679098 Train-f1: 0.4250055617352614 Test-f1: 0.4256306381556967 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4368/20000] Train-Loss: 0.6567294327302667 Test-Loss: 0.19249505809062123 Train-f1: 0.41905541670257695 Test-f1: 0.42702535176346623 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4369/20000] Train-Loss: 0.6555286184893468 Test-Loss: 0.19342983157900492 Train-f1: 0.43018934422467997 Test-f1: 0.38877263581488936 Train-acc: 0.7450331125827815 Test-acc: 0.6842105263157895\n",
      "Epoch [4370/20000] Train-Loss: 0.6518523931536347 Test-Loss: 0.19388324824652858 Train-f1: 0.41509758244033745 Test-f1: 0.40378576378576375 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [4371/20000] Train-Loss: 0.6542558943878166 Test-Loss: 0.19336174744524498 Train-f1: 0.4195536844186232 Test-f1: 0.40381289060034087 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4372/20000] Train-Loss: 0.6535465410351191 Test-Loss: 0.193563656455958 Train-f1: 0.4289573261899734 Test-f1: 0.40381289060034087 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4373/20000] Train-Loss: 0.6425829726614305 Test-Loss: 0.20437230272087575 Train-f1: 0.4150011638619233 Test-f1: 0.3953113553113553 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4374/20000] Train-Loss: 0.6578838055406745 Test-Loss: 0.19651929407670551 Train-f1: 0.4159888916907096 Test-f1: 0.403968253968254 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4375/20000] Train-Loss: 0.654937514435484 Test-Loss: 0.19262088539417518 Train-f1: 0.42639751552795035 Test-f1: 0.40193236714975844 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4376/20000] Train-Loss: 0.652032104362514 Test-Loss: 0.1929442654235807 Train-f1: 0.42311436322603485 Test-f1: 0.4202012072434608 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4377/20000] Train-Loss: 0.6547594407951375 Test-Loss: 0.1926565881940867 Train-f1: 0.42166654885605903 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4378/20000] Train-Loss: 0.654641833947551 Test-Loss: 0.19580051278600627 Train-f1: 0.40272564519057674 Test-f1: 0.39338345864661656 Train-acc: 0.7119205298013245 Test-acc: 0.6842105263157895\n",
      "Epoch [4379/20000] Train-Loss: 0.6532097983438383 Test-Loss: 0.1925897601421962 Train-f1: 0.4127769736580805 Test-f1: 0.3947478253682927 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [4380/20000] Train-Loss: 0.6516224922865387 Test-Loss: 0.19906818957006495 Train-f1: 0.4198697302958889 Test-f1: 0.39777812245531036 Train-acc: 0.7317880794701986 Test-acc: 0.6842105263157895\n",
      "Epoch [4381/20000] Train-Loss: 0.6517818600142047 Test-Loss: 0.19960176749936118 Train-f1: 0.4176419891407395 Test-f1: 0.407749766573296 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4382/20000] Train-Loss: 0.6541168380653003 Test-Loss: 0.19238931848275853 Train-f1: 0.41837963129064965 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4383/20000] Train-Loss: 0.6498674152158956 Test-Loss: 0.1937065156318295 Train-f1: 0.4211956178887538 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4384/20000] Train-Loss: 0.6542489765465062 Test-Loss: 0.1951397497807949 Train-f1: 0.4172183767592679 Test-f1: 0.41287828450619146 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4385/20000] Train-Loss: 0.655118093545386 Test-Loss: 0.192843232051841 Train-f1: 0.41150284274075377 Test-f1: 0.4098471168893704 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4386/20000] Train-Loss: 0.6535458457548858 Test-Loss: 0.19258342384390018 Train-f1: 0.42401005078203163 Test-f1: 0.4098471168893704 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4387/20000] Train-Loss: 0.6542470904593906 Test-Loss: 0.19236902066861522 Train-f1: 0.42391304347826086 Test-f1: 0.4265359477124183 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4388/20000] Train-Loss: 0.6533188329253372 Test-Loss: 0.1923613875947779 Train-f1: 0.4181352732486894 Test-f1: 0.4163765578329386 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "Epoch [4389/20000] Train-Loss: 0.6535187900295666 Test-Loss: 0.19434323744837342 Train-f1: 0.4191760819892739 Test-f1: 0.41421602787456446 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4390/20000] Train-Loss: 0.654252764882561 Test-Loss: 0.19234550802067957 Train-f1: 0.4221137474517219 Test-f1: 0.4202012072434608 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4391/20000] Train-Loss: 0.652289903524583 Test-Loss: 0.19323010006902047 Train-f1: 0.40809475817163354 Test-f1: 0.4098471168893704 Train-acc: 0.7185430463576159 Test-acc: 0.7105263157894737\n",
      "Epoch [4392/20000] Train-Loss: 0.6527317828741489 Test-Loss: 0.1928150389851323 Train-f1: 0.41705327147656385 Test-f1: 0.4098471168893704 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4393/20000] Train-Loss: 0.6530616343851247 Test-Loss: 0.19233942247592517 Train-f1: 0.41385706492089475 Test-f1: 0.42519841269841263 Train-acc: 0.7218543046357616 Test-acc: 0.7368421052631579\n",
      "Epoch [4394/20000] Train-Loss: 0.6480590258089624 Test-Loss: 0.19470612000602186 Train-f1: 0.42199270880886697 Test-f1: 0.42702535176346623 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4395/20000] Train-Loss: 0.6543332548441662 Test-Loss: 0.19281355462474267 Train-f1: 0.41479003651538865 Test-f1: 0.42702535176346623 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4396/20000] Train-Loss: 0.65432393667237 Test-Loss: 0.1931640040130466 Train-f1: 0.4153775672407146 Test-f1: 0.4202012072434608 Train-acc: 0.7284768211920529 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4397/20000] Train-Loss: 0.6516781722646617 Test-Loss: 0.19223021833064874 Train-f1: 0.42159885075322256 Test-f1: 0.42519841269841263 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4398/20000] Train-Loss: 0.6505869238807253 Test-Loss: 0.19378237417904265 Train-f1: 0.42176510180043747 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4399/20000] Train-Loss: 0.652170058078665 Test-Loss: 0.19229396706532384 Train-f1: 0.4309324927764644 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4400/20000] Train-Loss: 0.6536711255545453 Test-Loss: 0.1942108097404035 Train-f1: 0.42475353507797636 Test-f1: 0.4025672877846791 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4401/20000] Train-Loss: 0.6546107893801718 Test-Loss: 0.1922616131802441 Train-f1: 0.42562030045667765 Test-f1: 0.4163765578329386 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4402/20000] Train-Loss: 0.6539633093109648 Test-Loss: 0.19273013870034011 Train-f1: 0.40984524334226535 Test-f1: 0.42022634339762915 Train-acc: 0.7185430463576159 Test-acc: 0.7236842105263158\n",
      "Epoch [4403/20000] Train-Loss: 0.6526268745180999 Test-Loss: 0.1924576232900032 Train-f1: 0.4245614035087719 Test-f1: 0.38877263581488936 Train-acc: 0.7417218543046358 Test-acc: 0.6842105263157895\n",
      "Epoch [4404/20000] Train-Loss: 0.6533008714969045 Test-Loss: 0.19243544913985536 Train-f1: 0.4142486822777154 Test-f1: 0.3944216706148934 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [4405/20000] Train-Loss: 0.6510126036562391 Test-Loss: 0.197781869533161 Train-f1: 0.42399169208801196 Test-f1: 0.41369152821963 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4406/20000] Train-Loss: 0.6550769720375559 Test-Loss: 0.192857155599364 Train-f1: 0.4198870720856466 Test-f1: 0.3975433999344907 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4407/20000] Train-Loss: 0.6504701031595692 Test-Loss: 0.192257049340105 Train-f1: 0.4167916489683489 Test-f1: 0.437513491075135 Train-acc: 0.7251655629139073 Test-acc: 0.75\n",
      "Epoch [4408/20000] Train-Loss: 0.6550574668565105 Test-Loss: 0.19229342771832425 Train-f1: 0.4290989270849145 Test-f1: 0.42702535176346623 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4409/20000] Train-Loss: 0.6526637083414071 Test-Loss: 0.19384611114822514 Train-f1: 0.4259649000433373 Test-f1: 0.41421602787456446 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4410/20000] Train-Loss: 0.6546506607366644 Test-Loss: 0.1934186610760038 Train-f1: 0.42815324752388195 Test-f1: 0.403968253968254 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4411/20000] Train-Loss: 0.652416809980855 Test-Loss: 0.19221352830729396 Train-f1: 0.42504152447287885 Test-f1: 0.4256306381556967 Train-acc: 0.7384105960264901 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4412/20000] Train-Loss: 0.6519629271151269 Test-Loss: 0.19215540329539657 Train-f1: 0.420997674910167 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4413/20000] Train-Loss: 0.6502976600781478 Test-Loss: 0.19220531376256136 Train-f1: 0.43448838324089933 Test-f1: 0.4256306381556967 Train-acc: 0.7483443708609272 Test-acc: 0.7368421052631579\n",
      "Epoch [4414/20000] Train-Loss: 0.6521255311672971 Test-Loss: 0.19224065430412068 Train-f1: 0.4184810099976314 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4415/20000] Train-Loss: 0.6509909562894672 Test-Loss: 0.1938444009880495 Train-f1: 0.423482453958777 Test-f1: 0.403968253968254 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4416/20000] Train-Loss: 0.6496298488491845 Test-Loss: 0.19678751057892002 Train-f1: 0.42351550549297734 Test-f1: 0.4236024844720497 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4417/20000] Train-Loss: 0.6505647126976378 Test-Loss: 0.19196500589070417 Train-f1: 0.42612197504345906 Test-f1: 0.4163765578329386 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4418/20000] Train-Loss: 0.6500729373246843 Test-Loss: 0.19198256288708468 Train-f1: 0.41626267809025935 Test-f1: 0.4265359477124183 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4419/20000] Train-Loss: 0.6527513659574662 Test-Loss: 0.19311877020048165 Train-f1: 0.4233351518457901 Test-f1: 0.3975433999344907 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4420/20000] Train-Loss: 0.6513198000527946 Test-Loss: 0.19422186046374967 Train-f1: 0.4262154835177451 Test-f1: 0.41287828450619146 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4421/20000] Train-Loss: 0.6514535865093692 Test-Loss: 0.19241002286555728 Train-f1: 0.41878129261209696 Test-f1: 0.4256306381556967 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4422/20000] Train-Loss: 0.6532284196844866 Test-Loss: 0.19200279587213312 Train-f1: 0.42755504715342685 Test-f1: 0.4018569254185693 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4423/20000] Train-Loss: 0.6518357539351652 Test-Loss: 0.19224324507390306 Train-f1: 0.4205266357503309 Test-f1: 0.4098471168893704 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4424/20000] Train-Loss: 0.6487003112344282 Test-Loss: 0.1929174986774393 Train-f1: 0.4374249863611565 Test-f1: 0.42702535176346623 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [4425/20000] Train-Loss: 0.6514943585112023 Test-Loss: 0.19232674071925548 Train-f1: 0.42336106048053024 Test-f1: 0.4098471168893704 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4426/20000] Train-Loss: 0.6501322865053818 Test-Loss: 0.19271008958310296 Train-f1: 0.41269621089176234 Test-f1: 0.4055205890822329 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4427/20000] Train-Loss: 0.6509682087869874 Test-Loss: 0.19381899712450973 Train-f1: 0.4260496778216586 Test-f1: 0.403968253968254 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4428/20000] Train-Loss: 0.6518668250670372 Test-Loss: 0.19183431571706455 Train-f1: 0.42255705242318947 Test-f1: 0.42702535176346623 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4429/20000] Train-Loss: 0.6496079715588873 Test-Loss: 0.19223301434771758 Train-f1: 0.4224851931055552 Test-f1: 0.38877263581488936 Train-acc: 0.7350993377483444 Test-acc: 0.6842105263157895\n",
      "Epoch [4430/20000] Train-Loss: 0.6515788396858633 Test-Loss: 0.19193338698721835 Train-f1: 0.4165686383381357 Test-f1: 0.4098130747500651 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4431/20000] Train-Loss: 0.6509859691651575 Test-Loss: 0.1943656867916867 Train-f1: 0.41388285964023963 Test-f1: 0.39194805194805193 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "Epoch [4432/20000] Train-Loss: 0.6516157548899577 Test-Loss: 0.19325822733951603 Train-f1: 0.4262617275962472 Test-f1: 0.4243137254901961 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4433/20000] Train-Loss: 0.6489216063033867 Test-Loss: 0.19180354060980517 Train-f1: 0.4259422679394381 Test-f1: 0.4163765578329386 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4434/20000] Train-Loss: 0.6498550342129783 Test-Loss: 0.19411714151138712 Train-f1: 0.4347378667779168 Test-f1: 0.4163765578329386 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4435/20000] Train-Loss: 0.6530707773021708 Test-Loss: 0.1920520389195444 Train-f1: 0.42809066889167013 Test-f1: 0.40193236714975844 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4436/20000] Train-Loss: 0.6484026522516306 Test-Loss: 0.19300738654962668 Train-f1: 0.40906511927788525 Test-f1: 0.4256306381556967 Train-acc: 0.7185430463576159 Test-acc: 0.7368421052631579\n",
      "Epoch [4437/20000] Train-Loss: 0.6511290889024706 Test-Loss: 0.19203377670377528 Train-f1: 0.4211150372987829 Test-f1: 0.4163765578329386 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4438/20000] Train-Loss: 0.6521176346485484 Test-Loss: 0.19562529651012084 Train-f1: 0.4228070175438596 Test-f1: 0.40361535448491975 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4439/20000] Train-Loss: 0.6507488192791224 Test-Loss: 0.19186260218523887 Train-f1: 0.4216885279828132 Test-f1: 0.3947478253682927 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4440/20000] Train-Loss: 0.6518015362573214 Test-Loss: 0.19174044874745955 Train-f1: 0.41459000471953844 Test-f1: 0.4265359477124183 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4441/20000] Train-Loss: 0.6510517594458692 Test-Loss: 0.19326096941446289 Train-f1: 0.4271185742521634 Test-f1: 0.3975433999344907 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4442/20000] Train-Loss: 0.6516409338202481 Test-Loss: 0.192494360568964 Train-f1: 0.41740409724520944 Test-f1: 0.3884795955218491 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4443/20000] Train-Loss: 0.6508262552013755 Test-Loss: 0.1916645872339124 Train-f1: 0.4270349469887759 Test-f1: 0.4256306381556967 Train-acc: 0.7384105960264901 Test-acc: 0.7368421052631579\n",
      "Epoch [4444/20000] Train-Loss: 0.6475846107819915 Test-Loss: 0.1917091785571329 Train-f1: 0.4283689025002107 Test-f1: 0.41633139989304374 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4445/20000] Train-Loss: 0.6510562612055983 Test-Loss: 0.19162280568619136 Train-f1: 0.42016732488048214 Test-f1: 0.4163765578329386 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4446/20000] Train-Loss: 0.650888797318261 Test-Loss: 0.1926340099959509 Train-f1: 0.41834788286599134 Test-f1: 0.4098471168893704 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4447/20000] Train-Loss: 0.6500989604539018 Test-Loss: 0.1916291208135864 Train-f1: 0.4297343249712979 Test-f1: 0.4256306381556967 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4448/20000] Train-Loss: 0.65335878939241 Test-Loss: 0.1920432148290984 Train-f1: 0.423078873654725 Test-f1: 0.4098130747500651 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4449/20000] Train-Loss: 0.6474690838160004 Test-Loss: 0.1916640981892782 Train-f1: 0.43124817130076093 Test-f1: 0.4272316024211532 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4450/20000] Train-Loss: 0.6508944657695872 Test-Loss: 0.19193660985129127 Train-f1: 0.4169547540774536 Test-f1: 0.4098471168893704 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4451/20000] Train-Loss: 0.6495708172912195 Test-Loss: 0.19412343862028344 Train-f1: 0.41620907731601886 Test-f1: 0.403968253968254 Train-acc: 0.7251655629139073 Test-acc: 0.6973684210526315\n",
      "Epoch [4452/20000] Train-Loss: 0.6514639228477562 Test-Loss: 0.19224728475639077 Train-f1: 0.4208054208875879 Test-f1: 0.4188159258581794 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4453/20000] Train-Loss: 0.647998496298182 Test-Loss: 0.1939472715332983 Train-f1: 0.41631262912176775 Test-f1: 0.3900471648674143 Train-acc: 0.7251655629139073 Test-acc: 0.6842105263157895\n",
      "Epoch [4454/20000] Train-Loss: 0.6497135371800418 Test-Loss: 0.19219837250337435 Train-f1: 0.4231263455266129 Test-f1: 0.38877263581488936 Train-acc: 0.7417218543046358 Test-acc: 0.6842105263157895\n",
      "Epoch [4455/20000] Train-Loss: 0.645271165538963 Test-Loss: 0.1926944554092692 Train-f1: 0.422003550542321 Test-f1: 0.41633139989304374 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4456/20000] Train-Loss: 0.6537616918042294 Test-Loss: 0.193017285499085 Train-f1: 0.41915665773783806 Test-f1: 0.41421602787456446 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4457/20000] Train-Loss: 0.6516411356957459 Test-Loss: 0.1927018351445417 Train-f1: 0.4225063775848444 Test-f1: 0.3992867589605914 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4458/20000] Train-Loss: 0.6488343827161078 Test-Loss: 0.19233718703737132 Train-f1: 0.42424369226954306 Test-f1: 0.437513491075135 Train-acc: 0.7350993377483444 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [4459/20000] Train-Loss: 0.6514945335690416 Test-Loss: 0.19159805445750028 Train-f1: 0.41514376582073087 Test-f1: 0.4272316024211532 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4460/20000] Train-Loss: 0.6493159503843139 Test-Loss: 0.19376290426444942 Train-f1: 0.4155154005918165 Test-f1: 0.41454173067076294 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4461/20000] Train-Loss: 0.6483727102216842 Test-Loss: 0.19570017105406723 Train-f1: 0.41467787441114046 Test-f1: 0.3769047619047619 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4462/20000] Train-Loss: 0.6529328961887039 Test-Loss: 0.1926409758757293 Train-f1: 0.41581272084805654 Test-f1: 0.3992867589605914 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4463/20000] Train-Loss: 0.6489559858202364 Test-Loss: 0.1920203629901908 Train-f1: 0.428259236075984 Test-f1: 0.3991149190696547 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4464/20000] Train-Loss: 0.652335880131211 Test-Loss: 0.19215189651875805 Train-f1: 0.41666420660384446 Test-f1: 0.4163765578329386 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4465/20000] Train-Loss: 0.6511206739002398 Test-Loss: 0.19160019629534403 Train-f1: 0.42680979778867567 Test-f1: 0.4202012072434608 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4466/20000] Train-Loss: 0.6490821406337628 Test-Loss: 0.19244217260239646 Train-f1: 0.41637290963969836 Test-f1: 0.41454173067076294 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4467/20000] Train-Loss: 0.6496203760508359 Test-Loss: 0.1917876167925532 Train-f1: 0.4225374310480693 Test-f1: 0.3955106953565471 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4468/20000] Train-Loss: 0.6503070482736979 Test-Loss: 0.1920826951709108 Train-f1: 0.42087943453874893 Test-f1: 0.38877263581488936 Train-acc: 0.7417218543046358 Test-acc: 0.6842105263157895\n",
      "Epoch [4469/20000] Train-Loss: 0.6487424308382468 Test-Loss: 0.19192856398400887 Train-f1: 0.42435720772738306 Test-f1: 0.4081818181818182 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4470/20000] Train-Loss: 0.6506933041646855 Test-Loss: 0.1921470600861423 Train-f1: 0.4251500272776868 Test-f1: 0.4098471168893704 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4471/20000] Train-Loss: 0.6507254108927046 Test-Loss: 0.19195645535038933 Train-f1: 0.42934570991517174 Test-f1: 0.4303918200739589 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4472/20000] Train-Loss: 0.6436457088741813 Test-Loss: 0.19935652145102709 Train-f1: 0.4322855265351759 Test-f1: 0.4094396651941409 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4473/20000] Train-Loss: 0.6486487213471327 Test-Loss: 0.19200867616260486 Train-f1: 0.4170997252349401 Test-f1: 0.4256306381556967 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4474/20000] Train-Loss: 0.6482169263090312 Test-Loss: 0.1928652068445328 Train-f1: 0.42624113475177305 Test-f1: 0.40378576378576375 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4475/20000] Train-Loss: 0.6502538899367757 Test-Loss: 0.1920271285347667 Train-f1: 0.40731772350146905 Test-f1: 0.3884795955218491 Train-acc: 0.7152317880794702 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4476/20000] Train-Loss: 0.6492684569781743 Test-Loss: 0.1914907060713871 Train-f1: 0.42304330172681465 Test-f1: 0.42707070707070705 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4477/20000] Train-Loss: 0.6506587194705118 Test-Loss: 0.19224665056206422 Train-f1: 0.4178587405830038 Test-f1: 0.4202012072434608 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4478/20000] Train-Loss: 0.6501082675226822 Test-Loss: 0.19137735128345695 Train-f1: 0.41890303241117993 Test-f1: 0.4163765578329386 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4479/20000] Train-Loss: 0.6504913382949249 Test-Loss: 0.19144316941294107 Train-f1: 0.4062956216098019 Test-f1: 0.4202012072434608 Train-acc: 0.7152317880794702 Test-acc: 0.7236842105263158\n",
      "Epoch [4480/20000] Train-Loss: 0.6491409161278968 Test-Loss: 0.19162710343210443 Train-f1: 0.43145555516299056 Test-f1: 0.4303918200739589 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4481/20000] Train-Loss: 0.6466870507160685 Test-Loss: 0.19145402442059634 Train-f1: 0.4310483246233746 Test-f1: 0.42623992593060206 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4482/20000] Train-Loss: 0.6464375555926279 Test-Loss: 0.1913075413739781 Train-f1: 0.42782843633907464 Test-f1: 0.42702535176346623 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4483/20000] Train-Loss: 0.6465266178806717 Test-Loss: 0.19259126729897397 Train-f1: 0.4176806442376333 Test-f1: 0.4055205890822329 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4484/20000] Train-Loss: 0.6479767035282119 Test-Loss: 0.19308233280799972 Train-f1: 0.4207243169080625 Test-f1: 0.3827185427185428 Train-acc: 0.7317880794701986 Test-acc: 0.6710526315789473\n",
      "Epoch [4485/20000] Train-Loss: 0.6483392628921879 Test-Loss: 0.19286308305811317 Train-f1: 0.41963516174076715 Test-f1: 0.4243137254901961 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4486/20000] Train-Loss: 0.6471676816375512 Test-Loss: 0.19184586641799836 Train-f1: 0.4098171848244598 Test-f1: 0.38877263581488936 Train-acc: 0.7218543046357616 Test-acc: 0.6842105263157895\n",
      "Epoch [4487/20000] Train-Loss: 0.6476441672434347 Test-Loss: 0.1935592456626212 Train-f1: 0.4339692597361351 Test-f1: 0.413538961038961 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4488/20000] Train-Loss: 0.6485462376635269 Test-Loss: 0.19146305842487452 Train-f1: 0.415614575330966 Test-f1: 0.4202012072434608 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4489/20000] Train-Loss: 0.6506261973020318 Test-Loss: 0.19201452714724265 Train-f1: 0.418262899233621 Test-f1: 0.4098471168893704 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4490/20000] Train-Loss: 0.6483383143930053 Test-Loss: 0.19150084150647922 Train-f1: 0.4284560650224868 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4491/20000] Train-Loss: 0.6480876561304245 Test-Loss: 0.1927478981909002 Train-f1: 0.4294294801064451 Test-f1: 0.41421602787456446 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4492/20000] Train-Loss: 0.6475798775044753 Test-Loss: 0.1915496980412022 Train-f1: 0.42928140166582984 Test-f1: 0.4163765578329386 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4493/20000] Train-Loss: 0.6464205054386366 Test-Loss: 0.19296351456348437 Train-f1: 0.42902157168068095 Test-f1: 0.403968253968254 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4494/20000] Train-Loss: 0.6462553738000838 Test-Loss: 0.19119615058949072 Train-f1: 0.42519665400406736 Test-f1: 0.41599511599511596 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4495/20000] Train-Loss: 0.6469646570995646 Test-Loss: 0.19187510160983065 Train-f1: 0.41406678427955024 Test-f1: 0.4098471168893704 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4496/20000] Train-Loss: 0.6486384861592944 Test-Loss: 0.193637179354842 Train-f1: 0.41839853024492635 Test-f1: 0.40378576378576375 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4497/20000] Train-Loss: 0.6458942723729232 Test-Loss: 0.19255043021116605 Train-f1: 0.4269080901197916 Test-f1: 0.40378576378576375 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4498/20000] Train-Loss: 0.6472632289150092 Test-Loss: 0.19166857108202617 Train-f1: 0.43587090003578244 Test-f1: 0.41633139989304374 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4499/20000] Train-Loss: 0.647115193890669 Test-Loss: 0.19130220885843868 Train-f1: 0.4182134890938952 Test-f1: 0.40193236714975844 Train-acc: 0.7251655629139073 Test-acc: 0.7105263157894737\n",
      "Epoch [4500/20000] Train-Loss: 0.648223885491789 Test-Loss: 0.19172106036504338 Train-f1: 0.41780811930405964 Test-f1: 0.4202012072434608 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4501/20000] Train-Loss: 0.6458828096637688 Test-Loss: 0.19123861228065048 Train-f1: 0.4361719808739677 Test-f1: 0.4148280156310388 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4502/20000] Train-Loss: 0.6533821737356892 Test-Loss: 0.19138412582668884 Train-f1: 0.4225637885282711 Test-f1: 0.4202012072434608 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4503/20000] Train-Loss: 0.6486199252426241 Test-Loss: 0.19178993944102865 Train-f1: 0.41132315521628493 Test-f1: 0.4202012072434608 Train-acc: 0.7218543046357616 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4504/20000] Train-Loss: 0.6485797583776817 Test-Loss: 0.191130980111955 Train-f1: 0.4218385302753919 Test-f1: 0.4148280156310388 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4505/20000] Train-Loss: 0.6476351339489631 Test-Loss: 0.19102668708379641 Train-f1: 0.4231164175610326 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4506/20000] Train-Loss: 0.6462948952580805 Test-Loss: 0.1913814465360108 Train-f1: 0.43659152234867227 Test-f1: 0.3947478253682927 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [4507/20000] Train-Loss: 0.6459885970564537 Test-Loss: 0.19289842515490363 Train-f1: 0.4309467642738907 Test-f1: 0.41421602787456446 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4508/20000] Train-Loss: 0.6462560730144986 Test-Loss: 0.1911229466558936 Train-f1: 0.41957245357668427 Test-f1: 0.42519841269841263 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4509/20000] Train-Loss: 0.6482471869024747 Test-Loss: 0.19131008783588416 Train-f1: 0.42958878577253135 Test-f1: 0.3955106953565471 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4510/20000] Train-Loss: 0.6468432636813255 Test-Loss: 0.19096184790748863 Train-f1: 0.4239376061605641 Test-f1: 0.4369302783936931 Train-acc: 0.7384105960264901 Test-acc: 0.75\n",
      "Epoch [4511/20000] Train-Loss: 0.6472059533194454 Test-Loss: 0.1912846409703549 Train-f1: 0.41987422170569866 Test-f1: 0.4256306381556967 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4512/20000] Train-Loss: 0.6467699212249294 Test-Loss: 0.19205019628666345 Train-f1: 0.4326916256157635 Test-f1: 0.4163765578329386 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4513/20000] Train-Loss: 0.6466621048694973 Test-Loss: 0.19515965742137284 Train-f1: 0.4181868220819168 Test-f1: 0.37158132045088565 Train-acc: 0.7251655629139073 Test-acc: 0.6710526315789473\n",
      "Epoch [4514/20000] Train-Loss: 0.649700816132415 Test-Loss: 0.19131030855916784 Train-f1: 0.415531474243043 Test-f1: 0.4098471168893704 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4515/20000] Train-Loss: 0.6456957136228885 Test-Loss: 0.19441569225617694 Train-f1: 0.4266707090248051 Test-f1: 0.413538961038961 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4516/20000] Train-Loss: 0.6471643487315387 Test-Loss: 0.19108571751568063 Train-f1: 0.42222528645063856 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4517/20000] Train-Loss: 0.6482307258401883 Test-Loss: 0.1910321094094192 Train-f1: 0.4384047109502423 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4518/20000] Train-Loss: 0.6475769050682306 Test-Loss: 0.19110948726743787 Train-f1: 0.4214542651593011 Test-f1: 0.4202012072434608 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4519/20000] Train-Loss: 0.6472000349595367 Test-Loss: 0.1910170195926484 Train-f1: 0.41776561313991734 Test-f1: 0.4098471168893704 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4520/20000] Train-Loss: 0.6473200658477184 Test-Loss: 0.1911163857396732 Train-f1: 0.4355900711602022 Test-f1: 0.4188159258581794 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4521/20000] Train-Loss: 0.6456295713736901 Test-Loss: 0.1917121645859099 Train-f1: 0.42625657399833783 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4522/20000] Train-Loss: 0.6468136515203229 Test-Loss: 0.19105161364352616 Train-f1: 0.41963516174076715 Test-f1: 0.4098130747500651 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4523/20000] Train-Loss: 0.6468150627855419 Test-Loss: 0.19108182287305406 Train-f1: 0.4333788419445989 Test-f1: 0.3944216706148934 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4524/20000] Train-Loss: 0.647144790218725 Test-Loss: 0.19391522836356828 Train-f1: 0.42259662322019426 Test-f1: 0.4025672877846791 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4525/20000] Train-Loss: 0.6467620585396799 Test-Loss: 0.19195951480658885 Train-f1: 0.4200754314762432 Test-f1: 0.403968253968254 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4526/20000] Train-Loss: 0.648466582739278 Test-Loss: 0.1910834897582438 Train-f1: 0.4131151185946388 Test-f1: 0.4098471168893704 Train-acc: 0.7218543046357616 Test-acc: 0.7105263157894737\n",
      "Epoch [4527/20000] Train-Loss: 0.6453824638465011 Test-Loss: 0.19128794574645258 Train-f1: 0.42370419615971217 Test-f1: 0.4098130747500651 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4528/20000] Train-Loss: 0.6462902425245255 Test-Loss: 0.1917665963240892 Train-f1: 0.43826690550828484 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4529/20000] Train-Loss: 0.6467605568618788 Test-Loss: 0.19098282589830345 Train-f1: 0.414500490185527 Test-f1: 0.4303918200739589 Train-acc: 0.7251655629139073 Test-acc: 0.7368421052631579\n",
      "Epoch [4530/20000] Train-Loss: 0.6463520585442614 Test-Loss: 0.19114557202619864 Train-f1: 0.4399671398128572 Test-f1: 0.4202012072434608 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4531/20000] Train-Loss: 0.6457116715320116 Test-Loss: 0.19076530616199497 Train-f1: 0.4362232094630392 Test-f1: 0.437513491075135 Train-acc: 0.7516556291390728 Test-acc: 0.75\n",
      "Epoch [4532/20000] Train-Loss: 0.6455084783355999 Test-Loss: 0.19095190982964336 Train-f1: 0.4228322090159546 Test-f1: 0.3992867589605914 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4533/20000] Train-Loss: 0.6443430507820959 Test-Loss: 0.19076643272834998 Train-f1: 0.4311978455323652 Test-f1: 0.42702535176346623 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4534/20000] Train-Loss: 0.6462444044528404 Test-Loss: 0.19118389951860615 Train-f1: 0.4237875803221707 Test-f1: 0.4163765578329386 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4535/20000] Train-Loss: 0.6475597937867548 Test-Loss: 0.19089168027434572 Train-f1: 0.42075402652265464 Test-f1: 0.3947478253682927 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4536/20000] Train-Loss: 0.6460806257397443 Test-Loss: 0.19078911534449372 Train-f1: 0.430761504458812 Test-f1: 0.4163765578329386 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4537/20000] Train-Loss: 0.6456404592978372 Test-Loss: 0.19123720775376743 Train-f1: 0.43524495955416265 Test-f1: 0.4303918200739589 Train-acc: 0.7483443708609272 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4538/20000] Train-Loss: 0.6441236491541569 Test-Loss: 0.19067846279925044 Train-f1: 0.4291719226218925 Test-f1: 0.41599511599511596 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4539/20000] Train-Loss: 0.6442170657316296 Test-Loss: 0.19208557274189628 Train-f1: 0.4354648290934766 Test-f1: 0.41287828450619146 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4540/20000] Train-Loss: 0.6443170080741232 Test-Loss: 0.19151731687303747 Train-f1: 0.42493259480686946 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4541/20000] Train-Loss: 0.6436228538136967 Test-Loss: 0.19130766715538441 Train-f1: 0.4356650441756825 Test-f1: 0.4188159258581794 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4542/20000] Train-Loss: 0.6459736396781797 Test-Loss: 0.19056456538666885 Train-f1: 0.4165627264621259 Test-f1: 0.42702535176346623 Train-acc: 0.7284768211920529 Test-acc: 0.7368421052631579\n",
      "Epoch [4543/20000] Train-Loss: 0.6463918547747722 Test-Loss: 0.1931024060844618 Train-f1: 0.424750770076961 Test-f1: 0.3930313588850174 Train-acc: 0.7417218543046358 Test-acc: 0.6842105263157895\n",
      "Epoch [4544/20000] Train-Loss: 0.6480224315449947 Test-Loss: 0.19065421054250536 Train-f1: 0.4307815577093872 Test-f1: 0.4163765578329386 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4545/20000] Train-Loss: 0.643862437161155 Test-Loss: 0.1935915222793295 Train-f1: 0.42209492635024554 Test-f1: 0.39338345864661656 Train-acc: 0.7317880794701986 Test-acc: 0.6842105263157895\n",
      "Epoch [4546/20000] Train-Loss: 0.6449368031528114 Test-Loss: 0.19200946971101246 Train-f1: 0.43437985957382363 Test-f1: 0.403968253968254 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4547/20000] Train-Loss: 0.6460599874904546 Test-Loss: 0.19097514520777328 Train-f1: 0.4245246341980632 Test-f1: 0.3992867589605914 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4548/20000] Train-Loss: 0.6447439781461078 Test-Loss: 0.19079268209489403 Train-f1: 0.4387273596169875 Test-f1: 0.4303918200739589 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [4549/20000] Train-Loss: 0.646947676127622 Test-Loss: 0.19072841692715092 Train-f1: 0.4227327410538912 Test-f1: 0.4188159258581794 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4550/20000] Train-Loss: 0.6455763448657216 Test-Loss: 0.19206951259433971 Train-f1: 0.42343732853411764 Test-f1: 0.4030252100840336 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4551/20000] Train-Loss: 0.6451480663974797 Test-Loss: 0.19300196170174203 Train-f1: 0.4333425388934916 Test-f1: 0.4243137254901961 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4552/20000] Train-Loss: 0.6449537821233597 Test-Loss: 0.19054738211669334 Train-f1: 0.4278683346796789 Test-f1: 0.4256306381556967 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4553/20000] Train-Loss: 0.64758123690334 Test-Loss: 0.19169132814250975 Train-f1: 0.4159440874126366 Test-f1: 0.3947478253682927 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4554/20000] Train-Loss: 0.6443832527378729 Test-Loss: 0.19045525636292399 Train-f1: 0.42785812207003665 Test-f1: 0.4265359477124183 Train-acc: 0.7384105960264901 Test-acc: 0.7368421052631579\n",
      "Epoch [4555/20000] Train-Loss: 0.6454293134016901 Test-Loss: 0.19163021034023442 Train-f1: 0.44150693406196834 Test-f1: 0.403968253968254 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4556/20000] Train-Loss: 0.6434411808869335 Test-Loss: 0.19053844860045016 Train-f1: 0.4294061802975504 Test-f1: 0.4055205890822329 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4557/20000] Train-Loss: 0.644232517050646 Test-Loss: 0.19063490454126866 Train-f1: 0.4267493578678053 Test-f1: 0.4098471168893704 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4558/20000] Train-Loss: 0.645916775230355 Test-Loss: 0.1907153601684671 Train-f1: 0.4290598249141264 Test-f1: 0.4163765578329386 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4559/20000] Train-Loss: 0.6453978654587 Test-Loss: 0.1913971319773989 Train-f1: 0.4264311077155717 Test-f1: 0.3992867589605914 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4560/20000] Train-Loss: 0.6472967542898497 Test-Loss: 0.19100603094487523 Train-f1: 0.42861978071174134 Test-f1: 0.4098471168893704 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4561/20000] Train-Loss: 0.646844480465467 Test-Loss: 0.19037396511471188 Train-f1: 0.4323366360568552 Test-f1: 0.4163765578329386 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4562/20000] Train-Loss: 0.6458834028971203 Test-Loss: 0.1906467754563238 Train-f1: 0.43132025311361855 Test-f1: 0.4188159258581794 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4563/20000] Train-Loss: 0.6443376511901949 Test-Loss: 0.1908693844033502 Train-f1: 0.4329519474682879 Test-f1: 0.4098471168893704 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4564/20000] Train-Loss: 0.6471245705688636 Test-Loss: 0.19055076434854082 Train-f1: 0.4196623591818289 Test-f1: 0.38877263581488936 Train-acc: 0.7350993377483444 Test-acc: 0.6842105263157895\n",
      "Epoch [4565/20000] Train-Loss: 0.6458804539137165 Test-Loss: 0.19050854779691662 Train-f1: 0.4331395989693824 Test-f1: 0.4098471168893704 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4566/20000] Train-Loss: 0.645453046460118 Test-Loss: 0.19090187164695166 Train-f1: 0.418584202011708 Test-f1: 0.4098471168893704 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4567/20000] Train-Loss: 0.646176348203168 Test-Loss: 0.1903691260739459 Train-f1: 0.4298782763863473 Test-f1: 0.42519841269841263 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4568/20000] Train-Loss: 0.6438999620790841 Test-Loss: 0.19054331862797236 Train-f1: 0.4298058524227251 Test-f1: 0.4098471168893704 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4569/20000] Train-Loss: 0.6433762106996106 Test-Loss: 0.19078667150754233 Train-f1: 0.43204370879942766 Test-f1: 0.42702535176346623 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4570/20000] Train-Loss: 0.6488243337816657 Test-Loss: 0.19239768482344471 Train-f1: 0.43022094364342356 Test-f1: 0.41287828450619146 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4571/20000] Train-Loss: 0.6442186411870514 Test-Loss: 0.190293712625904 Train-f1: 0.4284369984987316 Test-f1: 0.42702535176346623 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4572/20000] Train-Loss: 0.6436132576920022 Test-Loss: 0.19317918583462132 Train-f1: 0.44042662569495244 Test-f1: 0.3566610644257703 Train-acc: 0.7549668874172185 Test-acc: 0.6578947368421053\n",
      "Epoch [4573/20000] Train-Loss: 0.6437536885388585 Test-Loss: 0.19053525466087423 Train-f1: 0.44036413196858415 Test-f1: 0.3992867589605914 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "Epoch [4574/20000] Train-Loss: 0.642698507913738 Test-Loss: 0.19247565940331676 Train-f1: 0.4343926699697162 Test-f1: 0.403968253968254 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4575/20000] Train-Loss: 0.6435982606617893 Test-Loss: 0.19237733234892643 Train-f1: 0.430253545670168 Test-f1: 0.4025672877846791 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4576/20000] Train-Loss: 0.644580720776816 Test-Loss: 0.190303247106086 Train-f1: 0.41839287231359884 Test-f1: 0.4055205890822329 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4577/20000] Train-Loss: 0.6452590291064773 Test-Loss: 0.191328196179234 Train-f1: 0.42960445637643724 Test-f1: 0.41287828450619146 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4578/20000] Train-Loss: 0.6448192184073902 Test-Loss: 0.19187668611219078 Train-f1: 0.4341309103146559 Test-f1: 0.4243137254901961 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4579/20000] Train-Loss: 0.6419262532482319 Test-Loss: 0.19081137944215276 Train-f1: 0.4337357577431621 Test-f1: 0.437513491075135 Train-acc: 0.7483443708609272 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [4580/20000] Train-Loss: 0.6443594037782978 Test-Loss: 0.19023361601549515 Train-f1: 0.43427267857949514 Test-f1: 0.41304492789864133 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4581/20000] Train-Loss: 0.6417289515193767 Test-Loss: 0.19019118087216658 Train-f1: 0.43242971075086095 Test-f1: 0.41599511599511596 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4582/20000] Train-Loss: 0.6449035221075038 Test-Loss: 0.19043395126863913 Train-f1: 0.4351983160748847 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4583/20000] Train-Loss: 0.6477280671371952 Test-Loss: 0.19038035204125328 Train-f1: 0.4291871249318057 Test-f1: 0.4303918200739589 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4584/20000] Train-Loss: 0.6411094690204034 Test-Loss: 0.19404211539742697 Train-f1: 0.4307873910127432 Test-f1: 0.38306822198656765 Train-acc: 0.7483443708609272 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [4585/20000] Train-Loss: 0.6451890454517903 Test-Loss: 0.1901508067367273 Train-f1: 0.430489500764845 Test-f1: 0.437513491075135 Train-acc: 0.7417218543046358 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [4586/20000] Train-Loss: 0.6445042374553693 Test-Loss: 0.1900956593367365 Train-f1: 0.4375892360265027 Test-f1: 0.4163765578329386 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4587/20000] Train-Loss: 0.6444524808282294 Test-Loss: 0.1901680388076912 Train-f1: 0.4347318456376935 Test-f1: 0.41599511599511596 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4588/20000] Train-Loss: 0.6423999854245128 Test-Loss: 0.19123285004142027 Train-f1: 0.42659815005138746 Test-f1: 0.4148280156310388 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4589/20000] Train-Loss: 0.6461164907289839 Test-Loss: 0.1905342601390354 Train-f1: 0.41866793984244455 Test-f1: 0.4098471168893704 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4590/20000] Train-Loss: 0.6424476475937704 Test-Loss: 0.1902826381714253 Train-f1: 0.43088673625659996 Test-f1: 0.41633139989304374 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4591/20000] Train-Loss: 0.6459407226860687 Test-Loss: 0.19124051902124622 Train-f1: 0.4177726450749203 Test-f1: 0.39338345864661656 Train-acc: 0.7317880794701986 Test-acc: 0.6842105263157895\n",
      "Epoch [4592/20000] Train-Loss: 0.6428080528686195 Test-Loss: 0.19109509558898102 Train-f1: 0.4215099615699714 Test-f1: 0.41587454059472106 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4593/20000] Train-Loss: 0.6428494358060888 Test-Loss: 0.19070961641575224 Train-f1: 0.42965304899468926 Test-f1: 0.4092921163343698 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4594/20000] Train-Loss: 0.6443534090468717 Test-Loss: 0.19026567462146537 Train-f1: 0.44371066815163396 Test-f1: 0.4098471168893704 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4595/20000] Train-Loss: 0.6430980773542252 Test-Loss: 0.1900184805353746 Train-f1: 0.42868092896317983 Test-f1: 0.41578947368421043 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4596/20000] Train-Loss: 0.6382604977620661 Test-Loss: 0.1902928597381364 Train-f1: 0.4389600511345478 Test-f1: 0.437513491075135 Train-acc: 0.7516556291390728 Test-acc: 0.75\n",
      "Epoch [4597/20000] Train-Loss: 0.6429790606575441 Test-Loss: 0.19130509279186925 Train-f1: 0.42321596244131454 Test-f1: 0.403968253968254 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4598/20000] Train-Loss: 0.642366057314086 Test-Loss: 0.1899861338412371 Train-f1: 0.43609881692606756 Test-f1: 0.41633139989304374 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4599/20000] Train-Loss: 0.6402213580208657 Test-Loss: 0.19371359657350615 Train-f1: 0.4191964727715227 Test-f1: 0.40361535448491975 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4600/20000] Train-Loss: 0.6429553617343846 Test-Loss: 0.19025226427657663 Train-f1: 0.43238888642252216 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4601/20000] Train-Loss: 0.6431952084844795 Test-Loss: 0.19131041028461357 Train-f1: 0.42846115633000875 Test-f1: 0.38285714285714284 Train-acc: 0.7450331125827815 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [4602/20000] Train-Loss: 0.6413549013122042 Test-Loss: 0.18996236044305592 Train-f1: 0.4286163844750569 Test-f1: 0.41599511599511596 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4603/20000] Train-Loss: 0.6414342967695328 Test-Loss: 0.1962518637398872 Train-f1: 0.4232118057438994 Test-f1: 0.4134165729574624 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4604/20000] Train-Loss: 0.6439269948035212 Test-Loss: 0.19081192686668072 Train-f1: 0.43983086518960696 Test-f1: 0.38877263581488936 Train-acc: 0.7582781456953642 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4605/20000] Train-Loss: 0.6442251350226706 Test-Loss: 0.18991231859504934 Train-f1: 0.4339861751152074 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4606/20000] Train-Loss: 0.6435448736211402 Test-Loss: 0.1899958182012778 Train-f1: 0.435840069151333 Test-f1: 0.4055205890822329 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4607/20000] Train-Loss: 0.6409852061189357 Test-Loss: 0.19050520889905667 Train-f1: 0.4311904352493057 Test-f1: 0.4098471168893704 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4608/20000] Train-Loss: 0.6435990826564814 Test-Loss: 0.19017863126665635 Train-f1: 0.4226296862761184 Test-f1: 0.4303918200739589 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4609/20000] Train-Loss: 0.6419056955862799 Test-Loss: 0.19206153010692656 Train-f1: 0.42331411215484893 Test-f1: 0.38285714285714284 Train-acc: 0.7384105960264901 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [4610/20000] Train-Loss: 0.6421818780912841 Test-Loss: 0.18986064190219973 Train-f1: 0.43243082677246714 Test-f1: 0.41599511599511596 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4611/20000] Train-Loss: 0.6413354095518353 Test-Loss: 0.19298412184535202 Train-f1: 0.4285638525736851 Test-f1: 0.40361535448491975 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4612/20000] Train-Loss: 0.6428357169010636 Test-Loss: 0.18986362413540278 Train-f1: 0.4275890775890776 Test-f1: 0.41599511599511596 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4613/20000] Train-Loss: 0.643185457814637 Test-Loss: 0.19065223308649 Train-f1: 0.4125217974513749 Test-f1: 0.4243137254901961 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "Epoch [4614/20000] Train-Loss: 0.6433997081230654 Test-Loss: 0.19024383539886164 Train-f1: 0.4166115051221434 Test-f1: 0.4098471168893704 Train-acc: 0.7284768211920529 Test-acc: 0.7105263157894737\n",
      "Epoch [4615/20000] Train-Loss: 0.6411993493037526 Test-Loss: 0.19218507797168444 Train-f1: 0.4181352732486894 Test-f1: 0.403968253968254 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4616/20000] Train-Loss: 0.640466075964486 Test-Loss: 0.19196872932908357 Train-f1: 0.4262069696755496 Test-f1: 0.401038961038961 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4617/20000] Train-Loss: 0.642467731928324 Test-Loss: 0.1903177423212298 Train-f1: 0.4261995961175168 Test-f1: 0.4170266040688576 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4618/20000] Train-Loss: 0.6409737498933231 Test-Loss: 0.19065977796299055 Train-f1: 0.42170214052757515 Test-f1: 0.4092921163343698 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4619/20000] Train-Loss: 0.6401799833965439 Test-Loss: 0.18982476235323262 Train-f1: 0.4440290886585177 Test-f1: 0.42519841269841263 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [4620/20000] Train-Loss: 0.6428980795543563 Test-Loss: 0.19051138189722855 Train-f1: 0.43253846556416164 Test-f1: 0.4098471168893704 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4621/20000] Train-Loss: 0.6390523318146639 Test-Loss: 0.19217362996768395 Train-f1: 0.44400257755684436 Test-f1: 0.401038961038961 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4622/20000] Train-Loss: 0.6392423186576479 Test-Loss: 0.1898721021755228 Train-f1: 0.42948717948717946 Test-f1: 0.3947478253682927 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4623/20000] Train-Loss: 0.6408899504736447 Test-Loss: 0.18986696100118855 Train-f1: 0.427713340499473 Test-f1: 0.4055205890822329 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4624/20000] Train-Loss: 0.6387198980429601 Test-Loss: 0.18998004275499633 Train-f1: 0.4293003713113152 Test-f1: 0.419345725515886 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4625/20000] Train-Loss: 0.639816228670772 Test-Loss: 0.1930892319193675 Train-f1: 0.42275299947835154 Test-f1: 0.40361535448491975 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4626/20000] Train-Loss: 0.6390739503315881 Test-Loss: 0.19006486348740195 Train-f1: 0.4243635837313649 Test-f1: 0.4303918200739589 Train-acc: 0.7350993377483444 Test-acc: 0.7368421052631579\n",
      "Epoch [4627/20000] Train-Loss: 0.6411007999932351 Test-Loss: 0.18984425093685547 Train-f1: 0.42817866395539994 Test-f1: 0.40510510510510506 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4628/20000] Train-Loss: 0.6410100260619227 Test-Loss: 0.1934307586497308 Train-f1: 0.44345904821331794 Test-f1: 0.40361535448491975 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4629/20000] Train-Loss: 0.6386097256375407 Test-Loss: 0.19028811919732871 Train-f1: 0.43557462560996124 Test-f1: 0.4098471168893704 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4630/20000] Train-Loss: 0.6412161216452616 Test-Loss: 0.19233874036676063 Train-f1: 0.4365083868527163 Test-f1: 0.4243137254901961 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4631/20000] Train-Loss: 0.6436386085409189 Test-Loss: 0.1897869345230209 Train-f1: 0.42949027737602175 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4632/20000] Train-Loss: 0.6412063200079249 Test-Loss: 0.18973585185044964 Train-f1: 0.4403255636145724 Test-f1: 0.4369302783936931 Train-acc: 0.7549668874172185 Test-acc: 0.75\n",
      "Epoch [4633/20000] Train-Loss: 0.6393451613333629 Test-Loss: 0.1900884408240009 Train-f1: 0.42577095784642766 Test-f1: 0.3947478253682927 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4634/20000] Train-Loss: 0.6444420773993262 Test-Loss: 0.18979831956513687 Train-f1: 0.4183851609383524 Test-f1: 0.38877263581488936 Train-acc: 0.7284768211920529 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4635/20000] Train-Loss: 0.6431083599307816 Test-Loss: 0.18955459826730775 Train-f1: 0.42942932971446035 Test-f1: 0.4163765578329386 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4636/20000] Train-Loss: 0.6406422658488214 Test-Loss: 0.19012613007425225 Train-f1: 0.4245277907878723 Test-f1: 0.38877263581488936 Train-acc: 0.7384105960264901 Test-acc: 0.6842105263157895\n",
      "Epoch [4637/20000] Train-Loss: 0.6416496294907827 Test-Loss: 0.19033560756815074 Train-f1: 0.42854967782165865 Test-f1: 0.38877263581488936 Train-acc: 0.7417218543046358 Test-acc: 0.6842105263157895\n",
      "Epoch [4638/20000] Train-Loss: 0.6401219302390977 Test-Loss: 0.19107387864700498 Train-f1: 0.4293160591854601 Test-f1: 0.39338345864661656 Train-acc: 0.7417218543046358 Test-acc: 0.6842105263157895\n",
      "Epoch [4639/20000] Train-Loss: 0.6411610441602723 Test-Loss: 0.18985000864720045 Train-f1: 0.4352803753346378 Test-f1: 0.406845725515886 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4640/20000] Train-Loss: 0.641776830286141 Test-Loss: 0.19214810316404976 Train-f1: 0.4366591502290838 Test-f1: 0.4243137254901961 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4641/20000] Train-Loss: 0.6391659519563689 Test-Loss: 0.18969947166012605 Train-f1: 0.42291196490913513 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4642/20000] Train-Loss: 0.6407323378353273 Test-Loss: 0.19083267958974992 Train-f1: 0.43401128758633745 Test-f1: 0.39338345864661656 Train-acc: 0.7483443708609272 Test-acc: 0.6842105263157895\n",
      "Epoch [4643/20000] Train-Loss: 0.6429555519768344 Test-Loss: 0.19078623256588173 Train-f1: 0.42544407012754426 Test-f1: 0.403968253968254 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4644/20000] Train-Loss: 0.6422598103646051 Test-Loss: 0.18960866118773792 Train-f1: 0.4362482483356662 Test-f1: 0.4055205890822329 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4645/20000] Train-Loss: 0.6420675946169867 Test-Loss: 0.19034580681275082 Train-f1: 0.4318967043467506 Test-f1: 0.3992867589605914 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4646/20000] Train-Loss: 0.6368408273466294 Test-Loss: 0.18947248130409036 Train-f1: 0.42471715751393824 Test-f1: 0.41599511599511596 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4647/20000] Train-Loss: 0.6370600526665803 Test-Loss: 0.19356154963735278 Train-f1: 0.41269621089176234 Test-f1: 0.40065876152832675 Train-acc: 0.7218543046357616 Test-acc: 0.6973684210526315\n",
      "Epoch [4648/20000] Train-Loss: 0.6414402870838183 Test-Loss: 0.19027494684157223 Train-f1: 0.4250919057652152 Test-f1: 0.40732311759117756 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4649/20000] Train-Loss: 0.6400515716696002 Test-Loss: 0.18983494909037812 Train-f1: 0.43271584213817843 Test-f1: 0.42623992593060206 Train-acc: 0.7483443708609272 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4650/20000] Train-Loss: 0.6398470716364576 Test-Loss: 0.18937149952749383 Train-f1: 0.4226276423407997 Test-f1: 0.4256306381556967 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4651/20000] Train-Loss: 0.6414340203126447 Test-Loss: 0.18951929769165984 Train-f1: 0.43314898052244155 Test-f1: 0.437513491075135 Train-acc: 0.7483443708609272 Test-acc: 0.75\n",
      "Epoch [4652/20000] Train-Loss: 0.6355697355210358 Test-Loss: 0.19664554013239824 Train-f1: 0.4257719664545311 Test-f1: 0.4016263510036027 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "Epoch [4653/20000] Train-Loss: 0.6386042224828286 Test-Loss: 0.18995453205671847 Train-f1: 0.43576494427558254 Test-f1: 0.3947478253682927 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4654/20000] Train-Loss: 0.6434589226700013 Test-Loss: 0.18950188671691856 Train-f1: 0.4270716632449144 Test-f1: 0.40539227895392277 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4655/20000] Train-Loss: 0.639094905713227 Test-Loss: 0.1896441309673964 Train-f1: 0.4340394088669951 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4656/20000] Train-Loss: 0.638263395171789 Test-Loss: 0.19962406960134013 Train-f1: 0.42965731346438385 Test-f1: 0.4047514647514648 Train-acc: 0.7450331125827815 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4657/20000] Train-Loss: 0.6384777875917844 Test-Loss: 0.189370280230674 Train-f1: 0.42576053442959916 Test-f1: 0.40510510510510506 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4658/20000] Train-Loss: 0.6389496034659032 Test-Loss: 0.18982339445930468 Train-f1: 0.44193810678055223 Test-f1: 0.41633139989304374 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4659/20000] Train-Loss: 0.6414219091940092 Test-Loss: 0.1904681644627396 Train-f1: 0.42759650393102355 Test-f1: 0.40378576378576375 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4660/20000] Train-Loss: 0.6390887254507139 Test-Loss: 0.1901327884496742 Train-f1: 0.4189723650163428 Test-f1: 0.3992867589605914 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4661/20000] Train-Loss: 0.6397951083804682 Test-Loss: 0.1905458362120362 Train-f1: 0.43625785729436367 Test-f1: 0.401038961038961 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4662/20000] Train-Loss: 0.6393090510165875 Test-Loss: 0.18921127268000795 Train-f1: 0.42613722843604496 Test-f1: 0.42702535176346623 Train-acc: 0.7384105960264901 Test-acc: 0.7368421052631579\n",
      "Epoch [4663/20000] Train-Loss: 0.6425361351774046 Test-Loss: 0.19032509298450054 Train-f1: 0.41899716116049407 Test-f1: 0.403968253968254 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4664/20000] Train-Loss: 0.6389979773513925 Test-Loss: 0.19103824255511898 Train-f1: 0.4478510170807267 Test-f1: 0.403968253968254 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [4665/20000] Train-Loss: 0.639163083660007 Test-Loss: 0.18949720803011613 Train-f1: 0.43291459864312004 Test-f1: 0.4303918200739589 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4666/20000] Train-Loss: 0.6388378324257362 Test-Loss: 0.1898042143786473 Train-f1: 0.4126553063931409 Test-f1: 0.3991149190696547 Train-acc: 0.7284768211920529 Test-acc: 0.6973684210526315\n",
      "Epoch [4667/20000] Train-Loss: 0.6403328836442206 Test-Loss: 0.1925494560817258 Train-f1: 0.4242018992938523 Test-f1: 0.38306822198656765 Train-acc: 0.7350993377483444 Test-acc: 0.6710526315789473\n",
      "Epoch [4668/20000] Train-Loss: 0.642328771609334 Test-Loss: 0.1926185609638333 Train-f1: 0.4215186445922946 Test-f1: 0.4127799736495389 Train-acc: 0.7317880794701986 Test-acc: 0.7105263157894737\n",
      "Epoch [4669/20000] Train-Loss: 0.6372830007513183 Test-Loss: 0.19365192516277038 Train-f1: 0.4334407791897421 Test-f1: 0.4096494355317885 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4670/20000] Train-Loss: 0.6417230255401394 Test-Loss: 0.19251717214665962 Train-f1: 0.4286638582271281 Test-f1: 0.39333450683565097 Train-acc: 0.7450331125827815 Test-acc: 0.6842105263157895\n",
      "Epoch [4671/20000] Train-Loss: 0.639443330595789 Test-Loss: 0.1901753088450504 Train-f1: 0.4345335515548282 Test-f1: 0.401038961038961 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4672/20000] Train-Loss: 0.634666724543181 Test-Loss: 0.19207573331322203 Train-f1: 0.42714203565267395 Test-f1: 0.43392041748206134 Train-acc: 0.7483443708609272 Test-acc: 0.75\n",
      "Epoch [4673/20000] Train-Loss: 0.6380696658917415 Test-Loss: 0.1939757749852734 Train-f1: 0.4317174732052472 Test-f1: 0.4061776061776062 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4674/20000] Train-Loss: 0.6383152674989541 Test-Loss: 0.18934661295484395 Train-f1: 0.4276878174652083 Test-f1: 0.4055205890822329 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4675/20000] Train-Loss: 0.6371002310098031 Test-Loss: 0.19068684023742055 Train-f1: 0.4396172913473485 Test-f1: 0.403968253968254 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [4676/20000] Train-Loss: 0.6396081440851613 Test-Loss: 0.1903960245720481 Train-f1: 0.42921856925124297 Test-f1: 0.4243137254901961 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4677/20000] Train-Loss: 0.6363471856254173 Test-Loss: 0.19091918138035022 Train-f1: 0.42665733468128675 Test-f1: 0.4163765578329386 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4678/20000] Train-Loss: 0.6348615995851712 Test-Loss: 0.1924955300493569 Train-f1: 0.4391506842771502 Test-f1: 0.40177653108929406 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [4679/20000] Train-Loss: 0.6407071600895645 Test-Loss: 0.18977488582763577 Train-f1: 0.4214937137645756 Test-f1: 0.4098471168893704 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4680/20000] Train-Loss: 0.6351330026461047 Test-Loss: 0.18917856050120324 Train-f1: 0.4383326380197469 Test-f1: 0.41478927203065136 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4681/20000] Train-Loss: 0.6356492167187923 Test-Loss: 0.19623431311330564 Train-f1: 0.4236904220710519 Test-f1: 0.4074109554921158 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4682/20000] Train-Loss: 0.6373594431761243 Test-Loss: 0.18906311030614525 Train-f1: 0.4328097279237513 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4683/20000] Train-Loss: 0.6379332051844723 Test-Loss: 0.19121050933236078 Train-f1: 0.42947123798187625 Test-f1: 0.39352935352935353 Train-acc: 0.7417218543046358 Test-acc: 0.6842105263157895\n",
      "Epoch [4684/20000] Train-Loss: 0.6373966927007679 Test-Loss: 0.19296221820507678 Train-f1: 0.4291871249318057 Test-f1: 0.40065876152832675 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4685/20000] Train-Loss: 0.6398240249163264 Test-Loss: 0.18902326921667073 Train-f1: 0.4204926241114538 Test-f1: 0.41599511599511596 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4686/20000] Train-Loss: 0.6397162727947239 Test-Loss: 0.18904209260774643 Train-f1: 0.43751564455569464 Test-f1: 0.42702535176346623 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [4687/20000] Train-Loss: 0.6377264375890271 Test-Loss: 0.18905097648814165 Train-f1: 0.4419206050896989 Test-f1: 0.41633139989304374 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4688/20000] Train-Loss: 0.6380178079476868 Test-Loss: 0.19147009157641937 Train-f1: 0.4361838262740706 Test-f1: 0.3900471648674143 Train-acc: 0.7483443708609272 Test-acc: 0.6842105263157895\n",
      "Epoch [4689/20000] Train-Loss: 0.6369854967484457 Test-Loss: 0.18923521165191684 Train-f1: 0.4308817262093808 Test-f1: 0.40585024204346487 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4690/20000] Train-Loss: 0.6373034763811587 Test-Loss: 0.19048350256375104 Train-f1: 0.4326498346629187 Test-f1: 0.403968253968254 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4691/20000] Train-Loss: 0.6393200513797018 Test-Loss: 0.1899225673347553 Train-f1: 0.43952201885320336 Test-f1: 0.403968253968254 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4692/20000] Train-Loss: 0.6384088375030572 Test-Loss: 0.188908875253586 Train-f1: 0.43759019706272284 Test-f1: 0.437513491075135 Train-acc: 0.7516556291390728 Test-acc: 0.75\n",
      "Epoch [4693/20000] Train-Loss: 0.640941596734889 Test-Loss: 0.19238290608597777 Train-f1: 0.41581272084805654 Test-f1: 0.4236024844720497 Train-acc: 0.7251655629139073 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4694/20000] Train-Loss: 0.6378876238296498 Test-Loss: 0.18883801318843416 Train-f1: 0.4205385106020607 Test-f1: 0.4163765578329386 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4695/20000] Train-Loss: 0.6376264066291468 Test-Loss: 0.18946204807325143 Train-f1: 0.4296072836994256 Test-f1: 0.42702535176346623 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [4696/20000] Train-Loss: 0.6391813418221042 Test-Loss: 0.1888767344417535 Train-f1: 0.4207483870967742 Test-f1: 0.4163765578329386 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4697/20000] Train-Loss: 0.6376710745261904 Test-Loss: 0.18926781837565998 Train-f1: 0.4343799723100449 Test-f1: 0.406845725515886 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4698/20000] Train-Loss: 0.6389396328699154 Test-Loss: 0.18948873999906685 Train-f1: 0.43550003253302105 Test-f1: 0.4092921163343698 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4699/20000] Train-Loss: 0.6393065307531625 Test-Loss: 0.19000943485175167 Train-f1: 0.44091508076329866 Test-f1: 0.403968253968254 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [4700/20000] Train-Loss: 0.6389166045411963 Test-Loss: 0.18897313627443524 Train-f1: 0.44816270812329506 Test-f1: 0.41578947368421043 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4701/20000] Train-Loss: 0.6374200412563683 Test-Loss: 0.18975898632632243 Train-f1: 0.4337815567603716 Test-f1: 0.401038961038961 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4702/20000] Train-Loss: 0.6378476620339437 Test-Loss: 0.1890319699397676 Train-f1: 0.41935408241017524 Test-f1: 0.4055205890822329 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4703/20000] Train-Loss: 0.6387012080987589 Test-Loss: 0.18970152517264208 Train-f1: 0.44194381815235373 Test-f1: 0.4098471168893704 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4704/20000] Train-Loss: 0.6374850453269097 Test-Loss: 0.19038288057302558 Train-f1: 0.43230413091711234 Test-f1: 0.403968253968254 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4705/20000] Train-Loss: 0.6379673243584301 Test-Loss: 0.1913156951025419 Train-f1: 0.43043183767885546 Test-f1: 0.403968253968254 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4706/20000] Train-Loss: 0.6331888581818298 Test-Loss: 0.18931158280935192 Train-f1: 0.43914188836724044 Test-f1: 0.4163765578329386 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4707/20000] Train-Loss: 0.6371403152248676 Test-Loss: 0.18978964322051603 Train-f1: 0.44253473499441787 Test-f1: 0.403968253968254 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4708/20000] Train-Loss: 0.6391532180535411 Test-Loss: 0.19155201086022575 Train-f1: 0.44017532926370534 Test-f1: 0.40361535448491975 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [4709/20000] Train-Loss: 0.6365551826661928 Test-Loss: 0.190510571435575 Train-f1: 0.4343926699697162 Test-f1: 0.41421602787456446 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4710/20000] Train-Loss: 0.6384512496179903 Test-Loss: 0.1888465997850407 Train-f1: 0.4374659409533234 Test-f1: 0.41599511599511596 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4711/20000] Train-Loss: 0.6363545916919534 Test-Loss: 0.18888450086337133 Train-f1: 0.4450310559006211 Test-f1: 0.41304492789864133 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4712/20000] Train-Loss: 0.6370836063259196 Test-Loss: 0.18887369073271074 Train-f1: 0.4456010947292405 Test-f1: 0.40193236714975844 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4713/20000] Train-Loss: 0.6359175242163482 Test-Loss: 0.18866534351014536 Train-f1: 0.4291981774318687 Test-f1: 0.4369302783936931 Train-acc: 0.7450331125827815 Test-acc: 0.75\n",
      "Epoch [4714/20000] Train-Loss: 0.6372662850437895 Test-Loss: 0.18889795599269038 Train-f1: 0.4211345492308691 Test-f1: 0.4256306381556967 Train-acc: 0.7317880794701986 Test-acc: 0.7368421052631579\n",
      "Epoch [4715/20000] Train-Loss: 0.6352828825614288 Test-Loss: 0.1888992752787468 Train-f1: 0.4421929302773779 Test-f1: 0.41333670012415047 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4716/20000] Train-Loss: 0.6379740293740395 Test-Loss: 0.1896553604066881 Train-f1: 0.425555744685343 Test-f1: 0.4243137254901961 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4717/20000] Train-Loss: 0.6365779124708303 Test-Loss: 0.18897944111068343 Train-f1: 0.4323707254713966 Test-f1: 0.4303918200739589 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4718/20000] Train-Loss: 0.6364359126900675 Test-Loss: 0.19098081489498508 Train-f1: 0.43581398568368934 Test-f1: 0.4030252100840336 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4719/20000] Train-Loss: 0.6351765254454501 Test-Loss: 0.19124377295189463 Train-f1: 0.4300718778077268 Test-f1: 0.4127799736495389 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4720/20000] Train-Loss: 0.6395065858025173 Test-Loss: 0.18907721120922025 Train-f1: 0.4404960051134548 Test-f1: 0.4098471168893704 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4721/20000] Train-Loss: 0.6391616533751641 Test-Loss: 0.18869421458953872 Train-f1: 0.4239953590977076 Test-f1: 0.41578947368421043 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4722/20000] Train-Loss: 0.6359815426098231 Test-Loss: 0.1947563168671905 Train-f1: 0.44044513677575203 Test-f1: 0.43275356654289626 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4723/20000] Train-Loss: 0.632946609998316 Test-Loss: 0.19710298382136818 Train-f1: 0.4346722593961183 Test-f1: 0.41827724124475657 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4724/20000] Train-Loss: 0.6396471805396639 Test-Loss: 0.18936962795780232 Train-f1: 0.43850962740685173 Test-f1: 0.43392041748206134 Train-acc: 0.7549668874172185 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [4725/20000] Train-Loss: 0.6374545819837839 Test-Loss: 0.18864859992709346 Train-f1: 0.45439741404658085 Test-f1: 0.4055205890822329 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [4726/20000] Train-Loss: 0.6384234688712054 Test-Loss: 0.18948814202848022 Train-f1: 0.43267145096690934 Test-f1: 0.403968253968254 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4727/20000] Train-Loss: 0.6344244581933948 Test-Loss: 0.18860220682984138 Train-f1: 0.44824463049106217 Test-f1: 0.41333670012415047 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4728/20000] Train-Loss: 0.6386247872390381 Test-Loss: 0.1889562421965329 Train-f1: 0.4309324927764644 Test-f1: 0.437513491075135 Train-acc: 0.7450331125827815 Test-acc: 0.75\n",
      "Epoch [4729/20000] Train-Loss: 0.6375182178995721 Test-Loss: 0.1897854596300732 Train-f1: 0.41400861866752453 Test-f1: 0.40378576378576375 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4730/20000] Train-Loss: 0.6385450076982455 Test-Loss: 0.18902343741852218 Train-f1: 0.4309816293161489 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4731/20000] Train-Loss: 0.6374763101827513 Test-Loss: 0.18861795464208922 Train-f1: 0.4244001668556828 Test-f1: 0.41578947368421043 Train-acc: 0.7384105960264901 Test-acc: 0.7236842105263158\n",
      "Epoch [4732/20000] Train-Loss: 0.6364836407259127 Test-Loss: 0.1889006565607324 Train-f1: 0.43707685046863853 Test-f1: 0.3992867589605914 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [4733/20000] Train-Loss: 0.6338349083720328 Test-Loss: 0.1939206369109813 Train-f1: 0.4381802081766996 Test-f1: 0.4134165729574624 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4734/20000] Train-Loss: 0.6379328829687071 Test-Loss: 0.19797110756742234 Train-f1: 0.4432495402227595 Test-f1: 0.397674959311788 Train-acc: 0.7582781456953642 Test-acc: 0.6842105263157895\n",
      "Epoch [4735/20000] Train-Loss: 0.6416005051318987 Test-Loss: 0.18871909142062068 Train-f1: 0.43362318840579706 Test-f1: 0.41304492789864133 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4736/20000] Train-Loss: 0.6361061220377175 Test-Loss: 0.18929884120284768 Train-f1: 0.42408521620457884 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4737/20000] Train-Loss: 0.6359424659805513 Test-Loss: 0.18841118856834427 Train-f1: 0.44611248966087674 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [4738/20000] Train-Loss: 0.6328348296669312 Test-Loss: 0.18844997710169448 Train-f1: 0.43052476168531373 Test-f1: 0.42702535176346623 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4739/20000] Train-Loss: 0.6359590524643818 Test-Loss: 0.18854332379663905 Train-f1: 0.4374167085740501 Test-f1: 0.4256306381556967 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [4740/20000] Train-Loss: 0.6352162510399008 Test-Loss: 0.18845590270516405 Train-f1: 0.4318484383000512 Test-f1: 0.41304492789864133 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4741/20000] Train-Loss: 0.6355595187881509 Test-Loss: 0.1885342328634587 Train-f1: 0.41760326099762024 Test-f1: 0.4163765578329386 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4742/20000] Train-Loss: 0.6326076761743467 Test-Loss: 0.19397931457067902 Train-f1: 0.4253985067490529 Test-f1: 0.4134165729574624 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4743/20000] Train-Loss: 0.6355671310220768 Test-Loss: 0.18903475361932548 Train-f1: 0.4326716588452449 Test-f1: 0.4202012072434608 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4744/20000] Train-Loss: 0.6382538721649845 Test-Loss: 0.18843547058628993 Train-f1: 0.44017532926370534 Test-f1: 0.41304492789864133 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4745/20000] Train-Loss: 0.6357393277603857 Test-Loss: 0.18965955175569446 Train-f1: 0.42008441002783653 Test-f1: 0.40378576378576375 Train-acc: 0.7317880794701986 Test-acc: 0.6973684210526315\n",
      "Epoch [4746/20000] Train-Loss: 0.6356582899575969 Test-Loss: 0.188560532827264 Train-f1: 0.4423681515617 Test-f1: 0.4303918200739589 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [4747/20000] Train-Loss: 0.633866534988423 Test-Loss: 0.18896254498685933 Train-f1: 0.44641377866645493 Test-f1: 0.41389826908781996 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [4748/20000] Train-Loss: 0.6366186019777794 Test-Loss: 0.18902859370502853 Train-f1: 0.427181103364849 Test-f1: 0.41587454059472106 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4749/20000] Train-Loss: 0.6372526479734497 Test-Loss: 0.18841302153916473 Train-f1: 0.4432479374728615 Test-f1: 0.41599511599511596 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4750/20000] Train-Loss: 0.6372490159584385 Test-Loss: 0.18855044913238858 Train-f1: 0.44356121214925287 Test-f1: 0.4098471168893704 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4751/20000] Train-Loss: 0.6344512223405331 Test-Loss: 0.19164043125384242 Train-f1: 0.4259358719022933 Test-f1: 0.40361535448491975 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4752/20000] Train-Loss: 0.6349442891492315 Test-Loss: 0.18847397570209343 Train-f1: 0.43634310418030714 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4753/20000] Train-Loss: 0.6332224462829265 Test-Loss: 0.18888078582348308 Train-f1: 0.43290714510949657 Test-f1: 0.406845725515886 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4754/20000] Train-Loss: 0.6325355397745107 Test-Loss: 0.188286221809558 Train-f1: 0.4430242454230579 Test-f1: 0.41599511599511596 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [4755/20000] Train-Loss: 0.6349407202120713 Test-Loss: 0.18936551368721033 Train-f1: 0.42362050163576886 Test-f1: 0.42623992593060206 Train-acc: 0.7384105960264901 Test-acc: 0.7368421052631579\n",
      "Epoch [4756/20000] Train-Loss: 0.6318644061689509 Test-Loss: 0.18888319154286223 Train-f1: 0.44111176086494763 Test-f1: 0.4098471168893704 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4757/20000] Train-Loss: 0.6334611893096717 Test-Loss: 0.18875069866692115 Train-f1: 0.4356792144026186 Test-f1: 0.406845725515886 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4758/20000] Train-Loss: 0.6343608266223342 Test-Loss: 0.18830995288321636 Train-f1: 0.4381546637249338 Test-f1: 0.41599511599511596 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4759/20000] Train-Loss: 0.635313988641533 Test-Loss: 0.19017342535671886 Train-f1: 0.4369711794347313 Test-f1: 0.401038961038961 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4760/20000] Train-Loss: 0.6348303232761721 Test-Loss: 0.19293299753290014 Train-f1: 0.43959607691315006 Test-f1: 0.4096494355317885 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4761/20000] Train-Loss: 0.6347319354043313 Test-Loss: 0.18950247093167333 Train-f1: 0.4338297179336851 Test-f1: 0.4163765578329386 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4762/20000] Train-Loss: 0.6366134388618533 Test-Loss: 0.18874605577090378 Train-f1: 0.4402934223334724 Test-f1: 0.4163765578329386 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4763/20000] Train-Loss: 0.6350704953030101 Test-Loss: 0.1882528041080666 Train-f1: 0.432500617741537 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4764/20000] Train-Loss: 0.6356782435187964 Test-Loss: 0.1909980531910906 Train-f1: 0.4301252392585669 Test-f1: 0.41231983187612753 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4765/20000] Train-Loss: 0.6340619125561158 Test-Loss: 0.19232579172829897 Train-f1: 0.436783586866031 Test-f1: 0.4213903743315508 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4766/20000] Train-Loss: 0.6345005002151688 Test-Loss: 0.1888950907328924 Train-f1: 0.44167357848093375 Test-f1: 0.4202012072434608 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4767/20000] Train-Loss: 0.6356938272133832 Test-Loss: 0.1909138666770048 Train-f1: 0.440676246830093 Test-f1: 0.40361535448491975 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4768/20000] Train-Loss: 0.6380259554167508 Test-Loss: 0.18823218656368715 Train-f1: 0.43635917935612395 Test-f1: 0.4055205890822329 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4769/20000] Train-Loss: 0.6354618157321046 Test-Loss: 0.18836916918055077 Train-f1: 0.4325503999336468 Test-f1: 0.42702535176346623 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4770/20000] Train-Loss: 0.6340555157302403 Test-Loss: 0.188679262597645 Train-f1: 0.4310659409533234 Test-f1: 0.4098130747500651 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4771/20000] Train-Loss: 0.6333603016274364 Test-Loss: 0.1890746218155639 Train-f1: 0.4263162596275235 Test-f1: 0.43392041748206134 Train-acc: 0.7417218543046358 Test-acc: 0.75\n",
      "Epoch [4772/20000] Train-Loss: 0.6346368426322074 Test-Loss: 0.19023569683991604 Train-f1: 0.44125896934116116 Test-f1: 0.41287828450619146 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4773/20000] Train-Loss: 0.6357832690796995 Test-Loss: 0.18874823350433273 Train-f1: 0.44165276039493817 Test-f1: 0.4098471168893704 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4774/20000] Train-Loss: 0.6364446066678007 Test-Loss: 0.18898397702030875 Train-f1: 0.42989310887354587 Test-f1: 0.4098471168893704 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4775/20000] Train-Loss: 0.6347067196913533 Test-Loss: 0.1888169167950025 Train-f1: 0.44156334231805927 Test-f1: 0.4098471168893704 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4776/20000] Train-Loss: 0.6342625538052573 Test-Loss: 0.18810202389569183 Train-f1: 0.4425465838509317 Test-f1: 0.42702535176346623 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [4777/20000] Train-Loss: 0.6333538902556063 Test-Loss: 0.1919984110498197 Train-f1: 0.44184687800527156 Test-f1: 0.40361535448491975 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4778/20000] Train-Loss: 0.6341923463377217 Test-Loss: 0.1888151785944434 Train-f1: 0.4384150211370087 Test-f1: 0.3992867589605914 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [4779/20000] Train-Loss: 0.6348604253883756 Test-Loss: 0.19246118321572522 Train-f1: 0.43796046720575027 Test-f1: 0.41349845201238383 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4780/20000] Train-Loss: 0.6351539880029424 Test-Loss: 0.18907076599486966 Train-f1: 0.44057417182163416 Test-f1: 0.403968253968254 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4781/20000] Train-Loss: 0.6358908139656743 Test-Loss: 0.18798305961475328 Train-f1: 0.4387344589438883 Test-f1: 0.41304492789864133 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4782/20000] Train-Loss: 0.6340240598112125 Test-Loss: 0.1880110168125624 Train-f1: 0.4383323383705761 Test-f1: 0.42519841269841263 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4783/20000] Train-Loss: 0.6274645246875687 Test-Loss: 0.18803158771687487 Train-f1: 0.43156680775055334 Test-f1: 0.41333670012415047 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4784/20000] Train-Loss: 0.6343308692292956 Test-Loss: 0.19326108887441484 Train-f1: 0.44001366239072504 Test-f1: 0.4072359978474148 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4785/20000] Train-Loss: 0.6335004239213362 Test-Loss: 0.19145784870511734 Train-f1: 0.4368204492801321 Test-f1: 0.41369152821963 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4786/20000] Train-Loss: 0.6329033246838633 Test-Loss: 0.18793555418899893 Train-f1: 0.43065427586385674 Test-f1: 0.42519841269841263 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4787/20000] Train-Loss: 0.6342042345057285 Test-Loss: 0.18814393702837998 Train-f1: 0.44452189136601633 Test-f1: 0.4098130747500651 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [4788/20000] Train-Loss: 0.6344626586470853 Test-Loss: 0.18981868415180736 Train-f1: 0.42719277504105096 Test-f1: 0.42086154491143485 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4789/20000] Train-Loss: 0.6329474241951552 Test-Loss: 0.18794831366108602 Train-f1: 0.440687432867884 Test-f1: 0.41333670012415047 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4790/20000] Train-Loss: 0.6343764064381998 Test-Loss: 0.1878785840157609 Train-f1: 0.42742045276037216 Test-f1: 0.42519841269841263 Train-acc: 0.7384105960264901 Test-acc: 0.7368421052631579\n",
      "Epoch [4791/20000] Train-Loss: 0.6334376570166361 Test-Loss: 0.18834771894296531 Train-f1: 0.4355654761904762 Test-f1: 0.4202012072434608 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4792/20000] Train-Loss: 0.6323058385871425 Test-Loss: 0.18986695072259774 Train-f1: 0.44047176244498176 Test-f1: 0.41421602787456446 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4793/20000] Train-Loss: 0.6327960902097197 Test-Loss: 0.19076324296477384 Train-f1: 0.4263406710768091 Test-f1: 0.40361535448491975 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4794/20000] Train-Loss: 0.6353139345947081 Test-Loss: 0.18791806053351576 Train-f1: 0.4319781444336604 Test-f1: 0.41599511599511596 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4795/20000] Train-Loss: 0.6311206977136692 Test-Loss: 0.18809704986310813 Train-f1: 0.4350378930450386 Test-f1: 0.41304492789864133 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4796/20000] Train-Loss: 0.6303379375874768 Test-Loss: 0.190399940398567 Train-f1: 0.4175615826838758 Test-f1: 0.40361535448491975 Train-acc: 0.7350993377483444 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4797/20000] Train-Loss: 0.633249319767626 Test-Loss: 0.18782615297804578 Train-f1: 0.4341000499680382 Test-f1: 0.42702535176346623 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4798/20000] Train-Loss: 0.6329791333180674 Test-Loss: 0.18951622070143753 Train-f1: 0.44286603032403704 Test-f1: 0.39338345864661656 Train-acc: 0.7615894039735099 Test-acc: 0.6842105263157895\n",
      "Epoch [4799/20000] Train-Loss: 0.627547679760534 Test-Loss: 0.18929287803573588 Train-f1: 0.4324051168606329 Test-f1: 0.4163765578329386 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4800/20000] Train-Loss: 0.6324433370487358 Test-Loss: 0.18797044592635073 Train-f1: 0.4441026575385657 Test-f1: 0.4163765578329386 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [4801/20000] Train-Loss: 0.6326829078251617 Test-Loss: 0.1890651746916747 Train-f1: 0.4456855791962175 Test-f1: 0.403968253968254 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "Epoch [4802/20000] Train-Loss: 0.6333287043408065 Test-Loss: 0.18852851190902853 Train-f1: 0.42435720772738306 Test-f1: 0.4098471168893704 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4803/20000] Train-Loss: 0.6324307193055303 Test-Loss: 0.18997402589812373 Train-f1: 0.4531826050747408 Test-f1: 0.403968253968254 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [4804/20000] Train-Loss: 0.6349114784571106 Test-Loss: 0.19174552615560003 Train-f1: 0.42895335442713856 Test-f1: 0.4213903743315508 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4805/20000] Train-Loss: 0.6344809032065323 Test-Loss: 0.1878440513326045 Train-f1: 0.42081089247942555 Test-f1: 0.41578947368421043 Train-acc: 0.7350993377483444 Test-acc: 0.7236842105263158\n",
      "Epoch [4806/20000] Train-Loss: 0.629429524073615 Test-Loss: 0.18808506300070055 Train-f1: 0.44371447355407 Test-f1: 0.41389826908781996 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4807/20000] Train-Loss: 0.6315374676489859 Test-Loss: 0.18888087510836718 Train-f1: 0.42834773485365873 Test-f1: 0.40378576378576375 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4808/20000] Train-Loss: 0.6279911680397023 Test-Loss: 0.1903005465305592 Train-f1: 0.42912383558074396 Test-f1: 0.42623992593060206 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4809/20000] Train-Loss: 0.6331103435971024 Test-Loss: 0.18969455124197132 Train-f1: 0.4324819145665508 Test-f1: 0.42702535176346623 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4810/20000] Train-Loss: 0.6339500015082615 Test-Loss: 0.18820879897246515 Train-f1: 0.43713938129996527 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4811/20000] Train-Loss: 0.6343069456236654 Test-Loss: 0.18793639286682992 Train-f1: 0.4457004465192549 Test-f1: 0.4202012072434608 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4812/20000] Train-Loss: 0.6380356602144458 Test-Loss: 0.18769950309017597 Train-f1: 0.4415619569834345 Test-f1: 0.41304492789864133 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4813/20000] Train-Loss: 0.6329206240978912 Test-Loss: 0.1876624992854007 Train-f1: 0.4340142535563837 Test-f1: 0.41599511599511596 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4814/20000] Train-Loss: 0.6275627801826456 Test-Loss: 0.18817441388818065 Train-f1: 0.4386024227379755 Test-f1: 0.3992867589605914 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [4815/20000] Train-Loss: 0.6302051868223981 Test-Loss: 0.1882322936417264 Train-f1: 0.4465012678431221 Test-f1: 0.42702535176346623 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [4816/20000] Train-Loss: 0.6353526310670004 Test-Loss: 0.18772899247463112 Train-f1: 0.4274207433914807 Test-f1: 0.41578947368421043 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4817/20000] Train-Loss: 0.6303825291781356 Test-Loss: 0.18805136608070508 Train-f1: 0.4421929302773779 Test-f1: 0.4275134910751349 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4818/20000] Train-Loss: 0.6329259350737881 Test-Loss: 0.1875500180297325 Train-f1: 0.4329932716975652 Test-f1: 0.4265359477124183 Train-acc: 0.7450331125827815 Test-acc: 0.7368421052631579\n",
      "Epoch [4819/20000] Train-Loss: 0.6344438786002664 Test-Loss: 0.18870852460301493 Train-f1: 0.43745110783626034 Test-f1: 0.3900471648674143 Train-acc: 0.7516556291390728 Test-acc: 0.6842105263157895\n",
      "Epoch [4820/20000] Train-Loss: 0.6299351085314933 Test-Loss: 0.19225870020190036 Train-f1: 0.4271988260055224 Test-f1: 0.4134165729574624 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4821/20000] Train-Loss: 0.6330629418938343 Test-Loss: 0.18939897767967623 Train-f1: 0.4271587404003525 Test-f1: 0.41287828450619146 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4822/20000] Train-Loss: 0.6326186090664361 Test-Loss: 0.18790842712365402 Train-f1: 0.44307563231746905 Test-f1: 0.42623992593060206 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [4823/20000] Train-Loss: 0.6326828305313145 Test-Loss: 0.18981322146717328 Train-f1: 0.44018663862390534 Test-f1: 0.41369152821963 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4824/20000] Train-Loss: 0.63309745190066 Test-Loss: 0.19032407616654703 Train-f1: 0.4357113817203261 Test-f1: 0.41369152821963 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4825/20000] Train-Loss: 0.6327265543806638 Test-Loss: 0.188523582652616 Train-f1: 0.4434118489610263 Test-f1: 0.401038961038961 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4826/20000] Train-Loss: 0.6322551829658039 Test-Loss: 0.18750804786052566 Train-f1: 0.43758735868448106 Test-f1: 0.41304492789864133 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4827/20000] Train-Loss: 0.6322066759719993 Test-Loss: 0.1875057977229923 Train-f1: 0.4314769403118358 Test-f1: 0.41599511599511596 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4828/20000] Train-Loss: 0.6308008726723936 Test-Loss: 0.1902122146977887 Train-f1: 0.44315474561026164 Test-f1: 0.40361535448491975 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [4829/20000] Train-Loss: 0.6276379456512228 Test-Loss: 0.18750160971799887 Train-f1: 0.44426582610979776 Test-f1: 0.4163765578329386 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4830/20000] Train-Loss: 0.6303667209556086 Test-Loss: 0.18745220414867586 Train-f1: 0.4375020205415308 Test-f1: 0.42702535176346623 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [4831/20000] Train-Loss: 0.6335967849320341 Test-Loss: 0.1874768171854578 Train-f1: 0.44170003416511194 Test-f1: 0.41599511599511596 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4832/20000] Train-Loss: 0.6331471712479843 Test-Loss: 0.18741039650729352 Train-f1: 0.43870470184254096 Test-f1: 0.42519841269841263 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4833/20000] Train-Loss: 0.6322522184719892 Test-Loss: 0.1905480522000579 Train-f1: 0.4398426685198948 Test-f1: 0.4026501035196688 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [4834/20000] Train-Loss: 0.6321742815776231 Test-Loss: 0.18806871297476607 Train-f1: 0.4457390173894016 Test-f1: 0.4081377151799687 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [4835/20000] Train-Loss: 0.6328533621979608 Test-Loss: 0.187479717529231 Train-f1: 0.43799283154121865 Test-f1: 0.41578947368421043 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4836/20000] Train-Loss: 0.6321540544707062 Test-Loss: 0.18916937497651043 Train-f1: 0.43473358377937465 Test-f1: 0.403968253968254 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4837/20000] Train-Loss: 0.6309911021285093 Test-Loss: 0.18974617686378817 Train-f1: 0.4327795980469381 Test-f1: 0.401038961038961 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "Epoch [4838/20000] Train-Loss: 0.6306418954379914 Test-Loss: 0.18743394640561475 Train-f1: 0.4296198156682028 Test-f1: 0.41373737373737374 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4839/20000] Train-Loss: 0.6295824816172512 Test-Loss: 0.1874089063271388 Train-f1: 0.4246371136173767 Test-f1: 0.4265359477124183 Train-acc: 0.7384105960264901 Test-acc: 0.7368421052631579\n",
      "Epoch [4840/20000] Train-Loss: 0.6315451385712179 Test-Loss: 0.1885896033800084 Train-f1: 0.42887129179810357 Test-f1: 0.4016385048643113 Train-acc: 0.7384105960264901 Test-acc: 0.6973684210526315\n",
      "Epoch [4841/20000] Train-Loss: 0.6317823570909905 Test-Loss: 0.1874422745613053 Train-f1: 0.43330875576036865 Test-f1: 0.42672672672672673 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [4842/20000] Train-Loss: 0.6307092836639081 Test-Loss: 0.18794353997927912 Train-f1: 0.4410663310582958 Test-f1: 0.4202012072434608 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4843/20000] Train-Loss: 0.6316992431066576 Test-Loss: 0.18736635890853925 Train-f1: 0.4421890201798922 Test-f1: 0.41373737373737374 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4844/20000] Train-Loss: 0.6319501618451601 Test-Loss: 0.18727222480547123 Train-f1: 0.4374961500135882 Test-f1: 0.4265359477124183 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4845/20000] Train-Loss: 0.6323823903082758 Test-Loss: 0.18813126286176984 Train-f1: 0.42927143778207605 Test-f1: 0.403968253968254 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4846/20000] Train-Loss: 0.6304100870526393 Test-Loss: 0.18748884728858034 Train-f1: 0.4404761904761905 Test-f1: 0.4275134910751349 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4847/20000] Train-Loss: 0.6316890479130767 Test-Loss: 0.1874194224228967 Train-f1: 0.43416890965464655 Test-f1: 0.41599511599511596 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4848/20000] Train-Loss: 0.6301754689262874 Test-Loss: 0.18884149310455248 Train-f1: 0.4400310876590175 Test-f1: 0.41421602787456446 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4849/20000] Train-Loss: 0.6300049865783746 Test-Loss: 0.18827518987401046 Train-f1: 0.4418003362830828 Test-f1: 0.403968253968254 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4850/20000] Train-Loss: 0.6288239279143792 Test-Loss: 0.1873947573519679 Train-f1: 0.4391377843106916 Test-f1: 0.4275134910751349 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4851/20000] Train-Loss: 0.6274998345457047 Test-Loss: 0.18767856779874578 Train-f1: 0.43021961622351934 Test-f1: 0.38317460317460317 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4852/20000] Train-Loss: 0.6332031481959359 Test-Loss: 0.18899039078355045 Train-f1: 0.4222062552831784 Test-f1: 0.41287828450619146 Train-acc: 0.7350993377483444 Test-acc: 0.7105263157894737\n",
      "Epoch [4853/20000] Train-Loss: 0.6316722801927248 Test-Loss: 0.1884854272627144 Train-f1: 0.4341862135642729 Test-f1: 0.40378576378576375 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [4854/20000] Train-Loss: 0.631929767442852 Test-Loss: 0.18751114862454688 Train-f1: 0.43520037421984137 Test-f1: 0.4098471168893704 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4855/20000] Train-Loss: 0.6292840225855741 Test-Loss: 0.18846486292738998 Train-f1: 0.43629338446788113 Test-f1: 0.41287828450619146 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4856/20000] Train-Loss: 0.6311853989022067 Test-Loss: 0.18715885530825627 Train-f1: 0.4495826569781075 Test-f1: 0.42519841269841263 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [4857/20000] Train-Loss: 0.6291842892282633 Test-Loss: 0.19038216494964774 Train-f1: 0.4458769862690593 Test-f1: 0.40361535448491975 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [4858/20000] Train-Loss: 0.6315069779799419 Test-Loss: 0.1872354344021092 Train-f1: 0.4345779237159261 Test-f1: 0.41578947368421043 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4859/20000] Train-Loss: 0.6312648011076842 Test-Loss: 0.18780259303615157 Train-f1: 0.45338659147869675 Test-f1: 0.41020120724346076 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [4860/20000] Train-Loss: 0.6291167048982401 Test-Loss: 0.18901167619254114 Train-f1: 0.4385804132973944 Test-f1: 0.401038961038961 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [4861/20000] Train-Loss: 0.6313309281092696 Test-Loss: 0.18737714704326558 Train-f1: 0.44548498469155107 Test-f1: 0.4088710166306303 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [4862/20000] Train-Loss: 0.6300855299039935 Test-Loss: 0.18726065444247575 Train-f1: 0.4363512949039265 Test-f1: 0.41599511599511596 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4863/20000] Train-Loss: 0.6284849080779271 Test-Loss: 0.1874601827760358 Train-f1: 0.43522030045667764 Test-f1: 0.4092921163343698 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4864/20000] Train-Loss: 0.6307711568196148 Test-Loss: 0.18705273272494854 Train-f1: 0.43160388852551523 Test-f1: 0.41599511599511596 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4865/20000] Train-Loss: 0.6290946124764727 Test-Loss: 0.18812274273137164 Train-f1: 0.44047718893270493 Test-f1: 0.38923395445134573 Train-acc: 0.7549668874172185 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [4866/20000] Train-Loss: 0.6311066332943753 Test-Loss: 0.18703632749828614 Train-f1: 0.4371638598555963 Test-f1: 0.41304492789864133 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4867/20000] Train-Loss: 0.6283346101060634 Test-Loss: 0.18727590556371806 Train-f1: 0.4335496778216587 Test-f1: 0.41304492789864133 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4868/20000] Train-Loss: 0.6294010240928092 Test-Loss: 0.1872869965651665 Train-f1: 0.4401334556301165 Test-f1: 0.41676894150705596 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4869/20000] Train-Loss: 0.629724554562479 Test-Loss: 0.19247382120836576 Train-f1: 0.43305430575979037 Test-f1: 0.4140247678018576 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4870/20000] Train-Loss: 0.6302798110191096 Test-Loss: 0.18730196160488147 Train-f1: 0.4463837535811045 Test-f1: 0.406845725515886 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [4871/20000] Train-Loss: 0.6266949832787269 Test-Loss: 0.18837540640707925 Train-f1: 0.44200790591148625 Test-f1: 0.4239189247219479 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [4872/20000] Train-Loss: 0.6294144558275371 Test-Loss: 0.18928153686029334 Train-f1: 0.4359252512394315 Test-f1: 0.401038961038961 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4873/20000] Train-Loss: 0.6303647319927065 Test-Loss: 0.1877316730525562 Train-f1: 0.4429770992366412 Test-f1: 0.4202012072434608 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [4874/20000] Train-Loss: 0.6306227250453526 Test-Loss: 0.18710144909431847 Train-f1: 0.43809634223729593 Test-f1: 0.41599511599511596 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4875/20000] Train-Loss: 0.6273028706439905 Test-Loss: 0.1874034571472819 Train-f1: 0.4426526474413587 Test-f1: 0.41676894150705596 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [4876/20000] Train-Loss: 0.6262124242750676 Test-Loss: 0.19145298122235774 Train-f1: 0.43311511183851603 Test-f1: 0.4096494355317885 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4877/20000] Train-Loss: 0.6311576439410734 Test-Loss: 0.18800148897104296 Train-f1: 0.4365113500597372 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4878/20000] Train-Loss: 0.6277844875193893 Test-Loss: 0.18793782872746412 Train-f1: 0.4353248195735836 Test-f1: 0.4206357225129832 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4879/20000] Train-Loss: 0.6302404708448557 Test-Loss: 0.18765590497953033 Train-f1: 0.44013002364066195 Test-f1: 0.4098471168893704 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4880/20000] Train-Loss: 0.6280526306746258 Test-Loss: 0.18829681731122533 Train-f1: 0.44278404122130793 Test-f1: 0.403968253968254 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4881/20000] Train-Loss: 0.626600966205324 Test-Loss: 0.19540446810386344 Train-f1: 0.44795417348608835 Test-f1: 0.41699225878330354 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4882/20000] Train-Loss: 0.6301604213417891 Test-Loss: 0.18694652697577802 Train-f1: 0.44118674429019256 Test-f1: 0.4163765578329386 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4883/20000] Train-Loss: 0.6263987253682358 Test-Loss: 0.18726070211929752 Train-f1: 0.4407764754616104 Test-f1: 0.3884160756501182 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [4884/20000] Train-Loss: 0.628209871038643 Test-Loss: 0.1922381739417616 Train-f1: 0.45137695852534565 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [4885/20000] Train-Loss: 0.6291160173639118 Test-Loss: 0.18763940005798435 Train-f1: 0.44605490825682176 Test-f1: 0.4202012072434608 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [4886/20000] Train-Loss: 0.6278684324788628 Test-Loss: 0.18701353498433146 Train-f1: 0.44019850862719523 Test-f1: 0.41304492789864133 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4887/20000] Train-Loss: 0.6285920460170737 Test-Loss: 0.18894883685008693 Train-f1: 0.4323873141537664 Test-f1: 0.40361535448491975 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4888/20000] Train-Loss: 0.6306406703816122 Test-Loss: 0.18708779553840432 Train-f1: 0.44751587467727305 Test-f1: 0.4163765578329385 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4889/20000] Train-Loss: 0.6284221099008889 Test-Loss: 0.18682860419792152 Train-f1: 0.43967759213727503 Test-f1: 0.42690058479532167 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4890/20000] Train-Loss: 0.6271295385071223 Test-Loss: 0.186900861703726 Train-f1: 0.4427285051840212 Test-f1: 0.42702535176346623 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [4891/20000] Train-Loss: 0.6292277767955662 Test-Loss: 0.1890632334903635 Train-f1: 0.4339926635822762 Test-f1: 0.4026501035196688 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4892/20000] Train-Loss: 0.6303742342313323 Test-Loss: 0.18686159753266077 Train-f1: 0.443280901372378 Test-f1: 0.41578947368421043 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [4893/20000] Train-Loss: 0.6270545947096385 Test-Loss: 0.1884481701577183 Train-f1: 0.4416280943284627 Test-f1: 0.4142160278745644 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4894/20000] Train-Loss: 0.6271724190243229 Test-Loss: 0.19002843563874883 Train-f1: 0.4400606485712869 Test-f1: 0.4141172328815349 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4895/20000] Train-Loss: 0.6272892811681967 Test-Loss: 0.1870660158879396 Train-f1: 0.4490757808499744 Test-f1: 0.41539227895392283 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4896/20000] Train-Loss: 0.6309779773722957 Test-Loss: 0.18736507599032792 Train-f1: 0.44351472237668477 Test-f1: 0.406845725515886 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4897/20000] Train-Loss: 0.6282989911232801 Test-Loss: 0.1866661548048421 Train-f1: 0.44293387251471084 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [4898/20000] Train-Loss: 0.6285842960184107 Test-Loss: 0.18682605047511566 Train-f1: 0.4440744718940207 Test-f1: 0.41539227895392283 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4899/20000] Train-Loss: 0.6258985318073441 Test-Loss: 0.1910742459466912 Train-f1: 0.42489997543080965 Test-f1: 0.4134165729574624 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [4900/20000] Train-Loss: 0.6271817701859533 Test-Loss: 0.18681395155360575 Train-f1: 0.43683998362078685 Test-f1: 0.40099616858237547 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4901/20000] Train-Loss: 0.6275216343298593 Test-Loss: 0.18680257126235666 Train-f1: 0.44892834495878253 Test-f1: 0.41373737373737374 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [4902/20000] Train-Loss: 0.6250271321508943 Test-Loss: 0.19138361173610768 Train-f1: 0.4433975784531752 Test-f1: 0.39394957983193274 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4903/20000] Train-Loss: 0.6307458488192937 Test-Loss: 0.18671150000898706 Train-f1: 0.43751715763910887 Test-f1: 0.41599511599511596 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4904/20000] Train-Loss: 0.627428751509581 Test-Loss: 0.1875275951982282 Train-f1: 0.43513092118906727 Test-f1: 0.41421602787456446 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4905/20000] Train-Loss: 0.6288665131516622 Test-Loss: 0.19075731507506904 Train-f1: 0.4333573500233923 Test-f1: 0.4231559290382819 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4906/20000] Train-Loss: 0.6241659410420907 Test-Loss: 0.18938775937424188 Train-f1: 0.4435200463163989 Test-f1: 0.4010628019323672 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4907/20000] Train-Loss: 0.6220375824015983 Test-Loss: 0.1996665760356982 Train-f1: 0.4459461758134108 Test-f1: 0.38493054115695624 Train-acc: 0.7615894039735099 Test-acc: 0.6710526315789473\n",
      "Epoch [4908/20000] Train-Loss: 0.6313837532422306 Test-Loss: 0.1870302209293824 Train-f1: 0.43244166402702994 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4909/20000] Train-Loss: 0.6268268867127397 Test-Loss: 0.18657387754231003 Train-f1: 0.4411614810679775 Test-f1: 0.41599511599511596 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4910/20000] Train-Loss: 0.6300799704303267 Test-Loss: 0.18654941637105912 Train-f1: 0.44013823632198185 Test-f1: 0.41599511599511596 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4911/20000] Train-Loss: 0.630557802903869 Test-Loss: 0.1876464577239939 Train-f1: 0.4369939443861693 Test-f1: 0.40378576378576375 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [4912/20000] Train-Loss: 0.6275735163599124 Test-Loss: 0.18887720250732803 Train-f1: 0.4405812708061948 Test-f1: 0.41369152821963 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4913/20000] Train-Loss: 0.6274277568971204 Test-Loss: 0.1865058025156525 Train-f1: 0.43881825902444416 Test-f1: 0.41304492789864133 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4914/20000] Train-Loss: 0.6275848666778676 Test-Loss: 0.18652650880924218 Train-f1: 0.4373870034764364 Test-f1: 0.42690058479532167 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4915/20000] Train-Loss: 0.6287581639261075 Test-Loss: 0.1900279697658937 Train-f1: 0.4326498346629187 Test-f1: 0.41202266953292954 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4916/20000] Train-Loss: 0.6298328780777872 Test-Loss: 0.18827867875807977 Train-f1: 0.4380940302563638 Test-f1: 0.41459657459657456 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4917/20000] Train-Loss: 0.6267217850938782 Test-Loss: 0.1883080042236654 Train-f1: 0.4363509986855183 Test-f1: 0.4030252100840336 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4918/20000] Train-Loss: 0.6270168963063597 Test-Loss: 0.18721549712671962 Train-f1: 0.4369404406703531 Test-f1: 0.4030252100840336 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [4919/20000] Train-Loss: 0.626873003157814 Test-Loss: 0.19052219504292447 Train-f1: 0.44621790594805644 Test-f1: 0.42402476780185766 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4920/20000] Train-Loss: 0.6289154140620493 Test-Loss: 0.19052902258255758 Train-f1: 0.43558115719406043 Test-f1: 0.4231559290382819 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4921/20000] Train-Loss: 0.6273164879855784 Test-Loss: 0.19025979760337713 Train-f1: 0.44283410138248847 Test-f1: 0.42474323062558356 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [4922/20000] Train-Loss: 0.6258765800071296 Test-Loss: 0.1867212973717961 Train-f1: 0.44461676316515025 Test-f1: 0.406845725515886 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4923/20000] Train-Loss: 0.6273962287940671 Test-Loss: 0.18853719779632752 Train-f1: 0.44674025906224807 Test-f1: 0.40361535448491975 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [4924/20000] Train-Loss: 0.6279903950633914 Test-Loss: 0.1885018022852585 Train-f1: 0.428432311146291 Test-f1: 0.42245169082125605 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4925/20000] Train-Loss: 0.6263583185119059 Test-Loss: 0.18860987396970058 Train-f1: 0.43706239576561556 Test-f1: 0.43399209486166007 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4926/20000] Train-Loss: 0.6281226802685616 Test-Loss: 0.18633707844274636 Train-f1: 0.4377219558964526 Test-f1: 0.42690058479532167 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [4927/20000] Train-Loss: 0.6282999765510069 Test-Loss: 0.18639722880405443 Train-f1: 0.4211530763934025 Test-f1: 0.41599511599511596 Train-acc: 0.7317880794701986 Test-acc: 0.7236842105263158\n",
      "Epoch [4928/20000] Train-Loss: 0.6258728659676419 Test-Loss: 0.18686406876742956 Train-f1: 0.42020678659777866 Test-f1: 0.44114805109728134 Train-acc: 0.7350993377483444 Test-acc: 0.75\n",
      "Epoch [4929/20000] Train-Loss: 0.6271906133441062 Test-Loss: 0.18840896947355296 Train-f1: 0.44090576090576095 Test-f1: 0.401038961038961 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [4930/20000] Train-Loss: 0.6278641081047185 Test-Loss: 0.18639663974765808 Train-f1: 0.43204370879942766 Test-f1: 0.41373737373737374 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4931/20000] Train-Loss: 0.6271259884898812 Test-Loss: 0.18768839728189124 Train-f1: 0.43041015546539185 Test-f1: 0.401038961038961 Train-acc: 0.7417218543046358 Test-acc: 0.6973684210526315\n",
      "Epoch [4932/20000] Train-Loss: 0.6231430676363477 Test-Loss: 0.18679563276635378 Train-f1: 0.43731187879965283 Test-f1: 0.4018569254185693 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4933/20000] Train-Loss: 0.6288869820682365 Test-Loss: 0.18619934318363998 Train-f1: 0.4340704965260126 Test-f1: 0.4265359477124183 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4934/20000] Train-Loss: 0.6259473210654185 Test-Loss: 0.1861980577834854 Train-f1: 0.44097030209933435 Test-f1: 0.4150529687115053 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4935/20000] Train-Loss: 0.627477522626938 Test-Loss: 0.18646896151929498 Train-f1: 0.4340802533503263 Test-f1: 0.4202012072434608 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [4936/20000] Train-Loss: 0.6274836508751869 Test-Loss: 0.18711127369534297 Train-f1: 0.43427267857949514 Test-f1: 0.401038961038961 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4937/20000] Train-Loss: 0.6263551073583168 Test-Loss: 0.1883100082794054 Train-f1: 0.4515351603762186 Test-f1: 0.40361535448491975 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [4938/20000] Train-Loss: 0.6252568111894341 Test-Loss: 0.18845630615667974 Train-f1: 0.4419047619047619 Test-f1: 0.4140786749482402 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4939/20000] Train-Loss: 0.622837357058217 Test-Loss: 0.18633074553416026 Train-f1: 0.4478510170807267 Test-f1: 0.4274425110041549 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [4940/20000] Train-Loss: 0.6276292524914039 Test-Loss: 0.1862666083658453 Train-f1: 0.44810368555920155 Test-f1: 0.41599511599511596 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4941/20000] Train-Loss: 0.6255366919829013 Test-Loss: 0.18729014352976175 Train-f1: 0.43510599078341017 Test-f1: 0.4144360902255639 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4942/20000] Train-Loss: 0.6201307556610557 Test-Loss: 0.18844217661182908 Train-f1: 0.44973409937650216 Test-f1: 0.43392041748206134 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [4943/20000] Train-Loss: 0.627905591899209 Test-Loss: 0.18624250335807774 Train-f1: 0.4406907605256521 Test-f1: 0.41676894150705596 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4944/20000] Train-Loss: 0.623470935857338 Test-Loss: 0.1929452661933851 Train-f1: 0.44351472237668477 Test-f1: 0.41699225878330354 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4945/20000] Train-Loss: 0.6255361669529832 Test-Loss: 0.18603886080568763 Train-f1: 0.43870223606572745 Test-f1: 0.4265359477124183 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4946/20000] Train-Loss: 0.6266719003671604 Test-Loss: 0.19389179343027682 Train-f1: 0.4502989574811881 Test-f1: 0.4060048594238112 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [4947/20000] Train-Loss: 0.6272639063784012 Test-Loss: 0.1867047096858757 Train-f1: 0.44359525366719615 Test-f1: 0.4274425110041549 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4948/20000] Train-Loss: 0.6268073282033733 Test-Loss: 0.18600119843646737 Train-f1: 0.43746324597388425 Test-f1: 0.4150529687115053 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4949/20000] Train-Loss: 0.6272839271191342 Test-Loss: 0.18600773194849068 Train-f1: 0.4391578552104935 Test-f1: 0.4265359477124183 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4950/20000] Train-Loss: 0.6249574645664383 Test-Loss: 0.18792320257261616 Train-f1: 0.4456435120486167 Test-f1: 0.4030252100840336 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "Epoch [4951/20000] Train-Loss: 0.6273454277780306 Test-Loss: 0.18620355152616372 Train-f1: 0.4315933314762382 Test-f1: 0.41373737373737374 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [4952/20000] Train-Loss: 0.6226581979074091 Test-Loss: 0.1860532721067871 Train-f1: 0.43928461301264266 Test-f1: 0.41373737373737374 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4953/20000] Train-Loss: 0.6245479908110855 Test-Loss: 0.18612207202017636 Train-f1: 0.44416638842968476 Test-f1: 0.42672672672672673 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [4954/20000] Train-Loss: 0.6286968234816661 Test-Loss: 0.18890305746011687 Train-f1: 0.4364417426545087 Test-f1: 0.4141172328815349 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4955/20000] Train-Loss: 0.6251361692634859 Test-Loss: 0.18688413071956828 Train-f1: 0.444306244713568 Test-f1: 0.41676894150705596 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [4956/20000] Train-Loss: 0.626574249147083 Test-Loss: 0.1871355043068326 Train-f1: 0.44255319148936173 Test-f1: 0.40378576378576375 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4957/20000] Train-Loss: 0.6273697293627113 Test-Loss: 0.18623701045994545 Train-f1: 0.43771723880500957 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4958/20000] Train-Loss: 0.6254165979008132 Test-Loss: 0.1860850046984575 Train-f1: 0.4390476190476191 Test-f1: 0.4150529687115053 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4959/20000] Train-Loss: 0.6266446966169327 Test-Loss: 0.18601783819327922 Train-f1: 0.44777508935379273 Test-f1: 0.41578947368421043 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4960/20000] Train-Loss: 0.6210555692951565 Test-Loss: 0.18737448880649196 Train-f1: 0.428484011932175 Test-f1: 0.43392041748206134 Train-acc: 0.7450331125827815 Test-acc: 0.75\n",
      "Epoch [4961/20000] Train-Loss: 0.624628826109167 Test-Loss: 0.18752946655002606 Train-f1: 0.43591193764070024 Test-f1: 0.401038961038961 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [4962/20000] Train-Loss: 0.6289082881969519 Test-Loss: 0.1863264489227236 Train-f1: 0.4364506020122052 Test-f1: 0.420623885560876 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4963/20000] Train-Loss: 0.6246136220552129 Test-Loss: 0.18697040004877138 Train-f1: 0.4308406828614163 Test-f1: 0.4144360902255639 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "Epoch [4964/20000] Train-Loss: 0.6250859376211475 Test-Loss: 0.18932441813016201 Train-f1: 0.4369939443861693 Test-f1: 0.42413273001508306 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4965/20000] Train-Loss: 0.6239256275993099 Test-Loss: 0.18935790845110714 Train-f1: 0.44024174656892806 Test-f1: 0.4231559290382819 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4966/20000] Train-Loss: 0.6228586115523811 Test-Loss: 0.18625691694021254 Train-f1: 0.446414909522091 Test-f1: 0.4274425110041549 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [4967/20000] Train-Loss: 0.6249157261319116 Test-Loss: 0.19153996678865343 Train-f1: 0.44841784403820933 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [4968/20000] Train-Loss: 0.6219175223213517 Test-Loss: 0.18865611271002897 Train-f1: 0.4442671394799055 Test-f1: 0.41202266953292954 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4969/20000] Train-Loss: 0.6248476197546823 Test-Loss: 0.1867278311750895 Train-f1: 0.42583114412537365 Test-f1: 0.41459657459657456 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "Epoch [4970/20000] Train-Loss: 0.6222210705078215 Test-Loss: 0.1870369079489 Train-f1: 0.44719612258632296 Test-f1: 0.44114805109728134 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [4971/20000] Train-Loss: 0.6253976717600664 Test-Loss: 0.18591412876929775 Train-f1: 0.43799070244621846 Test-f1: 0.40337454059472105 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [4972/20000] Train-Loss: 0.6262360948008189 Test-Loss: 0.18671119018980614 Train-f1: 0.4436855802374818 Test-f1: 0.420623885560876 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4973/20000] Train-Loss: 0.6259071704780896 Test-Loss: 0.1858200837961583 Train-f1: 0.43786018237082064 Test-f1: 0.4150529687115053 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4974/20000] Train-Loss: 0.6204566544662491 Test-Loss: 0.18766907351258366 Train-f1: 0.4400642242008466 Test-f1: 0.4275134910751349 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [4975/20000] Train-Loss: 0.628551389248174 Test-Loss: 0.18570119618313996 Train-f1: 0.44209872045198556 Test-f1: 0.4150529687115053 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [4976/20000] Train-Loss: 0.6243030669741805 Test-Loss: 0.18602358911279535 Train-f1: 0.44286603032403704 Test-f1: 0.4369302783936931 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "Epoch [4977/20000] Train-Loss: 0.623390080842945 Test-Loss: 0.18587216439432805 Train-f1: 0.44854926100477704 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4978/20000] Train-Loss: 0.6234317341883826 Test-Loss: 0.18644148179466313 Train-f1: 0.45267296622135333 Test-f1: 0.4274425110041549 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [4979/20000] Train-Loss: 0.6248063065620395 Test-Loss: 0.1857812372508373 Train-f1: 0.4406128474908454 Test-f1: 0.42690058479532167 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [4980/20000] Train-Loss: 0.6230822811460878 Test-Loss: 0.18586477027611994 Train-f1: 0.4485139022051774 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4981/20000] Train-Loss: 0.6224885797414549 Test-Loss: 0.18814595404785925 Train-f1: 0.42821434899986455 Test-f1: 0.4104721832632071 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4982/20000] Train-Loss: 0.6239107798966406 Test-Loss: 0.18873718212552734 Train-f1: 0.4379663444325098 Test-f1: 0.42402476780185766 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4983/20000] Train-Loss: 0.6251001201309886 Test-Loss: 0.18639000752761722 Train-f1: 0.46052672147995893 Test-f1: 0.4312146382568918 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [4984/20000] Train-Loss: 0.6254086549298515 Test-Loss: 0.18667256569777235 Train-f1: 0.42931547619047616 Test-f1: 0.4365339280025486 Train-acc: 0.7450331125827815 Test-acc: 0.75\n",
      "Epoch [4985/20000] Train-Loss: 0.6257516382333396 Test-Loss: 0.18688159447943029 Train-f1: 0.435193515704154 Test-f1: 0.41421602787456446 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [4986/20000] Train-Loss: 0.624608596189765 Test-Loss: 0.18572337816336817 Train-f1: 0.44430631276178467 Test-f1: 0.42690058479532167 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [4987/20000] Train-Loss: 0.6213936233983386 Test-Loss: 0.186476625337004 Train-f1: 0.43799283154121865 Test-f1: 0.41333670012415047 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [4988/20000] Train-Loss: 0.6223864257207651 Test-Loss: 0.18751730679061776 Train-f1: 0.45033343648460467 Test-f1: 0.40065876152832675 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [4989/20000] Train-Loss: 0.6239845857342025 Test-Loss: 0.18573366295583424 Train-f1: 0.4485731871325219 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [4990/20000] Train-Loss: 0.6222716226950832 Test-Loss: 0.1872585191847312 Train-f1: 0.44050953535198084 Test-f1: 0.4244261652957305 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [4991/20000] Train-Loss: 0.6218069469602654 Test-Loss: 0.18568770939131962 Train-f1: 0.4334083494637448 Test-f1: 0.4265359477124183 Train-acc: 0.7483443708609272 Test-acc: 0.7368421052631579\n",
      "Epoch [4992/20000] Train-Loss: 0.6200423301731096 Test-Loss: 0.19737316743460948 Train-f1: 0.447598243582194 Test-f1: 0.4055677655677655 Train-acc: 0.7649006622516556 Test-acc: 0.6842105263157895\n",
      "Epoch [4993/20000] Train-Loss: 0.6272432059384183 Test-Loss: 0.18697951235887636 Train-f1: 0.44417269682170346 Test-f1: 0.4016385048643113 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [4994/20000] Train-Loss: 0.6222758029452228 Test-Loss: 0.19297603462388557 Train-f1: 0.44077136276082 Test-f1: 0.4178951396861845 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [4995/20000] Train-Loss: 0.6232212717550767 Test-Loss: 0.18596636110742318 Train-f1: 0.4458271242186399 Test-f1: 0.40099616858237547 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4996/20000] Train-Loss: 0.6231256961038514 Test-Loss: 0.1856770322704703 Train-f1: 0.43946661735963755 Test-f1: 0.40099616858237547 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [4997/20000] Train-Loss: 0.6243236911008513 Test-Loss: 0.18824819565284487 Train-f1: 0.43417764268828096 Test-f1: 0.41202266953292954 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [4998/20000] Train-Loss: 0.6233299821290755 Test-Loss: 0.18560165093488717 Train-f1: 0.4327133139845424 Test-f1: 0.41304492789864133 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [4999/20000] Train-Loss: 0.6229037425252845 Test-Loss: 0.1901899696740675 Train-f1: 0.442695699214268 Test-f1: 0.41202266953292954 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5000/20000] Train-Loss: 0.6240291504082478 Test-Loss: 0.18576684647437172 Train-f1: 0.4401335994888992 Test-f1: 0.40099616858237547 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5001/20000] Train-Loss: 0.6228214794130934 Test-Loss: 0.1863001732428836 Train-f1: 0.4391377843106916 Test-f1: 0.41539227895392283 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5002/20000] Train-Loss: 0.6223832668991888 Test-Loss: 0.18971294091551358 Train-f1: 0.4496961225863229 Test-f1: 0.4231559290382819 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5003/20000] Train-Loss: 0.6234619141502669 Test-Loss: 0.18581442750724345 Train-f1: 0.43865976498126136 Test-f1: 0.41304492789864133 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5004/20000] Train-Loss: 0.6243965365235068 Test-Loss: 0.18698920024678803 Train-f1: 0.4449918287279083 Test-f1: 0.4141172328815349 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5005/20000] Train-Loss: 0.6211043333999021 Test-Loss: 0.18563560328635506 Train-f1: 0.44125896934116116 Test-f1: 0.4274425110041549 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [5006/20000] Train-Loss: 0.6226243823842214 Test-Loss: 0.1868450390310397 Train-f1: 0.42641943072671573 Test-f1: 0.41459657459657456 Train-acc: 0.7384105960264901 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5007/20000] Train-Loss: 0.6227227630121894 Test-Loss: 0.18548559165278186 Train-f1: 0.4418091766643884 Test-f1: 0.42672672672672673 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5008/20000] Train-Loss: 0.6249729217372254 Test-Loss: 0.18535464987020442 Train-f1: 0.4458465583020743 Test-f1: 0.4150529687115053 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5009/20000] Train-Loss: 0.6200733155994737 Test-Loss: 0.18741367169897477 Train-f1: 0.44309835139424775 Test-f1: 0.4198334875115634 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5010/20000] Train-Loss: 0.6229071737560895 Test-Loss: 0.18599344736023238 Train-f1: 0.4420110509957408 Test-f1: 0.4088710166306303 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5011/20000] Train-Loss: 0.6211730530378008 Test-Loss: 0.18534034545776146 Train-f1: 0.44804164588209716 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5012/20000] Train-Loss: 0.6210519123550189 Test-Loss: 0.18618814363938652 Train-f1: 0.4535210979368916 Test-f1: 0.40103491218276294 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5013/20000] Train-Loss: 0.6215418289004652 Test-Loss: 0.1859692587616441 Train-f1: 0.426745091007836 Test-f1: 0.40099616858237547 Train-acc: 0.7417218543046358 Test-acc: 0.7105263157894737\n",
      "Epoch [5014/20000] Train-Loss: 0.6234283806512063 Test-Loss: 0.18630333180285424 Train-f1: 0.4350817341862118 Test-f1: 0.4030252100840336 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [5015/20000] Train-Loss: 0.6227041716394591 Test-Loss: 0.19082310957460882 Train-f1: 0.4453341013824885 Test-f1: 0.4231559290382819 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5016/20000] Train-Loss: 0.6250966912574034 Test-Loss: 0.18549200355115109 Train-f1: 0.4515570451054322 Test-f1: 0.41578947368421043 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5017/20000] Train-Loss: 0.6237580902694354 Test-Loss: 0.18663463266099573 Train-f1: 0.4424442512853095 Test-f1: 0.4363714200094182 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5018/20000] Train-Loss: 0.6211570759412142 Test-Loss: 0.18522254481818878 Train-f1: 0.43172397793473216 Test-f1: 0.4265359477124183 Train-acc: 0.7417218543046358 Test-acc: 0.7368421052631579\n",
      "Epoch [5019/20000] Train-Loss: 0.6209662999824263 Test-Loss: 0.18548743337580684 Train-f1: 0.4535668027224082 Test-f1: 0.42690058479532167 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5020/20000] Train-Loss: 0.6211295316906715 Test-Loss: 0.1873862424266917 Train-f1: 0.4509843565184594 Test-f1: 0.3900471648674143 Train-acc: 0.7682119205298014 Test-acc: 0.6842105263157895\n",
      "Epoch [5021/20000] Train-Loss: 0.6229954076038278 Test-Loss: 0.18576397358356356 Train-f1: 0.44177695594528393 Test-f1: 0.4312146382568918 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5022/20000] Train-Loss: 0.6192643673301833 Test-Loss: 0.18658329322774486 Train-f1: 0.45825528088832773 Test-f1: 0.41333670012415047 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5023/20000] Train-Loss: 0.6223841635463166 Test-Loss: 0.18601533793996852 Train-f1: 0.43748042176822277 Test-f1: 0.420623885560876 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5024/20000] Train-Loss: 0.6238494831471673 Test-Loss: 0.18551345190065283 Train-f1: 0.44247316912223356 Test-f1: 0.4202012072434608 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5025/20000] Train-Loss: 0.6204030072270353 Test-Loss: 0.1867449900124661 Train-f1: 0.44728778467908903 Test-f1: 0.4026501035196688 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [5026/20000] Train-Loss: 0.6230622742779139 Test-Loss: 0.18566743021487322 Train-f1: 0.44360400444938825 Test-f1: 0.4312146382568918 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5027/20000] Train-Loss: 0.6192827595472679 Test-Loss: 0.1859496289348793 Train-f1: 0.4433819477373797 Test-f1: 0.42702535176346623 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5028/20000] Train-Loss: 0.6217345652570372 Test-Loss: 0.18565946847223752 Train-f1: 0.4361707523245985 Test-f1: 0.420623885560876 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5029/20000] Train-Loss: 0.623852395788312 Test-Loss: 0.1857865000521331 Train-f1: 0.4441639813820265 Test-f1: 0.4030252100840336 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [5030/20000] Train-Loss: 0.6223345254996661 Test-Loss: 0.18598801759022263 Train-f1: 0.444393627954779 Test-f1: 0.41459657459657456 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5031/20000] Train-Loss: 0.6190532655813393 Test-Loss: 0.18555102676136753 Train-f1: 0.4381379165974894 Test-f1: 0.40099616858237547 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [5032/20000] Train-Loss: 0.6228408952785821 Test-Loss: 0.18795600005874458 Train-f1: 0.4336845788941598 Test-f1: 0.4096494355317885 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [5033/20000] Train-Loss: 0.6249111015525383 Test-Loss: 0.1873898723349067 Train-f1: 0.4375083599035695 Test-f1: 0.40361535448491975 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [5034/20000] Train-Loss: 0.615582371908398 Test-Loss: 0.18509443421503535 Train-f1: 0.45302697302697303 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5035/20000] Train-Loss: 0.6225832486620322 Test-Loss: 0.1875614914534459 Train-f1: 0.443280901372378 Test-f1: 0.4141172328815349 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5036/20000] Train-Loss: 0.6227752020254406 Test-Loss: 0.1862836028928237 Train-f1: 0.43014801909956973 Test-f1: 0.38923395445134573 Train-acc: 0.7417218543046358 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [5037/20000] Train-Loss: 0.622893329454336 Test-Loss: 0.18505492617123706 Train-f1: 0.4436855802374818 Test-f1: 0.4150529687115053 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5038/20000] Train-Loss: 0.6216673707455809 Test-Loss: 0.18505102817493183 Train-f1: 0.45395549591152634 Test-f1: 0.42690058479532167 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5039/20000] Train-Loss: 0.6168569502353339 Test-Loss: 0.19648929766737536 Train-f1: 0.45577504859743306 Test-f1: 0.41171186537040194 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [5040/20000] Train-Loss: 0.6204909262241436 Test-Loss: 0.18535189661888796 Train-f1: 0.44621636753264304 Test-f1: 0.40099616858237547 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5041/20000] Train-Loss: 0.6217665886928421 Test-Loss: 0.18498921888825864 Train-f1: 0.4386904761904762 Test-f1: 0.4150529687115053 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5042/20000] Train-Loss: 0.624328665908227 Test-Loss: 0.1874170142215445 Train-f1: 0.436349525934333 Test-f1: 0.41202266953292954 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [5043/20000] Train-Loss: 0.6210636888771922 Test-Loss: 0.18649550550275484 Train-f1: 0.4510483870967742 Test-f1: 0.41369152821963 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5044/20000] Train-Loss: 0.6210948169093274 Test-Loss: 0.18510657648934806 Train-f1: 0.44704383400915393 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5045/20000] Train-Loss: 0.6220933705378442 Test-Loss: 0.1876404837568905 Train-f1: 0.4420110509957408 Test-f1: 0.4096494355317885 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5046/20000] Train-Loss: 0.6217862344260174 Test-Loss: 0.18565196623617417 Train-f1: 0.44930859176410776 Test-f1: 0.420623885560876 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5047/20000] Train-Loss: 0.6214680373193086 Test-Loss: 0.18640032202855347 Train-f1: 0.44977132579650564 Test-f1: 0.41369152821963 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5048/20000] Train-Loss: 0.6177725186405164 Test-Loss: 0.19069831953616578 Train-f1: 0.4351286124499194 Test-f1: 0.4072359978474148 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "Epoch [5049/20000] Train-Loss: 0.6212184147186005 Test-Loss: 0.1857795657256985 Train-f1: 0.4427024660732526 Test-f1: 0.420623885560876 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5050/20000] Train-Loss: 0.6206965553486392 Test-Loss: 0.18486761879432498 Train-f1: 0.45072995969547697 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5051/20000] Train-Loss: 0.6240698894281395 Test-Loss: 0.18706242892098876 Train-f1: 0.44550304332259216 Test-f1: 0.41202266953292954 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5052/20000] Train-Loss: 0.6210407327461892 Test-Loss: 0.18591154613213598 Train-f1: 0.4357685220776967 Test-f1: 0.41459657459657456 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5053/20000] Train-Loss: 0.6203833376402477 Test-Loss: 0.1854861588105421 Train-f1: 0.44108441271499166 Test-f1: 0.4202012072434608 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5054/20000] Train-Loss: 0.6211204043548043 Test-Loss: 0.19084133270361198 Train-f1: 0.4387344589438883 Test-f1: 0.4189247560646381 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5055/20000] Train-Loss: 0.6196775425100812 Test-Loss: 0.18511985357082414 Train-f1: 0.44555321534800185 Test-f1: 0.420623885560876 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5056/20000] Train-Loss: 0.6207129867558461 Test-Loss: 0.18566049791164724 Train-f1: 0.45265049415992814 Test-f1: 0.420623885560876 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5057/20000] Train-Loss: 0.6247005573505444 Test-Loss: 0.1849433444811542 Train-f1: 0.4419047619047619 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5058/20000] Train-Loss: 0.6188928844647698 Test-Loss: 0.18503303663932255 Train-f1: 0.4424205971296343 Test-f1: 0.43768009768009775 Train-acc: 0.7582781456953642 Test-acc: 0.75\n",
      "Epoch [5059/20000] Train-Loss: 0.6192398284657719 Test-Loss: 0.18591638612784858 Train-f1: 0.4377665917247911 Test-f1: 0.41459657459657456 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5060/20000] Train-Loss: 0.6200575180068841 Test-Loss: 0.18573592312189002 Train-f1: 0.43746324597388425 Test-f1: 0.4098471168893704 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5061/20000] Train-Loss: 0.6205442407876876 Test-Loss: 0.18471120512181843 Train-f1: 0.4504628704628705 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5062/20000] Train-Loss: 0.6188729449148853 Test-Loss: 0.1896561985829909 Train-f1: 0.4472146102781608 Test-f1: 0.42413273001508306 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5063/20000] Train-Loss: 0.6237630232160202 Test-Loss: 0.1849291236831793 Train-f1: 0.4409420892849969 Test-f1: 0.4265359477124183 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [5064/20000] Train-Loss: 0.6178602589625287 Test-Loss: 0.18483360117102723 Train-f1: 0.44801770368934546 Test-f1: 0.42690058479532167 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5065/20000] Train-Loss: 0.6197447122943587 Test-Loss: 0.18916100442858694 Train-f1: 0.4366654588277924 Test-f1: 0.4240641711229946 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5066/20000] Train-Loss: 0.6204301697205425 Test-Loss: 0.18491545938281 Train-f1: 0.4518053774947358 Test-f1: 0.42690058479532167 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5067/20000] Train-Loss: 0.621713615799513 Test-Loss: 0.18668580512263327 Train-f1: 0.44312778877817643 Test-f1: 0.4096494355317885 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5068/20000] Train-Loss: 0.616128891879482 Test-Loss: 0.18572857270287263 Train-f1: 0.4412370855195543 Test-f1: 0.4163765578329385 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5069/20000] Train-Loss: 0.618656742295103 Test-Loss: 0.18632999752476076 Train-f1: 0.44374264715346695 Test-f1: 0.401038961038961 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [5070/20000] Train-Loss: 0.6186864906910442 Test-Loss: 0.18485635535972833 Train-f1: 0.4453341013824885 Test-f1: 0.4150529687115053 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5071/20000] Train-Loss: 0.6169979260718339 Test-Loss: 0.18799368808527903 Train-f1: 0.4405985634477254 Test-f1: 0.41990196078431374 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5072/20000] Train-Loss: 0.6204003331822407 Test-Loss: 0.18759291480518953 Train-f1: 0.44537393640841916 Test-f1: 0.4134165729574624 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5073/20000] Train-Loss: 0.618848543394059 Test-Loss: 0.1874520781870456 Train-f1: 0.43203547065969367 Test-f1: 0.4099744245524297 Train-acc: 0.7450331125827815 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5074/20000] Train-Loss: 0.6210370942335127 Test-Loss: 0.18468004018529202 Train-f1: 0.4485048573459155 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5075/20000] Train-Loss: 0.6195486085531209 Test-Loss: 0.18545895235743534 Train-f1: 0.4397149777646773 Test-f1: 0.41421602787456446 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5076/20000] Train-Loss: 0.6184701944174845 Test-Loss: 0.18551401223978006 Train-f1: 0.4568152007672966 Test-f1: 0.4309371379793915 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5077/20000] Train-Loss: 0.6205358096129003 Test-Loss: 0.1881829187465586 Train-f1: 0.44305143989673834 Test-f1: 0.4231559290382819 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5078/20000] Train-Loss: 0.6171727917090846 Test-Loss: 0.18620031223627745 Train-f1: 0.44401593011305246 Test-f1: 0.42408453775848043 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5079/20000] Train-Loss: 0.6158575884320729 Test-Loss: 0.18604568167476015 Train-f1: 0.4495505233813278 Test-f1: 0.401038961038961 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [5080/20000] Train-Loss: 0.6197374779469907 Test-Loss: 0.18477489765516825 Train-f1: 0.44007782253333855 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5081/20000] Train-Loss: 0.618945224038037 Test-Loss: 0.18466125634337707 Train-f1: 0.4425282251316315 Test-f1: 0.41578947368421043 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5082/20000] Train-Loss: 0.6180147182720028 Test-Loss: 0.19083636034427082 Train-f1: 0.450133919258345 Test-f1: 0.4178951396861845 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5083/20000] Train-Loss: 0.6185185578678115 Test-Loss: 0.18453917544050727 Train-f1: 0.4391407598622825 Test-f1: 0.41304492789864133 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5084/20000] Train-Loss: 0.6185939123423722 Test-Loss: 0.18591380698976484 Train-f1: 0.4445888810950301 Test-f1: 0.38923395445134573 Train-acc: 0.7615894039735099 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [5085/20000] Train-Loss: 0.6195564668822247 Test-Loss: 0.18444638316304385 Train-f1: 0.4425652841781874 Test-f1: 0.4031746031746032 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5086/20000] Train-Loss: 0.6203044034131402 Test-Loss: 0.1869931908423731 Train-f1: 0.4456439367996593 Test-f1: 0.42413273001508306 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5087/20000] Train-Loss: 0.6210988268719339 Test-Loss: 0.1855400374317519 Train-f1: 0.4392764145571258 Test-f1: 0.4206357225129832 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5088/20000] Train-Loss: 0.6190314996517753 Test-Loss: 0.1858426072446665 Train-f1: 0.44490015360983104 Test-f1: 0.4016385048643113 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [5089/20000] Train-Loss: 0.6177058437767539 Test-Loss: 0.18456992407481232 Train-f1: 0.44011904761904763 Test-f1: 0.41578947368421043 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5090/20000] Train-Loss: 0.6205885899764659 Test-Loss: 0.18591958430744535 Train-f1: 0.4458153583448663 Test-f1: 0.4141172328815349 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5091/20000] Train-Loss: 0.6174010890905971 Test-Loss: 0.18494660069106025 Train-f1: 0.44336883876357563 Test-f1: 0.43392041748206134 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "Epoch [5092/20000] Train-Loss: 0.6180888697640969 Test-Loss: 0.18597629510777058 Train-f1: 0.4415245892742229 Test-f1: 0.401038961038961 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [5093/20000] Train-Loss: 0.6187152872149054 Test-Loss: 0.1863173305749692 Train-f1: 0.44080877247967754 Test-f1: 0.4141172328815349 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5094/20000] Train-Loss: 0.6155323176613997 Test-Loss: 0.1851537477152714 Train-f1: 0.44994469743482723 Test-f1: 0.4392488262910798 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5095/20000] Train-Loss: 0.6094514131931585 Test-Loss: 0.18729877780698317 Train-f1: 0.4539778567593138 Test-f1: 0.4274425110041549 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5096/20000] Train-Loss: 0.6214026463313778 Test-Loss: 0.18460370098511192 Train-f1: 0.4396612847077576 Test-f1: 0.41578947368421043 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5097/20000] Train-Loss: 0.618436254237988 Test-Loss: 0.18475905163261744 Train-f1: 0.4499641962047978 Test-f1: 0.4031746031746032 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5098/20000] Train-Loss: 0.6216570289883532 Test-Loss: 0.18443759885945712 Train-f1: 0.44561444991789817 Test-f1: 0.42690058479532167 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5099/20000] Train-Loss: 0.6174888682115545 Test-Loss: 0.18886027187487547 Train-f1: 0.4503228600544297 Test-f1: 0.42402476780185766 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5100/20000] Train-Loss: 0.6217738337623638 Test-Loss: 0.18658560935521104 Train-f1: 0.4446032509377706 Test-f1: 0.4231559290382819 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5101/20000] Train-Loss: 0.62062043653973 Test-Loss: 0.18698348671628226 Train-f1: 0.4453259077814238 Test-f1: 0.4231559290382819 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5102/20000] Train-Loss: 0.618593556336125 Test-Loss: 0.1846637695463314 Train-f1: 0.44708214972171484 Test-f1: 0.4202012072434608 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5103/20000] Train-Loss: 0.617967572464142 Test-Loss: 0.18508873220648311 Train-f1: 0.43481574604157 Test-f1: 0.4206357225129832 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [5104/20000] Train-Loss: 0.617022405282191 Test-Loss: 0.18502599978862444 Train-f1: 0.4503228600544297 Test-f1: 0.4202012072434608 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5105/20000] Train-Loss: 0.6166471784915281 Test-Loss: 0.18584288454679107 Train-f1: 0.4382821582021773 Test-f1: 0.3900471648674143 Train-acc: 0.7516556291390728 Test-acc: 0.6842105263157895\n",
      "Epoch [5106/20000] Train-Loss: 0.6173476973635513 Test-Loss: 0.18649820070029727 Train-f1: 0.4438029420834013 Test-f1: 0.4005347593582888 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "Epoch [5107/20000] Train-Loss: 0.6187959336465176 Test-Loss: 0.18544416401214817 Train-f1: 0.45187051754215934 Test-f1: 0.41421602787456446 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5108/20000] Train-Loss: 0.617835692264015 Test-Loss: 0.18561872559654208 Train-f1: 0.4432479374728615 Test-f1: 0.41369152821963 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5109/20000] Train-Loss: 0.6174345758933958 Test-Loss: 0.18576801864804984 Train-f1: 0.4540704965260125 Test-f1: 0.4140786749482402 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5110/20000] Train-Loss: 0.6165577404137157 Test-Loss: 0.18561565158166893 Train-f1: 0.4440462533841142 Test-f1: 0.4275134910751349 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5111/20000] Train-Loss: 0.6173299725008735 Test-Loss: 0.18550333982136025 Train-f1: 0.44118674429019256 Test-f1: 0.41421602787456446 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5112/20000] Train-Loss: 0.6181766925606904 Test-Loss: 0.18633440496560372 Train-f1: 0.4485139022051774 Test-f1: 0.40261437908496733 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5113/20000] Train-Loss: 0.6164275779362299 Test-Loss: 0.18925790508927415 Train-f1: 0.4292560707557044 Test-f1: 0.4005347593582888 Train-acc: 0.7450331125827815 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [5114/20000] Train-Loss: 0.6179324241319948 Test-Loss: 0.18438951914614365 Train-f1: 0.45003635041802986 Test-f1: 0.4369302783936931 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5115/20000] Train-Loss: 0.6197499579824387 Test-Loss: 0.18676508253816587 Train-f1: 0.44293394995336044 Test-f1: 0.42402476780185766 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5116/20000] Train-Loss: 0.6191038279786504 Test-Loss: 0.18614625896662304 Train-f1: 0.4549530881448005 Test-f1: 0.4141172328815349 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5117/20000] Train-Loss: 0.617865777183473 Test-Loss: 0.1857004845187378 Train-f1: 0.44180769081826277 Test-f1: 0.4026501035196688 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [5118/20000] Train-Loss: 0.6164724009371318 Test-Loss: 0.18443247800102608 Train-f1: 0.4382058348718772 Test-f1: 0.4369302783936931 Train-acc: 0.7549668874172185 Test-acc: 0.75\n",
      "Epoch [5119/20000] Train-Loss: 0.6182707105609885 Test-Loss: 0.18675917072041875 Train-f1: 0.4489093701996928 Test-f1: 0.4005347593582888 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [5120/20000] Train-Loss: 0.619382250774187 Test-Loss: 0.1849233009065669 Train-f1: 0.44927143778207607 Test-f1: 0.420623885560876 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5121/20000] Train-Loss: 0.6167612458295259 Test-Loss: 0.1844883270194604 Train-f1: 0.4457502056027467 Test-f1: 0.41373737373737374 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5122/20000] Train-Loss: 0.6178065515175478 Test-Loss: 0.18469880644171693 Train-f1: 0.45302697302697303 Test-f1: 0.3971709042131578 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [5123/20000] Train-Loss: 0.6182280600964171 Test-Loss: 0.18414667032465717 Train-f1: 0.4425552619876004 Test-f1: 0.42690058479532167 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5124/20000] Train-Loss: 0.615066199650489 Test-Loss: 0.188670457305484 Train-f1: 0.4374167085740501 Test-f1: 0.4099744245524297 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5125/20000] Train-Loss: 0.6187331889654057 Test-Loss: 0.1840251676273914 Train-f1: 0.4471431921944088 Test-f1: 0.4031746031746032 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5126/20000] Train-Loss: 0.6172713483165851 Test-Loss: 0.18462419416604087 Train-f1: 0.4356515028644675 Test-f1: 0.420623885560876 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5127/20000] Train-Loss: 0.6105258136978631 Test-Loss: 0.19449510565127942 Train-f1: 0.4588418673525056 Test-f1: 0.41699225878330354 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5128/20000] Train-Loss: 0.6208995367427922 Test-Loss: 0.1845296689124818 Train-f1: 0.4453060334180316 Test-f1: 0.4206357225129832 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5129/20000] Train-Loss: 0.6158716301151997 Test-Loss: 0.18683691514455508 Train-f1: 0.4513379835129994 Test-f1: 0.4240641711229946 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5130/20000] Train-Loss: 0.6186071367371248 Test-Loss: 0.18481606612288243 Train-f1: 0.4468765045894 Test-f1: 0.420623885560876 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5131/20000] Train-Loss: 0.6157847101793533 Test-Loss: 0.18452400764523083 Train-f1: 0.4439751296287799 Test-f1: 0.4274425110041549 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5132/20000] Train-Loss: 0.6159673230667115 Test-Loss: 0.1849189684800885 Train-f1: 0.4524541462822113 Test-f1: 0.420623885560876 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5133/20000] Train-Loss: 0.6165834834191918 Test-Loss: 0.18395117593481644 Train-f1: 0.4485139022051774 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5134/20000] Train-Loss: 0.6171371598808008 Test-Loss: 0.18410563628449036 Train-f1: 0.44035689773507214 Test-f1: 0.42690058479532167 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [5135/20000] Train-Loss: 0.6167263596359934 Test-Loss: 0.18434170001072314 Train-f1: 0.45237191650853886 Test-f1: 0.4369302783936931 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5136/20000] Train-Loss: 0.6125548378726551 Test-Loss: 0.18802956856829656 Train-f1: 0.44662936647758433 Test-f1: 0.4072359978474148 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5137/20000] Train-Loss: 0.6145907014798935 Test-Loss: 0.18469689396325947 Train-f1: 0.44808344544654277 Test-f1: 0.4309371379793915 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5138/20000] Train-Loss: 0.6139298501392101 Test-Loss: 0.18523149131838546 Train-f1: 0.436783586866031 Test-f1: 0.4163765578329385 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5139/20000] Train-Loss: 0.6173217139776025 Test-Loss: 0.18531604268847374 Train-f1: 0.4402217760100912 Test-f1: 0.41369152821963 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5140/20000] Train-Loss: 0.6143746486162697 Test-Loss: 0.1847778547810568 Train-f1: 0.44903660674148566 Test-f1: 0.42702535176346623 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5141/20000] Train-Loss: 0.6131782704443494 Test-Loss: 0.19265492907152482 Train-f1: 0.45301161710007165 Test-f1: 0.4178951396861845 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5142/20000] Train-Loss: 0.6154996045114073 Test-Loss: 0.1849968709971414 Train-f1: 0.4574178354615867 Test-f1: 0.43392041748206134 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5143/20000] Train-Loss: 0.6176905023829964 Test-Loss: 0.18472951915339889 Train-f1: 0.44429466273230833 Test-f1: 0.41421602787456446 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5144/20000] Train-Loss: 0.6146251575141761 Test-Loss: 0.18422281428824308 Train-f1: 0.44841049102092034 Test-f1: 0.4275134910751349 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5145/20000] Train-Loss: 0.6160379921073262 Test-Loss: 0.1840484086425919 Train-f1: 0.45010711499308165 Test-f1: 0.4031746031746032 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5146/20000] Train-Loss: 0.6147863900724699 Test-Loss: 0.18494800715826318 Train-f1: 0.4504556778572647 Test-f1: 0.4309371379793915 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5147/20000] Train-Loss: 0.6191966112412476 Test-Loss: 0.18412139420697365 Train-f1: 0.44013110258661864 Test-f1: 0.4369302783936931 Train-acc: 0.7549668874172185 Test-acc: 0.75\n",
      "Epoch [5148/20000] Train-Loss: 0.6110551782722847 Test-Loss: 0.18418092596417 Train-f1: 0.4472146102781608 Test-f1: 0.4202012072434608 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5149/20000] Train-Loss: 0.6145856248301458 Test-Loss: 0.18468150030405067 Train-f1: 0.45075459470669055 Test-f1: 0.4468029372611329 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [5150/20000] Train-Loss: 0.6161803585415222 Test-Loss: 0.18418771470110942 Train-f1: 0.44013002364066195 Test-f1: 0.4202012072434608 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5151/20000] Train-Loss: 0.6152504254406476 Test-Loss: 0.1839302954815626 Train-f1: 0.4484155647857547 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5152/20000] Train-Loss: 0.6151148673357194 Test-Loss: 0.18777735618444583 Train-f1: 0.44377888155781253 Test-f1: 0.4005347593582888 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [5153/20000] Train-Loss: 0.6140777308236398 Test-Loss: 0.18373238647727377 Train-f1: 0.4422627484653142 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5154/20000] Train-Loss: 0.6163687775456639 Test-Loss: 0.18462700279035033 Train-f1: 0.4537267080745342 Test-f1: 0.420623885560876 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5155/20000] Train-Loss: 0.6144683150047647 Test-Loss: 0.19132757165545533 Train-f1: 0.4452273186531558 Test-f1: 0.39468501647606125 Train-acc: 0.7615894039735099 Test-acc: 0.6842105263157895\n",
      "Epoch [5156/20000] Train-Loss: 0.6156425046160703 Test-Loss: 0.1866875271345578 Train-f1: 0.446414909522091 Test-f1: 0.43389185977421274 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5157/20000] Train-Loss: 0.6151222143718298 Test-Loss: 0.18388361126043476 Train-f1: 0.4520587182241318 Test-f1: 0.4031746031746032 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5158/20000] Train-Loss: 0.6149754143791628 Test-Loss: 0.1877660894003758 Train-f1: 0.452834812378936 Test-f1: 0.42402476780185766 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5159/20000] Train-Loss: 0.6156606408160985 Test-Loss: 0.1889725393495438 Train-f1: 0.44640815886367485 Test-f1: 0.3977068109330211 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "Epoch [5160/20000] Train-Loss: 0.6151580096411023 Test-Loss: 0.18512116727393857 Train-f1: 0.4396036957211836 Test-f1: 0.38923395445134573 Train-acc: 0.7549668874172185 Test-acc: 0.6842105263157895\n",
      "Epoch [5161/20000] Train-Loss: 0.6132377815725265 Test-Loss: 0.18495847226284035 Train-f1: 0.45175080702866693 Test-f1: 0.41369152821963 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5162/20000] Train-Loss: 0.6148985373293926 Test-Loss: 0.1838305059557531 Train-f1: 0.4400179131213614 Test-f1: 0.4265359477124183 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [5163/20000] Train-Loss: 0.6144153039591916 Test-Loss: 0.18400679411768575 Train-f1: 0.4398466600987863 Test-f1: 0.4369302783936931 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "Epoch [5164/20000] Train-Loss: 0.6151772017978425 Test-Loss: 0.1877653658418094 Train-f1: 0.4487576109595245 Test-f1: 0.4231559290382819 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5165/20000] Train-Loss: 0.6155764528494776 Test-Loss: 0.1844126340875853 Train-f1: 0.44035689773507214 Test-f1: 0.41421602787456446 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5166/20000] Train-Loss: 0.6171934684892977 Test-Loss: 0.18375162456763297 Train-f1: 0.45243277002951104 Test-f1: 0.42690058479532167 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5167/20000] Train-Loss: 0.6173469134049575 Test-Loss: 0.18366144479617125 Train-f1: 0.44398059988915606 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5168/20000] Train-Loss: 0.6159392221807954 Test-Loss: 0.188144220003307 Train-f1: 0.4467562570711112 Test-f1: 0.4231559290382819 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5169/20000] Train-Loss: 0.6148542392504861 Test-Loss: 0.18392714703713062 Train-f1: 0.44929499343161583 Test-f1: 0.4369302783936931 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5170/20000] Train-Loss: 0.6135254382347101 Test-Loss: 0.18386031484972573 Train-f1: 0.4521746387352893 Test-f1: 0.4274425110041549 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5171/20000] Train-Loss: 0.6167289931139747 Test-Loss: 0.1836794807996081 Train-f1: 0.4379663444325098 Test-f1: 0.4150529687115053 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5172/20000] Train-Loss: 0.615111180277429 Test-Loss: 0.18391889496862343 Train-f1: 0.45553986338412644 Test-f1: 0.40337454059472105 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5173/20000] Train-Loss: 0.6159335635364724 Test-Loss: 0.18381967605608387 Train-f1: 0.435961665587058 Test-f1: 0.4369302783936931 Train-acc: 0.7549668874172185 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5174/20000] Train-Loss: 0.6154839381811258 Test-Loss: 0.1835410779985352 Train-f1: 0.4479964457469777 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5175/20000] Train-Loss: 0.6159815985157293 Test-Loss: 0.18565264408705115 Train-f1: 0.4370057890872706 Test-f1: 0.4231559290382819 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5176/20000] Train-Loss: 0.6136078901175666 Test-Loss: 0.18425654106483097 Train-f1: 0.4384281236005374 Test-f1: 0.401038961038961 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [5177/20000] Train-Loss: 0.6150830650700545 Test-Loss: 0.1864523178011566 Train-f1: 0.4454112089604661 Test-f1: 0.4096494355317885 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5178/20000] Train-Loss: 0.6175335288204472 Test-Loss: 0.18570739777763368 Train-f1: 0.447762021020553 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5179/20000] Train-Loss: 0.6137495154747307 Test-Loss: 0.18641262516207563 Train-f1: 0.44888544291529364 Test-f1: 0.41414891538010556 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5180/20000] Train-Loss: 0.6122852114191825 Test-Loss: 0.1845714940687964 Train-f1: 0.4544255744255744 Test-f1: 0.401038961038961 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [5181/20000] Train-Loss: 0.6145842308684968 Test-Loss: 0.1837410061506321 Train-f1: 0.45450111853558195 Test-f1: 0.42702535176346623 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5182/20000] Train-Loss: 0.6143471164612094 Test-Loss: 0.18755919584075137 Train-f1: 0.4573924178871496 Test-f1: 0.4231559290382819 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5183/20000] Train-Loss: 0.6126898679525281 Test-Loss: 0.18354157179063588 Train-f1: 0.45089354048675956 Test-f1: 0.42690058479532167 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5184/20000] Train-Loss: 0.6138067245389003 Test-Loss: 0.18341931921126128 Train-f1: 0.4482241317579663 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5185/20000] Train-Loss: 0.613800550166182 Test-Loss: 0.18405384303259345 Train-f1: 0.4533551554828151 Test-f1: 0.391454746803584 Train-acc: 0.7715231788079471 Test-acc: 0.6842105263157895\n",
      "Epoch [5186/20000] Train-Loss: 0.6192656842957671 Test-Loss: 0.1837903390956212 Train-f1: 0.43713938129996527 Test-f1: 0.4369302783936931 Train-acc: 0.7516556291390728 Test-acc: 0.75\n",
      "Epoch [5187/20000] Train-Loss: 0.613764066608241 Test-Loss: 0.18769055453162894 Train-f1: 0.44974618629808794 Test-f1: 0.41990196078431374 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5188/20000] Train-Loss: 0.6135734923364403 Test-Loss: 0.18515333153260402 Train-f1: 0.44584723553851074 Test-f1: 0.4231559290382819 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5189/20000] Train-Loss: 0.617396372123515 Test-Loss: 0.1846173944217334 Train-f1: 0.4356482446993396 Test-f1: 0.4141172328815349 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [5190/20000] Train-Loss: 0.6117550990273996 Test-Loss: 0.18412135665825766 Train-f1: 0.4418458666803038 Test-f1: 0.40337454059472105 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5191/20000] Train-Loss: 0.6140662281620431 Test-Loss: 0.18339587648918276 Train-f1: 0.44540997464074383 Test-f1: 0.42690058479532167 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5192/20000] Train-Loss: 0.6118869006532424 Test-Loss: 0.18439464958343643 Train-f1: 0.4482840607395768 Test-f1: 0.41421602787456446 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5193/20000] Train-Loss: 0.6113778101826723 Test-Loss: 0.18362018973458713 Train-f1: 0.4533273622928795 Test-f1: 0.42702535176346623 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5194/20000] Train-Loss: 0.6145806324592806 Test-Loss: 0.18438414726224706 Train-f1: 0.45031438370401167 Test-f1: 0.41421602787456446 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5195/20000] Train-Loss: 0.6162283471583317 Test-Loss: 0.18450533094626903 Train-f1: 0.4464070484626994 Test-f1: 0.41369152821963 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5196/20000] Train-Loss: 0.6157770667085904 Test-Loss: 0.18414691371219305 Train-f1: 0.45243277002951104 Test-f1: 0.41421602787456446 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5197/20000] Train-Loss: 0.6151436732286346 Test-Loss: 0.1835975316112617 Train-f1: 0.44298147259632537 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5198/20000] Train-Loss: 0.6133099890092522 Test-Loss: 0.18336205764819416 Train-f1: 0.45125913513010285 Test-f1: 0.42690058479532167 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5199/20000] Train-Loss: 0.61467959025282 Test-Loss: 0.18419767480945182 Train-f1: 0.4374587825997979 Test-f1: 0.420623885560876 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5200/20000] Train-Loss: 0.6137270927822417 Test-Loss: 0.19023550784336823 Train-f1: 0.4544527488414721 Test-f1: 0.41699225878330354 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5201/20000] Train-Loss: 0.6147492711738616 Test-Loss: 0.1885252384648243 Train-f1: 0.45261583061551997 Test-f1: 0.42402476780185766 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5202/20000] Train-Loss: 0.6127453535968937 Test-Loss: 0.18422061276888466 Train-f1: 0.4580521200154437 Test-f1: 0.44114805109728134 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5203/20000] Train-Loss: 0.6129889570169635 Test-Loss: 0.18454384500663712 Train-f1: 0.44707452725250285 Test-f1: 0.38923395445134573 Train-acc: 0.7615894039735099 Test-acc: 0.6842105263157895\n",
      "Epoch [5204/20000] Train-Loss: 0.6131357995680671 Test-Loss: 0.18643167580732192 Train-f1: 0.44533466533466537 Test-f1: 0.4099744245524297 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5205/20000] Train-Loss: 0.6137705228635942 Test-Loss: 0.18330105746675637 Train-f1: 0.44053832248536395 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5206/20000] Train-Loss: 0.6134109372027634 Test-Loss: 0.1833374497478485 Train-f1: 0.44616374192270236 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5207/20000] Train-Loss: 0.6147392584992982 Test-Loss: 0.18335456396073088 Train-f1: 0.4542676474383791 Test-f1: 0.4150529687115053 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5208/20000] Train-Loss: 0.6145248789376795 Test-Loss: 0.1873856186818914 Train-f1: 0.447726574500768 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5209/20000] Train-Loss: 0.6117767117405254 Test-Loss: 0.18558302234870644 Train-f1: 0.4496664367878716 Test-f1: 0.40656862745098044 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5210/20000] Train-Loss: 0.6128005763558594 Test-Loss: 0.1836680334232718 Train-f1: 0.44042739099766115 Test-f1: 0.420623885560876 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5211/20000] Train-Loss: 0.6153789102174937 Test-Loss: 0.18329244173710177 Train-f1: 0.45011583061552 Test-f1: 0.4369302783936931 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5212/20000] Train-Loss: 0.6114443896860757 Test-Loss: 0.18341294514210812 Train-f1: 0.4416651990286905 Test-f1: 0.44722222222222213 Train-acc: 0.7582781456953642 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [5213/20000] Train-Loss: 0.6118181327798784 Test-Loss: 0.18328036057163974 Train-f1: 0.4589872404405935 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5214/20000] Train-Loss: 0.6157189059814163 Test-Loss: 0.18349245476353632 Train-f1: 0.4436125077243761 Test-f1: 0.4202012072434608 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5215/20000] Train-Loss: 0.6110656592201134 Test-Loss: 0.18533537637328226 Train-f1: 0.4469847680468414 Test-f1: 0.42102910129569066 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5216/20000] Train-Loss: 0.6113978008349737 Test-Loss: 0.18305568190744897 Train-f1: 0.44731528124905573 Test-f1: 0.41373737373737374 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5217/20000] Train-Loss: 0.6090096847568534 Test-Loss: 0.18416135414130147 Train-f1: 0.4450879765395895 Test-f1: 0.4275134910751349 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5218/20000] Train-Loss: 0.6146500484668141 Test-Loss: 0.1858833013285923 Train-f1: 0.4421013304885445 Test-f1: 0.4134165729574624 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5219/20000] Train-Loss: 0.6118156875205017 Test-Loss: 0.1842510127059321 Train-f1: 0.4491401701746529 Test-f1: 0.4140786749482402 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5220/20000] Train-Loss: 0.6128260451795883 Test-Loss: 0.18292131797514602 Train-f1: 0.45613547702185897 Test-f1: 0.4150529687115053 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5221/20000] Train-Loss: 0.6116279200493353 Test-Loss: 0.18294688344162435 Train-f1: 0.4470285899506245 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5222/20000] Train-Loss: 0.611570826306759 Test-Loss: 0.18614391311551984 Train-f1: 0.4521880871065635 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5223/20000] Train-Loss: 0.6144850530509939 Test-Loss: 0.18296863532511437 Train-f1: 0.4541478326490414 Test-f1: 0.4031746031746032 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5224/20000] Train-Loss: 0.6132306517416845 Test-Loss: 0.1843041471972192 Train-f1: 0.4478379258680216 Test-f1: 0.42702535176346623 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5225/20000] Train-Loss: 0.610419447699321 Test-Loss: 0.18460365065972467 Train-f1: 0.45533811854699413 Test-f1: 0.4231559290382819 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5226/20000] Train-Loss: 0.6139575557659391 Test-Loss: 0.18359811519396502 Train-f1: 0.4457211986237226 Test-f1: 0.4016385048643113 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "Epoch [5227/20000] Train-Loss: 0.6118612520598343 Test-Loss: 0.18676294363064921 Train-f1: 0.4583489156502892 Test-f1: 0.41202266953292954 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5228/20000] Train-Loss: 0.6119202467529654 Test-Loss: 0.1840357538272915 Train-f1: 0.4423046165030898 Test-f1: 0.44084415584415576 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "Epoch [5229/20000] Train-Loss: 0.6141855720067757 Test-Loss: 0.18396907859939401 Train-f1: 0.45030721966205844 Test-f1: 0.41369152821963 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5230/20000] Train-Loss: 0.6151290833797586 Test-Loss: 0.1844873611266473 Train-f1: 0.4567190023906441 Test-f1: 0.40361535448491975 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [5231/20000] Train-Loss: 0.6108382827704966 Test-Loss: 0.18306890545560664 Train-f1: 0.4462971620521877 Test-f1: 0.4369302783936931 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [5232/20000] Train-Loss: 0.6138537775060136 Test-Loss: 0.18375470131541513 Train-f1: 0.4370057890872706 Test-f1: 0.44114805109728134 Train-acc: 0.7516556291390728 Test-acc: 0.75\n",
      "Epoch [5233/20000] Train-Loss: 0.6126137716156931 Test-Loss: 0.18513819874584647 Train-f1: 0.4504628704628705 Test-f1: 0.41202266953292954 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5234/20000] Train-Loss: 0.6127215631932711 Test-Loss: 0.18283536741595927 Train-f1: 0.4445746982066848 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5235/20000] Train-Loss: 0.6113019125729554 Test-Loss: 0.1853296842140944 Train-f1: 0.4442150537634409 Test-f1: 0.4099744245524297 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5236/20000] Train-Loss: 0.6117016787704153 Test-Loss: 0.1831886098458921 Train-f1: 0.4547873102729464 Test-f1: 0.4369302783936931 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5237/20000] Train-Loss: 0.6143181854559349 Test-Loss: 0.1840303727929108 Train-f1: 0.45183836975650477 Test-f1: 0.40361535448491975 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [5238/20000] Train-Loss: 0.6118233657723258 Test-Loss: 0.18280194886392223 Train-f1: 0.44477498621317635 Test-f1: 0.41599511599511596 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5239/20000] Train-Loss: 0.6093814628963681 Test-Loss: 0.18302603945283666 Train-f1: 0.453312629399586 Test-f1: 0.4381306381556967 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5240/20000] Train-Loss: 0.6131584068108673 Test-Loss: 0.18501621378406244 Train-f1: 0.44193810678055223 Test-f1: 0.4231559290382819 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5241/20000] Train-Loss: 0.6098989274327394 Test-Loss: 0.18270648564424552 Train-f1: 0.4484936167674866 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5242/20000] Train-Loss: 0.6072508338610012 Test-Loss: 0.18349467576473352 Train-f1: 0.4484065196548418 Test-f1: 0.41389826908781996 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5243/20000] Train-Loss: 0.6138071689949884 Test-Loss: 0.1839799484330736 Train-f1: 0.45332267876475746 Test-f1: 0.41369152821963 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5244/20000] Train-Loss: 0.6133909446689157 Test-Loss: 0.18291174042053543 Train-f1: 0.45804315826831665 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5245/20000] Train-Loss: 0.6127567764836057 Test-Loss: 0.1883748563562672 Train-f1: 0.45065338856685555 Test-f1: 0.4074109554921158 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [5246/20000] Train-Loss: 0.6114365350044287 Test-Loss: 0.18309596720989055 Train-f1: 0.45267080745341615 Test-f1: 0.44722222222222213 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [5247/20000] Train-Loss: 0.6121766252368848 Test-Loss: 0.1826351431739015 Train-f1: 0.4378690327034698 Test-f1: 0.4265359477124183 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [5248/20000] Train-Loss: 0.6173339557932234 Test-Loss: 0.18352684937576924 Train-f1: 0.4470285899506245 Test-f1: 0.4098471168893704 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5249/20000] Train-Loss: 0.6113831055967079 Test-Loss: 0.18368664743662508 Train-f1: 0.4444007670182167 Test-f1: 0.41369152821963 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5250/20000] Train-Loss: 0.6108826458988803 Test-Loss: 0.18494223855396694 Train-f1: 0.4471323048513061 Test-f1: 0.4231559290382819 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5251/20000] Train-Loss: 0.6128526651004028 Test-Loss: 0.18859004105469374 Train-f1: 0.448111524087755 Test-f1: 0.41699225878330354 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5252/20000] Train-Loss: 0.6120000887172172 Test-Loss: 0.18325201078141012 Train-f1: 0.4488902674386545 Test-f1: 0.41373737373737374 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5253/20000] Train-Loss: 0.6091129219714511 Test-Loss: 0.18262128110557824 Train-f1: 0.4491255218220017 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5254/20000] Train-Loss: 0.611962204984529 Test-Loss: 0.18268247999962636 Train-f1: 0.4453060334180316 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5255/20000] Train-Loss: 0.6103671724607167 Test-Loss: 0.18340926705585536 Train-f1: 0.459669634791586 Test-f1: 0.4202012072434608 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5256/20000] Train-Loss: 0.6126081467314282 Test-Loss: 0.18467187294558773 Train-f1: 0.4630267214799589 Test-f1: 0.43202390084379677 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5257/20000] Train-Loss: 0.6088552653134499 Test-Loss: 0.18733598978807833 Train-f1: 0.44642663569492835 Test-f1: 0.43202390084379677 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5258/20000] Train-Loss: 0.6123114031378403 Test-Loss: 0.18293877115631382 Train-f1: 0.4551997142943199 Test-f1: 0.4150529687115053 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5259/20000] Train-Loss: 0.6104608371280978 Test-Loss: 0.18297048824550582 Train-f1: 0.4509843565184594 Test-f1: 0.40099616858237547 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5260/20000] Train-Loss: 0.6101547431933422 Test-Loss: 0.18390869454724393 Train-f1: 0.4439609217920297 Test-f1: 0.41202266953292954 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5261/20000] Train-Loss: 0.609442922601191 Test-Loss: 0.18294000376101996 Train-f1: 0.45583121877207206 Test-f1: 0.4202012072434608 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5262/20000] Train-Loss: 0.6093313068309347 Test-Loss: 0.18298286962050242 Train-f1: 0.449001660051264 Test-f1: 0.40099616858237547 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5263/20000] Train-Loss: 0.6101744302664825 Test-Loss: 0.18625033347208617 Train-f1: 0.44126117890218214 Test-f1: 0.43389185977421274 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5264/20000] Train-Loss: 0.6111351721088634 Test-Loss: 0.18243892755666113 Train-f1: 0.4404416369688164 Test-f1: 0.4265359477124183 Train-acc: 0.7516556291390728 Test-acc: 0.7368421052631579\n",
      "Epoch [5265/20000] Train-Loss: 0.6109723022908651 Test-Loss: 0.1863847569620826 Train-f1: 0.44871493978684923 Test-f1: 0.4231559290382819 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5266/20000] Train-Loss: 0.6123512869796597 Test-Loss: 0.1824668137198303 Train-f1: 0.4477715212396955 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5267/20000] Train-Loss: 0.6108595274710285 Test-Loss: 0.18656738621429075 Train-f1: 0.4533105684510745 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5268/20000] Train-Loss: 0.6109686692334252 Test-Loss: 0.18239814619898861 Train-f1: 0.44112153690776507 Test-f1: 0.4150529687115053 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5269/20000] Train-Loss: 0.6086106962020551 Test-Loss: 0.18244128419585298 Train-f1: 0.4497722567287785 Test-f1: 0.4150529687115053 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5270/20000] Train-Loss: 0.6091075702576004 Test-Loss: 0.18751248402665416 Train-f1: 0.44412514909624734 Test-f1: 0.4240641711229946 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5271/20000] Train-Loss: 0.610470853788727 Test-Loss: 0.18559559487645938 Train-f1: 0.44835938512585216 Test-f1: 0.43202390084379677 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5272/20000] Train-Loss: 0.606314096162452 Test-Loss: 0.19186580658483568 Train-f1: 0.4481235843073298 Test-f1: 0.42620365912373614 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5273/20000] Train-Loss: 0.6134333997953619 Test-Loss: 0.18242719480043315 Train-f1: 0.43636488780801175 Test-f1: 0.41599511599511596 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [5274/20000] Train-Loss: 0.6086549636033876 Test-Loss: 0.18292304372722684 Train-f1: 0.45122042148370056 Test-f1: 0.42702535176346623 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5275/20000] Train-Loss: 0.6079710208988547 Test-Loss: 0.18500262090578995 Train-f1: 0.455646481178396 Test-f1: 0.4231559290382819 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5276/20000] Train-Loss: 0.6092289694346817 Test-Loss: 0.182745034814233 Train-f1: 0.45373764600179695 Test-f1: 0.4163765578329386 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5277/20000] Train-Loss: 0.6087972086566401 Test-Loss: 0.18960331086957577 Train-f1: 0.4487576109595245 Test-f1: 0.4060048594238112 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [5278/20000] Train-Loss: 0.6098091710084146 Test-Loss: 0.18620192545716424 Train-f1: 0.46112689294306614 Test-f1: 0.41202266953292954 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5279/20000] Train-Loss: 0.6110544997046936 Test-Loss: 0.1831597935614328 Train-f1: 0.44577785334972664 Test-f1: 0.4095000495000495 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5280/20000] Train-Loss: 0.6093205846119005 Test-Loss: 0.18542679084948235 Train-f1: 0.45315973948705385 Test-f1: 0.4034984520123839 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [5281/20000] Train-Loss: 0.6100062155559944 Test-Loss: 0.1838707232320899 Train-f1: 0.44636262513904335 Test-f1: 0.4231559290382819 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5282/20000] Train-Loss: 0.6065711949060629 Test-Loss: 0.1834415342510035 Train-f1: 0.44902716221887456 Test-f1: 0.4307137823416893 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5283/20000] Train-Loss: 0.6108891356226485 Test-Loss: 0.18287642944584187 Train-f1: 0.4499348857599671 Test-f1: 0.4098471168893704 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5284/20000] Train-Loss: 0.6118827998434563 Test-Loss: 0.18311946269937374 Train-f1: 0.43914316397760106 Test-f1: 0.41369152821963 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [5285/20000] Train-Loss: 0.608432731637248 Test-Loss: 0.18759447207847316 Train-f1: 0.43600926903134035 Test-f1: 0.3977068109330211 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [5286/20000] Train-Loss: 0.6080946314772183 Test-Loss: 0.1846937082929155 Train-f1: 0.4657869895308491 Test-f1: 0.3977068109330211 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [5287/20000] Train-Loss: 0.6091668078767882 Test-Loss: 0.18343105554471736 Train-f1: 0.44430631276178467 Test-f1: 0.41202266953292954 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5288/20000] Train-Loss: 0.608996054356486 Test-Loss: 0.1823513046669536 Train-f1: 0.44961883876357567 Test-f1: 0.41599511599511596 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5289/20000] Train-Loss: 0.6101344385656567 Test-Loss: 0.18223577000480762 Train-f1: 0.4466778323314826 Test-f1: 0.41373737373737374 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5290/20000] Train-Loss: 0.6113252289026052 Test-Loss: 0.18214448940681616 Train-f1: 0.4545654634065216 Test-f1: 0.4150529687115053 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5291/20000] Train-Loss: 0.6079850973974618 Test-Loss: 0.1828870752129659 Train-f1: 0.4579302720416947 Test-f1: 0.4468029372611329 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [5292/20000] Train-Loss: 0.6110680937283239 Test-Loss: 0.18220585275547913 Train-f1: 0.443608402004383 Test-f1: 0.4150529687115053 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5293/20000] Train-Loss: 0.6096848401664651 Test-Loss: 0.1822475554511358 Train-f1: 0.4542947599664018 Test-f1: 0.4265359477124183 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5294/20000] Train-Loss: 0.6057612049142455 Test-Loss: 0.19042853477530047 Train-f1: 0.4499482979798216 Test-f1: 0.39763872942977424 Train-acc: 0.7682119205298014 Test-acc: 0.6842105263157895\n",
      "Epoch [5295/20000] Train-Loss: 0.6116442116575032 Test-Loss: 0.1832318502038594 Train-f1: 0.44470944985191646 Test-f1: 0.41369152821963 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5296/20000] Train-Loss: 0.6085533488674753 Test-Loss: 0.18219485412662134 Train-f1: 0.4457643122643035 Test-f1: 0.41567460317460314 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5297/20000] Train-Loss: 0.6097593802379477 Test-Loss: 0.184397625212156 Train-f1: 0.4553334364846046 Test-f1: 0.41202266953292954 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5298/20000] Train-Loss: 0.6115848563443621 Test-Loss: 0.18208374109301415 Train-f1: 0.4464998228343425 Test-f1: 0.4150529687115053 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5299/20000] Train-Loss: 0.6098228886880327 Test-Loss: 0.18349086192485026 Train-f1: 0.46511339406861796 Test-f1: 0.4231559290382819 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [5300/20000] Train-Loss: 0.6073285044747798 Test-Loss: 0.1823270251657466 Train-f1: 0.4543455023099918 Test-f1: 0.4381306381556967 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5301/20000] Train-Loss: 0.6102316332029166 Test-Loss: 0.1821344213713329 Train-f1: 0.4580436817872854 Test-f1: 0.41599511599511596 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5302/20000] Train-Loss: 0.6097711716612468 Test-Loss: 0.18445935357305548 Train-f1: 0.44577252833211245 Test-f1: 0.4099744245524297 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5303/20000] Train-Loss: 0.6082903738232032 Test-Loss: 0.18260380588458208 Train-f1: 0.4575617138267728 Test-f1: 0.4202012072434608 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5304/20000] Train-Loss: 0.6068246524067717 Test-Loss: 0.1824987846828252 Train-f1: 0.4417966903073286 Test-f1: 0.41599511599511596 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5305/20000] Train-Loss: 0.6103852325209659 Test-Loss: 0.18213328675158463 Train-f1: 0.4441195235917295 Test-f1: 0.41599511599511596 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5306/20000] Train-Loss: 0.6085238591932257 Test-Loss: 0.18199666274333406 Train-f1: 0.4580233602875112 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5307/20000] Train-Loss: 0.6097800054663541 Test-Loss: 0.18457351916297976 Train-f1: 0.4630227743271222 Test-f1: 0.4231559290382819 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5308/20000] Train-Loss: 0.6052296308238476 Test-Loss: 0.18210656295405722 Train-f1: 0.45279387279387284 Test-f1: 0.4150529687115053 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5309/20000] Train-Loss: 0.6047044406904548 Test-Loss: 0.19010655654622738 Train-f1: 0.45596940389943297 Test-f1: 0.39763872942977424 Train-acc: 0.7781456953642384 Test-acc: 0.6842105263157895\n",
      "Epoch [5310/20000] Train-Loss: 0.6104257197321508 Test-Loss: 0.1822103166452169 Train-f1: 0.45288806175902946 Test-f1: 0.4369302783936931 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5311/20000] Train-Loss: 0.6089131773336307 Test-Loss: 0.18539133690106752 Train-f1: 0.45353570789791764 Test-f1: 0.41202266953292954 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5312/20000] Train-Loss: 0.6087809171881844 Test-Loss: 0.18306495328545463 Train-f1: 0.44708214972171484 Test-f1: 0.4231559290382819 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5313/20000] Train-Loss: 0.6092664768340369 Test-Loss: 0.18230585660697896 Train-f1: 0.4503239591949269 Test-f1: 0.4369302783936931 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5314/20000] Train-Loss: 0.6075470322995443 Test-Loss: 0.18184045813433464 Train-f1: 0.4491401701746529 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5315/20000] Train-Loss: 0.6094571645544065 Test-Loss: 0.18177425030518873 Train-f1: 0.4474460239521729 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5316/20000] Train-Loss: 0.6102002780077898 Test-Loss: 0.18228752261382306 Train-f1: 0.42957357131914814 Test-f1: 0.4202012072434608 Train-acc: 0.7417218543046358 Test-acc: 0.7236842105263158\n",
      "Epoch [5317/20000] Train-Loss: 0.6061589304957241 Test-Loss: 0.18839555251145973 Train-f1: 0.4488148942851161 Test-f1: 0.41419027312417506 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5318/20000] Train-Loss: 0.6102136487640044 Test-Loss: 0.1819661287450848 Train-f1: 0.44446105453299706 Test-f1: 0.4369302783936931 Train-acc: 0.7582781456953642 Test-acc: 0.75\n",
      "Epoch [5319/20000] Train-Loss: 0.6079210999161028 Test-Loss: 0.18332257759072929 Train-f1: 0.46014641439397846 Test-f1: 0.4231559290382819 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5320/20000] Train-Loss: 0.6107705343948987 Test-Loss: 0.18282317824643982 Train-f1: 0.4551662174303684 Test-f1: 0.41369152821963 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5321/20000] Train-Loss: 0.6095069274612518 Test-Loss: 0.18177268646039604 Train-f1: 0.4601233280552016 Test-f1: 0.4265359477124183 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5322/20000] Train-Loss: 0.6099970058491213 Test-Loss: 0.18262046058409645 Train-f1: 0.44804164588209716 Test-f1: 0.41369152821963 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5323/20000] Train-Loss: 0.6067177804627633 Test-Loss: 0.183121714237775 Train-f1: 0.44469888700772053 Test-f1: 0.41202266953292954 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5324/20000] Train-Loss: 0.6095614493747544 Test-Loss: 0.18179602697668848 Train-f1: 0.44268585131894483 Test-f1: 0.4150529687115053 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5325/20000] Train-Loss: 0.6059749924619791 Test-Loss: 0.1821276792310991 Train-f1: 0.4580986465902145 Test-f1: 0.3969267139479905 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5326/20000] Train-Loss: 0.6109171352546463 Test-Loss: 0.18316517128062623 Train-f1: 0.4467562570711112 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5327/20000] Train-Loss: 0.6080553039574886 Test-Loss: 0.18223367103941468 Train-f1: 0.44826392583572094 Test-f1: 0.4312146382568918 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5328/20000] Train-Loss: 0.6104655198699493 Test-Loss: 0.1821404871933472 Train-f1: 0.4545825971961361 Test-f1: 0.41599511599511596 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5329/20000] Train-Loss: 0.6069071074244405 Test-Loss: 0.1816601388801323 Train-f1: 0.44876272986915505 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5330/20000] Train-Loss: 0.6065009206503357 Test-Loss: 0.18219570883214553 Train-f1: 0.4458465583020743 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5331/20000] Train-Loss: 0.6079346023743096 Test-Loss: 0.18157395964452366 Train-f1: 0.45149359973858944 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5332/20000] Train-Loss: 0.6035613535056484 Test-Loss: 0.18259481662352767 Train-f1: 0.44477498621317635 Test-f1: 0.42702535176346623 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5333/20000] Train-Loss: 0.6067650649617676 Test-Loss: 0.18721609851396182 Train-f1: 0.4507924391130498 Test-f1: 0.42620365912373614 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5334/20000] Train-Loss: 0.6067975936143509 Test-Loss: 0.18374922436237592 Train-f1: 0.4440744718940207 Test-f1: 0.4231559290382819 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5335/20000] Train-Loss: 0.6069831931995553 Test-Loss: 0.18156545513384606 Train-f1: 0.4580233602875112 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5336/20000] Train-Loss: 0.608084775662603 Test-Loss: 0.18276013471694735 Train-f1: 0.44102464834172145 Test-f1: 0.40361535448491975 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [5337/20000] Train-Loss: 0.6076941632625065 Test-Loss: 0.18208507001397078 Train-f1: 0.45243488575996704 Test-f1: 0.43742690058479533 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5338/20000] Train-Loss: 0.6061423677785538 Test-Loss: 0.18274200392651582 Train-f1: 0.447394540942928 Test-f1: 0.41202266953292954 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5339/20000] Train-Loss: 0.6047957774746369 Test-Loss: 0.18394896017126644 Train-f1: 0.4518340064243539 Test-f1: 0.43389185977421274 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5340/20000] Train-Loss: 0.6050117802196913 Test-Loss: 0.18279414388586615 Train-f1: 0.449070129167512 Test-f1: 0.42702535176346623 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5341/20000] Train-Loss: 0.6087337685737273 Test-Loss: 0.1827410420498342 Train-f1: 0.4531682665347188 Test-f1: 0.4423050379572119 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5342/20000] Train-Loss: 0.6059441042846282 Test-Loss: 0.18527604866747252 Train-f1: 0.45261583061551997 Test-f1: 0.43202390084379677 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5343/20000] Train-Loss: 0.6061385053429781 Test-Loss: 0.18740578968494792 Train-f1: 0.44613293739640036 Test-f1: 0.41699225878330354 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5344/20000] Train-Loss: 0.6087116139535013 Test-Loss: 0.1833985095515094 Train-f1: 0.4485048573459155 Test-f1: 0.4034984520123839 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [5345/20000] Train-Loss: 0.6063476812794869 Test-Loss: 0.18220404174552976 Train-f1: 0.45056874160322435 Test-f1: 0.4095000495000495 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5346/20000] Train-Loss: 0.600818087517873 Test-Loss: 0.18216474077705483 Train-f1: 0.44673297307837334 Test-f1: 0.41539227895392283 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5347/20000] Train-Loss: 0.6101070468630795 Test-Loss: 0.18150757427351644 Train-f1: 0.4573924178871496 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5348/20000] Train-Loss: 0.6033068102199051 Test-Loss: 0.18720376756365054 Train-f1: 0.4433819477373797 Test-f1: 0.41202266953292954 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5349/20000] Train-Loss: 0.6072778463899334 Test-Loss: 0.18146144074656617 Train-f1: 0.44632616487455196 Test-f1: 0.41567460317460314 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5350/20000] Train-Loss: 0.6072358178163868 Test-Loss: 0.18394400820906945 Train-f1: 0.45138968815615527 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5351/20000] Train-Loss: 0.6078626554690459 Test-Loss: 0.1858876000119312 Train-f1: 0.4580267214799589 Test-f1: 0.4240641711229946 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5352/20000] Train-Loss: 0.6073400553756755 Test-Loss: 0.18167129946417154 Train-f1: 0.4520121139815537 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5353/20000] Train-Loss: 0.607086180198361 Test-Loss: 0.18134746913445662 Train-f1: 0.4530849974398361 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5354/20000] Train-Loss: 0.6087559359407481 Test-Loss: 0.18127601049363687 Train-f1: 0.44428573744424266 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5355/20000] Train-Loss: 0.6102132893561717 Test-Loss: 0.1817449561004991 Train-f1: 0.45261583061551997 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5356/20000] Train-Loss: 0.6062584859260715 Test-Loss: 0.18167385676034922 Train-f1: 0.4609045354932141 Test-f1: 0.4270253517634662 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5357/20000] Train-Loss: 0.6058324623816587 Test-Loss: 0.18587984225161505 Train-f1: 0.4555910755910756 Test-f1: 0.41363211951447243 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5358/20000] Train-Loss: 0.6084000751687211 Test-Loss: 0.1812391960987321 Train-f1: 0.45822994210090984 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5359/20000] Train-Loss: 0.6062579044564234 Test-Loss: 0.18146242341662927 Train-f1: 0.45757179207560894 Test-f1: 0.43742690058479533 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5360/20000] Train-Loss: 0.6051476009791054 Test-Loss: 0.18190632189506958 Train-f1: 0.4518100737227407 Test-f1: 0.4163765578329386 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5361/20000] Train-Loss: 0.6058944707422592 Test-Loss: 0.1811934866599806 Train-f1: 0.45127515405661106 Test-f1: 0.4150529687115053 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5362/20000] Train-Loss: 0.6049420211680017 Test-Loss: 0.1844328205081235 Train-f1: 0.44545871282948307 Test-f1: 0.43389185977421274 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5363/20000] Train-Loss: 0.6055190407110691 Test-Loss: 0.18542525134598412 Train-f1: 0.44984327473320623 Test-f1: 0.43389185977421274 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5364/20000] Train-Loss: 0.6072069077323412 Test-Loss: 0.18133906818644996 Train-f1: 0.44106063130374606 Test-f1: 0.4265359477124183 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [5365/20000] Train-Loss: 0.6087673056098272 Test-Loss: 0.18168133746564577 Train-f1: 0.45770730143310134 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5366/20000] Train-Loss: 0.6059382813090101 Test-Loss: 0.181929033862107 Train-f1: 0.44792241019214707 Test-f1: 0.4452525252525252 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [5367/20000] Train-Loss: 0.60022088333124 Test-Loss: 0.18603129974846366 Train-f1: 0.4406907605256521 Test-f1: 0.4383314450509837 Train-acc: 0.7549668874172185 Test-acc: 0.75\n",
      "Epoch [5368/20000] Train-Loss: 0.6094148707396136 Test-Loss: 0.18379315585721565 Train-f1: 0.44927760061212024 Test-f1: 0.4231559290382819 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5369/20000] Train-Loss: 0.6060570881537147 Test-Loss: 0.18114575940481323 Train-f1: 0.4574820788530466 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5370/20000] Train-Loss: 0.6047043173976331 Test-Loss: 0.18151246940021498 Train-f1: 0.4479233103788264 Test-f1: 0.41567460317460314 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5371/20000] Train-Loss: 0.6077076350141247 Test-Loss: 0.181260920385058 Train-f1: 0.45146492184660125 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5372/20000] Train-Loss: 0.6075200745159364 Test-Loss: 0.1815858758580118 Train-f1: 0.4601233280552016 Test-f1: 0.4369302783936931 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5373/20000] Train-Loss: 0.6059478591184237 Test-Loss: 0.18603324369725363 Train-f1: 0.4543455023099918 Test-f1: 0.43389185977421274 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5374/20000] Train-Loss: 0.6062356124371203 Test-Loss: 0.1836275548997941 Train-f1: 0.44762742057440186 Test-f1: 0.4240641711229946 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5375/20000] Train-Loss: 0.603233251490879 Test-Loss: 0.19422784062292148 Train-f1: 0.44727762803234505 Test-f1: 0.3916993575530161 Train-acc: 0.7649006622516556 Test-acc: 0.6710526315789473\n",
      "Epoch [5376/20000] Train-Loss: 0.6092869275707659 Test-Loss: 0.18147499366088496 Train-f1: 0.4529920926834895 Test-f1: 0.4381306381556967 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5377/20000] Train-Loss: 0.604837735490155 Test-Loss: 0.18101313907084068 Train-f1: 0.4586833855799373 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5378/20000] Train-Loss: 0.6040069352212133 Test-Loss: 0.1826198312926996 Train-f1: 0.4485483870967742 Test-f1: 0.4134165729574624 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5379/20000] Train-Loss: 0.6031303706825883 Test-Loss: 0.18100429043115232 Train-f1: 0.45852216748768476 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5380/20000] Train-Loss: 0.6038573009503788 Test-Loss: 0.18235821856392986 Train-f1: 0.4406778429440606 Test-f1: 0.41202266953292954 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5381/20000] Train-Loss: 0.6069577555116872 Test-Loss: 0.1845679483821919 Train-f1: 0.4522653484210662 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5382/20000] Train-Loss: 0.6043362851404236 Test-Loss: 0.1883576214656814 Train-f1: 0.4551490236382323 Test-f1: 0.41699225878330354 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5383/20000] Train-Loss: 0.6046891212977643 Test-Loss: 0.18241903528339543 Train-f1: 0.4443112815596037 Test-f1: 0.41202266953292954 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5384/20000] Train-Loss: 0.6047820483893448 Test-Loss: 0.18166095790062153 Train-f1: 0.4493895986578914 Test-f1: 0.4262515262515262 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5385/20000] Train-Loss: 0.6062116738946406 Test-Loss: 0.18220403174143923 Train-f1: 0.4538878377160803 Test-f1: 0.41202266953292954 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5386/20000] Train-Loss: 0.6039423734103617 Test-Loss: 0.18209699226810414 Train-f1: 0.45313558274626126 Test-f1: 0.4099744245524297 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5387/20000] Train-Loss: 0.6029587360576597 Test-Loss: 0.18118335122204343 Train-f1: 0.4657805204812764 Test-f1: 0.4150529687115053 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [5388/20000] Train-Loss: 0.6047488440240011 Test-Loss: 0.18141247101621227 Train-f1: 0.4387178981937603 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5389/20000] Train-Loss: 0.6028673047581434 Test-Loss: 0.18102831746846804 Train-f1: 0.4534788659343819 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5390/20000] Train-Loss: 0.6037441222539784 Test-Loss: 0.18662201412104001 Train-f1: 0.45048061631367065 Test-f1: 0.42802587851257545 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5391/20000] Train-Loss: 0.6054515735646082 Test-Loss: 0.1808925902157916 Train-f1: 0.44542726679712985 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5392/20000] Train-Loss: 0.6010745009489987 Test-Loss: 0.18949818375701472 Train-f1: 0.44594329933841265 Test-f1: 0.3917235630358844 Train-acc: 0.7615894039735099 Test-acc: 0.6842105263157895\n",
      "Epoch [5393/20000] Train-Loss: 0.6064134262516786 Test-Loss: 0.18278435004473056 Train-f1: 0.45572577276524645 Test-f1: 0.4231559290382819 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5394/20000] Train-Loss: 0.6052515315536177 Test-Loss: 0.18127571985463511 Train-f1: 0.44230436230436226 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5395/20000] Train-Loss: 0.6047084259886432 Test-Loss: 0.18226143687274635 Train-f1: 0.4555910755910756 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5396/20000] Train-Loss: 0.6046549410646413 Test-Loss: 0.1811499521459645 Train-f1: 0.44019850862719523 Test-f1: 0.40099616858237547 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5397/20000] Train-Loss: 0.6042245439065319 Test-Loss: 0.18123571183789702 Train-f1: 0.44778081177396223 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5398/20000] Train-Loss: 0.6054442929738879 Test-Loss: 0.18080444412797245 Train-f1: 0.44778081177396223 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5399/20000] Train-Loss: 0.6014684579861695 Test-Loss: 0.18598128358123742 Train-f1: 0.44233305156382074 Test-f1: 0.41990196078431374 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5400/20000] Train-Loss: 0.6063015256854548 Test-Loss: 0.18117361139089175 Train-f1: 0.4524388565135594 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5401/20000] Train-Loss: 0.602416272567545 Test-Loss: 0.18189247916394127 Train-f1: 0.45000079876746985 Test-f1: 0.42702535176346623 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5402/20000] Train-Loss: 0.6080792844007 Test-Loss: 0.18104735845815942 Train-f1: 0.4420451616229314 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5403/20000] Train-Loss: 0.5989974729520645 Test-Loss: 0.18139059474790048 Train-f1: 0.4509909638275391 Test-f1: 0.4452525252525252 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [5404/20000] Train-Loss: 0.6058030213449248 Test-Loss: 0.18215410481954383 Train-f1: 0.4352050443048556 Test-f1: 0.4231559290382819 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [5405/20000] Train-Loss: 0.5996886587152915 Test-Loss: 0.18218742603051935 Train-f1: 0.43591193764070024 Test-f1: 0.39990929705215417 Train-acc: 0.7483443708609272 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5406/20000] Train-Loss: 0.6049524052710807 Test-Loss: 0.1808037228179176 Train-f1: 0.44192098195381424 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5407/20000] Train-Loss: 0.6051981023341083 Test-Loss: 0.1817850416047359 Train-f1: 0.4596592256534516 Test-f1: 0.4231559290382819 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5408/20000] Train-Loss: 0.6028898296500984 Test-Loss: 0.18148922517516533 Train-f1: 0.44356896596417555 Test-f1: 0.4452525252525252 Train-acc: 0.7615894039735099 Test-acc: 0.7631578947368421\n",
      "Epoch [5409/20000] Train-Loss: 0.6057312503827235 Test-Loss: 0.1819425912333663 Train-f1: 0.44763463491371025 Test-f1: 0.4007605466428997 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "Epoch [5410/20000] Train-Loss: 0.6026508111533886 Test-Loss: 0.18153504616499985 Train-f1: 0.44397515527950315 Test-f1: 0.41389826908781996 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5411/20000] Train-Loss: 0.6062721264056683 Test-Loss: 0.1810717859960324 Train-f1: 0.4404055299539171 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5412/20000] Train-Loss: 0.6040641700207522 Test-Loss: 0.1814535993527263 Train-f1: 0.4510932308868352 Test-f1: 0.44722222222222213 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [5413/20000] Train-Loss: 0.6070110630134028 Test-Loss: 0.18106691256321605 Train-f1: 0.4367107802591674 Test-f1: 0.42702535176346623 Train-acc: 0.7483443708609272 Test-acc: 0.7368421052631579\n",
      "Epoch [5414/20000] Train-Loss: 0.6085751313645618 Test-Loss: 0.18126377955021228 Train-f1: 0.45602825827229754 Test-f1: 0.4258350100603622 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5415/20000] Train-Loss: 0.6025494880926717 Test-Loss: 0.18116281564167847 Train-f1: 0.4546441908819497 Test-f1: 0.44866696126086314 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [5416/20000] Train-Loss: 0.6018873936299612 Test-Loss: 0.18055839273192686 Train-f1: 0.4563556614932251 Test-f1: 0.4150529687115053 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5417/20000] Train-Loss: 0.6019438704908038 Test-Loss: 0.18259291039417017 Train-f1: 0.44267954795251885 Test-f1: 0.4231559290382819 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5418/20000] Train-Loss: 0.6035852781898727 Test-Loss: 0.18252405740356292 Train-f1: 0.45265049415992814 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5419/20000] Train-Loss: 0.6045669068127159 Test-Loss: 0.18300102398516305 Train-f1: 0.46052672147995893 Test-f1: 0.4231559290382819 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5420/20000] Train-Loss: 0.6064256692504789 Test-Loss: 0.18065364058632055 Train-f1: 0.4393995443416435 Test-f1: 0.4265359477124183 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5421/20000] Train-Loss: 0.6011711305718495 Test-Loss: 0.18237786069701514 Train-f1: 0.44090441932168545 Test-f1: 0.4019495798319328 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5422/20000] Train-Loss: 0.6035790952664872 Test-Loss: 0.1869832999182592 Train-f1: 0.43881049102092035 Test-f1: 0.41699225878330354 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5423/20000] Train-Loss: 0.6026970346992407 Test-Loss: 0.1807119970460474 Train-f1: 0.45391565398926836 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5424/20000] Train-Loss: 0.6040533443079864 Test-Loss: 0.18410632244176145 Train-f1: 0.4559247648902821 Test-f1: 0.4231559290382819 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5425/20000] Train-Loss: 0.6038017492063925 Test-Loss: 0.1810664277222261 Train-f1: 0.4605383022774327 Test-f1: 0.4258350100603622 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5426/20000] Train-Loss: 0.6032868033615397 Test-Loss: 0.1858529338794097 Train-f1: 0.45061850251063823 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5427/20000] Train-Loss: 0.6043067813768713 Test-Loss: 0.18261421023965207 Train-f1: 0.45512004687019625 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5428/20000] Train-Loss: 0.6021635452328571 Test-Loss: 0.18160905408769756 Train-f1: 0.4636361389197874 Test-f1: 0.41202266953292954 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5429/20000] Train-Loss: 0.6036684175475037 Test-Loss: 0.1804326752543167 Train-f1: 0.45538918766779873 Test-f1: 0.4265359477124183 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5430/20000] Train-Loss: 0.6035566296897389 Test-Loss: 0.18042956443696875 Train-f1: 0.45483238940762283 Test-f1: 0.4265359477124183 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5431/20000] Train-Loss: 0.5999369488983063 Test-Loss: 0.18148759159134129 Train-f1: 0.45591026245207383 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5432/20000] Train-Loss: 0.6015367044154503 Test-Loss: 0.18056038005056887 Train-f1: 0.455862775217614 Test-f1: 0.4265359477124183 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5433/20000] Train-Loss: 0.6024054076160239 Test-Loss: 0.1829257292130047 Train-f1: 0.4413533834586466 Test-f1: 0.4007605466428997 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [5434/20000] Train-Loss: 0.6058172083230952 Test-Loss: 0.18062554354823052 Train-f1: 0.4513441548275711 Test-f1: 0.43742690058479533 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5435/20000] Train-Loss: 0.605609444475347 Test-Loss: 0.18054560503148454 Train-f1: 0.4589087076725401 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5436/20000] Train-Loss: 0.6007925748949008 Test-Loss: 0.18127939429882603 Train-f1: 0.44784738187192696 Test-f1: 0.41389826908781996 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5437/20000] Train-Loss: 0.5954326988545265 Test-Loss: 0.18098521759186606 Train-f1: 0.4520144499178982 Test-f1: 0.44866696126086314 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5438/20000] Train-Loss: 0.601671499321033 Test-Loss: 0.18058392420266917 Train-f1: 0.4441910941181014 Test-f1: 0.41567460317460314 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5439/20000] Train-Loss: 0.6037559155732443 Test-Loss: 0.1818037983536096 Train-f1: 0.45072262974924404 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5440/20000] Train-Loss: 0.6029043569511646 Test-Loss: 0.18509078475772175 Train-f1: 0.455006755995217 Test-f1: 0.41414891538010556 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5441/20000] Train-Loss: 0.602190815404835 Test-Loss: 0.18099404453442638 Train-f1: 0.4452578304986613 Test-f1: 0.4274425110041549 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5442/20000] Train-Loss: 0.6064563433475876 Test-Loss: 0.18031392387212639 Train-f1: 0.44998501047974215 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5443/20000] Train-Loss: 0.6009271355873945 Test-Loss: 0.18054437223583017 Train-f1: 0.4483039709529776 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5444/20000] Train-Loss: 0.6055994580042159 Test-Loss: 0.18208043856820286 Train-f1: 0.44040243327422174 Test-f1: 0.4122994652406417 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5445/20000] Train-Loss: 0.6037382124733174 Test-Loss: 0.18076018010844716 Train-f1: 0.45067972917729004 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5446/20000] Train-Loss: 0.6006869137811125 Test-Loss: 0.1871050197343491 Train-f1: 0.45543456754082934 Test-f1: 0.41699225878330354 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5447/20000] Train-Loss: 0.6035045304672192 Test-Loss: 0.18132366103123088 Train-f1: 0.45066443339017426 Test-f1: 0.4305997407800165 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5448/20000] Train-Loss: 0.6015569012672897 Test-Loss: 0.1812859563059441 Train-f1: 0.4518100737227407 Test-f1: 0.4468029372611329 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [5449/20000] Train-Loss: 0.6035534013148197 Test-Loss: 0.18107394184475675 Train-f1: 0.4482241317579663 Test-f1: 0.42924472722988105 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5450/20000] Train-Loss: 0.6020719881865222 Test-Loss: 0.18349466032164152 Train-f1: 0.461080743662842 Test-f1: 0.4134165729574624 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5451/20000] Train-Loss: 0.6063133195084256 Test-Loss: 0.193542695465334 Train-f1: 0.44772261623325454 Test-f1: 0.42802587851257545 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5452/20000] Train-Loss: 0.6011956406432116 Test-Loss: 0.18031461379049968 Train-f1: 0.44992983862247077 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5453/20000] Train-Loss: 0.6026466378942443 Test-Loss: 0.1835806855129391 Train-f1: 0.44642593236267203 Test-f1: 0.4138763197586727 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5454/20000] Train-Loss: 0.6033369201968972 Test-Loss: 0.18316027063884818 Train-f1: 0.4480433033428425 Test-f1: 0.4134165729574624 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5455/20000] Train-Loss: 0.6012153439989555 Test-Loss: 0.18485447052507398 Train-f1: 0.4605692326237653 Test-f1: 0.43202390084379677 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5456/20000] Train-Loss: 0.5991317735896673 Test-Loss: 0.18085675519895764 Train-f1: 0.45803553864471713 Test-f1: 0.4452525252525252 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [5457/20000] Train-Loss: 0.5981583700030022 Test-Loss: 0.1805713130905369 Train-f1: 0.45264902363823234 Test-f1: 0.44722222222222213 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [5458/20000] Train-Loss: 0.5999861134579035 Test-Loss: 0.18032662312911243 Train-f1: 0.4375841295422279 Test-f1: 0.4150529687115053 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5459/20000] Train-Loss: 0.6026215864835113 Test-Loss: 0.1804496242722413 Train-f1: 0.4448042472803075 Test-f1: 0.44722222222222213 Train-acc: 0.7615894039735099 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [5460/20000] Train-Loss: 0.6025717576855154 Test-Loss: 0.18007530531563645 Train-f1: 0.44613293739640036 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5461/20000] Train-Loss: 0.6024789439762703 Test-Loss: 0.1803118516560746 Train-f1: 0.44493820992116867 Test-f1: 0.41567460317460314 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5462/20000] Train-Loss: 0.5987689795749169 Test-Loss: 0.18456583126106066 Train-f1: 0.45740707587486185 Test-f1: 0.4122994652406417 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5463/20000] Train-Loss: 0.6005880024147248 Test-Loss: 0.18335747823501025 Train-f1: 0.4607437267868466 Test-f1: 0.4240641711229946 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5464/20000] Train-Loss: 0.6004323046383905 Test-Loss: 0.18244185802154075 Train-f1: 0.4417702932134172 Test-f1: 0.4231559290382819 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5465/20000] Train-Loss: 0.602438597735316 Test-Loss: 0.18074700073729555 Train-f1: 0.4630227743271222 Test-f1: 0.4354006968641116 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [5466/20000] Train-Loss: 0.6011998510021002 Test-Loss: 0.18043599868982396 Train-f1: 0.45810476447946036 Test-f1: 0.44722222222222213 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [5467/20000] Train-Loss: 0.5943468482413522 Test-Loss: 0.18311660175391997 Train-f1: 0.441179774781129 Test-f1: 0.42702535176346623 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [5468/20000] Train-Loss: 0.6070961419862245 Test-Loss: 0.18197974801140282 Train-f1: 0.45911815234882114 Test-f1: 0.4231559290382819 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5469/20000] Train-Loss: 0.6035627876867327 Test-Loss: 0.1813836459904638 Train-f1: 0.4616563962259327 Test-f1: 0.41202266953292954 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5470/20000] Train-Loss: 0.6014952525822631 Test-Loss: 0.18058595676730735 Train-f1: 0.4474691531978496 Test-f1: 0.4258350100603622 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5471/20000] Train-Loss: 0.5994164142980536 Test-Loss: 0.18074838209058572 Train-f1: 0.46036053990397685 Test-f1: 0.4452525252525252 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [5472/20000] Train-Loss: 0.601226352041227 Test-Loss: 0.18154576432993536 Train-f1: 0.43709214606377567 Test-f1: 0.4231559290382819 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [5473/20000] Train-Loss: 0.5998433958509765 Test-Loss: 0.18240510678073224 Train-f1: 0.4418985554469425 Test-f1: 0.43389185977421274 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5474/20000] Train-Loss: 0.5984500582776348 Test-Loss: 0.18180630893737507 Train-f1: 0.4483039709529776 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5475/20000] Train-Loss: 0.6006116164387126 Test-Loss: 0.18018853021838765 Train-f1: 0.45799183062208276 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5476/20000] Train-Loss: 0.6014371743810173 Test-Loss: 0.1822778102067212 Train-f1: 0.45588931494179824 Test-f1: 0.4231559290382819 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5477/20000] Train-Loss: 0.6011841551331607 Test-Loss: 0.18004116939751033 Train-f1: 0.4592050317059339 Test-f1: 0.43742690058479533 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5478/20000] Train-Loss: 0.6029949865467021 Test-Loss: 0.18077740004522322 Train-f1: 0.45265049415992814 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5479/20000] Train-Loss: 0.5984233812549913 Test-Loss: 0.1800139928632694 Train-f1: 0.46302334162433534 Test-f1: 0.4265359477124183 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5480/20000] Train-Loss: 0.6015090172159198 Test-Loss: 0.181620280009618 Train-f1: 0.4582319872092423 Test-f1: 0.4231559290382819 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5481/20000] Train-Loss: 0.6005328997409948 Test-Loss: 0.1835557941770376 Train-f1: 0.46019357336430505 Test-f1: 0.4231559290382819 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5482/20000] Train-Loss: 0.602946137131433 Test-Loss: 0.18159245409850908 Train-f1: 0.45348728965270324 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5483/20000] Train-Loss: 0.6000790111940537 Test-Loss: 0.18138697215749924 Train-f1: 0.4551490236382323 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5484/20000] Train-Loss: 0.5965696881538576 Test-Loss: 0.17985576956850347 Train-f1: 0.4528706307550666 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5485/20000] Train-Loss: 0.5993577506409353 Test-Loss: 0.18388693191752825 Train-f1: 0.4489432703003337 Test-f1: 0.4231559290382819 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5486/20000] Train-Loss: 0.5989805752330714 Test-Loss: 0.17977396695406903 Train-f1: 0.44870619946091644 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5487/20000] Train-Loss: 0.6034704729784243 Test-Loss: 0.1844692243128031 Train-f1: 0.44874661984646735 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5488/20000] Train-Loss: 0.596483345166103 Test-Loss: 0.1811288635797114 Train-f1: 0.4670182779480692 Test-f1: 0.43392041748206134 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [5489/20000] Train-Loss: 0.6017962972976191 Test-Loss: 0.1814373072801625 Train-f1: 0.4491598150051388 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5490/20000] Train-Loss: 0.6010656300896812 Test-Loss: 0.1811344823661048 Train-f1: 0.44687370600414084 Test-f1: 0.4122994652406417 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5491/20000] Train-Loss: 0.5990950773199237 Test-Loss: 0.17982828221140526 Train-f1: 0.45556935817805383 Test-f1: 0.41567460317460314 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5492/20000] Train-Loss: 0.6007518073659067 Test-Loss: 0.18428781884721313 Train-f1: 0.45572577276524645 Test-f1: 0.4134165729574624 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5493/20000] Train-Loss: 0.5999563361376108 Test-Loss: 0.18542268959356878 Train-f1: 0.44761904761904764 Test-f1: 0.4074109554921158 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [5494/20000] Train-Loss: 0.6000209490108347 Test-Loss: 0.18031510454861227 Train-f1: 0.4391449747806484 Test-f1: 0.4368484410737932 Train-acc: 0.7582781456953642 Test-acc: 0.75\n",
      "Epoch [5495/20000] Train-Loss: 0.5990001446639045 Test-Loss: 0.18094779962408272 Train-f1: 0.45854909809322175 Test-f1: 0.4383314450509837 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5496/20000] Train-Loss: 0.6024498081947832 Test-Loss: 0.18026653677496346 Train-f1: 0.46485295403238663 Test-f1: 0.4368484410737932 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [5497/20000] Train-Loss: 0.6008554032388543 Test-Loss: 0.18120986363244246 Train-f1: 0.4512519637074798 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5498/20000] Train-Loss: 0.5995808262682445 Test-Loss: 0.18082150729035618 Train-f1: 0.4527656813541209 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5499/20000] Train-Loss: 0.6031885890561371 Test-Loss: 0.18280024258198307 Train-f1: 0.44632616487455196 Test-f1: 0.4231559290382819 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5500/20000] Train-Loss: 0.600355657176507 Test-Loss: 0.18035679241499938 Train-f1: 0.45826700181538893 Test-f1: 0.4354006968641116 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5501/20000] Train-Loss: 0.5930678228011679 Test-Loss: 0.18764020743538026 Train-f1: 0.4489422161218061 Test-f1: 0.41699225878330354 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5502/20000] Train-Loss: 0.6007649265898959 Test-Loss: 0.1816063667383915 Train-f1: 0.4497722567287785 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5503/20000] Train-Loss: 0.6022752910453402 Test-Loss: 0.18214317539274158 Train-f1: 0.44570950043813634 Test-f1: 0.4242273837682733 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5504/20000] Train-Loss: 0.5983572361616156 Test-Loss: 0.18606815492243609 Train-f1: 0.4508557107053347 Test-f1: 0.4060048594238112 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [5505/20000] Train-Loss: 0.6010810993012031 Test-Loss: 0.18081026902207614 Train-f1: 0.4530146577924826 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5506/20000] Train-Loss: 0.5997974461218422 Test-Loss: 0.18055919002152188 Train-f1: 0.4537577500798977 Test-f1: 0.41857707509881426 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5507/20000] Train-Loss: 0.5959302081081566 Test-Loss: 0.18573255661147894 Train-f1: 0.46401891252955085 Test-f1: 0.4099744245524297 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5508/20000] Train-Loss: 0.6016279167519706 Test-Loss: 0.1794586794199884 Train-f1: 0.453407047634765 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5509/20000] Train-Loss: 0.600067479310366 Test-Loss: 0.18070541605999793 Train-f1: 0.4437639501227708 Test-f1: 0.43069828722002634 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5510/20000] Train-Loss: 0.6009630399790027 Test-Loss: 0.1803082010819842 Train-f1: 0.43771831470158845 Test-f1: 0.41941245593419507 Train-acc: 0.7516556291390728 Test-acc: 0.7236842105263158\n",
      "Epoch [5511/20000] Train-Loss: 0.5983983067316403 Test-Loss: 0.18871044298316403 Train-f1: 0.4529919137466307 Test-f1: 0.40611358790463276 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [5512/20000] Train-Loss: 0.6002270628417518 Test-Loss: 0.18006076151414827 Train-f1: 0.4589294665234515 Test-f1: 0.4239775910364146 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5513/20000] Train-Loss: 0.5958151494503234 Test-Loss: 0.18295208416308048 Train-f1: 0.4479296066252588 Test-f1: 0.4134165729574624 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5514/20000] Train-Loss: 0.599259678858127 Test-Loss: 0.18159483222150102 Train-f1: 0.4521558248523047 Test-f1: 0.4242273837682733 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5515/20000] Train-Loss: 0.6018971659231718 Test-Loss: 0.18044064979708013 Train-f1: 0.451523915461624 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5516/20000] Train-Loss: 0.5978275514326197 Test-Loss: 0.18015543836967032 Train-f1: 0.44820222197051846 Test-f1: 0.4354006968641116 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [5517/20000] Train-Loss: 0.5988837216223668 Test-Loss: 0.18349734817101582 Train-f1: 0.4606122240903562 Test-f1: 0.4242273837682733 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5518/20000] Train-Loss: 0.5966180570348176 Test-Loss: 0.17987507931888888 Train-f1: 0.4570099255583127 Test-f1: 0.44759481497806186 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5519/20000] Train-Loss: 0.5976579142484554 Test-Loss: 0.17987851238442523 Train-f1: 0.45265049415992814 Test-f1: 0.4354006968641116 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5520/20000] Train-Loss: 0.5967022793013872 Test-Loss: 0.17973601116745838 Train-f1: 0.4496136530967572 Test-f1: 0.4369302783936931 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5521/20000] Train-Loss: 0.5982226965220725 Test-Loss: 0.17957714792527157 Train-f1: 0.4482241317579663 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5522/20000] Train-Loss: 0.5971423422438734 Test-Loss: 0.18031108805756568 Train-f1: 0.46385146624113843 Test-f1: 0.4448826291079812 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [5523/20000] Train-Loss: 0.5998939627829616 Test-Loss: 0.18066982069343507 Train-f1: 0.44977132579650564 Test-f1: 0.42402476780185766 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5524/20000] Train-Loss: 0.6002584823111285 Test-Loss: 0.17963388677509706 Train-f1: 0.44565496433046103 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5525/20000] Train-Loss: 0.5983088462367844 Test-Loss: 0.17966412596034895 Train-f1: 0.45572577276524645 Test-f1: 0.4262515262515262 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5526/20000] Train-Loss: 0.5976194365105776 Test-Loss: 0.18029645066283792 Train-f1: 0.44862786309680197 Test-f1: 0.4068599033816426 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5527/20000] Train-Loss: 0.5990946644838705 Test-Loss: 0.18012463313837382 Train-f1: 0.4567551408642087 Test-f1: 0.43642806805597506 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5528/20000] Train-Loss: 0.6018312036404587 Test-Loss: 0.18126230668585555 Train-f1: 0.4497933513027853 Test-f1: 0.4389639222941721 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5529/20000] Train-Loss: 0.601488703530637 Test-Loss: 0.17945396549608153 Train-f1: 0.4565942150738008 Test-f1: 0.43742690058479533 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5530/20000] Train-Loss: 0.5952817558253084 Test-Loss: 0.1795767004289443 Train-f1: 0.4531826050747408 Test-f1: 0.43742690058479533 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5531/20000] Train-Loss: 0.5941016574153355 Test-Loss: 0.18808939959991167 Train-f1: 0.4426454667439848 Test-f1: 0.40611358790463276 Train-acc: 0.7582781456953642 Test-acc: 0.6973684210526315\n",
      "Epoch [5532/20000] Train-Loss: 0.602812532323578 Test-Loss: 0.18117989306747506 Train-f1: 0.4503657036662901 Test-f1: 0.42413273001508306 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5533/20000] Train-Loss: 0.595049805616668 Test-Loss: 0.17946271712410408 Train-f1: 0.4465638647697861 Test-f1: 0.40374260231724596 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5534/20000] Train-Loss: 0.5949819003616006 Test-Loss: 0.18348192877722524 Train-f1: 0.4528223952765539 Test-f1: 0.4122994652406417 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5535/20000] Train-Loss: 0.6003183915031505 Test-Loss: 0.179554531620824 Train-f1: 0.45015113427479675 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5536/20000] Train-Loss: 0.5991889638470641 Test-Loss: 0.17945101455039128 Train-f1: 0.4498254433738305 Test-f1: 0.4365359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5537/20000] Train-Loss: 0.5986970922571964 Test-Loss: 0.1800717537040245 Train-f1: 0.46625336927223715 Test-f1: 0.44678185391418274 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [5538/20000] Train-Loss: 0.599895955759851 Test-Loss: 0.18677740242164206 Train-f1: 0.45619203491543914 Test-f1: 0.4182792600703048 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5539/20000] Train-Loss: 0.5997751946949091 Test-Loss: 0.1799726412067852 Train-f1: 0.44779631689531574 Test-f1: 0.4354006968641116 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5540/20000] Train-Loss: 0.5971857087624342 Test-Loss: 0.17933770210147684 Train-f1: 0.45765419225273246 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5541/20000] Train-Loss: 0.6007211296758119 Test-Loss: 0.18015119195313636 Train-f1: 0.45149359973858944 Test-f1: 0.4305997407800165 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5542/20000] Train-Loss: 0.5963590008524801 Test-Loss: 0.18115119209482994 Train-f1: 0.4557075101876908 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5543/20000] Train-Loss: 0.5945664766933297 Test-Loss: 0.1797303702182742 Train-f1: 0.43451389300465026 Test-f1: 0.41062801932367154 Train-acc: 0.7450331125827815 Test-acc: 0.7236842105263158\n",
      "Epoch [5544/20000] Train-Loss: 0.5953388145060632 Test-Loss: 0.18739778968363557 Train-f1: 0.4500047032874357 Test-f1: 0.3962791086583888 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [5545/20000] Train-Loss: 0.6006665799638973 Test-Loss: 0.17944450468236872 Train-f1: 0.4587453586085597 Test-f1: 0.43742690058479533 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5546/20000] Train-Loss: 0.5932286340406583 Test-Loss: 0.1802749000710733 Train-f1: 0.4427881773399015 Test-f1: 0.4468029372611329 Train-acc: 0.7615894039735099 Test-acc: 0.7631578947368421\n",
      "Epoch [5547/20000] Train-Loss: 0.6002165239597969 Test-Loss: 0.18002170325981734 Train-f1: 0.44902815775142224 Test-f1: 0.41941245593419507 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5548/20000] Train-Loss: 0.5988951315944824 Test-Loss: 0.17949506634567755 Train-f1: 0.45605990783410144 Test-f1: 0.4369302783936931 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5549/20000] Train-Loss: 0.5972367676635152 Test-Loss: 0.17946698603135086 Train-f1: 0.45838827838827834 Test-f1: 0.4150529687115053 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5550/20000] Train-Loss: 0.5980688054130919 Test-Loss: 0.18595855758335475 Train-f1: 0.44363759609311215 Test-f1: 0.42846358707106313 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5551/20000] Train-Loss: 0.5985863442039734 Test-Loss: 0.18144097279457072 Train-f1: 0.455862775217614 Test-f1: 0.42413273001508306 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5552/20000] Train-Loss: 0.5981865708083092 Test-Loss: 0.18043434793206947 Train-f1: 0.44136136734707493 Test-f1: 0.43455108359133127 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5553/20000] Train-Loss: 0.5993567649941802 Test-Loss: 0.17944107373204163 Train-f1: 0.45803906141528206 Test-f1: 0.4369302783936931 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5554/20000] Train-Loss: 0.5975310103900473 Test-Loss: 0.18192001112588913 Train-f1: 0.4530849974398361 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5555/20000] Train-Loss: 0.5944537741458003 Test-Loss: 0.17946953907856944 Train-f1: 0.45441953258597756 Test-f1: 0.41567460317460314 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5556/20000] Train-Loss: 0.598684070796179 Test-Loss: 0.17954828685762556 Train-f1: 0.455006755995217 Test-f1: 0.4239775910364146 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5557/20000] Train-Loss: 0.5962192320422405 Test-Loss: 0.17954248618429022 Train-f1: 0.4392561284868977 Test-f1: 0.4368484410737932 Train-acc: 0.7582781456953642 Test-acc: 0.75\n",
      "Epoch [5558/20000] Train-Loss: 0.5962884139086653 Test-Loss: 0.18190066553612677 Train-f1: 0.45061850251063823 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5559/20000] Train-Loss: 0.5953732272941891 Test-Loss: 0.18450616085627697 Train-f1: 0.4551662174303684 Test-f1: 0.41699225878330354 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5560/20000] Train-Loss: 0.592384724131875 Test-Loss: 0.1804089689565218 Train-f1: 0.47084379459994763 Test-f1: 0.43392041748206134 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [5561/20000] Train-Loss: 0.5958571381946184 Test-Loss: 0.17952805669662197 Train-f1: 0.4567622117340087 Test-f1: 0.42430685593476297 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5562/20000] Train-Loss: 0.5920486780868677 Test-Loss: 0.1800552089141545 Train-f1: 0.4547512501933289 Test-f1: 0.42702535176346623 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5563/20000] Train-Loss: 0.5980369454522872 Test-Loss: 0.18408915849422552 Train-f1: 0.4487576109595245 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5564/20000] Train-Loss: 0.596203647149639 Test-Loss: 0.17945645410472955 Train-f1: 0.4463826894223556 Test-f1: 0.4369302783936931 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5565/20000] Train-Loss: 0.5966182962449335 Test-Loss: 0.17902097694459415 Train-f1: 0.44885236285002783 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5566/20000] Train-Loss: 0.5975305598462585 Test-Loss: 0.17957051955022868 Train-f1: 0.4518286230481352 Test-f1: 0.4365709407962929 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5567/20000] Train-Loss: 0.5959990521168363 Test-Loss: 0.17910531169260988 Train-f1: 0.45155100838379436 Test-f1: 0.41567460317460314 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5568/20000] Train-Loss: 0.5982904428261315 Test-Loss: 0.18231922092201294 Train-f1: 0.46118579508291335 Test-f1: 0.4007605466428997 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [5569/20000] Train-Loss: 0.5976879766607526 Test-Loss: 0.1790104643127269 Train-f1: 0.4529624846401886 Test-f1: 0.43742690058479533 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5570/20000] Train-Loss: 0.5939044523147804 Test-Loss: 0.18927634333850957 Train-f1: 0.456585297040813 Test-f1: 0.4060048594238112 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [5571/20000] Train-Loss: 0.5967691077034895 Test-Loss: 0.18208138836070326 Train-f1: 0.45947220547261614 Test-f1: 0.4240641711229946 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5572/20000] Train-Loss: 0.5977168067093265 Test-Loss: 0.1834114704412584 Train-f1: 0.45613547702185897 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5573/20000] Train-Loss: 0.5960125636353459 Test-Loss: 0.1821975312666019 Train-f1: 0.4545766424656107 Test-f1: 0.41202266953292954 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5574/20000] Train-Loss: 0.5949741249916379 Test-Loss: 0.17897915258061187 Train-f1: 0.45884134797425224 Test-f1: 0.4265359477124183 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5575/20000] Train-Loss: 0.5949273469872881 Test-Loss: 0.17984349166428298 Train-f1: 0.4575788181748447 Test-f1: 0.41202266953292954 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5576/20000] Train-Loss: 0.5958187363538068 Test-Loss: 0.18292826163972264 Train-f1: 0.4630227743271222 Test-f1: 0.4134165729574624 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5577/20000] Train-Loss: 0.5979837366536715 Test-Loss: 0.18389106540751396 Train-f1: 0.4565942150738008 Test-f1: 0.4231559290382819 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5578/20000] Train-Loss: 0.5973159804143385 Test-Loss: 0.18052634774010715 Train-f1: 0.45703372878131054 Test-f1: 0.42402476780185766 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5579/20000] Train-Loss: 0.5961504287732341 Test-Loss: 0.17994217700081822 Train-f1: 0.45229726246326757 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5580/20000] Train-Loss: 0.5956774816591058 Test-Loss: 0.1806954543259222 Train-f1: 0.46088056121990945 Test-f1: 0.41202266953292954 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5581/20000] Train-Loss: 0.5960637667307905 Test-Loss: 0.18823803364862932 Train-f1: 0.4551665042459345 Test-f1: 0.41754215933320415 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5582/20000] Train-Loss: 0.5972285249360696 Test-Loss: 0.1804991867054204 Train-f1: 0.44034989364895694 Test-f1: 0.4134165729574624 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5583/20000] Train-Loss: 0.5977841354960832 Test-Loss: 0.17888492191304353 Train-f1: 0.4536556802781969 Test-f1: 0.4150529687115053 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5584/20000] Train-Loss: 0.5972987996309125 Test-Loss: 0.17865749493651875 Train-f1: 0.46057390028192946 Test-f1: 0.4265359477124183 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5585/20000] Train-Loss: 0.5963393188386444 Test-Loss: 0.18084039063649704 Train-f1: 0.4568993093420574 Test-f1: 0.43455108359133127 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5586/20000] Train-Loss: 0.5948391325157637 Test-Loss: 0.1787805462001479 Train-f1: 0.46328193186627536 Test-f1: 0.4265359477124183 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5587/20000] Train-Loss: 0.5957089693622953 Test-Loss: 0.1789991391212931 Train-f1: 0.44842153495462905 Test-f1: 0.41373737373737374 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5588/20000] Train-Loss: 0.5998795845441219 Test-Loss: 0.18018985203120064 Train-f1: 0.4392476754015216 Test-f1: 0.4122994652406417 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5589/20000] Train-Loss: 0.5973020766203289 Test-Loss: 0.1790162805381909 Train-f1: 0.4574457803854609 Test-f1: 0.44759481497806186 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [5590/20000] Train-Loss: 0.5967611907707919 Test-Loss: 0.17980965505121846 Train-f1: 0.4465759360014638 Test-f1: 0.4231559290382819 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5591/20000] Train-Loss: 0.594735073701235 Test-Loss: 0.17920122342483488 Train-f1: 0.4584748485972067 Test-f1: 0.42702535176346623 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5592/20000] Train-Loss: 0.595920174373278 Test-Loss: 0.18033205960301546 Train-f1: 0.4432587355800234 Test-f1: 0.4122994652406417 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5593/20000] Train-Loss: 0.5899750895757533 Test-Loss: 0.18754810909829905 Train-f1: 0.45175080702866693 Test-f1: 0.4074109554921158 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [5594/20000] Train-Loss: 0.5990453496221785 Test-Loss: 0.18041638061917956 Train-f1: 0.46220785974581224 Test-f1: 0.4134165729574624 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5595/20000] Train-Loss: 0.5947032345538078 Test-Loss: 0.18054343527028907 Train-f1: 0.4596592256534516 Test-f1: 0.4468029372611329 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [5596/20000] Train-Loss: 0.5938786663588411 Test-Loss: 0.17886344609733584 Train-f1: 0.4483039709529776 Test-f1: 0.436784004167251 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [5597/20000] Train-Loss: 0.5909181498722793 Test-Loss: 0.1797016794113124 Train-f1: 0.45117866004962776 Test-f1: 0.3969267139479905 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5598/20000] Train-Loss: 0.5947994963912706 Test-Loss: 0.17897927656732154 Train-f1: 0.4333103599267788 Test-f1: 0.41295692040807896 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [5599/20000] Train-Loss: 0.5972314514522812 Test-Loss: 0.17933593113558552 Train-f1: 0.4497253336891137 Test-f1: 0.41811693332540295 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5600/20000] Train-Loss: 0.5936725664831972 Test-Loss: 0.18023402613347977 Train-f1: 0.4630430360843986 Test-f1: 0.42402476780185766 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5601/20000] Train-Loss: 0.5939313723439368 Test-Loss: 0.1822058001695675 Train-f1: 0.4584833606110202 Test-f1: 0.4240641711229946 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5602/20000] Train-Loss: 0.5985695405971472 Test-Loss: 0.17965133758178156 Train-f1: 0.4476656676656677 Test-f1: 0.42402476780185766 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5603/20000] Train-Loss: 0.5945419926583286 Test-Loss: 0.1794014385220436 Train-f1: 0.4582319872092423 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5604/20000] Train-Loss: 0.5955164583048099 Test-Loss: 0.18069122010847827 Train-f1: 0.45364706215770045 Test-f1: 0.4231559290382819 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5605/20000] Train-Loss: 0.5960512879394511 Test-Loss: 0.18256104330356066 Train-f1: 0.4620816459526137 Test-f1: 0.4242273837682733 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5606/20000] Train-Loss: 0.5931194543087732 Test-Loss: 0.17937301611288017 Train-f1: 0.46481825866441245 Test-f1: 0.44655844155844154 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [5607/20000] Train-Loss: 0.5949967736980786 Test-Loss: 0.1788760931954239 Train-f1: 0.44221248861156426 Test-f1: 0.41539227895392283 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5608/20000] Train-Loss: 0.5958141336954623 Test-Loss: 0.1787794624781361 Train-f1: 0.46592132505175987 Test-f1: 0.4239775910364146 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [5609/20000] Train-Loss: 0.5945321402776168 Test-Loss: 0.17883814434542541 Train-f1: 0.45844010351125136 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5610/20000] Train-Loss: 0.59423808318527 Test-Loss: 0.17848635991388262 Train-f1: 0.45283907600980766 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5611/20000] Train-Loss: 0.593608047085398 Test-Loss: 0.17909357817415006 Train-f1: 0.4448396353033739 Test-f1: 0.4452525252525252 Train-acc: 0.7615894039735099 Test-acc: 0.7631578947368421\n",
      "Epoch [5612/20000] Train-Loss: 0.5922296807816763 Test-Loss: 0.1789468564103208 Train-f1: 0.441310397494143 Test-f1: 0.41062801932367154 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5613/20000] Train-Loss: 0.5957625157376849 Test-Loss: 0.18200656952574884 Train-f1: 0.4437565769279132 Test-f1: 0.4134165729574624 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5614/20000] Train-Loss: 0.5918423896936575 Test-Loss: 0.17918599810452246 Train-f1: 0.45921041034813487 Test-f1: 0.4468029372611329 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [5615/20000] Train-Loss: 0.5934105685128079 Test-Loss: 0.17850516275555164 Train-f1: 0.4508557107053347 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5616/20000] Train-Loss: 0.5898847348075102 Test-Loss: 0.1792633785007348 Train-f1: 0.45004511268467784 Test-f1: 0.4383314450509837 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5617/20000] Train-Loss: 0.594889692062241 Test-Loss: 0.17955214665724037 Train-f1: 0.45653179929689996 Test-f1: 0.4122994652406417 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5618/20000] Train-Loss: 0.594628183615233 Test-Loss: 0.1795069413468714 Train-f1: 0.4446522517893989 Test-f1: 0.41202266953292954 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5619/20000] Train-Loss: 0.5957217738135219 Test-Loss: 0.17876781049819593 Train-f1: 0.44805450918006595 Test-f1: 0.4253968253968254 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5620/20000] Train-Loss: 0.5930809368073056 Test-Loss: 0.17942823962727886 Train-f1: 0.4564514489069649 Test-f1: 0.43069828722002634 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5621/20000] Train-Loss: 0.5937050911833162 Test-Loss: 0.18060749874378423 Train-f1: 0.44476840363937137 Test-f1: 0.41202266953292954 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5622/20000] Train-Loss: 0.5938456581400726 Test-Loss: 0.18012864814869614 Train-f1: 0.4626891516443755 Test-f1: 0.42402476780185766 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5623/20000] Train-Loss: 0.5936094367987615 Test-Loss: 0.1802849165750505 Train-f1: 0.463452306193851 Test-f1: 0.4134165729574624 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5624/20000] Train-Loss: 0.5892508179057036 Test-Loss: 0.17861382190794453 Train-f1: 0.45563592918431634 Test-f1: 0.44936443292607675 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5625/20000] Train-Loss: 0.5961381107447994 Test-Loss: 0.17931877892142592 Train-f1: 0.46088056121990945 Test-f1: 0.4134165729574624 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5626/20000] Train-Loss: 0.5920989315736271 Test-Loss: 0.178344514741592 Train-f1: 0.4586075290331186 Test-f1: 0.4150529687115053 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5627/20000] Train-Loss: 0.5955428245761032 Test-Loss: 0.1786171689653199 Train-f1: 0.46019357336430505 Test-f1: 0.4253968253968254 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5628/20000] Train-Loss: 0.5933997846472584 Test-Loss: 0.17810740037679657 Train-f1: 0.45106210314120077 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5629/20000] Train-Loss: 0.5952533797398668 Test-Loss: 0.17809168577866508 Train-f1: 0.45620837808807735 Test-f1: 0.4265359477124183 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5630/20000] Train-Loss: 0.5926773316232994 Test-Loss: 0.1795356948370065 Train-f1: 0.4580538302277433 Test-f1: 0.43455108359133127 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5631/20000] Train-Loss: 0.5926744690832625 Test-Loss: 0.17901750529934712 Train-f1: 0.45265049415992814 Test-f1: 0.41948862966890543 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5632/20000] Train-Loss: 0.5964301815745864 Test-Loss: 0.17883280613526473 Train-f1: 0.4608056350258569 Test-f1: 0.4253968253968254 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5633/20000] Train-Loss: 0.5931082635442403 Test-Loss: 0.17946125670103785 Train-f1: 0.4575877959637693 Test-f1: 0.44472049689440996 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [5634/20000] Train-Loss: 0.5957083059582442 Test-Loss: 0.18122265161046142 Train-f1: 0.4436118598382749 Test-f1: 0.4122994652406417 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5635/20000] Train-Loss: 0.5933425517408116 Test-Loss: 0.1792965440449128 Train-f1: 0.45563592918431634 Test-f1: 0.43069828722002634 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5636/20000] Train-Loss: 0.5939216992184222 Test-Loss: 0.17893331611978558 Train-f1: 0.44030325500524176 Test-f1: 0.41811693332540295 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [5637/20000] Train-Loss: 0.5957975114742042 Test-Loss: 0.17930555074249052 Train-f1: 0.45602660328159034 Test-f1: 0.41857707509881426 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5638/20000] Train-Loss: 0.5952279906795392 Test-Loss: 0.18071625879649614 Train-f1: 0.45640552995391703 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5639/20000] Train-Loss: 0.5901157430322521 Test-Loss: 0.1779433489811754 Train-f1: 0.4514077252108894 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5640/20000] Train-Loss: 0.5902960988133041 Test-Loss: 0.17897723312657068 Train-f1: 0.4464093544394502 Test-f1: 0.4381306381556967 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [5641/20000] Train-Loss: 0.5905573013872403 Test-Loss: 0.1784508974291343 Train-f1: 0.4491380349608198 Test-f1: 0.41373737373737374 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5642/20000] Train-Loss: 0.5901751354294792 Test-Loss: 0.18160544563893652 Train-f1: 0.44250464286115115 Test-f1: 0.4240641711229946 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5643/20000] Train-Loss: 0.5918802351492481 Test-Loss: 0.17882603986997475 Train-f1: 0.4487123514498902 Test-f1: 0.43642806805597506 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5644/20000] Train-Loss: 0.595281907783777 Test-Loss: 0.1786534777526432 Train-f1: 0.44911123565376637 Test-f1: 0.43628155628155635 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5645/20000] Train-Loss: 0.5971147590962081 Test-Loss: 0.1779057770133312 Train-f1: 0.4611854811854812 Test-f1: 0.4265359477124183 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5646/20000] Train-Loss: 0.5932465466638278 Test-Loss: 0.18191976655841688 Train-f1: 0.4429042087254853 Test-f1: 0.4099744245524297 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [5647/20000] Train-Loss: 0.5927335543098242 Test-Loss: 0.17884014907935641 Train-f1: 0.46006267281105984 Test-f1: 0.44472049689440996 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [5648/20000] Train-Loss: 0.5940938557949087 Test-Loss: 0.17807281797749822 Train-f1: 0.4564147058442652 Test-f1: 0.4483483483483483 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5649/20000] Train-Loss: 0.5959350683663155 Test-Loss: 0.17919437217313028 Train-f1: 0.4441026575385657 Test-f1: 0.43455108359133127 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5650/20000] Train-Loss: 0.593665151498868 Test-Loss: 0.17891341568627103 Train-f1: 0.445900368288428 Test-f1: 0.44472049689440996 Train-acc: 0.7649006622516556 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [5651/20000] Train-Loss: 0.5926413508073943 Test-Loss: 0.17789143495938148 Train-f1: 0.44867167919799505 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5652/20000] Train-Loss: 0.5954450634374937 Test-Loss: 0.1796287919156227 Train-f1: 0.46079973230717747 Test-f1: 0.4134165729574624 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5653/20000] Train-Loss: 0.5940755475922318 Test-Loss: 0.17826919329187188 Train-f1: 0.4574852201280562 Test-f1: 0.4253968253968254 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5654/20000] Train-Loss: 0.5901246574362548 Test-Loss: 0.18331306197734004 Train-f1: 0.45423214778053495 Test-f1: 0.41202266953292954 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5655/20000] Train-Loss: 0.5926488068508796 Test-Loss: 0.17824607842267218 Train-f1: 0.46286081245207933 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [5656/20000] Train-Loss: 0.5942041449641803 Test-Loss: 0.1792841823394741 Train-f1: 0.45640552995391703 Test-f1: 0.43069828722002634 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5657/20000] Train-Loss: 0.593078599395519 Test-Loss: 0.18220816895242256 Train-f1: 0.45578976431826634 Test-f1: 0.43455108359133127 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5658/20000] Train-Loss: 0.5903247886797032 Test-Loss: 0.17930443181046848 Train-f1: 0.45751330049261085 Test-f1: 0.4552490421455939 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [5659/20000] Train-Loss: 0.592294916916447 Test-Loss: 0.17863752181956363 Train-f1: 0.45668155437196967 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5660/20000] Train-Loss: 0.5937464898652876 Test-Loss: 0.17953930728430217 Train-f1: 0.4638666189704634 Test-f1: 0.42413273001508306 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5661/20000] Train-Loss: 0.5917133321933509 Test-Loss: 0.1797114208122409 Train-f1: 0.46287580682790264 Test-f1: 0.4240641711229946 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5662/20000] Train-Loss: 0.5929923069838365 Test-Loss: 0.1777983664672736 Train-f1: 0.448986097909123 Test-f1: 0.41567460317460314 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5663/20000] Train-Loss: 0.5897482152166945 Test-Loss: 0.17935378298630403 Train-f1: 0.4614581914851953 Test-f1: 0.4545938580861545 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [5664/20000] Train-Loss: 0.5889402238483169 Test-Loss: 0.18208961962246675 Train-f1: 0.4619009268136785 Test-f1: 0.4134165729574624 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5665/20000] Train-Loss: 0.5935513878803391 Test-Loss: 0.1777370725566119 Train-f1: 0.4631963645673324 Test-f1: 0.4265359477124183 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5666/20000] Train-Loss: 0.5919526538746306 Test-Loss: 0.18272112412743863 Train-f1: 0.4380919117032211 Test-f1: 0.4007605466428997 Train-acc: 0.7516556291390728 Test-acc: 0.6973684210526315\n",
      "Epoch [5667/20000] Train-Loss: 0.5918555334412787 Test-Loss: 0.17856940868374116 Train-f1: 0.4604100178503813 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [5668/20000] Train-Loss: 0.592635455847457 Test-Loss: 0.17993047251616173 Train-f1: 0.4484065196548418 Test-f1: 0.41202266953292954 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5669/20000] Train-Loss: 0.5951139670624953 Test-Loss: 0.1796427872395886 Train-f1: 0.4492238918621204 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5670/20000] Train-Loss: 0.5937563726915538 Test-Loss: 0.18051001213610696 Train-f1: 0.45571622790755156 Test-f1: 0.43455108359133127 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5671/20000] Train-Loss: 0.5899952972857103 Test-Loss: 0.17796380149785485 Train-f1: 0.4597746050978337 Test-f1: 0.4365359477124183 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5672/20000] Train-Loss: 0.5893180413964613 Test-Loss: 0.1831850057725486 Train-f1: 0.45343179760827096 Test-f1: 0.4007605466428997 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [5673/20000] Train-Loss: 0.591069172180294 Test-Loss: 0.17794827531677948 Train-f1: 0.45823065422488013 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5674/20000] Train-Loss: 0.592775523040863 Test-Loss: 0.1780702649170563 Train-f1: 0.4543956043956044 Test-f1: 0.4253968253968254 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5675/20000] Train-Loss: 0.5927852479253938 Test-Loss: 0.17912993299291816 Train-f1: 0.46033359435773785 Test-f1: 0.43426704014939316 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5676/20000] Train-Loss: 0.5919959987913608 Test-Loss: 0.17872097530593156 Train-f1: 0.4462952101661779 Test-f1: 0.44472049689440996 Train-acc: 0.7649006622516556 Test-acc: 0.7631578947368421\n",
      "Epoch [5677/20000] Train-Loss: 0.5890491020026181 Test-Loss: 0.1873705870798115 Train-f1: 0.43541117699430654 Test-f1: 0.4074109554921158 Train-acc: 0.7483443708609272 Test-acc: 0.6973684210526315\n",
      "Epoch [5678/20000] Train-Loss: 0.5944147508998259 Test-Loss: 0.1785035254249589 Train-f1: 0.44762742057440186 Test-f1: 0.41811693332540295 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5679/20000] Train-Loss: 0.592125470499731 Test-Loss: 0.17747290039295333 Train-f1: 0.4633962264150943 Test-f1: 0.4150529687115053 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5680/20000] Train-Loss: 0.5897886885741511 Test-Loss: 0.18037379198101014 Train-f1: 0.4599411044228822 Test-f1: 0.4234512409615009 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5681/20000] Train-Loss: 0.5924714343573242 Test-Loss: 0.17873064720660892 Train-f1: 0.4607341611656384 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5682/20000] Train-Loss: 0.5881077000344775 Test-Loss: 0.17834083390469424 Train-f1: 0.4572983600697082 Test-f1: 0.43751349107513493 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5683/20000] Train-Loss: 0.5899051691492057 Test-Loss: 0.1918402983097837 Train-f1: 0.45366198031104477 Test-f1: 0.401367053998633 Train-acc: 0.7715231788079471 Test-acc: 0.6842105263157895\n",
      "best score_dx!!\n",
      "Epoch [5684/20000] Train-Loss: 0.5930123375158902 Test-Loss: 0.17746899514832143 Train-f1: 0.45822994210090984 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5685/20000] Train-Loss: 0.591000915965543 Test-Loss: 0.17830547920623727 Train-f1: 0.46508659292032506 Test-f1: 0.43642806805597506 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [5686/20000] Train-Loss: 0.5943869967985967 Test-Loss: 0.180960619098237 Train-f1: 0.4558241758241758 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5687/20000] Train-Loss: 0.5933802603116903 Test-Loss: 0.17760121410889657 Train-f1: 0.4532720615127771 Test-f1: 0.4265359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5688/20000] Train-Loss: 0.590885545974797 Test-Loss: 0.1784083754978472 Train-f1: 0.45942461285421576 Test-f1: 0.4068599033816426 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5689/20000] Train-Loss: 0.5924241523362561 Test-Loss: 0.17803248943913447 Train-f1: 0.45629754860524085 Test-f1: 0.43628155628155635 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5690/20000] Train-Loss: 0.5903939092633128 Test-Loss: 0.1825170005977934 Train-f1: 0.4635313639628412 Test-f1: 0.4134165729574624 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5691/20000] Train-Loss: 0.5928150356649224 Test-Loss: 0.18351110766929773 Train-f1: 0.46014641439397846 Test-f1: 0.4178951396861845 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5692/20000] Train-Loss: 0.5897925770451014 Test-Loss: 0.17756948255698327 Train-f1: 0.4502015787900183 Test-f1: 0.4483483483483483 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [5693/20000] Train-Loss: 0.591105939692918 Test-Loss: 0.17768626283477512 Train-f1: 0.4528223952765539 Test-f1: 0.4239775910364146 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5694/20000] Train-Loss: 0.5929031822265084 Test-Loss: 0.17772011134566856 Train-f1: 0.4581662471539915 Test-f1: 0.42430685593476297 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5695/20000] Train-Loss: 0.5908767238792779 Test-Loss: 0.1802925865603587 Train-f1: 0.4630227743271222 Test-f1: 0.4134165729574624 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5696/20000] Train-Loss: 0.5907964994268337 Test-Loss: 0.17790441384707523 Train-f1: 0.46117366620102784 Test-f1: 0.4239775910364146 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5697/20000] Train-Loss: 0.5893463055094558 Test-Loss: 0.17974656939955547 Train-f1: 0.4507924391130498 Test-f1: 0.4240641711229946 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5698/20000] Train-Loss: 0.5867627851695892 Test-Loss: 0.18385581849779695 Train-f1: 0.455507637017071 Test-f1: 0.40720901421745037 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [5699/20000] Train-Loss: 0.5914030816311558 Test-Loss: 0.18007376440674944 Train-f1: 0.4606978927236118 Test-f1: 0.4234512409615009 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5700/20000] Train-Loss: 0.5907249266376282 Test-Loss: 0.1778279321603265 Train-f1: 0.4614420062695925 Test-f1: 0.43628155628155635 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5701/20000] Train-Loss: 0.5899469117816618 Test-Loss: 0.17737036556886962 Train-f1: 0.4598522167487685 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5702/20000] Train-Loss: 0.5893550416478145 Test-Loss: 0.18385657432249886 Train-f1: 0.45629754860524085 Test-f1: 0.41827724124475657 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5703/20000] Train-Loss: 0.5914058601332575 Test-Loss: 0.18170226143596951 Train-f1: 0.45030316680931576 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5704/20000] Train-Loss: 0.5867905162012425 Test-Loss: 0.17793387124097415 Train-f1: 0.4589954055830452 Test-f1: 0.4369302783936931 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5705/20000] Train-Loss: 0.5886644407434888 Test-Loss: 0.17939379178174644 Train-f1: 0.448513224968741 Test-f1: 0.4240641711229946 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5706/20000] Train-Loss: 0.5890998614441831 Test-Loss: 0.17772299251930823 Train-f1: 0.45769242666391713 Test-f1: 0.43642806805597506 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5707/20000] Train-Loss: 0.5901532759097986 Test-Loss: 0.17750030293715038 Train-f1: 0.45333468515085834 Test-f1: 0.43742690058479533 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5708/20000] Train-Loss: 0.5896692682833347 Test-Loss: 0.18015089363237397 Train-f1: 0.45092132881295033 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5709/20000] Train-Loss: 0.5904267870483217 Test-Loss: 0.18233429910394594 Train-f1: 0.4557075101876908 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5710/20000] Train-Loss: 0.5905917228879961 Test-Loss: 0.17736069572659174 Train-f1: 0.447496443469322 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5711/20000] Train-Loss: 0.5849525397757338 Test-Loss: 0.18175895281365653 Train-f1: 0.4536556802781969 Test-f1: 0.4122994652406417 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5712/20000] Train-Loss: 0.5896615142596281 Test-Loss: 0.17729924205916694 Train-f1: 0.4609045354932141 Test-f1: 0.4239775910364146 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5713/20000] Train-Loss: 0.5868904193159986 Test-Loss: 0.178556971380456 Train-f1: 0.4593613944788613 Test-f1: 0.4452525252525252 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [5714/20000] Train-Loss: 0.5891088918840458 Test-Loss: 0.17988296717981123 Train-f1: 0.44619583325151435 Test-f1: 0.4007605466428997 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [5715/20000] Train-Loss: 0.5894430818686692 Test-Loss: 0.17724755540041479 Train-f1: 0.45953881325333434 Test-f1: 0.4265359477124183 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5716/20000] Train-Loss: 0.5851555408447437 Test-Loss: 0.18426978266131702 Train-f1: 0.45147563113172956 Test-f1: 0.42846358707106313 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5717/20000] Train-Loss: 0.5934963112020841 Test-Loss: 0.18006840321413417 Train-f1: 0.4648693574359773 Test-f1: 0.4234512409615009 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [5718/20000] Train-Loss: 0.5930655525094864 Test-Loss: 0.17826793001786714 Train-f1: 0.4539969906135804 Test-f1: 0.41811693332540295 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5719/20000] Train-Loss: 0.589242171569357 Test-Loss: 0.17740061110622293 Train-f1: 0.45096762812817826 Test-f1: 0.4249795283327874 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5720/20000] Train-Loss: 0.5923724665525941 Test-Loss: 0.17774676585673807 Train-f1: 0.455330762091128 Test-f1: 0.41539227895392283 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5721/20000] Train-Loss: 0.5906401378047417 Test-Loss: 0.17779897183145466 Train-f1: 0.4603379928795893 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5722/20000] Train-Loss: 0.5876468707292146 Test-Loss: 0.17746072927941065 Train-f1: 0.45357679914070886 Test-f1: 0.4124675324675325 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5723/20000] Train-Loss: 0.5877489743188303 Test-Loss: 0.17786304467900307 Train-f1: 0.4605428354615867 Test-f1: 0.4381306381556967 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5724/20000] Train-Loss: 0.5878126826918579 Test-Loss: 0.1802614246046774 Train-f1: 0.44619583325151435 Test-f1: 0.4007605466428997 Train-acc: 0.7615894039735099 Test-acc: 0.6973684210526315\n",
      "Epoch [5725/20000] Train-Loss: 0.5871208845561898 Test-Loss: 0.1775490186944967 Train-f1: 0.44777508935379273 Test-f1: 0.422660406885759 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5726/20000] Train-Loss: 0.5919725016779074 Test-Loss: 0.1779980137355699 Train-f1: 0.45229726246326757 Test-f1: 0.41811693332540295 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5727/20000] Train-Loss: 0.5892264201468417 Test-Loss: 0.18023853124278919 Train-f1: 0.4630227743271222 Test-f1: 0.4007605466428997 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [5728/20000] Train-Loss: 0.5879628847739558 Test-Loss: 0.17755802295005713 Train-f1: 0.45775454897406115 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [5729/20000] Train-Loss: 0.5927925570489065 Test-Loss: 0.17715321426606098 Train-f1: 0.44776002114809293 Test-f1: 0.41421602787456446 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5730/20000] Train-Loss: 0.5905634678014091 Test-Loss: 0.17947696231375415 Train-f1: 0.45885404630956234 Test-f1: 0.4134165729574624 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5731/20000] Train-Loss: 0.5880946156243528 Test-Loss: 0.17706406687084417 Train-f1: 0.4586833855799373 Test-f1: 0.41567460317460314 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5732/20000] Train-Loss: 0.5857094153854853 Test-Loss: 0.1782510001921844 Train-f1: 0.4548294968478962 Test-f1: 0.4452525252525252 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5733/20000] Train-Loss: 0.5920723532866805 Test-Loss: 0.1779870971247333 Train-f1: 0.4508557107053347 Test-f1: 0.4253968253968254 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5734/20000] Train-Loss: 0.5915024027913542 Test-Loss: 0.17779750846740341 Train-f1: 0.4531923617208637 Test-f1: 0.41421602787456446 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5735/20000] Train-Loss: 0.5898806777993338 Test-Loss: 0.17693855820696888 Train-f1: 0.44727762803234505 Test-f1: 0.4265359477124183 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5736/20000] Train-Loss: 0.5898800544055112 Test-Loss: 0.1777709166444857 Train-f1: 0.45374913516666054 Test-f1: 0.40374260231724596 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5737/20000] Train-Loss: 0.5902516157080447 Test-Loss: 0.17715824567756558 Train-f1: 0.45465386631859744 Test-f1: 0.4239775910364146 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5738/20000] Train-Loss: 0.5903364971472123 Test-Loss: 0.17750578527164237 Train-f1: 0.4519901251818375 Test-f1: 0.43628155628155635 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5739/20000] Train-Loss: 0.5856207791635148 Test-Loss: 0.17740259228964897 Train-f1: 0.4568484383000512 Test-f1: 0.40374260231724596 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5740/20000] Train-Loss: 0.5903104981711991 Test-Loss: 0.1769809090855409 Train-f1: 0.45445350671334594 Test-f1: 0.4150529687115053 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5741/20000] Train-Loss: 0.5883279057233025 Test-Loss: 0.17762534339932068 Train-f1: 0.4548915706693517 Test-f1: 0.43642806805597506 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5742/20000] Train-Loss: 0.588836222518991 Test-Loss: 0.17968200262442874 Train-f1: 0.4555120362176651 Test-f1: 0.4447942921400597 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5743/20000] Train-Loss: 0.5875481963524456 Test-Loss: 0.17769783886092763 Train-f1: 0.46891065028161805 Test-f1: 0.4253968253968254 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [5744/20000] Train-Loss: 0.5892037847345644 Test-Loss: 0.1806566208984531 Train-f1: 0.4555120362176651 Test-f1: 0.43202390084379677 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5745/20000] Train-Loss: 0.5882065732508477 Test-Loss: 0.17717576500012955 Train-f1: 0.4430873981912426 Test-f1: 0.4239775910364146 Train-acc: 0.7582781456953642 Test-acc: 0.7368421052631579\n",
      "Epoch [5746/20000] Train-Loss: 0.5881492858021617 Test-Loss: 0.1770665457995145 Train-f1: 0.4630875908645667 Test-f1: 0.4253968253968254 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5747/20000] Train-Loss: 0.5873046356161147 Test-Loss: 0.17813999367417227 Train-f1: 0.45706962094058873 Test-f1: 0.41811693332540295 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5748/20000] Train-Loss: 0.5868699373865705 Test-Loss: 0.1768485272831139 Train-f1: 0.4483935404867595 Test-f1: 0.4150529687115053 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [5749/20000] Train-Loss: 0.5886400302102442 Test-Loss: 0.17990139828153337 Train-f1: 0.4547915860110982 Test-f1: 0.42413273001508306 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5750/20000] Train-Loss: 0.5866384133302911 Test-Loss: 0.17833347962642465 Train-f1: 0.46259246385767067 Test-f1: 0.41935990338164253 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5751/20000] Train-Loss: 0.5908532715268102 Test-Loss: 0.17919455339532098 Train-f1: 0.4529920926834895 Test-f1: 0.4240641711229946 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5752/20000] Train-Loss: 0.5878592982935078 Test-Loss: 0.17847078363873753 Train-f1: 0.4452273186531558 Test-f1: 0.4134165729574624 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [5753/20000] Train-Loss: 0.5889255542054789 Test-Loss: 0.17702428995233313 Train-f1: 0.4557947704915845 Test-f1: 0.4239775910364146 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5754/20000] Train-Loss: 0.589188923251301 Test-Loss: 0.18000293883601023 Train-f1: 0.45994476708755216 Test-f1: 0.41202266953292954 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5755/20000] Train-Loss: 0.5847555771054226 Test-Loss: 0.1772040292719814 Train-f1: 0.45994476708755216 Test-f1: 0.4365359477124183 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5756/20000] Train-Loss: 0.5876123995142578 Test-Loss: 0.1818381357604274 Train-f1: 0.4508814633369794 Test-f1: 0.42846358707106313 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5757/20000] Train-Loss: 0.5885832988150337 Test-Loss: 0.17859911036836265 Train-f1: 0.4531923617208637 Test-f1: 0.41202266953292954 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5758/20000] Train-Loss: 0.5845652526534205 Test-Loss: 0.17769938510752273 Train-f1: 0.46535510611273095 Test-f1: 0.4382533218149657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [5759/20000] Train-Loss: 0.5896777749392009 Test-Loss: 0.18542972567659413 Train-f1: 0.45369211514393 Test-f1: 0.4060048594238112 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [5760/20000] Train-Loss: 0.587519208146394 Test-Loss: 0.17772289168864522 Train-f1: 0.45899732840579793 Test-f1: 0.43642806805597506 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5761/20000] Train-Loss: 0.5873848914092042 Test-Loss: 0.18019290264361504 Train-f1: 0.46607913466347817 Test-f1: 0.4134165729574624 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [5762/20000] Train-Loss: 0.58629671816343 Test-Loss: 0.1769561186691124 Train-f1: 0.45249579489747316 Test-f1: 0.4239775910364146 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5763/20000] Train-Loss: 0.5879723365106769 Test-Loss: 0.1802232715281499 Train-f1: 0.4568871142591009 Test-f1: 0.4007605466428997 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [5764/20000] Train-Loss: 0.5852105030953068 Test-Loss: 0.17690372555807404 Train-f1: 0.45261090482704713 Test-f1: 0.4365359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5765/20000] Train-Loss: 0.5866573711811277 Test-Loss: 0.17683469740998248 Train-f1: 0.4580289104995279 Test-f1: 0.43742690058479533 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5766/20000] Train-Loss: 0.587063495763396 Test-Loss: 0.1771824776954201 Train-f1: 0.45838827838827834 Test-f1: 0.4602863322690288 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [5767/20000] Train-Loss: 0.5871449129232452 Test-Loss: 0.17762729902362082 Train-f1: 0.4708902691511387 Test-f1: 0.44472049689440996 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [5768/20000] Train-Loss: 0.5865830169161209 Test-Loss: 0.1768247958959607 Train-f1: 0.4615183693548466 Test-f1: 0.43768009768009775 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5769/20000] Train-Loss: 0.587479337659762 Test-Loss: 0.17862153901680106 Train-f1: 0.4537240825458566 Test-f1: 0.4122994652406417 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5770/20000] Train-Loss: 0.5871238642534091 Test-Loss: 0.17845380939644656 Train-f1: 0.45769242666391713 Test-f1: 0.4134165729574624 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5771/20000] Train-Loss: 0.5908370177320573 Test-Loss: 0.1788395412683952 Train-f1: 0.45538918766779873 Test-f1: 0.4122994652406417 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5772/20000] Train-Loss: 0.5800896375932473 Test-Loss: 0.17767435915384464 Train-f1: 0.46738559639909977 Test-f1: 0.4452525252525252 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [5773/20000] Train-Loss: 0.5867890414910707 Test-Loss: 0.17731250987171074 Train-f1: 0.44136905957498096 Test-f1: 0.4602863322690288 Train-acc: 0.7549668874172185 Test-acc: 0.7763157894736842\n",
      "Epoch [5774/20000] Train-Loss: 0.5904140851185242 Test-Loss: 0.17728419425641986 Train-f1: 0.45271592064779415 Test-f1: 0.436390977443609 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5775/20000] Train-Loss: 0.5879427583430649 Test-Loss: 0.1768750712698902 Train-f1: 0.4529920926834895 Test-f1: 0.4365359477124183 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5776/20000] Train-Loss: 0.5848536764014897 Test-Loss: 0.17735044095522026 Train-f1: 0.45030667200611363 Test-f1: 0.4249675324675325 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5777/20000] Train-Loss: 0.5855825588128278 Test-Loss: 0.17976464849097845 Train-f1: 0.46506344779813025 Test-f1: 0.4447942921400597 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5778/20000] Train-Loss: 0.5871603696778851 Test-Loss: 0.17645853868429615 Train-f1: 0.4558241758241758 Test-f1: 0.4265359477124183 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5779/20000] Train-Loss: 0.5874244430805777 Test-Loss: 0.17941929599568376 Train-f1: 0.45302697302697303 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5780/20000] Train-Loss: 0.5879976613587858 Test-Loss: 0.1787902134369619 Train-f1: 0.4634368530020704 Test-f1: 0.43455108359133127 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5781/20000] Train-Loss: 0.5852787994909231 Test-Loss: 0.17992650060565188 Train-f1: 0.4493581780538302 Test-f1: 0.42413273001508306 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5782/20000] Train-Loss: 0.5864288462865267 Test-Loss: 0.1797710637792858 Train-f1: 0.45354838709677414 Test-f1: 0.4447942921400597 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5783/20000] Train-Loss: 0.5901244676051687 Test-Loss: 0.17909775961976462 Train-f1: 0.4564244167465644 Test-f1: 0.4007605466428997 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [5784/20000] Train-Loss: 0.5862441990140403 Test-Loss: 0.17773354668288244 Train-f1: 0.4530244475237183 Test-f1: 0.43574229691876754 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5785/20000] Train-Loss: 0.5860918963669163 Test-Loss: 0.17713045732514526 Train-f1: 0.45599314994815127 Test-f1: 0.4124675324675325 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5786/20000] Train-Loss: 0.5886784037988452 Test-Loss: 0.1790335850486236 Train-f1: 0.4632411301872841 Test-f1: 0.40252100840336136 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [5787/20000] Train-Loss: 0.587679204952271 Test-Loss: 0.18283705537108078 Train-f1: 0.46625336927223715 Test-f1: 0.4074109554921158 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [5788/20000] Train-Loss: 0.5837985813189217 Test-Loss: 0.18309348429450517 Train-f1: 0.45716006674429244 Test-f1: 0.42620365912373614 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5789/20000] Train-Loss: 0.5893142049994899 Test-Loss: 0.1800056264065699 Train-f1: 0.45199294868740775 Test-f1: 0.42402476780185766 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5790/20000] Train-Loss: 0.5869184343075035 Test-Loss: 0.17750114533342706 Train-f1: 0.45313558274626126 Test-f1: 0.41948862966890543 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5791/20000] Train-Loss: 0.5869624031613595 Test-Loss: 0.17837897119661453 Train-f1: 0.4586833855799373 Test-f1: 0.4134165729574624 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5792/20000] Train-Loss: 0.5865283442524494 Test-Loss: 0.17645817780359463 Train-f1: 0.4596077147991994 Test-f1: 0.4582159624413146 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [5793/20000] Train-Loss: 0.5836352193032819 Test-Loss: 0.1763182641615542 Train-f1: 0.45045263569346644 Test-f1: 0.4244497286750808 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5794/20000] Train-Loss: 0.5879723083381047 Test-Loss: 0.17658238008491817 Train-f1: 0.45639241743208914 Test-f1: 0.4365359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5795/20000] Train-Loss: 0.585953304073061 Test-Loss: 0.1766884140903448 Train-f1: 0.4508814633369794 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "Epoch [5796/20000] Train-Loss: 0.5859577834044581 Test-Loss: 0.17717455055642126 Train-f1: 0.4617180002741854 Test-f1: 0.42702535176346623 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5797/20000] Train-Loss: 0.5873803942704714 Test-Loss: 0.17762234156417395 Train-f1: 0.45056874160322435 Test-f1: 0.41811693332540295 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5798/20000] Train-Loss: 0.5830605585255121 Test-Loss: 0.1784773831638288 Train-f1: 0.4465155993431855 Test-f1: 0.4291725105189341 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5799/20000] Train-Loss: 0.5830960343261409 Test-Loss: 0.1814121867664525 Train-f1: 0.44085085975367455 Test-f1: 0.4134165729574624 Train-acc: 0.7549668874172185 Test-acc: 0.7105263157894737\n",
      "Epoch [5800/20000] Train-Loss: 0.5894373926122324 Test-Loss: 0.17787945760716442 Train-f1: 0.45916477941842554 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5801/20000] Train-Loss: 0.5871007761638265 Test-Loss: 0.17695093075452878 Train-f1: 0.4594081763766706 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5802/20000] Train-Loss: 0.5856770953797114 Test-Loss: 0.18144665575717814 Train-f1: 0.4641926244181883 Test-f1: 0.40252100840336136 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [5803/20000] Train-Loss: 0.5871420916761537 Test-Loss: 0.17840362851142952 Train-f1: 0.4649469089961566 Test-f1: 0.4240641711229946 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [5804/20000] Train-Loss: 0.5839108783575718 Test-Loss: 0.17731373418707422 Train-f1: 0.46639475960944254 Test-f1: 0.41857707509881426 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [5805/20000] Train-Loss: 0.5856339907751863 Test-Loss: 0.17975382301797996 Train-f1: 0.4596592256534516 Test-f1: 0.43455108359133127 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5806/20000] Train-Loss: 0.5880470543549571 Test-Loss: 0.1775866015893154 Train-f1: 0.45503312896275955 Test-f1: 0.4447942921400597 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5807/20000] Train-Loss: 0.5857886058593244 Test-Loss: 0.1818204177778227 Train-f1: 0.4568152007672966 Test-f1: 0.41754215933320415 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5808/20000] Train-Loss: 0.5877774835403954 Test-Loss: 0.17743009620571976 Train-f1: 0.45838827838827834 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [5809/20000] Train-Loss: 0.5880515595752575 Test-Loss: 0.17752249244489698 Train-f1: 0.46019357336430505 Test-f1: 0.44472049689440996 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [5810/20000] Train-Loss: 0.5868142113837712 Test-Loss: 0.17654358919975352 Train-f1: 0.4581662471539915 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5811/20000] Train-Loss: 0.5891372171561521 Test-Loss: 0.1782306731517176 Train-f1: 0.45563861976436454 Test-f1: 0.42402476780185766 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5812/20000] Train-Loss: 0.5819702947635772 Test-Loss: 0.1760673175355508 Train-f1: 0.4550115181679134 Test-f1: 0.4265359477124183 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5813/20000] Train-Loss: 0.5853178215337933 Test-Loss: 0.17887233245072517 Train-f1: 0.4558241758241758 Test-f1: 0.4122994652406417 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5814/20000] Train-Loss: 0.5894025590567715 Test-Loss: 0.17885111061566486 Train-f1: 0.4660592096075967 Test-f1: 0.4234512409615009 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [5815/20000] Train-Loss: 0.5867645413287509 Test-Loss: 0.17740485416278293 Train-f1: 0.4620884647095133 Test-f1: 0.4134165729574624 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5816/20000] Train-Loss: 0.5818803192159248 Test-Loss: 0.1759202844363126 Train-f1: 0.44426779313876086 Test-f1: 0.4265359477124183 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [5817/20000] Train-Loss: 0.5815690367567695 Test-Loss: 0.1787319997614403 Train-f1: 0.4356144533145824 Test-f1: 0.4128560719640181 Train-acc: 0.7483443708609272 Test-acc: 0.7236842105263158\n",
      "Epoch [5818/20000] Train-Loss: 0.5855509605870238 Test-Loss: 0.1760269146273582 Train-f1: 0.4667718770438195 Test-f1: 0.43768009768009775 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [5819/20000] Train-Loss: 0.5857981268029376 Test-Loss: 0.17769286861643457 Train-f1: 0.45569195523607886 Test-f1: 0.42402476780185766 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5820/20000] Train-Loss: 0.5844546286895124 Test-Loss: 0.17687763556788594 Train-f1: 0.4644013941119228 Test-f1: 0.4602863322690288 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [5821/20000] Train-Loss: 0.5835207504943909 Test-Loss: 0.1785161128874595 Train-f1: 0.4517229280159688 Test-f1: 0.42247442455242973 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5822/20000] Train-Loss: 0.5871869156499351 Test-Loss: 0.17877559170559484 Train-f1: 0.4433142754818823 Test-f1: 0.42402476780185766 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5823/20000] Train-Loss: 0.5831652141752961 Test-Loss: 0.17631593136537227 Train-f1: 0.47139498667975366 Test-f1: 0.4253968253968254 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [5824/20000] Train-Loss: 0.5844336162772593 Test-Loss: 0.17716582805073317 Train-f1: 0.4630885089499251 Test-f1: 0.44472049689440996 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [5825/20000] Train-Loss: 0.5872077521067902 Test-Loss: 0.18103622506728456 Train-f1: 0.4380536569564718 Test-f1: 0.4134165729574624 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [5826/20000] Train-Loss: 0.5879647700851978 Test-Loss: 0.17858870457903595 Train-f1: 0.45288806175902946 Test-f1: 0.4007605466428997 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [5827/20000] Train-Loss: 0.5855500258034658 Test-Loss: 0.17675936351086402 Train-f1: 0.46263433697486456 Test-f1: 0.40844720496894416 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5828/20000] Train-Loss: 0.5865419803597396 Test-Loss: 0.1768761356121576 Train-f1: 0.45720523792339024 Test-f1: 0.41948862966890543 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5829/20000] Train-Loss: 0.5800149711964496 Test-Loss: 0.1825936529636132 Train-f1: 0.46559556071751196 Test-f1: 0.4386138804049252 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [5830/20000] Train-Loss: 0.5873068679390762 Test-Loss: 0.1795342211842161 Train-f1: 0.456989676989677 Test-f1: 0.4122994652406417 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5831/20000] Train-Loss: 0.5854631151206304 Test-Loss: 0.17614791875614955 Train-f1: 0.4563706464547238 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5832/20000] Train-Loss: 0.5811499242308653 Test-Loss: 0.18309433738980682 Train-f1: 0.4601692688402469 Test-f1: 0.4074109554921158 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [5833/20000] Train-Loss: 0.583665168297192 Test-Loss: 0.18009834171601616 Train-f1: 0.4521556800008228 Test-f1: 0.4447942921400597 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5834/20000] Train-Loss: 0.5858501693093114 Test-Loss: 0.17719116681009742 Train-f1: 0.45765419225273246 Test-f1: 0.43455108359133127 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5835/20000] Train-Loss: 0.584377340037837 Test-Loss: 0.17759908733443636 Train-f1: 0.4589049198682028 Test-f1: 0.4134165729574624 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5836/20000] Train-Loss: 0.5830851727581485 Test-Loss: 0.17597405214940504 Train-f1: 0.44820222197051846 Test-f1: 0.40374260231724596 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [5837/20000] Train-Loss: 0.5843341278742284 Test-Loss: 0.18043767327174784 Train-f1: 0.4554013987011212 Test-f1: 0.43202390084379677 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5838/20000] Train-Loss: 0.585908554424163 Test-Loss: 0.17584325476779933 Train-f1: 0.4513748320644873 Test-f1: 0.4253968253968254 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5839/20000] Train-Loss: 0.5821776193159327 Test-Loss: 0.1757112740766704 Train-f1: 0.450554937584971 Test-f1: 0.4265359477124183 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [5840/20000] Train-Loss: 0.5860866092710253 Test-Loss: 0.1778659636899831 Train-f1: 0.46531480018214066 Test-f1: 0.4447942921400597 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [5841/20000] Train-Loss: 0.5801106948228071 Test-Loss: 0.18502001789670086 Train-f1: 0.45866441251056633 Test-f1: 0.41754215933320415 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5842/20000] Train-Loss: 0.5833985517326928 Test-Loss: 0.17691013011458995 Train-f1: 0.4644781080217322 Test-f1: 0.4302232667450059 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5843/20000] Train-Loss: 0.5830389038566086 Test-Loss: 0.17701259209388293 Train-f1: 0.44803044968216177 Test-f1: 0.43485919098822323 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5844/20000] Train-Loss: 0.5876180306765403 Test-Loss: 0.17601337810662893 Train-f1: 0.4510066736556803 Test-f1: 0.446829268292683 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [5845/20000] Train-Loss: 0.5858974155103432 Test-Loss: 0.17699311213215405 Train-f1: 0.4501347708894879 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [5846/20000] Train-Loss: 0.583030122255151 Test-Loss: 0.17563604253377466 Train-f1: 0.46251931974009663 Test-f1: 0.41567460317460314 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5847/20000] Train-Loss: 0.5832127080972014 Test-Loss: 0.1761096718066339 Train-f1: 0.4570353019909653 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [5848/20000] Train-Loss: 0.5840916201325838 Test-Loss: 0.17569816784443912 Train-f1: 0.4609045354932141 Test-f1: 0.4253968253968254 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5849/20000] Train-Loss: 0.586093539216898 Test-Loss: 0.1765126270637926 Train-f1: 0.45207139291256615 Test-f1: 0.43628155628155635 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5850/20000] Train-Loss: 0.5846441046648224 Test-Loss: 0.17702821607674135 Train-f1: 0.44486846486846493 Test-f1: 0.44472049689440996 Train-acc: 0.7615894039735099 Test-acc: 0.7631578947368421\n",
      "Epoch [5851/20000] Train-Loss: 0.5797772644681518 Test-Loss: 0.1762175272645432 Train-f1: 0.4630875908645667 Test-f1: 0.4369302783936931 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [5852/20000] Train-Loss: 0.5832109108366468 Test-Loss: 0.17590430404546864 Train-f1: 0.4538725482698318 Test-f1: 0.43742690058479533 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5853/20000] Train-Loss: 0.5829167739943526 Test-Loss: 0.17564623962268652 Train-f1: 0.4599310858310391 Test-f1: 0.4253968253968254 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5854/20000] Train-Loss: 0.5844232890691133 Test-Loss: 0.17995816085254962 Train-f1: 0.46088056121990945 Test-f1: 0.4134165729574624 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5855/20000] Train-Loss: 0.5860018819313347 Test-Loss: 0.17818768536503699 Train-f1: 0.46050614827287095 Test-f1: 0.4134165729574624 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5856/20000] Train-Loss: 0.5849311993083646 Test-Loss: 0.17802810197829338 Train-f1: 0.46118579508291335 Test-f1: 0.4122994652406417 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [5857/20000] Train-Loss: 0.5828755421720153 Test-Loss: 0.18056435071338864 Train-f1: 0.45313558274626126 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5858/20000] Train-Loss: 0.5833783299719224 Test-Loss: 0.1797475208167009 Train-f1: 0.4652710164087409 Test-f1: 0.4134165729574624 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5859/20000] Train-Loss: 0.581888794089513 Test-Loss: 0.17544455611031798 Train-f1: 0.4578483976005375 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5860/20000] Train-Loss: 0.5858208013603085 Test-Loss: 0.17621667568499594 Train-f1: 0.45173446151803487 Test-f1: 0.43642806805597506 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5861/20000] Train-Loss: 0.5846292299078522 Test-Loss: 0.17690577538570418 Train-f1: 0.4580876340847634 Test-f1: 0.4302232667450059 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5862/20000] Train-Loss: 0.5808407667013458 Test-Loss: 0.17809089206064851 Train-f1: 0.4551490236382323 Test-f1: 0.42413273001508306 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5863/20000] Train-Loss: 0.5831586424752288 Test-Loss: 0.17568592347169396 Train-f1: 0.46398268398268405 Test-f1: 0.436390977443609 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [5864/20000] Train-Loss: 0.5837269347660801 Test-Loss: 0.17709037818148746 Train-f1: 0.4634368530020704 Test-f1: 0.42413273001508306 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5865/20000] Train-Loss: 0.5820811672932563 Test-Loss: 0.18266016018309383 Train-f1: 0.45613547702185897 Test-f1: 0.39474782948005077 Train-acc: 0.7748344370860927 Test-acc: 0.6842105263157895\n",
      "Epoch [5866/20000] Train-Loss: 0.5851265218909891 Test-Loss: 0.17761803402384843 Train-f1: 0.440448773123659 Test-f1: 0.4007605466428997 Train-acc: 0.7549668874172185 Test-acc: 0.6973684210526315\n",
      "Epoch [5867/20000] Train-Loss: 0.5832320754285932 Test-Loss: 0.1801134501696258 Train-f1: 0.44895724799470765 Test-f1: 0.4007605466428997 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [5868/20000] Train-Loss: 0.584855705634384 Test-Loss: 0.1762810433181533 Train-f1: 0.4530816263643588 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5869/20000] Train-Loss: 0.5815095011643315 Test-Loss: 0.17563489484667974 Train-f1: 0.45609846427493766 Test-f1: 0.4253968253968254 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [5870/20000] Train-Loss: 0.5840174265659716 Test-Loss: 0.18098884322600398 Train-f1: 0.4586075290331186 Test-f1: 0.4074109554921158 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [5871/20000] Train-Loss: 0.581675836409269 Test-Loss: 0.17550479517166157 Train-f1: 0.4570490758994358 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [5872/20000] Train-Loss: 0.5827783423266256 Test-Loss: 0.1789063392431925 Train-f1: 0.4551850564502633 Test-f1: 0.4134165729574624 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5873/20000] Train-Loss: 0.5814308576898316 Test-Loss: 0.17594515683187045 Train-f1: 0.4605281737343045 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [5874/20000] Train-Loss: 0.584864811229917 Test-Loss: 0.17580962041207804 Train-f1: 0.4570213963936851 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [5875/20000] Train-Loss: 0.5832201749736019 Test-Loss: 0.17699571902804082 Train-f1: 0.4656963645673323 Test-f1: 0.4602863322690288 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [5876/20000] Train-Loss: 0.5817851716291093 Test-Loss: 0.18061916340347503 Train-f1: 0.4560572760572761 Test-f1: 0.4074109554921158 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [5877/20000] Train-Loss: 0.5825401078753923 Test-Loss: 0.17525671772265888 Train-f1: 0.4516535279204784 Test-f1: 0.4253968253968254 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5878/20000] Train-Loss: 0.5825681576223192 Test-Loss: 0.17572187105936718 Train-f1: 0.4578535208298999 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [5879/20000] Train-Loss: 0.5824818810316831 Test-Loss: 0.17603002486932212 Train-f1: 0.4519671795658903 Test-f1: 0.41857707509881426 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5880/20000] Train-Loss: 0.5834973196821442 Test-Loss: 0.17601095560669536 Train-f1: 0.4521249359959037 Test-f1: 0.4253968253968254 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5881/20000] Train-Loss: 0.5807568754891164 Test-Loss: 0.17716326448698555 Train-f1: 0.4631963645673324 Test-f1: 0.4068599033816426 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5882/20000] Train-Loss: 0.5850761000025333 Test-Loss: 0.17785780443895455 Train-f1: 0.45838827838827834 Test-f1: 0.4134165729574624 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5883/20000] Train-Loss: 0.580395163804115 Test-Loss: 0.17936512360005835 Train-f1: 0.46068527703316164 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5884/20000] Train-Loss: 0.5816101545979399 Test-Loss: 0.17521069025396718 Train-f1: 0.45822994210090984 Test-f1: 0.4265359477124183 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5885/20000] Train-Loss: 0.5821373167502341 Test-Loss: 0.17717712664218974 Train-f1: 0.45553825237918677 Test-f1: 0.40272034395153417 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [5886/20000] Train-Loss: 0.5790320998397304 Test-Loss: 0.17747596920251693 Train-f1: 0.4592477851355595 Test-f1: 0.4128560719640181 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5887/20000] Train-Loss: 0.5825526385103529 Test-Loss: 0.17626887344522654 Train-f1: 0.4551113564654875 Test-f1: 0.43628155628155635 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5888/20000] Train-Loss: 0.5781460486591924 Test-Loss: 0.1766169171768295 Train-f1: 0.4584447465435475 Test-f1: 0.44472049689440996 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [5889/20000] Train-Loss: 0.5839137212036122 Test-Loss: 0.17648581302837632 Train-f1: 0.44582933844678807 Test-f1: 0.41857707509881426 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [5890/20000] Train-Loss: 0.5803721088125036 Test-Loss: 0.1759273942860308 Train-f1: 0.4596077147991994 Test-f1: 0.43642806805597506 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5891/20000] Train-Loss: 0.5794852596518495 Test-Loss: 0.1813080885235601 Train-f1: 0.4570155366110121 Test-f1: 0.39665763844868324 Train-acc: 0.7748344370860927 Test-acc: 0.6842105263157895\n",
      "Epoch [5892/20000] Train-Loss: 0.5829159401384105 Test-Loss: 0.1840314296250367 Train-f1: 0.44762742057440186 Test-f1: 0.40611358790463276 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [5893/20000] Train-Loss: 0.5834380486657158 Test-Loss: 0.18113099072522026 Train-f1: 0.46395562691577874 Test-f1: 0.42846358707106313 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5894/20000] Train-Loss: 0.584418617686442 Test-Loss: 0.17520833936295724 Train-f1: 0.4497722567287785 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "Epoch [5895/20000] Train-Loss: 0.5832650384466541 Test-Loss: 0.17751941898737353 Train-f1: 0.4612211981566821 Test-f1: 0.42402476780185766 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5896/20000] Train-Loss: 0.5828053480820224 Test-Loss: 0.17741683642899514 Train-f1: 0.4603379928795893 Test-f1: 0.40252100840336136 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [5897/20000] Train-Loss: 0.5841835841306235 Test-Loss: 0.17695521425670352 Train-f1: 0.4559247648902821 Test-f1: 0.4068599033816426 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5898/20000] Train-Loss: 0.582303985620041 Test-Loss: 0.17504395409568582 Train-f1: 0.46026490922013313 Test-f1: 0.4253968253968254 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5899/20000] Train-Loss: 0.5783237094709234 Test-Loss: 0.176556775150101 Train-f1: 0.4448397272952433 Test-f1: 0.4249675324675325 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5900/20000] Train-Loss: 0.5817577262082649 Test-Loss: 0.17737018030782198 Train-f1: 0.45925158527688925 Test-f1: 0.4240641711229946 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5901/20000] Train-Loss: 0.5822438604317547 Test-Loss: 0.1862287933070812 Train-f1: 0.44867167919799505 Test-f1: 0.39665763844868324 Train-acc: 0.7649006622516556 Test-acc: 0.6842105263157895\n",
      "Epoch [5902/20000] Train-Loss: 0.5826505081317013 Test-Loss: 0.17552904512776757 Train-f1: 0.46271792364851355 Test-f1: 0.41062801932367154 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5903/20000] Train-Loss: 0.5828326878967366 Test-Loss: 0.17525654919171485 Train-f1: 0.4667815941611949 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [5904/20000] Train-Loss: 0.5803525910709956 Test-Loss: 0.17524046421209472 Train-f1: 0.46208110237579003 Test-f1: 0.4590643274853802 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [5905/20000] Train-Loss: 0.5798675775603713 Test-Loss: 0.18379462891033838 Train-f1: 0.4635747796050936 Test-f1: 0.4386138804049252 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5906/20000] Train-Loss: 0.5816124123037049 Test-Loss: 0.17821964742421076 Train-f1: 0.45044432198285256 Test-f1: 0.4234512409615009 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5907/20000] Train-Loss: 0.5833466264474667 Test-Loss: 0.1753812031104869 Train-f1: 0.4642384902491086 Test-f1: 0.4124675324675325 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5908/20000] Train-Loss: 0.5777474217628202 Test-Loss: 0.17822580783411665 Train-f1: 0.4575358808616549 Test-f1: 0.4134165729574624 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [5909/20000] Train-Loss: 0.5801173024796579 Test-Loss: 0.1750543294477516 Train-f1: 0.45808840460787864 Test-f1: 0.41295692040807896 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5910/20000] Train-Loss: 0.5807781077042635 Test-Loss: 0.17550594516281062 Train-f1: 0.45947538412374644 Test-f1: 0.4590643274853802 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [5911/20000] Train-Loss: 0.5796124259058333 Test-Loss: 0.18246360218942614 Train-f1: 0.44298207885304663 Test-f1: 0.39474782948005077 Train-acc: 0.7582781456953642 Test-acc: 0.6842105263157895\n",
      "Epoch [5912/20000] Train-Loss: 0.5831102866038276 Test-Loss: 0.17766016364230233 Train-f1: 0.4623336968375137 Test-f1: 0.42402476780185766 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [5913/20000] Train-Loss: 0.5822920938663475 Test-Loss: 0.17506823086155535 Train-f1: 0.45381643729602306 Test-f1: 0.42430685593476297 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [5914/20000] Train-Loss: 0.5809880269322885 Test-Loss: 0.17489908851909475 Train-f1: 0.46531480018214066 Test-f1: 0.43768009768009775 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [5915/20000] Train-Loss: 0.5832496659825521 Test-Loss: 0.18053673385724409 Train-f1: 0.4528706307550666 Test-f1: 0.4122994652406417 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5916/20000] Train-Loss: 0.5824480700486058 Test-Loss: 0.1775670663997013 Train-f1: 0.45665884272573376 Test-f1: 0.4419047619047619 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [5917/20000] Train-Loss: 0.5828198450957954 Test-Loss: 0.17721364309724474 Train-f1: 0.4540627745182905 Test-f1: 0.4447942921400597 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [5918/20000] Train-Loss: 0.5795447890216062 Test-Loss: 0.1757116316430623 Train-f1: 0.4663285667600441 Test-f1: 0.4365359477124183 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [5919/20000] Train-Loss: 0.5827156365206718 Test-Loss: 0.18009150556774586 Train-f1: 0.4656963645673323 Test-f1: 0.41827724124475657 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [5920/20000] Train-Loss: 0.5806958904460747 Test-Loss: 0.1747159857945825 Train-f1: 0.4512409693224634 Test-f1: 0.4150529687115053 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5921/20000] Train-Loss: 0.5841301876643133 Test-Loss: 0.17471663148938363 Train-f1: 0.45229726246326757 Test-f1: 0.41567460317460314 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [5922/20000] Train-Loss: 0.578900239896719 Test-Loss: 0.1820871407008116 Train-f1: 0.4558241758241758 Test-f1: 0.39474782948005077 Train-acc: 0.7748344370860927 Test-acc: 0.6842105263157895\n",
      "Epoch [5923/20000] Train-Loss: 0.5813456337511032 Test-Loss: 0.1747928679958542 Train-f1: 0.4551490236382323 Test-f1: 0.40374260231724596 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5924/20000] Train-Loss: 0.5810054184653527 Test-Loss: 0.17497324604744235 Train-f1: 0.4484028542454159 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "Epoch [5925/20000] Train-Loss: 0.5815770985036915 Test-Loss: 0.17484666461619616 Train-f1: 0.44511550706866665 Test-f1: 0.43742690058479533 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "Epoch [5926/20000] Train-Loss: 0.5791732743261494 Test-Loss: 0.17703973902207223 Train-f1: 0.46442252765902714 Test-f1: 0.40252100840336136 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [5927/20000] Train-Loss: 0.5782718794985447 Test-Loss: 0.17745405642820944 Train-f1: 0.4614062409503646 Test-f1: 0.40252100840336136 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [5928/20000] Train-Loss: 0.5825944882693874 Test-Loss: 0.1775558439815383 Train-f1: 0.4660464397016827 Test-f1: 0.40272034395153417 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [5929/20000] Train-Loss: 0.5806118451135373 Test-Loss: 0.1766337741353419 Train-f1: 0.4471559918965249 Test-f1: 0.4447942921400597 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [5930/20000] Train-Loss: 0.5793894109437079 Test-Loss: 0.17698939841444977 Train-f1: 0.457639751552795 Test-f1: 0.4291725105189341 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5931/20000] Train-Loss: 0.5850407784464218 Test-Loss: 0.1764058369087673 Train-f1: 0.44376950995857534 Test-f1: 0.41857707509881426 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [5932/20000] Train-Loss: 0.5796559763096994 Test-Loss: 0.17584332028272515 Train-f1: 0.45443420378904253 Test-f1: 0.40844720496894416 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5933/20000] Train-Loss: 0.5788207670907495 Test-Loss: 0.18048288136324586 Train-f1: 0.46675664587324306 Test-f1: 0.40720901421745037 Train-acc: 0.7913907284768212 Test-acc: 0.6973684210526315\n",
      "Epoch [5934/20000] Train-Loss: 0.5783992698178773 Test-Loss: 0.18009343039812462 Train-f1: 0.4549758229881733 Test-f1: 0.4167235630358844 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5935/20000] Train-Loss: 0.5763041679558343 Test-Loss: 0.17482440337391394 Train-f1: 0.4459940122436212 Test-f1: 0.41295692040807896 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [5936/20000] Train-Loss: 0.5820427784394506 Test-Loss: 0.1745078434388191 Train-f1: 0.4602200240153535 Test-f1: 0.4150529687115053 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5937/20000] Train-Loss: 0.5840612681591042 Test-Loss: 0.17495053597125063 Train-f1: 0.4578483976005375 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [5938/20000] Train-Loss: 0.5796957202598655 Test-Loss: 0.17646027831675498 Train-f1: 0.45420492800809215 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5939/20000] Train-Loss: 0.5797643189950209 Test-Loss: 0.1745194387582296 Train-f1: 0.4659063831737158 Test-f1: 0.41567460317460314 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [5940/20000] Train-Loss: 0.5784539959802116 Test-Loss: 0.1745572578542142 Train-f1: 0.4528177001759205 Test-f1: 0.42430685593476297 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5941/20000] Train-Loss: 0.5816015257922315 Test-Loss: 0.1750873978326296 Train-f1: 0.44917609272447984 Test-f1: 0.43574229691876754 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5942/20000] Train-Loss: 0.5813501600156609 Test-Loss: 0.17601766685902606 Train-f1: 0.4509642727358205 Test-f1: 0.41857707509881426 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5943/20000] Train-Loss: 0.576936981499092 Test-Loss: 0.17785837112528785 Train-f1: 0.4529920926834895 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [5944/20000] Train-Loss: 0.5825420538373433 Test-Loss: 0.17539239995857803 Train-f1: 0.4535207766380573 Test-f1: 0.43642806805597506 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5945/20000] Train-Loss: 0.5834433373927853 Test-Loss: 0.17937585971545422 Train-f1: 0.4511698455671291 Test-f1: 0.40252100840336136 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [5946/20000] Train-Loss: 0.5793576933245633 Test-Loss: 0.17694813566599552 Train-f1: 0.4525579743516584 Test-f1: 0.4128817587641117 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5947/20000] Train-Loss: 0.5804846938986772 Test-Loss: 0.1752175999958346 Train-f1: 0.4539778567593138 Test-f1: 0.4253968253968254 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5948/20000] Train-Loss: 0.5820887754755661 Test-Loss: 0.17467230645276158 Train-f1: 0.4533273622928795 Test-f1: 0.4253968253968254 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5949/20000] Train-Loss: 0.5779888485133774 Test-Loss: 0.17471650049538412 Train-f1: 0.45135700585403676 Test-f1: 0.40374260231724596 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [5950/20000] Train-Loss: 0.5796115417517251 Test-Loss: 0.17592936649745336 Train-f1: 0.45082323212382047 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [5951/20000] Train-Loss: 0.5777014715953569 Test-Loss: 0.17579695804216025 Train-f1: 0.45867181377565824 Test-f1: 0.4302232667450059 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5952/20000] Train-Loss: 0.5805215885187447 Test-Loss: 0.1759176871855957 Train-f1: 0.45967275582847356 Test-f1: 0.4302232667450059 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5953/20000] Train-Loss: 0.5823172639413806 Test-Loss: 0.17672481501765694 Train-f1: 0.45676445384740044 Test-f1: 0.4234512409615009 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [5954/20000] Train-Loss: 0.5797880547439085 Test-Loss: 0.17558691194724646 Train-f1: 0.4381983384358599 Test-f1: 0.4088146077440076 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [5955/20000] Train-Loss: 0.5808167516015057 Test-Loss: 0.17582701795911646 Train-f1: 0.4551850564502633 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [5956/20000] Train-Loss: 0.5756922105711259 Test-Loss: 0.18384800009210853 Train-f1: 0.4582732360767043 Test-f1: 0.4210688213504322 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [5957/20000] Train-Loss: 0.5830657744654741 Test-Loss: 0.1809836063413115 Train-f1: 0.4383030890530377 Test-f1: 0.4181515499425948 Train-acc: 0.7516556291390728 Test-acc: 0.7105263157894737\n",
      "Epoch [5958/20000] Train-Loss: 0.5831504319390728 Test-Loss: 0.1767242643191249 Train-f1: 0.46052672147995893 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5959/20000] Train-Loss: 0.5775715233535531 Test-Loss: 0.17471798343883882 Train-f1: 0.4587651309416043 Test-f1: 0.42430685593476297 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5960/20000] Train-Loss: 0.5784859408392874 Test-Loss: 0.17697179490964107 Train-f1: 0.4506576824278098 Test-f1: 0.40272034395153417 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [5961/20000] Train-Loss: 0.5794117684263139 Test-Loss: 0.17454357745635898 Train-f1: 0.47093674279527764 Test-f1: 0.41421602787456446 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [5962/20000] Train-Loss: 0.5784021308489241 Test-Loss: 0.17531375478138497 Train-f1: 0.460941551177268 Test-f1: 0.43485919098822323 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5963/20000] Train-Loss: 0.580794106282422 Test-Loss: 0.17421743051764874 Train-f1: 0.4587800873878972 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5964/20000] Train-Loss: 0.5823602043126781 Test-Loss: 0.17517894639743614 Train-f1: 0.455862775217614 Test-f1: 0.43628155628155635 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5965/20000] Train-Loss: 0.5796331336059376 Test-Loss: 0.17432696262770447 Train-f1: 0.45707701000609535 Test-f1: 0.41295692040807896 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5966/20000] Train-Loss: 0.5786169441640081 Test-Loss: 0.17449175784108426 Train-f1: 0.4542947599664018 Test-f1: 0.4590643274853802 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [5967/20000] Train-Loss: 0.5794860242132664 Test-Loss: 0.17462052506073888 Train-f1: 0.4656963645673323 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [5968/20000] Train-Loss: 0.5781653196386716 Test-Loss: 0.1749380105295525 Train-f1: 0.4521746387352893 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [5969/20000] Train-Loss: 0.5794705491605217 Test-Loss: 0.17617730781475568 Train-f1: 0.45119746762437607 Test-f1: 0.4545938580861545 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [5970/20000] Train-Loss: 0.5799072589881998 Test-Loss: 0.17532955156979238 Train-f1: 0.4513124268551912 Test-f1: 0.436390977443609 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5971/20000] Train-Loss: 0.5772975057193367 Test-Loss: 0.17796336917301087 Train-f1: 0.4616673283621953 Test-f1: 0.4007605466428997 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [5972/20000] Train-Loss: 0.5781603232897711 Test-Loss: 0.17706102626601525 Train-f1: 0.4638132476842154 Test-f1: 0.4291725105189341 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [5973/20000] Train-Loss: 0.5778826592617348 Test-Loss: 0.1775082467521372 Train-f1: 0.45954990150593195 Test-f1: 0.4447942921400597 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5974/20000] Train-Loss: 0.5766833796849306 Test-Loss: 0.17418414987961256 Train-f1: 0.4532720615127771 Test-f1: 0.4249795283327874 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5975/20000] Train-Loss: 0.5811247682862747 Test-Loss: 0.1750816838983765 Train-f1: 0.45356866399960294 Test-f1: 0.4088146077440076 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5976/20000] Train-Loss: 0.5788077970573372 Test-Loss: 0.17966481145143892 Train-f1: 0.4748172014535611 Test-f1: 0.4178951396861845 Train-acc: 0.7980132450331126 Test-acc: 0.7105263157894737\n",
      "Epoch [5977/20000] Train-Loss: 0.5784979742583103 Test-Loss: 0.1799513854327626 Train-f1: 0.46471464921846606 Test-f1: 0.41827724124475657 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [5978/20000] Train-Loss: 0.5804355739315541 Test-Loss: 0.1765765118443597 Train-f1: 0.4566334060505922 Test-f1: 0.42413273001508306 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [5979/20000] Train-Loss: 0.5759986072343298 Test-Loss: 0.17437517087469795 Train-f1: 0.45978770333609037 Test-f1: 0.42430685593476297 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5980/20000] Train-Loss: 0.5776041333578064 Test-Loss: 0.1758829852922147 Train-f1: 0.45553986338412644 Test-f1: 0.4068599033816426 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [5981/20000] Train-Loss: 0.5824485560258286 Test-Loss: 0.17640628913919423 Train-f1: 0.4653069369789682 Test-f1: 0.40252100840336136 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [5982/20000] Train-Loss: 0.5801380621037339 Test-Loss: 0.17857358211953103 Train-f1: 0.4519473458309905 Test-f1: 0.43426704014939316 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5983/20000] Train-Loss: 0.579242075680294 Test-Loss: 0.17430693398969802 Train-f1: 0.448435982558558 Test-f1: 0.4253968253968254 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [5984/20000] Train-Loss: 0.5769778281313084 Test-Loss: 0.17609413982336866 Train-f1: 0.45556935817805383 Test-f1: 0.4447942921400597 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [5985/20000] Train-Loss: 0.5757930497749804 Test-Loss: 0.18294332295631804 Train-f1: 0.4531826050747408 Test-f1: 0.39474782948005077 Train-acc: 0.7715231788079471 Test-acc: 0.6842105263157895\n",
      "Epoch [5986/20000] Train-Loss: 0.5769034815392083 Test-Loss: 0.17542353071615924 Train-f1: 0.46758945556662745 Test-f1: 0.4365339280025486 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [5987/20000] Train-Loss: 0.5788741637312714 Test-Loss: 0.17391317933659442 Train-f1: 0.45874747089745416 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [5988/20000] Train-Loss: 0.5805762315944619 Test-Loss: 0.17459004282250443 Train-f1: 0.4494955099749987 Test-f1: 0.4150529687115053 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [5989/20000] Train-Loss: 0.5805830606356537 Test-Loss: 0.18414276363972856 Train-f1: 0.4619181057890735 Test-f1: 0.4178951396861845 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [5990/20000] Train-Loss: 0.5806094829640753 Test-Loss: 0.17483351423719476 Train-f1: 0.4608108417724418 Test-f1: 0.4253968253968254 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [5991/20000] Train-Loss: 0.5732563082183836 Test-Loss: 0.17574387738447073 Train-f1: 0.46328193186627536 Test-f1: 0.4602863322690288 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [5992/20000] Train-Loss: 0.5835882194723538 Test-Loss: 0.1740364345098801 Train-f1: 0.44977132579650564 Test-f1: 0.436390977443609 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [5993/20000] Train-Loss: 0.5744139924717911 Test-Loss: 0.1747612883628 Train-f1: 0.4630875908645667 Test-f1: 0.4249795283327874 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [5994/20000] Train-Loss: 0.5773009946030743 Test-Loss: 0.18018686534055955 Train-f1: 0.45243277002951104 Test-f1: 0.4386138804049252 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [5995/20000] Train-Loss: 0.5753647696813231 Test-Loss: 0.17444138394338066 Train-f1: 0.45109915357938934 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "Epoch [5996/20000] Train-Loss: 0.5773588970985101 Test-Loss: 0.17541898542878384 Train-f1: 0.4597626596612012 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [5997/20000] Train-Loss: 0.5778902369450083 Test-Loss: 0.17458035711305886 Train-f1: 0.46335861247307886 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [5998/20000] Train-Loss: 0.5775884974735557 Test-Loss: 0.1744051226220047 Train-f1: 0.4651034727773336 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [5999/20000] Train-Loss: 0.5765270796073387 Test-Loss: 0.17736004203634326 Train-f1: 0.470522711119726 Test-f1: 0.4362023070097605 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6000/20000] Train-Loss: 0.5781256210115258 Test-Loss: 0.1738674696469135 Train-f1: 0.4510947086486654 Test-f1: 0.43628155628155635 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [6001/20000] Train-Loss: 0.5784560599219799 Test-Loss: 0.17397961682563912 Train-f1: 0.4661359250068927 Test-f1: 0.4253968253968254 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6002/20000] Train-Loss: 0.5758605829000665 Test-Loss: 0.17543475656388696 Train-f1: 0.465541578594628 Test-f1: 0.45103407755581665 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6003/20000] Train-Loss: 0.5770913206082152 Test-Loss: 0.17620442282854437 Train-f1: 0.4603641208709778 Test-f1: 0.42413273001508306 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6004/20000] Train-Loss: 0.572652373652116 Test-Loss: 0.1794922655693945 Train-f1: 0.46590441932168547 Test-f1: 0.42620365912373614 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6005/20000] Train-Loss: 0.5740865285132716 Test-Loss: 0.17690812008172876 Train-f1: 0.46088056121990945 Test-f1: 0.4389639222941721 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6006/20000] Train-Loss: 0.5760640147941222 Test-Loss: 0.1746352428747485 Train-f1: 0.4543064690978106 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6007/20000] Train-Loss: 0.5767046485105602 Test-Loss: 0.17813173408190716 Train-f1: 0.45838827838827834 Test-f1: 0.43455108359133127 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6008/20000] Train-Loss: 0.5776318589348832 Test-Loss: 0.17472547646603478 Train-f1: 0.4553392217101894 Test-f1: 0.43628155628155635 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6009/20000] Train-Loss: 0.5740913150380134 Test-Loss: 0.1760763701283238 Train-f1: 0.45838716691566883 Test-f1: 0.4068599033816426 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6010/20000] Train-Loss: 0.5752507801000097 Test-Loss: 0.1739520856435425 Train-f1: 0.4550115181679134 Test-f1: 0.41295692040807896 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6011/20000] Train-Loss: 0.5759219140608814 Test-Loss: 0.17379898236606298 Train-f1: 0.46224316803241894 Test-f1: 0.4265359477124183 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6012/20000] Train-Loss: 0.5823705681519935 Test-Loss: 0.17975822522640525 Train-f1: 0.45727983459326743 Test-f1: 0.4074109554921158 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6013/20000] Train-Loss: 0.5779560463665929 Test-Loss: 0.17498582914262817 Train-f1: 0.4557453180186739 Test-f1: 0.40844720496894416 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6014/20000] Train-Loss: 0.577171568998789 Test-Loss: 0.17424120239482543 Train-f1: 0.4676662259895793 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6015/20000] Train-Loss: 0.5764146662389633 Test-Loss: 0.17891678207823383 Train-f1: 0.4642944955240421 Test-f1: 0.41754215933320415 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6016/20000] Train-Loss: 0.5775689513785961 Test-Loss: 0.17595269350655934 Train-f1: 0.45871423838352543 Test-f1: 0.4134165729574624 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6017/20000] Train-Loss: 0.5743902084537126 Test-Loss: 0.18076678710107325 Train-f1: 0.4485048573459155 Test-f1: 0.40720901421745037 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [6018/20000] Train-Loss: 0.5700514905091142 Test-Loss: 0.17509599853884286 Train-f1: 0.4505543429429074 Test-f1: 0.4276190476190476 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6019/20000] Train-Loss: 0.578610276641922 Test-Loss: 0.17368140114005046 Train-f1: 0.44829071971306733 Test-f1: 0.4144537815126051 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [6020/20000] Train-Loss: 0.5759898835576293 Test-Loss: 0.1769366313001039 Train-f1: 0.4599411044228822 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6021/20000] Train-Loss: 0.5765869033159187 Test-Loss: 0.17416113448225298 Train-f1: 0.4462269996566025 Test-f1: 0.4150529687115053 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [6022/20000] Train-Loss: 0.5766233287102598 Test-Loss: 0.17448980036387904 Train-f1: 0.45110012854414394 Test-f1: 0.4124675324675325 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [6023/20000] Train-Loss: 0.5806288816700408 Test-Loss: 0.17727205748982677 Train-f1: 0.46481825866441245 Test-f1: 0.42413273001508306 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6024/20000] Train-Loss: 0.5801133925528245 Test-Loss: 0.17443539967138097 Train-f1: 0.4531603096664586 Test-f1: 0.41421602787456446 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6025/20000] Train-Loss: 0.5788531866235263 Test-Loss: 0.17811191343207125 Train-f1: 0.4551850564502633 Test-f1: 0.4240641711229946 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6026/20000] Train-Loss: 0.5801970253935813 Test-Loss: 0.17664465540315347 Train-f1: 0.46489487500377163 Test-f1: 0.4234512409615009 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6027/20000] Train-Loss: 0.5781358599267447 Test-Loss: 0.1743199062700706 Train-f1: 0.4505043522785458 Test-f1: 0.43628155628155635 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [6028/20000] Train-Loss: 0.5765040460755344 Test-Loss: 0.17898200639661813 Train-f1: 0.45233238940762277 Test-f1: 0.4178951396861845 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [6029/20000] Train-Loss: 0.5752011093389876 Test-Loss: 0.17379370067137725 Train-f1: 0.4658430277054951 Test-f1: 0.41421602787456446 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6030/20000] Train-Loss: 0.5712496919569104 Test-Loss: 0.17417660653313574 Train-f1: 0.46259396781968604 Test-f1: 0.4590643274853802 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6031/20000] Train-Loss: 0.5756090170741704 Test-Loss: 0.17576154091057483 Train-f1: 0.45895369352322996 Test-f1: 0.4068599033816426 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6032/20000] Train-Loss: 0.5758951475176439 Test-Loss: 0.17504809454641956 Train-f1: 0.4568993093420574 Test-f1: 0.41948862966890543 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6033/20000] Train-Loss: 0.5713736315881511 Test-Loss: 0.17851095335262293 Train-f1: 0.4588446036721899 Test-f1: 0.4007605466428997 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6034/20000] Train-Loss: 0.5749246382557702 Test-Loss: 0.1749356009408433 Train-f1: 0.45612790689586175 Test-f1: 0.43495105530543976 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6035/20000] Train-Loss: 0.5760082596122034 Test-Loss: 0.17384504866697917 Train-f1: 0.45374913516666054 Test-f1: 0.4590643274853802 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6036/20000] Train-Loss: 0.5775557130938014 Test-Loss: 0.1765870303337088 Train-f1: 0.4601242236024845 Test-f1: 0.4234512409615009 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6037/20000] Train-Loss: 0.5760268290413709 Test-Loss: 0.1822581716454022 Train-f1: 0.4581863904650015 Test-f1: 0.4178951396861845 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6038/20000] Train-Loss: 0.5793731039708576 Test-Loss: 0.17356127142782737 Train-f1: 0.44101974532144145 Test-f1: 0.4576190476190477 Train-acc: 0.7582781456953642 Test-acc: 0.7763157894736842\n",
      "Epoch [6039/20000] Train-Loss: 0.5756543617871194 Test-Loss: 0.1752616843417439 Train-f1: 0.4586833855799373 Test-f1: 0.40844720496894416 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6040/20000] Train-Loss: 0.5766901295256354 Test-Loss: 0.17350370028777318 Train-f1: 0.46592132505175987 Test-f1: 0.436784004167251 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6041/20000] Train-Loss: 0.57760515085686 Test-Loss: 0.17537332253541257 Train-f1: 0.4551662174303684 Test-f1: 0.40844720496894416 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6042/20000] Train-Loss: 0.5752806660922256 Test-Loss: 0.17355298267402222 Train-f1: 0.4575008950948801 Test-f1: 0.4253968253968254 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6043/20000] Train-Loss: 0.5783273694406652 Test-Loss: 0.1797268752480344 Train-f1: 0.46063360313450535 Test-f1: 0.4386138804049252 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6044/20000] Train-Loss: 0.5741342607924527 Test-Loss: 0.1744068477563005 Train-f1: 0.45357679914070886 Test-f1: 0.4602863322690288 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6045/20000] Train-Loss: 0.575968639549545 Test-Loss: 0.17542466204635448 Train-f1: 0.45343179760827096 Test-f1: 0.4068599033816426 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [6046/20000] Train-Loss: 0.5742210768785252 Test-Loss: 0.17623593429025355 Train-f1: 0.44974747474747473 Test-f1: 0.4291725105189341 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6047/20000] Train-Loss: 0.5745500656118294 Test-Loss: 0.17346724962318444 Train-f1: 0.4596744126034979 Test-f1: 0.43768009768009775 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6048/20000] Train-Loss: 0.5760036837205634 Test-Loss: 0.17476309143141486 Train-f1: 0.4555243020437761 Test-f1: 0.40844720496894416 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6049/20000] Train-Loss: 0.5721298271932969 Test-Loss: 0.1740730222717191 Train-f1: 0.4642496458438057 Test-f1: 0.44793650793650797 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6050/20000] Train-Loss: 0.5752592314447512 Test-Loss: 0.17587807487858906 Train-f1: 0.4474460239521729 Test-f1: 0.43485919098822323 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [6051/20000] Train-Loss: 0.5768693587436616 Test-Loss: 0.17531426269823533 Train-f1: 0.4574189808051569 Test-f1: 0.41948862966890543 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6052/20000] Train-Loss: 0.5725287663353744 Test-Loss: 0.17427414742352373 Train-f1: 0.4437161382005506 Test-f1: 0.43574229691876754 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "Epoch [6053/20000] Train-Loss: 0.5740923258257777 Test-Loss: 0.17899206607768692 Train-f1: 0.4633962264150943 Test-f1: 0.4386138804049252 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6054/20000] Train-Loss: 0.5718982820859738 Test-Loss: 0.1826026444065416 Train-f1: 0.45267515678504094 Test-f1: 0.4386138804049252 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6055/20000] Train-Loss: 0.5762534722572056 Test-Loss: 0.17664000287834716 Train-f1: 0.46417929737004027 Test-f1: 0.42247442455242973 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6056/20000] Train-Loss: 0.5756958235070904 Test-Loss: 0.18028187652655303 Train-f1: 0.44626413013509786 Test-f1: 0.4386138804049252 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6057/20000] Train-Loss: 0.5746789746058667 Test-Loss: 0.17318916277839366 Train-f1: 0.45313558274626126 Test-f1: 0.4145048194475317 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6058/20000] Train-Loss: 0.5752721785524132 Test-Loss: 0.17916448587410055 Train-f1: 0.45791605110776346 Test-f1: 0.42822708164603346 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6059/20000] Train-Loss: 0.5777847647657469 Test-Loss: 0.17416157532734317 Train-f1: 0.455862775217614 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6060/20000] Train-Loss: 0.5744323682380227 Test-Loss: 0.17330071359468985 Train-f1: 0.45538918766779873 Test-f1: 0.43628155628155635 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6061/20000] Train-Loss: 0.5763169134276526 Test-Loss: 0.1745327204711251 Train-f1: 0.4548479280159688 Test-f1: 0.43485919098822323 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6062/20000] Train-Loss: 0.5722410476734814 Test-Loss: 0.17336439955037714 Train-f1: 0.4543380095465569 Test-f1: 0.4144537815126051 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6063/20000] Train-Loss: 0.5746076924177241 Test-Loss: 0.17501409099552434 Train-f1: 0.4702331548932187 Test-f1: 0.4249675324675325 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [6064/20000] Train-Loss: 0.5755993889760423 Test-Loss: 0.1778370017318261 Train-f1: 0.4578483976005375 Test-f1: 0.4447942921400597 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6065/20000] Train-Loss: 0.5760119463322081 Test-Loss: 0.1739466499590777 Train-f1: 0.46286081245207933 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6066/20000] Train-Loss: 0.5769153991966085 Test-Loss: 0.17473998853526082 Train-f1: 0.4611842481499696 Test-f1: 0.4302232667450059 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6067/20000] Train-Loss: 0.5755326386391318 Test-Loss: 0.1737328290116742 Train-f1: 0.4485675080097691 Test-f1: 0.43574229691876754 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [6068/20000] Train-Loss: 0.5713276201778941 Test-Loss: 0.1732357023593766 Train-f1: 0.45947275317155445 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6069/20000] Train-Loss: 0.572522156114764 Test-Loss: 0.18321498176000284 Train-f1: 0.4587800873878972 Test-f1: 0.4178951396861845 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6070/20000] Train-Loss: 0.5767306597868237 Test-Loss: 0.17561165913598797 Train-f1: 0.4585294903456635 Test-f1: 0.40272034395153417 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6071/20000] Train-Loss: 0.5749387524099613 Test-Loss: 0.17417569930648955 Train-f1: 0.45607738468519454 Test-f1: 0.4068599033816426 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6072/20000] Train-Loss: 0.5754867260031802 Test-Loss: 0.17613537059454035 Train-f1: 0.46419907497882873 Test-f1: 0.43455108359133127 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6073/20000] Train-Loss: 0.5747973072548308 Test-Loss: 0.1730497615646819 Train-f1: 0.44672657697435636 Test-f1: 0.4368484410737932 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [6074/20000] Train-Loss: 0.5741458210009432 Test-Loss: 0.17465938562057218 Train-f1: 0.46241504354525825 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6075/20000] Train-Loss: 0.5742359488669295 Test-Loss: 0.1756676441832751 Train-f1: 0.4522555564451459 Test-f1: 0.40252100840336136 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [6076/20000] Train-Loss: 0.5743724203752901 Test-Loss: 0.1766290037962359 Train-f1: 0.46282963655567677 Test-f1: 0.40272034395153417 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [6077/20000] Train-Loss: 0.5720968722107942 Test-Loss: 0.17313904583287343 Train-f1: 0.4503769620566061 Test-f1: 0.41567460317460314 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [6078/20000] Train-Loss: 0.5762576286414404 Test-Loss: 0.17307324356670475 Train-f1: 0.45735302110698434 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6079/20000] Train-Loss: 0.5760838095128172 Test-Loss: 0.1740760567776967 Train-f1: 0.45249579489747316 Test-f1: 0.40844720496894416 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [6080/20000] Train-Loss: 0.5743350950945253 Test-Loss: 0.1746557426531061 Train-f1: 0.4632010003036019 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6081/20000] Train-Loss: 0.57360849883837 Test-Loss: 0.1858580981974112 Train-f1: 0.462916081722487 Test-f1: 0.4178951396861845 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6082/20000] Train-Loss: 0.5794213483678398 Test-Loss: 0.17767109841259449 Train-f1: 0.44936690100703025 Test-f1: 0.40252100840336136 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [6083/20000] Train-Loss: 0.5725877707436169 Test-Loss: 0.17369312970633155 Train-f1: 0.45819636456733237 Test-f1: 0.41567460317460314 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6084/20000] Train-Loss: 0.5740117895433353 Test-Loss: 0.17454665898054933 Train-f1: 0.4583735117169422 Test-f1: 0.44904646790222935 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6085/20000] Train-Loss: 0.5748939867495936 Test-Loss: 0.17414128383729 Train-f1: 0.4474820788530466 Test-f1: 0.43751349107513493 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [6086/20000] Train-Loss: 0.5748483919236025 Test-Loss: 0.17367485732191773 Train-f1: 0.4525324443370901 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6087/20000] Train-Loss: 0.5760789780380394 Test-Loss: 0.17350316060209384 Train-f1: 0.46057390028192946 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6088/20000] Train-Loss: 0.5739436726115303 Test-Loss: 0.17378079758638904 Train-f1: 0.4593834309256315 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6089/20000] Train-Loss: 0.5758170235339753 Test-Loss: 0.17293644249509244 Train-f1: 0.45781009855313426 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6090/20000] Train-Loss: 0.5748443703959726 Test-Loss: 0.17354623612883704 Train-f1: 0.4578483976005375 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6091/20000] Train-Loss: 0.576648462963889 Test-Loss: 0.173494876480895 Train-f1: 0.4590687694330319 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6092/20000] Train-Loss: 0.5726612825832268 Test-Loss: 0.1732859695903549 Train-f1: 0.45333468515085834 Test-f1: 0.4144537815126051 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6093/20000] Train-Loss: 0.5736282614607873 Test-Loss: 0.17523467736408543 Train-f1: 0.45962529848000255 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6094/20000] Train-Loss: 0.5744402438580575 Test-Loss: 0.17343668087616212 Train-f1: 0.4616253248618243 Test-f1: 0.44793650793650797 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6095/20000] Train-Loss: 0.5762049261846743 Test-Loss: 0.1743800438742355 Train-f1: 0.45610218910538247 Test-f1: 0.43485919098822323 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6096/20000] Train-Loss: 0.5716870697307457 Test-Loss: 0.17300350436247733 Train-f1: 0.4574602950378627 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6097/20000] Train-Loss: 0.5718591691635251 Test-Loss: 0.17701394721673738 Train-f1: 0.4574602950378627 Test-f1: 0.4128817587641117 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6098/20000] Train-Loss: 0.5714800756294232 Test-Loss: 0.17327950218669075 Train-f1: 0.45838827838827834 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6099/20000] Train-Loss: 0.5731143836736062 Test-Loss: 0.17275937354862886 Train-f1: 0.4584748485972067 Test-f1: 0.43628155628155635 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6100/20000] Train-Loss: 0.5743323848125497 Test-Loss: 0.1729692742129619 Train-f1: 0.45563592918431634 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6101/20000] Train-Loss: 0.574256062396821 Test-Loss: 0.17332682286053985 Train-f1: 0.45741751938695907 Test-f1: 0.4124675324675325 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6102/20000] Train-Loss: 0.574453197941821 Test-Loss: 0.17331752134130765 Train-f1: 0.45770730143310134 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6103/20000] Train-Loss: 0.5713106114930808 Test-Loss: 0.17318554219156937 Train-f1: 0.45826848725101554 Test-f1: 0.43628155628155635 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6104/20000] Train-Loss: 0.5746803632243163 Test-Loss: 0.17558419823297147 Train-f1: 0.46282963655567677 Test-f1: 0.40252100840336136 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [6105/20000] Train-Loss: 0.570510908871477 Test-Loss: 0.17317510435593808 Train-f1: 0.4620029243800377 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6106/20000] Train-Loss: 0.5754994569994788 Test-Loss: 0.17267214675049652 Train-f1: 0.4453302211639377 Test-f1: 0.436390977443609 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "Epoch [6107/20000] Train-Loss: 0.5704057148999232 Test-Loss: 0.17272229277132806 Train-f1: 0.4524064770490416 Test-f1: 0.41567460317460314 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6108/20000] Train-Loss: 0.5740868233561807 Test-Loss: 0.1743230898562175 Train-f1: 0.4597626596612012 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6109/20000] Train-Loss: 0.5723700802449349 Test-Loss: 0.17367578976769502 Train-f1: 0.43442908346134146 Test-f1: 0.4576190476190477 Train-acc: 0.7483443708609272 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6110/20000] Train-Loss: 0.5748499270666079 Test-Loss: 0.17264584628095261 Train-f1: 0.46033922171018943 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6111/20000] Train-Loss: 0.5722625794093643 Test-Loss: 0.17459059855414413 Train-f1: 0.45828292273343135 Test-f1: 0.44936443292607675 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6112/20000] Train-Loss: 0.5724289816883726 Test-Loss: 0.17744542249169626 Train-f1: 0.45653637292748817 Test-f1: 0.4447942921400597 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6113/20000] Train-Loss: 0.5700634899389815 Test-Loss: 0.17271348310040127 Train-f1: 0.457062580439322 Test-f1: 0.4124675324675325 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6114/20000] Train-Loss: 0.5695316101781962 Test-Loss: 0.1753941693289328 Train-f1: 0.45769895711337905 Test-f1: 0.43426704014939316 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6115/20000] Train-Loss: 0.570340118228829 Test-Loss: 0.1728755551481594 Train-f1: 0.46396321303960003 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6116/20000] Train-Loss: 0.5720537293294319 Test-Loss: 0.174706396495424 Train-f1: 0.46357182244279016 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6117/20000] Train-Loss: 0.5728136931010385 Test-Loss: 0.17288076385053602 Train-f1: 0.4603379928795893 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6118/20000] Train-Loss: 0.57224410489192 Test-Loss: 0.17244952782116937 Train-f1: 0.4526743865453543 Test-f1: 0.4253968253968254 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6119/20000] Train-Loss: 0.5721164804291291 Test-Loss: 0.17352912491369188 Train-f1: 0.4578483976005375 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6120/20000] Train-Loss: 0.5712262687282206 Test-Loss: 0.1784709929741444 Train-f1: 0.45864055299539175 Test-f1: 0.39665763844868324 Train-acc: 0.7781456953642384 Test-acc: 0.6842105263157895\n",
      "Epoch [6121/20000] Train-Loss: 0.5725405355270443 Test-Loss: 0.17285185716095516 Train-f1: 0.4609802378858703 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6122/20000] Train-Loss: 0.5718914161888922 Test-Loss: 0.1762088227959871 Train-f1: 0.4531498042875288 Test-f1: 0.4234512409615009 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6123/20000] Train-Loss: 0.5803668938821065 Test-Loss: 0.1734551446354347 Train-f1: 0.44564874551971323 Test-f1: 0.4088146077440076 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [6124/20000] Train-Loss: 0.5737927741611051 Test-Loss: 0.17246697736558655 Train-f1: 0.46592132505175987 Test-f1: 0.4149259191512713 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6125/20000] Train-Loss: 0.5720304245917226 Test-Loss: 0.17241958111708222 Train-f1: 0.46117366620102784 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6126/20000] Train-Loss: 0.5721465814421904 Test-Loss: 0.1747740364388019 Train-f1: 0.45440133356105183 Test-f1: 0.44069706037669415 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6127/20000] Train-Loss: 0.5717379828268219 Test-Loss: 0.1727215634670648 Train-f1: 0.4637454955821439 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6128/20000] Train-Loss: 0.5742227634592881 Test-Loss: 0.17341286821318758 Train-f1: 0.4609523809523809 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6129/20000] Train-Loss: 0.5723173746167092 Test-Loss: 0.1786148367361847 Train-f1: 0.463229538293232 Test-f1: 0.4178951396861845 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6130/20000] Train-Loss: 0.5722111741958517 Test-Loss: 0.17424357911731184 Train-f1: 0.463229538293232 Test-f1: 0.4088146077440076 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6131/20000] Train-Loss: 0.5718256589645244 Test-Loss: 0.1732931985788998 Train-f1: 0.44277724243768385 Test-f1: 0.4144537815126051 Train-acc: 0.7582781456953642 Test-acc: 0.7236842105263158\n",
      "Epoch [6132/20000] Train-Loss: 0.5714651645999651 Test-Loss: 0.17637832999392403 Train-f1: 0.461432295303263 Test-f1: 0.4122994652406417 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6133/20000] Train-Loss: 0.5724539237927918 Test-Loss: 0.17702134467950284 Train-f1: 0.4551662174303684 Test-f1: 0.43455108359133127 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6134/20000] Train-Loss: 0.5678766307201368 Test-Loss: 0.1729376378111742 Train-f1: 0.4607869260147438 Test-f1: 0.4590643274853802 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6135/20000] Train-Loss: 0.5717522476039619 Test-Loss: 0.17336130028047275 Train-f1: 0.46803458510775586 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6136/20000] Train-Loss: 0.5740419912041438 Test-Loss: 0.17226066266286816 Train-f1: 0.4630126457839939 Test-f1: 0.41421602787456446 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6137/20000] Train-Loss: 0.567627991856882 Test-Loss: 0.1745011817056997 Train-f1: 0.4574871166928209 Test-f1: 0.43485919098822323 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6138/20000] Train-Loss: 0.573980707791295 Test-Loss: 0.17248503100225684 Train-f1: 0.456655925528106 Test-f1: 0.43628155628155635 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6139/20000] Train-Loss: 0.5697652997412042 Test-Loss: 0.1736872848955268 Train-f1: 0.45898207885304665 Test-f1: 0.4184970128917304 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6140/20000] Train-Loss: 0.572532125916975 Test-Loss: 0.17552246050693995 Train-f1: 0.4614981816580513 Test-f1: 0.43455108359133127 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6141/20000] Train-Loss: 0.5742836947830516 Test-Loss: 0.17221957814355687 Train-f1: 0.4594679143902923 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6142/20000] Train-Loss: 0.5686109195654404 Test-Loss: 0.17392131048858067 Train-f1: 0.4636361389197874 Test-f1: 0.44904646790222935 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6143/20000] Train-Loss: 0.5726654231217275 Test-Loss: 0.17432672403668825 Train-f1: 0.44469924067211924 Test-f1: 0.43751349107513493 Train-acc: 0.7615894039735099 Test-acc: 0.75\n",
      "Epoch [6144/20000] Train-Loss: 0.5794706269312759 Test-Loss: 0.17441274394669995 Train-f1: 0.46190961630047234 Test-f1: 0.4088146077440076 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6145/20000] Train-Loss: 0.5723124363020489 Test-Loss: 0.1721853080800458 Train-f1: 0.4556658395368073 Test-f1: 0.43628155628155635 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6146/20000] Train-Loss: 0.571392904469445 Test-Loss: 0.17490730433581966 Train-f1: 0.46458193676963494 Test-f1: 0.4447942921400597 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6147/20000] Train-Loss: 0.5700920068238653 Test-Loss: 0.1739479992408232 Train-f1: 0.4572998842856896 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6148/20000] Train-Loss: 0.5717065785067632 Test-Loss: 0.17377611335640358 Train-f1: 0.4578483976005375 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6149/20000] Train-Loss: 0.5694258254185358 Test-Loss: 0.17863787151881244 Train-f1: 0.463229538293232 Test-f1: 0.39665763844868324 Train-acc: 0.7847682119205298 Test-acc: 0.6842105263157895\n",
      "Epoch [6150/20000] Train-Loss: 0.5720436953817527 Test-Loss: 0.17222682708578066 Train-f1: 0.4635147941320624 Test-f1: 0.43628155628155635 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6151/20000] Train-Loss: 0.5656044301753832 Test-Loss: 0.17214581061271447 Train-f1: 0.47084302770549513 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6152/20000] Train-Loss: 0.5715494078846707 Test-Loss: 0.17617603740553336 Train-f1: 0.46511339406861796 Test-f1: 0.4234512409615009 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6153/20000] Train-Loss: 0.5727151079813715 Test-Loss: 0.17620214581402918 Train-f1: 0.45553986338412644 Test-f1: 0.4447942921400597 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6154/20000] Train-Loss: 0.5715949117476665 Test-Loss: 0.17778511171298378 Train-f1: 0.46650871558394985 Test-f1: 0.4178951396861845 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6155/20000] Train-Loss: 0.5749819969288575 Test-Loss: 0.173104692296004 Train-f1: 0.45695332416812373 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6156/20000] Train-Loss: 0.5715454338851759 Test-Loss: 0.17304943789637062 Train-f1: 0.46779413050297486 Test-f1: 0.436390977443609 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6157/20000] Train-Loss: 0.5723063572166055 Test-Loss: 0.17216122634114586 Train-f1: 0.4632010003036019 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6158/20000] Train-Loss: 0.5721097166328497 Test-Loss: 0.17469481332881415 Train-f1: 0.4637214369016648 Test-f1: 0.4447942921400597 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6159/20000] Train-Loss: 0.5691731476642045 Test-Loss: 0.17206746777600804 Train-f1: 0.4615183693548466 Test-f1: 0.4149259191512713 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6160/20000] Train-Loss: 0.5696750787306545 Test-Loss: 0.1745787858457673 Train-f1: 0.4619484518075511 Test-f1: 0.43455108359133127 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6161/20000] Train-Loss: 0.5737554542379878 Test-Loss: 0.17207088040059929 Train-f1: 0.448014784922327 Test-f1: 0.41421602787456446 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [6162/20000] Train-Loss: 0.5715372199916531 Test-Loss: 0.173900884535874 Train-f1: 0.4586075290331186 Test-f1: 0.4068599033816426 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6163/20000] Train-Loss: 0.5716508671346646 Test-Loss: 0.17237825385917274 Train-f1: 0.46733487873498947 Test-f1: 0.436390977443609 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6164/20000] Train-Loss: 0.5688781968693462 Test-Loss: 0.17434712341825662 Train-f1: 0.45906729935200774 Test-f1: 0.4361616161616162 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6165/20000] Train-Loss: 0.5678694142557186 Test-Loss: 0.17185205507825546 Train-f1: 0.45674543501611176 Test-f1: 0.41421602787456446 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6166/20000] Train-Loss: 0.571340882036668 Test-Loss: 0.1736772600185766 Train-f1: 0.45783922171018937 Test-f1: 0.397364953886693 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6167/20000] Train-Loss: 0.5694519869502795 Test-Loss: 0.17559050756070096 Train-f1: 0.4496092742518387 Test-f1: 0.40272034395153417 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [6168/20000] Train-Loss: 0.5712436480893566 Test-Loss: 0.17251243617439996 Train-f1: 0.46239509215294355 Test-f1: 0.4590643274853802 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6169/20000] Train-Loss: 0.5702524176969177 Test-Loss: 0.17714254676146496 Train-f1: 0.4532600732600732 Test-f1: 0.40272034395153417 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [6170/20000] Train-Loss: 0.5639871058622659 Test-Loss: 0.17241121286033947 Train-f1: 0.46033922171018943 Test-f1: 0.43768009768009775 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6171/20000] Train-Loss: 0.5708592907082529 Test-Loss: 0.17287157702234277 Train-f1: 0.4572875033306688 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6172/20000] Train-Loss: 0.5696169014595553 Test-Loss: 0.17259218237499813 Train-f1: 0.45477132579650564 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6173/20000] Train-Loss: 0.5701468161033646 Test-Loss: 0.1738676036889886 Train-f1: 0.4528468027084518 Test-f1: 0.44472049689440996 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6174/20000] Train-Loss: 0.5713566174382454 Test-Loss: 0.17554030825110917 Train-f1: 0.4514681748449165 Test-f1: 0.42402476780185766 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [6175/20000] Train-Loss: 0.5690272991830604 Test-Loss: 0.18002986697051854 Train-f1: 0.4521556800008228 Test-f1: 0.42822708164603346 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6176/20000] Train-Loss: 0.5731883176943495 Test-Loss: 0.17263781735005695 Train-f1: 0.45741458224149156 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6177/20000] Train-Loss: 0.5702750655495383 Test-Loss: 0.17189506890452483 Train-f1: 0.46286081245207933 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6178/20000] Train-Loss: 0.5712638289869975 Test-Loss: 0.1744387816417699 Train-f1: 0.4576692688402469 Test-f1: 0.43426704014939316 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6179/20000] Train-Loss: 0.5709614828395159 Test-Loss: 0.1719727653800735 Train-f1: 0.4452578304986613 Test-f1: 0.41421602787456446 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [6180/20000] Train-Loss: 0.5688109954048053 Test-Loss: 0.17640585805495967 Train-f1: 0.4614823899662146 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6181/20000] Train-Loss: 0.5702187507564013 Test-Loss: 0.17847979723034255 Train-f1: 0.45121839448663703 Test-f1: 0.3967025338424158 Train-acc: 0.7682119205298014 Test-acc: 0.6842105263157895\n",
      "Epoch [6182/20000] Train-Loss: 0.5714689429558756 Test-Loss: 0.172353279423787 Train-f1: 0.46263433697486456 Test-f1: 0.41421602787456446 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6183/20000] Train-Loss: 0.5703722159100076 Test-Loss: 0.1738883788824661 Train-f1: 0.4656963645673323 Test-f1: 0.40844720496894416 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6184/20000] Train-Loss: 0.5710345496324357 Test-Loss: 0.17299672419011208 Train-f1: 0.46352215646578576 Test-f1: 0.43485919098822323 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6185/20000] Train-Loss: 0.5692034091911962 Test-Loss: 0.17174383051170475 Train-f1: 0.44784738187192696 Test-f1: 0.436390977443609 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [6186/20000] Train-Loss: 0.5679993008652663 Test-Loss: 0.1719361809535176 Train-f1: 0.4566703905413583 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6187/20000] Train-Loss: 0.5709043276085444 Test-Loss: 0.1721003984494056 Train-f1: 0.45650412721720024 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6188/20000] Train-Loss: 0.5699354759177825 Test-Loss: 0.17947636848894474 Train-f1: 0.44494879672299026 Test-f1: 0.39665763844868324 Train-acc: 0.7615894039735099 Test-acc: 0.6842105263157895\n",
      "Epoch [6189/20000] Train-Loss: 0.5727061765561575 Test-Loss: 0.1728501510516516 Train-f1: 0.47114936628894066 Test-f1: 0.42430685593476286 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [6190/20000] Train-Loss: 0.5649533134484845 Test-Loss: 0.17208127819754576 Train-f1: 0.47284822782516367 Test-f1: 0.4144537815126051 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6191/20000] Train-Loss: 0.5683258658264231 Test-Loss: 0.17172889305186093 Train-f1: 0.453407047634765 Test-f1: 0.4368484410737932 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6192/20000] Train-Loss: 0.5675255600630523 Test-Loss: 0.1721884943253326 Train-f1: 0.4572306104013421 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6193/20000] Train-Loss: 0.5678494987396081 Test-Loss: 0.17595102456991527 Train-f1: 0.46112771161139665 Test-f1: 0.42402476780185766 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6194/20000] Train-Loss: 0.5651456832165688 Test-Loss: 0.17166954055644656 Train-f1: 0.44892970235930524 Test-f1: 0.4145048194475317 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [6195/20000] Train-Loss: 0.56941783418993 Test-Loss: 0.17375712787807404 Train-f1: 0.45651159147869674 Test-f1: 0.4249675324675325 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6196/20000] Train-Loss: 0.5668834234097192 Test-Loss: 0.1806899592636526 Train-f1: 0.4574602950378627 Test-f1: 0.4167235630358844 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6197/20000] Train-Loss: 0.5664988226678824 Test-Loss: 0.17301337784853915 Train-f1: 0.4587263522515321 Test-f1: 0.43485919098822323 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6198/20000] Train-Loss: 0.5711850837436453 Test-Loss: 0.17724918949086296 Train-f1: 0.4414675623739419 Test-f1: 0.4386138804049252 Train-acc: 0.7549668874172185 Test-acc: 0.7368421052631579\n",
      "Epoch [6199/20000] Train-Loss: 0.5701827040739909 Test-Loss: 0.17284663509582257 Train-f1: 0.46622697952994213 Test-f1: 0.397364953886693 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [6200/20000] Train-Loss: 0.5704756786096882 Test-Loss: 0.17176540153170328 Train-f1: 0.4577446951263938 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6201/20000] Train-Loss: 0.5706919794303907 Test-Loss: 0.17263440728762905 Train-f1: 0.46672715349008975 Test-f1: 0.397364953886693 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [6202/20000] Train-Loss: 0.5650043806019547 Test-Loss: 0.172661350227958 Train-f1: 0.4646359229592763 Test-f1: 0.42430685593476286 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6203/20000] Train-Loss: 0.5684145699352227 Test-Loss: 0.18119280653932512 Train-f1: 0.4612211981566821 Test-f1: 0.4178951396861845 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6204/20000] Train-Loss: 0.5689930855238464 Test-Loss: 0.1728778374443129 Train-f1: 0.459259160633969 Test-f1: 0.4088146077440076 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6205/20000] Train-Loss: 0.5663546387381085 Test-Loss: 0.17734934788368328 Train-f1: 0.461784087789202 Test-f1: 0.4167235630358844 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6206/20000] Train-Loss: 0.5681092605409495 Test-Loss: 0.17322037522390488 Train-f1: 0.4606525071719812 Test-f1: 0.4302232667450059 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6207/20000] Train-Loss: 0.5656306912858118 Test-Loss: 0.17382116948270593 Train-f1: 0.4586075290331186 Test-f1: 0.397364953886693 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6208/20000] Train-Loss: 0.5676389813651942 Test-Loss: 0.1718175987194129 Train-f1: 0.45256974507979636 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6209/20000] Train-Loss: 0.568403929080616 Test-Loss: 0.17785407734216532 Train-f1: 0.4603641208709778 Test-f1: 0.4386138804049252 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6210/20000] Train-Loss: 0.5711027283754807 Test-Loss: 0.17157977878334293 Train-f1: 0.46348200357827307 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6211/20000] Train-Loss: 0.5674034610433982 Test-Loss: 0.17488594499077909 Train-f1: 0.4588446036721899 Test-f1: 0.42402476780185766 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6212/20000] Train-Loss: 0.5683874045754522 Test-Loss: 0.17205738656421626 Train-f1: 0.462225774935607 Test-f1: 0.4144537815126051 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6213/20000] Train-Loss: 0.568772979649967 Test-Loss: 0.17373770328353852 Train-f1: 0.45923547251820496 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6214/20000] Train-Loss: 0.570545037451246 Test-Loss: 0.1753338104381997 Train-f1: 0.44915637469124076 Test-f1: 0.4447942921400597 Train-acc: 0.7649006622516556 Test-acc: 0.75\n",
      "Epoch [6215/20000] Train-Loss: 0.5698846604202279 Test-Loss: 0.17272829859068622 Train-f1: 0.4486261474340944 Test-f1: 0.4088146077440076 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [6216/20000] Train-Loss: 0.565303721654095 Test-Loss: 0.17848088193504266 Train-f1: 0.4540925523107223 Test-f1: 0.4181515499425948 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6217/20000] Train-Loss: 0.5678169238169506 Test-Loss: 0.17180262718967554 Train-f1: 0.4642162006646715 Test-f1: 0.4582159624413146 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6218/20000] Train-Loss: 0.5704345640063674 Test-Loss: 0.17518244911703373 Train-f1: 0.4592519637074798 Test-f1: 0.40252100840336136 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [6219/20000] Train-Loss: 0.568411099727036 Test-Loss: 0.17140847277338428 Train-f1: 0.4518286230481352 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6220/20000] Train-Loss: 0.5691594818133788 Test-Loss: 0.17179224895404047 Train-f1: 0.45322521932827275 Test-f1: 0.41567460317460314 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6221/20000] Train-Loss: 0.5688048843807716 Test-Loss: 0.1731004960949772 Train-f1: 0.46791860106128247 Test-f1: 0.43485919098822323 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6222/20000] Train-Loss: 0.5631619498863484 Test-Loss: 0.17176324779513613 Train-f1: 0.4405814124210835 Test-f1: 0.41295692040807896 Train-acc: 0.7549668874172185 Test-acc: 0.7236842105263158\n",
      "Epoch [6223/20000] Train-Loss: 0.5706681882571333 Test-Loss: 0.1774021619074827 Train-f1: 0.45119746762437607 Test-f1: 0.4386138804049252 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6224/20000] Train-Loss: 0.5702187658443066 Test-Loss: 0.18104124804879487 Train-f1: 0.4603641208709778 Test-f1: 0.4074109554921158 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [6225/20000] Train-Loss: 0.5661787052142245 Test-Loss: 0.17273482204275883 Train-f1: 0.4657001627710054 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6226/20000] Train-Loss: 0.5696940089658651 Test-Loss: 0.17145682913855886 Train-f1: 0.45267203427662395 Test-f1: 0.44709236709236705 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6227/20000] Train-Loss: 0.5686664180382274 Test-Loss: 0.17331403437125342 Train-f1: 0.46050761835389514 Test-f1: 0.4302232667450059 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6228/20000] Train-Loss: 0.5673611500065915 Test-Loss: 0.17285353372097018 Train-f1: 0.46053252821306734 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6229/20000] Train-Loss: 0.5668444801574609 Test-Loss: 0.17336288619365542 Train-f1: 0.4666714396100118 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6230/20000] Train-Loss: 0.567815360179678 Test-Loss: 0.1734126935063529 Train-f1: 0.47084302770549513 Test-f1: 0.42430685593476286 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [6231/20000] Train-Loss: 0.569954639972784 Test-Loss: 0.17358871096188525 Train-f1: 0.4482241317579663 Test-f1: 0.45103407755581665 Train-acc: 0.7649006622516556 Test-acc: 0.7631578947368421\n",
      "Epoch [6232/20000] Train-Loss: 0.5636893386888184 Test-Loss: 0.1725092341152762 Train-f1: 0.46638951034728227 Test-f1: 0.42702535176346623 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6233/20000] Train-Loss: 0.5691244406892699 Test-Loss: 0.17164936045593113 Train-f1: 0.46088050314465406 Test-f1: 0.4028833182321555 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6234/20000] Train-Loss: 0.5672437656502672 Test-Loss: 0.1729020239026953 Train-f1: 0.4542894381604059 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6235/20000] Train-Loss: 0.5677327652546953 Test-Loss: 0.17249538440079926 Train-f1: 0.4603605007532076 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6236/20000] Train-Loss: 0.5658892065428842 Test-Loss: 0.17149644451627533 Train-f1: 0.466712150809154 Test-f1: 0.40374260231724596 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6237/20000] Train-Loss: 0.5703353127819253 Test-Loss: 0.17205352148966313 Train-f1: 0.45232941597032933 Test-f1: 0.4093581780538303 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6238/20000] Train-Loss: 0.57033464353776 Test-Loss: 0.17137563087136598 Train-f1: 0.45970407701750987 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6239/20000] Train-Loss: 0.5677926026158739 Test-Loss: 0.17422938037848976 Train-f1: 0.4655120348165912 Test-f1: 0.40252100840336136 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [6240/20000] Train-Loss: 0.5678189090501321 Test-Loss: 0.171587668404323 Train-f1: 0.458481545447267 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6241/20000] Train-Loss: 0.5703427443958073 Test-Loss: 0.17125871587825528 Train-f1: 0.456077882638222 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6242/20000] Train-Loss: 0.5686710412239524 Test-Loss: 0.1741987764046166 Train-f1: 0.45752613027910416 Test-f1: 0.397364953886693 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [6243/20000] Train-Loss: 0.5681796732736478 Test-Loss: 0.17149576891358495 Train-f1: 0.45617227889234063 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6244/20000] Train-Loss: 0.5656476667674032 Test-Loss: 0.1748483506646614 Train-f1: 0.462916081722487 Test-f1: 0.4447942921400597 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6245/20000] Train-Loss: 0.5642916268790665 Test-Loss: 0.17120123999937117 Train-f1: 0.4589032745970284 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6246/20000] Train-Loss: 0.5676739865368348 Test-Loss: 0.1714233378447861 Train-f1: 0.46531480018214066 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6247/20000] Train-Loss: 0.5679032762802765 Test-Loss: 0.1714582442117903 Train-f1: 0.45715523660195834 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6248/20000] Train-Loss: 0.5715067069682159 Test-Loss: 0.174162158512249 Train-f1: 0.4682345082113377 Test-f1: 0.43426704014939316 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6249/20000] Train-Loss: 0.5658576073529673 Test-Loss: 0.1826970600069131 Train-f1: 0.4599411044228822 Test-f1: 0.42822708164603346 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6250/20000] Train-Loss: 0.5667988298397686 Test-Loss: 0.17253629227646583 Train-f1: 0.4581662471539915 Test-f1: 0.44069706037669415 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6251/20000] Train-Loss: 0.5651282237092012 Test-Loss: 0.1751048679914521 Train-f1: 0.4587800873878972 Test-f1: 0.3914438502673797 Train-acc: 0.7781456953642384 Test-acc: 0.6842105263157895\n",
      "Epoch [6252/20000] Train-Loss: 0.5689482287717148 Test-Loss: 0.17287374892203286 Train-f1: 0.4553494855455555 Test-f1: 0.4088146077440076 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6253/20000] Train-Loss: 0.568621707713901 Test-Loss: 0.1714165856733102 Train-f1: 0.45746195002856993 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6254/20000] Train-Loss: 0.5636757453321887 Test-Loss: 0.18247719036916615 Train-f1: 0.4565139022051774 Test-f1: 0.3967025338424158 Train-acc: 0.7748344370860927 Test-acc: 0.6842105263157895\n",
      "Epoch [6255/20000] Train-Loss: 0.5686164431223323 Test-Loss: 0.17211972249428992 Train-f1: 0.45871665400598777 Test-f1: 0.4088146077440076 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6256/20000] Train-Loss: 0.5651903385313737 Test-Loss: 0.17109103361815195 Train-f1: 0.465419789290757 Test-f1: 0.4253968253968254 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6257/20000] Train-Loss: 0.5680353689349492 Test-Loss: 0.17275661147336632 Train-f1: 0.4505084969101273 Test-f1: 0.45103407755581665 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [6258/20000] Train-Loss: 0.569196480116217 Test-Loss: 0.1738965312999989 Train-f1: 0.4649159084642956 Test-f1: 0.40252100840336136 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [6259/20000] Train-Loss: 0.5633734768069603 Test-Loss: 0.17804534910457795 Train-f1: 0.46471464921846606 Test-f1: 0.41754215933320415 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6260/20000] Train-Loss: 0.5679458646139778 Test-Loss: 0.17146154409138584 Train-f1: 0.4506463638696955 Test-f1: 0.4253968253968254 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [6261/20000] Train-Loss: 0.5653605860475669 Test-Loss: 0.17234112925089792 Train-f1: 0.463006148272871 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6262/20000] Train-Loss: 0.5686498776168185 Test-Loss: 0.1716039817462895 Train-f1: 0.45293752259339026 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6263/20000] Train-Loss: 0.568042324562141 Test-Loss: 0.1713757712492152 Train-f1: 0.46033922171018943 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6264/20000] Train-Loss: 0.560739590427644 Test-Loss: 0.17163512113669763 Train-f1: 0.4558986636265754 Test-f1: 0.4265359477124183 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6265/20000] Train-Loss: 0.5666034087329314 Test-Loss: 0.17222032318210842 Train-f1: 0.4609523809523809 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6266/20000] Train-Loss: 0.5683689519537725 Test-Loss: 0.17348315580815893 Train-f1: 0.4530849974398361 Test-f1: 0.4302232667450059 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6267/20000] Train-Loss: 0.5664468939568462 Test-Loss: 0.1722725361221472 Train-f1: 0.4662740615809208 Test-f1: 0.397364953886693 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [6268/20000] Train-Loss: 0.5683628627884002 Test-Loss: 0.1813417496374247 Train-f1: 0.46239509215294355 Test-f1: 0.4386138804049252 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6269/20000] Train-Loss: 0.5661669136122706 Test-Loss: 0.171344316517429 Train-f1: 0.4658402430327861 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6270/20000] Train-Loss: 0.565460840612904 Test-Loss: 0.176078276483844 Train-f1: 0.45538918766779873 Test-f1: 0.4386138804049252 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6271/20000] Train-Loss: 0.5664831114230937 Test-Loss: 0.1734696958649227 Train-f1: 0.45997490525189877 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6272/20000] Train-Loss: 0.5647091503607448 Test-Loss: 0.1744764082648091 Train-f1: 0.46592132505175987 Test-f1: 0.4389639222941721 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6273/20000] Train-Loss: 0.567641045660163 Test-Loss: 0.17122479662771647 Train-f1: 0.45411988070289794 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6274/20000] Train-Loss: 0.5654079192192862 Test-Loss: 0.17172694751978063 Train-f1: 0.47358504954754677 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [6275/20000] Train-Loss: 0.5648848872603325 Test-Loss: 0.17408410331230106 Train-f1: 0.4603641208709778 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6276/20000] Train-Loss: 0.5642845092668741 Test-Loss: 0.17321819954764375 Train-f1: 0.4513065762734344 Test-f1: 0.397364953886693 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [6277/20000] Train-Loss: 0.5649869962156053 Test-Loss: 0.17390937484796865 Train-f1: 0.4516535279204784 Test-f1: 0.4447942921400597 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6278/20000] Train-Loss: 0.5668173451252401 Test-Loss: 0.17601762284850397 Train-f1: 0.46800614827287107 Test-f1: 0.4386138804049252 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6279/20000] Train-Loss: 0.5648992347341577 Test-Loss: 0.17205458884387834 Train-f1: 0.4627221634863249 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6280/20000] Train-Loss: 0.5639294713286674 Test-Loss: 0.17226531518845214 Train-f1: 0.45952991295951584 Test-f1: 0.4068599033816426 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6281/20000] Train-Loss: 0.5658026073670667 Test-Loss: 0.1708703218567103 Train-f1: 0.4599310858310391 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6282/20000] Train-Loss: 0.5659954646731264 Test-Loss: 0.17115413218028938 Train-f1: 0.4587089636325629 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6283/20000] Train-Loss: 0.5632433981612355 Test-Loss: 0.1720350938332134 Train-f1: 0.4630518742900415 Test-f1: 0.4249675324675325 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6284/20000] Train-Loss: 0.5627223690488625 Test-Loss: 0.1733275575458168 Train-f1: 0.4528010491753364 Test-f1: 0.45103407755581665 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6285/20000] Train-Loss: 0.5661553770668869 Test-Loss: 0.17375526533341887 Train-f1: 0.45211342093356244 Test-f1: 0.42402476780185766 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6286/20000] Train-Loss: 0.5661605591331544 Test-Loss: 0.17074630944192137 Train-f1: 0.45822994210090984 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6287/20000] Train-Loss: 0.5644703706836889 Test-Loss: 0.174940657857315 Train-f1: 0.4636463343686518 Test-f1: 0.42402476780185766 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6288/20000] Train-Loss: 0.5599046721034933 Test-Loss: 0.17597267799745048 Train-f1: 0.46058537480502804 Test-f1: 0.43052100840336127 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6289/20000] Train-Loss: 0.567441449236049 Test-Loss: 0.17065929413025868 Train-f1: 0.45381579708403963 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6290/20000] Train-Loss: 0.5657865015738247 Test-Loss: 0.1739163396384141 Train-f1: 0.4453302211639377 Test-f1: 0.4088146077440076 Train-acc: 0.7615894039735099 Test-acc: 0.7105263157894737\n",
      "Epoch [6291/20000] Train-Loss: 0.5589839154108363 Test-Loss: 0.18116934572637602 Train-f1: 0.46341112786163646 Test-f1: 0.39508780209623817 Train-acc: 0.7847682119205298 Test-acc: 0.6842105263157895\n",
      "Epoch [6292/20000] Train-Loss: 0.5661004124472164 Test-Loss: 0.17194047710622593 Train-f1: 0.44325169365707745 Test-f1: 0.4576190476190477 Train-acc: 0.7615894039735099 Test-acc: 0.7763157894736842\n",
      "Epoch [6293/20000] Train-Loss: 0.5653756287865626 Test-Loss: 0.1721789952438778 Train-f1: 0.47045030642152935 Test-f1: 0.42430685593476286 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [6294/20000] Train-Loss: 0.5648915013443373 Test-Loss: 0.17473454665926727 Train-f1: 0.4504487450784246 Test-f1: 0.4134165729574624 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [6295/20000] Train-Loss: 0.5605673444802567 Test-Loss: 0.17269181340461315 Train-f1: 0.45643369892504976 Test-f1: 0.41587454059472106 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6296/20000] Train-Loss: 0.5645905372862419 Test-Loss: 0.1706175143261598 Train-f1: 0.46158841369467557 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6297/20000] Train-Loss: 0.5688866668465767 Test-Loss: 0.17099223479159917 Train-f1: 0.4633962264150943 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6298/20000] Train-Loss: 0.5613777555331713 Test-Loss: 0.17283242316897585 Train-f1: 0.4542894381604059 Test-f1: 0.4419047619047619 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6299/20000] Train-Loss: 0.5676812985281698 Test-Loss: 0.172325642757474 Train-f1: 0.4540988105543266 Test-f1: 0.397364953886693 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [6300/20000] Train-Loss: 0.5597487792796275 Test-Loss: 0.19136537389216818 Train-f1: 0.4707353069735631 Test-f1: 0.401367053998633 Train-acc: 0.7947019867549668 Test-acc: 0.6842105263157895\n",
      "Epoch [6301/20000] Train-Loss: 0.568016628529192 Test-Loss: 0.17436549306479057 Train-f1: 0.4733430277054952 Test-f1: 0.4447942921400597 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [6302/20000] Train-Loss: 0.5661325028773133 Test-Loss: 0.1720290499437399 Train-f1: 0.4516535279204784 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6303/20000] Train-Loss: 0.5680849399630741 Test-Loss: 0.17150228987687313 Train-f1: 0.4562304199193837 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6304/20000] Train-Loss: 0.5634260423029116 Test-Loss: 0.170821532446427 Train-f1: 0.455862775217614 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6305/20000] Train-Loss: 0.567054015815115 Test-Loss: 0.17220065943993892 Train-f1: 0.4494715338840039 Test-f1: 0.45103407755581665 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [6306/20000] Train-Loss: 0.5648877431927776 Test-Loss: 0.1711822235661492 Train-f1: 0.45337877086318396 Test-f1: 0.4144537815126051 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6307/20000] Train-Loss: 0.5648444713694062 Test-Loss: 0.1718579867554311 Train-f1: 0.4545954815095293 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6308/20000] Train-Loss: 0.5635430137592073 Test-Loss: 0.17250631380127618 Train-f1: 0.450871091206505 Test-f1: 0.397364953886693 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [6309/20000] Train-Loss: 0.5674423513271524 Test-Loss: 0.17078762816108672 Train-f1: 0.4671158494890834 Test-f1: 0.43628155628155635 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6310/20000] Train-Loss: 0.5656939312574276 Test-Loss: 0.17297730246664805 Train-f1: 0.4601692688402469 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6311/20000] Train-Loss: 0.561722170753277 Test-Loss: 0.17060479086828206 Train-f1: 0.46127864928121076 Test-f1: 0.41295692040807896 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6312/20000] Train-Loss: 0.5634946930653392 Test-Loss: 0.18469624974506982 Train-f1: 0.4617413355874894 Test-f1: 0.4319434951013898 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6313/20000] Train-Loss: 0.5620588682897586 Test-Loss: 0.17109555099074747 Train-f1: 0.47097317005361405 Test-f1: 0.4582159624413146 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6314/20000] Train-Loss: 0.5638984614044689 Test-Loss: 0.17454889679395802 Train-f1: 0.4570213963936851 Test-f1: 0.4122994652406417 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6315/20000] Train-Loss: 0.5656817498787101 Test-Loss: 0.17359347274998657 Train-f1: 0.4662692076023017 Test-f1: 0.43426704014939316 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6316/20000] Train-Loss: 0.5695771180803064 Test-Loss: 0.17136140633500802 Train-f1: 0.4568871142591009 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6317/20000] Train-Loss: 0.5653453003012993 Test-Loss: 0.17115884934165224 Train-f1: 0.4601233280552016 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6318/20000] Train-Loss: 0.566189177698639 Test-Loss: 0.17884396868083377 Train-f1: 0.4553326743300973 Test-f1: 0.42846358707106313 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6319/20000] Train-Loss: 0.5661539432524417 Test-Loss: 0.1725423270223638 Train-f1: 0.4588752030492563 Test-f1: 0.4406970603766943 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6320/20000] Train-Loss: 0.5638363170581838 Test-Loss: 0.17042506954669326 Train-f1: 0.4524820788530466 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6321/20000] Train-Loss: 0.5626338877599997 Test-Loss: 0.17076344382617717 Train-f1: 0.4587651309416043 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6322/20000] Train-Loss: 0.5659150725809761 Test-Loss: 0.1710199368447201 Train-f1: 0.46239509215294355 Test-f1: 0.4253968253968254 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6323/20000] Train-Loss: 0.5609631960930214 Test-Loss: 0.17252172315503175 Train-f1: 0.4701494466237717 Test-f1: 0.4419047619047619 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6324/20000] Train-Loss: 0.5654624455411588 Test-Loss: 0.17476005771591374 Train-f1: 0.4537756471716203 Test-f1: 0.4447942921400597 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6325/20000] Train-Loss: 0.565449987235901 Test-Loss: 0.17030589854192651 Train-f1: 0.4525919848705959 Test-f1: 0.43628155628155635 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6326/20000] Train-Loss: 0.5603530668663703 Test-Loss: 0.1756552253291714 Train-f1: 0.4540085940006596 Test-f1: 0.42247442455242973 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6327/20000] Train-Loss: 0.5613555268106232 Test-Loss: 0.17740516268999523 Train-f1: 0.45378272898374467 Test-f1: 0.42846358707106313 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6328/20000] Train-Loss: 0.5623730474916425 Test-Loss: 0.17895619153046682 Train-f1: 0.45037179393925475 Test-f1: 0.40651253536240367 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [6329/20000] Train-Loss: 0.5647069405686822 Test-Loss: 0.17120067146246204 Train-f1: 0.46263433697486456 Test-f1: 0.42430685593476286 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6330/20000] Train-Loss: 0.5630367930120799 Test-Loss: 0.17255462394374319 Train-f1: 0.4587800873878972 Test-f1: 0.397364953886693 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6331/20000] Train-Loss: 0.563832152333287 Test-Loss: 0.17219419639833275 Train-f1: 0.44820222197051846 Test-f1: 0.4068599033816426 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6332/20000] Train-Loss: 0.5667047126233765 Test-Loss: 0.17007369988322618 Train-f1: 0.46511339406861796 Test-f1: 0.4253968253968254 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6333/20000] Train-Loss: 0.5632775910253154 Test-Loss: 0.17684169079367382 Train-f1: 0.4541441000722518 Test-f1: 0.4386138804049252 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6334/20000] Train-Loss: 0.5640625889804699 Test-Loss: 0.17093397169869143 Train-f1: 0.44752613993648216 Test-f1: 0.4576190476190477 Train-acc: 0.7649006622516556 Test-acc: 0.7763157894736842\n",
      "Epoch [6335/20000] Train-Loss: 0.5600309769219876 Test-Loss: 0.17594958803642533 Train-f1: 0.45643369892504976 Test-f1: 0.40272034395153417 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [6336/20000] Train-Loss: 0.5618931236845193 Test-Loss: 0.1713733713694075 Train-f1: 0.45819370999826503 Test-f1: 0.42195596518177164 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6337/20000] Train-Loss: 0.5621440315447789 Test-Loss: 0.17267794320016858 Train-f1: 0.4620816459526137 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6338/20000] Train-Loss: 0.5625693399927144 Test-Loss: 0.17096272283774006 Train-f1: 0.462572003315039 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6339/20000] Train-Loss: 0.5647226101936215 Test-Loss: 0.1719646843219307 Train-f1: 0.45571622790755156 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6340/20000] Train-Loss: 0.5635418836564551 Test-Loss: 0.17457605920830951 Train-f1: 0.45607738468519454 Test-f1: 0.43455108359133127 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6341/20000] Train-Loss: 0.5657486938849453 Test-Loss: 0.17007930926662412 Train-f1: 0.4494715338840039 Test-f1: 0.4253968253968254 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [6342/20000] Train-Loss: 0.5635020177501147 Test-Loss: 0.1746589471992147 Train-f1: 0.4545236576545899 Test-f1: 0.3914438502673797 Train-acc: 0.7715231788079471 Test-acc: 0.6842105263157895\n",
      "Epoch [6343/20000] Train-Loss: 0.5584010713601077 Test-Loss: 0.17156874152271068 Train-f1: 0.46709165288090376 Test-f1: 0.43485919098822323 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6344/20000] Train-Loss: 0.5657449689213297 Test-Loss: 0.17017712115440312 Train-f1: 0.4561867644956882 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6345/20000] Train-Loss: 0.5635285417258528 Test-Loss: 0.17106739221993136 Train-f1: 0.46485295403238663 Test-f1: 0.42430685593476286 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6346/20000] Train-Loss: 0.563280604532474 Test-Loss: 0.17048273222058247 Train-f1: 0.4510800262810418 Test-f1: 0.4253968253968254 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [6347/20000] Train-Loss: 0.5663583675050772 Test-Loss: 0.17083444650313015 Train-f1: 0.4588895103472822 Test-f1: 0.43628155628155635 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6348/20000] Train-Loss: 0.5624678148155474 Test-Loss: 0.1760493871643261 Train-f1: 0.45997490525189877 Test-f1: 0.42815154994259474 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6349/20000] Train-Loss: 0.5616587160928018 Test-Loss: 0.17511916202181185 Train-f1: 0.4532779428170638 Test-f1: 0.40272034395153417 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [6350/20000] Train-Loss: 0.5635539389122947 Test-Loss: 0.16998169108922523 Train-f1: 0.46939988381965236 Test-f1: 0.4253968253968254 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6351/20000] Train-Loss: 0.5619353645344125 Test-Loss: 0.1715599906884243 Train-f1: 0.46252388788019994 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6352/20000] Train-Loss: 0.5615848790460675 Test-Loss: 0.1721009047514306 Train-f1: 0.4581483934443802 Test-f1: 0.45103407755581665 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6353/20000] Train-Loss: 0.5623191541373113 Test-Loss: 0.1697017816176795 Train-f1: 0.46406314418663736 Test-f1: 0.4144537815126051 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6354/20000] Train-Loss: 0.566488123635993 Test-Loss: 0.1701720228570226 Train-f1: 0.4610310428602757 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6355/20000] Train-Loss: 0.565385630389124 Test-Loss: 0.18053127007151537 Train-f1: 0.4501403887070038 Test-f1: 0.4074109554921158 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [6356/20000] Train-Loss: 0.5654439572976204 Test-Loss: 0.17039476179983706 Train-f1: 0.44806440561836247 Test-f1: 0.4253968253968254 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [6357/20000] Train-Loss: 0.5644097714772619 Test-Loss: 0.17361997774482793 Train-f1: 0.44872089589474917 Test-f1: 0.43455108359133127 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [6358/20000] Train-Loss: 0.5632952383361073 Test-Loss: 0.17072894351190165 Train-f1: 0.4707353069735631 Test-f1: 0.4144537815126051 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6359/20000] Train-Loss: 0.5601983254896398 Test-Loss: 0.16970155779926627 Train-f1: 0.4557188201693288 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6360/20000] Train-Loss: 0.5617171599123597 Test-Loss: 0.17023801410168854 Train-f1: 0.4601692688402469 Test-f1: 0.4582159624413146 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6361/20000] Train-Loss: 0.5664812618568605 Test-Loss: 0.1722008947353669 Train-f1: 0.45456339930393225 Test-f1: 0.42430685593476286 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6362/20000] Train-Loss: 0.5635977595318074 Test-Loss: 0.17263282064524393 Train-f1: 0.466037879736226 Test-f1: 0.43485919098822323 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6363/20000] Train-Loss: 0.5610044309035285 Test-Loss: 0.17480207367858833 Train-f1: 0.45982334869431646 Test-f1: 0.3914438502673797 Train-acc: 0.7781456953642384 Test-acc: 0.6842105263157895\n",
      "Epoch [6364/20000] Train-Loss: 0.5635855444859771 Test-Loss: 0.17046430472079768 Train-f1: 0.4566001960022069 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6365/20000] Train-Loss: 0.5625555773064217 Test-Loss: 0.1715528071202801 Train-f1: 0.45799189508345484 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6366/20000] Train-Loss: 0.5598793975580805 Test-Loss: 0.17008912027378684 Train-f1: 0.4578166644403311 Test-f1: 0.41295692040807896 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6367/20000] Train-Loss: 0.5659565895611581 Test-Loss: 0.1710745702354215 Train-f1: 0.4570490758994358 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6368/20000] Train-Loss: 0.5640084398676617 Test-Loss: 0.17134878816840515 Train-f1: 0.4548138880200188 Test-f1: 0.44069706037669415 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6369/20000] Train-Loss: 0.5614101471545458 Test-Loss: 0.1698762089376313 Train-f1: 0.4590003179559477 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6370/20000] Train-Loss: 0.5541516415526517 Test-Loss: 0.18201649369123618 Train-f1: 0.4630267214799589 Test-f1: 0.42846358707106313 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6371/20000] Train-Loss: 0.5640166018546288 Test-Loss: 0.16978878165014047 Train-f1: 0.46739188240677076 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6372/20000] Train-Loss: 0.5601189603826557 Test-Loss: 0.18374791937250445 Train-f1: 0.4632010003036019 Test-f1: 0.401367053998633 Train-acc: 0.7847682119205298 Test-acc: 0.6842105263157895\n",
      "Epoch [6373/20000] Train-Loss: 0.5655014448376228 Test-Loss: 0.17019734557354504 Train-f1: 0.4658613765798904 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6374/20000] Train-Loss: 0.5631142581977933 Test-Loss: 0.17203017318893443 Train-f1: 0.4523492317257306 Test-f1: 0.4088146077440076 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [6375/20000] Train-Loss: 0.560987682565435 Test-Loss: 0.17273271387529252 Train-f1: 0.461432295303263 Test-f1: 0.44472049689440996 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6376/20000] Train-Loss: 0.5619612665322603 Test-Loss: 0.17059819752616728 Train-f1: 0.45660610554937586 Test-f1: 0.4028833182321555 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6377/20000] Train-Loss: 0.5624851102788891 Test-Loss: 0.1715383481379325 Train-f1: 0.4691095870497136 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6378/20000] Train-Loss: 0.5591366968745407 Test-Loss: 0.17528347819509799 Train-f1: 0.45860671299771294 Test-f1: 0.4074109554921158 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6379/20000] Train-Loss: 0.5625335924112115 Test-Loss: 0.1702362511279425 Train-f1: 0.45322521932827275 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6380/20000] Train-Loss: 0.5622011923473312 Test-Loss: 0.17470451577182672 Train-f1: 0.4610077198786876 Test-f1: 0.4386138804049252 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6381/20000] Train-Loss: 0.564495878992077 Test-Loss: 0.17545684813662624 Train-f1: 0.4676085869272496 Test-f1: 0.42822708164603346 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [6382/20000] Train-Loss: 0.5626662911441589 Test-Loss: 0.17408281582593838 Train-f1: 0.4533957509944617 Test-f1: 0.43426704014939316 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6383/20000] Train-Loss: 0.5626363408224747 Test-Loss: 0.17293636503962545 Train-f1: 0.4534458057378261 Test-f1: 0.4447942921400597 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6384/20000] Train-Loss: 0.5629957851273476 Test-Loss: 0.17422432355178574 Train-f1: 0.4501597448156124 Test-f1: 0.4447942921400597 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6385/20000] Train-Loss: 0.5588226411023555 Test-Loss: 0.16963291629379693 Train-f1: 0.4661363902508566 Test-f1: 0.4253968253968254 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6386/20000] Train-Loss: 0.5603801232770759 Test-Loss: 0.17164034500480138 Train-f1: 0.45235809226655344 Test-f1: 0.45103407755581665 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6387/20000] Train-Loss: 0.5605171688841354 Test-Loss: 0.17029508132869953 Train-f1: 0.46033922171018943 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6388/20000] Train-Loss: 0.5571883582335081 Test-Loss: 0.17197534632815528 Train-f1: 0.4646320454349202 Test-f1: 0.397364953886693 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [6389/20000] Train-Loss: 0.561567042391984 Test-Loss: 0.17047469967553133 Train-f1: 0.45949386336483106 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6390/20000] Train-Loss: 0.5643461669401693 Test-Loss: 0.16958257559029008 Train-f1: 0.4612135738195254 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6391/20000] Train-Loss: 0.5642557121306122 Test-Loss: 0.17291089756642833 Train-f1: 0.4603379928795893 Test-f1: 0.43426704014939316 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6392/20000] Train-Loss: 0.5619007739940202 Test-Loss: 0.17080166729351862 Train-f1: 0.47054976339783783 Test-f1: 0.43485919098822323 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [6393/20000] Train-Loss: 0.5633616339965688 Test-Loss: 0.17214573487092308 Train-f1: 0.45885343116235483 Test-f1: 0.45103407755581665 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6394/20000] Train-Loss: 0.5636577834581226 Test-Loss: 0.16969239555662463 Train-f1: 0.44468433741070806 Test-f1: 0.4253968253968254 Train-acc: 0.7615894039735099 Test-acc: 0.7368421052631579\n",
      "Epoch [6395/20000] Train-Loss: 0.5638262668252519 Test-Loss: 0.16969779304045302 Train-f1: 0.45267203427662395 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6396/20000] Train-Loss: 0.562052358110634 Test-Loss: 0.16957384828410604 Train-f1: 0.4601652011723954 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6397/20000] Train-Loss: 0.5619219002281458 Test-Loss: 0.1699739119403009 Train-f1: 0.46335861247307886 Test-f1: 0.4582159624413146 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6398/20000] Train-Loss: 0.5596996445031207 Test-Loss: 0.170963738694591 Train-f1: 0.45970407701750987 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6399/20000] Train-Loss: 0.5592339179752728 Test-Loss: 0.17378341384342785 Train-f1: 0.46138951034728226 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6400/20000] Train-Loss: 0.5596872703699675 Test-Loss: 0.1695754917033879 Train-f1: 0.46106568920736146 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6401/20000] Train-Loss: 0.5615057037424166 Test-Loss: 0.1710598749952106 Train-f1: 0.4462952101661779 Test-f1: 0.446829268292683 Train-acc: 0.7649006622516556 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6402/20000] Train-Loss: 0.5607281595150668 Test-Loss: 0.16915370142814162 Train-f1: 0.4572967255684337 Test-f1: 0.42564459930313586 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6403/20000] Train-Loss: 0.5600411488729871 Test-Loss: 0.17430844508478602 Train-f1: 0.46410819949281484 Test-f1: 0.4122994652406417 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6404/20000] Train-Loss: 0.5603373054520897 Test-Loss: 0.1693418695081271 Train-f1: 0.45609846427493766 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6405/20000] Train-Loss: 0.5594524220800571 Test-Loss: 0.17251539833341803 Train-f1: 0.4628224649292788 Test-f1: 0.4403558383409921 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6406/20000] Train-Loss: 0.5602018902696287 Test-Loss: 0.17025004616122313 Train-f1: 0.45313558274626126 Test-f1: 0.42564459930313586 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6407/20000] Train-Loss: 0.5597670853278216 Test-Loss: 0.17290064672466174 Train-f1: 0.4516477006344267 Test-f1: 0.43455108359133127 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6408/20000] Train-Loss: 0.5621144527563903 Test-Loss: 0.16917931827710245 Train-f1: 0.4630641737826876 Test-f1: 0.4253968253968254 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6409/20000] Train-Loss: 0.5598669374077946 Test-Loss: 0.1700092347162482 Train-f1: 0.4574871166928209 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6410/20000] Train-Loss: 0.5577249194435056 Test-Loss: 0.1694796203752718 Train-f1: 0.4693775269327052 Test-f1: 0.4253968253968254 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6411/20000] Train-Loss: 0.5608955568046903 Test-Loss: 0.1710134799929959 Train-f1: 0.45370937635838293 Test-f1: 0.41987577639751555 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6412/20000] Train-Loss: 0.5593272450398662 Test-Loss: 0.17420856951476543 Train-f1: 0.4417905894913445 Test-f1: 0.41363211951447243 Train-acc: 0.7582781456953642 Test-acc: 0.7105263157894737\n",
      "Epoch [6413/20000] Train-Loss: 0.5602143883110652 Test-Loss: 0.17121820043830507 Train-f1: 0.45746195002856993 Test-f1: 0.41948862966890543 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6414/20000] Train-Loss: 0.5607947933863145 Test-Loss: 0.16939168337438915 Train-f1: 0.4545674086636238 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6415/20000] Train-Loss: 0.5611294953886384 Test-Loss: 0.17054559853896145 Train-f1: 0.45695332416812373 Test-f1: 0.4088146077440076 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6416/20000] Train-Loss: 0.5603492747803591 Test-Loss: 0.17093966729554558 Train-f1: 0.47068849594139905 Test-f1: 0.41948862966890543 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [6417/20000] Train-Loss: 0.5558266140610169 Test-Loss: 0.17579073363230682 Train-f1: 0.47219607191954516 Test-f1: 0.40651253536240367 Train-acc: 0.7980132450331126 Test-acc: 0.6973684210526315\n",
      "Epoch [6418/20000] Train-Loss: 0.5601914397613842 Test-Loss: 0.17065347506361928 Train-f1: 0.45994476708755216 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6419/20000] Train-Loss: 0.5605136982464349 Test-Loss: 0.16917482263167685 Train-f1: 0.46015770922647264 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6420/20000] Train-Loss: 0.557827179078945 Test-Loss: 0.17338283013271638 Train-f1: 0.46458193676963494 Test-f1: 0.4122994652406417 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6421/20000] Train-Loss: 0.5568161509634683 Test-Loss: 0.17049243960348381 Train-f1: 0.4626524227403886 Test-f1: 0.43485919098822323 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6422/20000] Train-Loss: 0.5605464077661633 Test-Loss: 0.17106123669488413 Train-f1: 0.46779353049465655 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6423/20000] Train-Loss: 0.5600566035950327 Test-Loss: 0.1704652956652247 Train-f1: 0.4558671663622997 Test-f1: 0.4088146077440076 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6424/20000] Train-Loss: 0.5601506657522523 Test-Loss: 0.1689849650324588 Train-f1: 0.45687894129141127 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6425/20000] Train-Loss: 0.5608074431677167 Test-Loss: 0.16919832280148792 Train-f1: 0.4555243020437761 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6426/20000] Train-Loss: 0.559006653450884 Test-Loss: 0.17043029019610403 Train-f1: 0.46043085501232656 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6427/20000] Train-Loss: 0.5622772173941254 Test-Loss: 0.17136935773542325 Train-f1: 0.4572875033306688 Test-f1: 0.45103407755581665 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6428/20000] Train-Loss: 0.5576919739712125 Test-Loss: 0.1721390418927279 Train-f1: 0.44585576165950247 Test-f1: 0.41626928471248253 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [6429/20000] Train-Loss: 0.5613882728432011 Test-Loss: 0.17097291998602443 Train-f1: 0.45949386336483106 Test-f1: 0.41948862966890543 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6430/20000] Train-Loss: 0.5617866394034255 Test-Loss: 0.17138830137510228 Train-f1: 0.4620029243800377 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6431/20000] Train-Loss: 0.5597383579851972 Test-Loss: 0.1693603966863819 Train-f1: 0.4651034727773336 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6432/20000] Train-Loss: 0.5605131279486371 Test-Loss: 0.16988607578384904 Train-f1: 0.4601692688402469 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6433/20000] Train-Loss: 0.5580524488744768 Test-Loss: 0.1742512672985349 Train-f1: 0.455862775217614 Test-f1: 0.41363211951447243 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6434/20000] Train-Loss: 0.5629714580877976 Test-Loss: 0.17230907876055299 Train-f1: 0.45481836752819965 Test-f1: 0.41987577639751555 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6435/20000] Train-Loss: 0.5572232936282187 Test-Loss: 0.1693497993291881 Train-f1: 0.4656963645673323 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6436/20000] Train-Loss: 0.5615533244191934 Test-Loss: 0.1727544258574104 Train-f1: 0.4585988921472793 Test-f1: 0.42939958592132504 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6437/20000] Train-Loss: 0.5611590361791514 Test-Loss: 0.1783587870190842 Train-f1: 0.4474691531978496 Test-f1: 0.41754215933320415 Train-acc: 0.7649006622516556 Test-acc: 0.7105263157894737\n",
      "Epoch [6438/20000] Train-Loss: 0.5604742471671044 Test-Loss: 0.16898554786941977 Train-f1: 0.4494955099749987 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "Epoch [6439/20000] Train-Loss: 0.5604098568848386 Test-Loss: 0.1734616264322938 Train-f1: 0.46087751823013223 Test-f1: 0.40272034395153417 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [6440/20000] Train-Loss: 0.5546056429809884 Test-Loss: 0.16937946574255533 Train-f1: 0.4683430277054951 Test-f1: 0.4149259191512713 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [6441/20000] Train-Loss: 0.5589923843095325 Test-Loss: 0.1699920286398268 Train-f1: 0.4559625065754531 Test-f1: 0.4093581780538303 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6442/20000] Train-Loss: 0.5608362383580722 Test-Loss: 0.16933177329829568 Train-f1: 0.4509192433456902 Test-f1: 0.44709236709236705 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [6443/20000] Train-Loss: 0.5575011481332923 Test-Loss: 0.1718099274133933 Train-f1: 0.4610554937584971 Test-f1: 0.4088146077440076 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6444/20000] Train-Loss: 0.560941034246583 Test-Loss: 0.17047455628984334 Train-f1: 0.4729041567751245 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [6445/20000] Train-Loss: 0.5580017729612351 Test-Loss: 0.17004935698285353 Train-f1: 0.4551749936750042 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6446/20000] Train-Loss: 0.5567433807712302 Test-Loss: 0.17373432138791936 Train-f1: 0.4505182760501909 Test-f1: 0.43426704014939316 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [6447/20000] Train-Loss: 0.5597771299279081 Test-Loss: 0.17178626994021307 Train-f1: 0.4679815898833728 Test-f1: 0.45103407755581665 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6448/20000] Train-Loss: 0.5601631904748197 Test-Loss: 0.1708973576811404 Train-f1: 0.45695332416812373 Test-f1: 0.397364953886693 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [6449/20000] Train-Loss: 0.5602969762561738 Test-Loss: 0.16903993792592106 Train-f1: 0.45906729935200774 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6450/20000] Train-Loss: 0.5560886509723831 Test-Loss: 0.17058105872125023 Train-f1: 0.45246754396932565 Test-f1: 0.4483006535947712 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6451/20000] Train-Loss: 0.5573343079411482 Test-Loss: 0.17016168273683577 Train-f1: 0.4567526987087291 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6452/20000] Train-Loss: 0.5596714426967239 Test-Loss: 0.17266702169489048 Train-f1: 0.4532720615127771 Test-f1: 0.4134165729574624 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [6453/20000] Train-Loss: 0.5580187020560194 Test-Loss: 0.17187755366868554 Train-f1: 0.4656963645673323 Test-f1: 0.45103407755581665 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6454/20000] Train-Loss: 0.558160139272235 Test-Loss: 0.1762794850650166 Train-f1: 0.45374913516666054 Test-f1: 0.4042235630358844 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [6455/20000] Train-Loss: 0.560521543401458 Test-Loss: 0.16873048158329412 Train-f1: 0.4598189256081765 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6456/20000] Train-Loss: 0.5603376738228597 Test-Loss: 0.1690492886214982 Train-f1: 0.4599310858310391 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6457/20000] Train-Loss: 0.5593341392191683 Test-Loss: 0.17063290790430824 Train-f1: 0.4530736820562103 Test-f1: 0.4037866173296331 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [6458/20000] Train-Loss: 0.5597219176851276 Test-Loss: 0.1689793841506345 Train-f1: 0.46053252821306734 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6459/20000] Train-Loss: 0.5575289269614914 Test-Loss: 0.16868196914050018 Train-f1: 0.4557453180186739 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6460/20000] Train-Loss: 0.5563751315426205 Test-Loss: 0.17184716088984314 Train-f1: 0.45885077276524644 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6461/20000] Train-Loss: 0.561375247397871 Test-Loss: 0.1687081913363853 Train-f1: 0.4640497059038669 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6462/20000] Train-Loss: 0.5587576183909291 Test-Loss: 0.16905101151341576 Train-f1: 0.45803306399466415 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6463/20000] Train-Loss: 0.5586468171573957 Test-Loss: 0.1737712766139181 Train-f1: 0.46239509215294355 Test-f1: 0.4447942921400597 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6464/20000] Train-Loss: 0.5563796526306863 Test-Loss: 0.16999158630768874 Train-f1: 0.45638755132658815 Test-f1: 0.4483006535947712 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6465/20000] Train-Loss: 0.5588633004346848 Test-Loss: 0.16885533805703093 Train-f1: 0.4607940446650124 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6466/20000] Train-Loss: 0.5577500896722319 Test-Loss: 0.16973893886450292 Train-f1: 0.46572818728031445 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6467/20000] Train-Loss: 0.5577923598937429 Test-Loss: 0.1807815223454887 Train-f1: 0.45249579489747316 Test-f1: 0.42822708164603346 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6468/20000] Train-Loss: 0.55952713071 Test-Loss: 0.16985117852269585 Train-f1: 0.46838973265831785 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6469/20000] Train-Loss: 0.560739772373041 Test-Loss: 0.17046836814912508 Train-f1: 0.46674594798533675 Test-f1: 0.42430685593476286 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6470/20000] Train-Loss: 0.5543607614090668 Test-Loss: 0.1694028598713952 Train-f1: 0.4457629444933719 Test-f1: 0.4093581780538303 Train-acc: 0.7615894039735099 Test-acc: 0.7236842105263158\n",
      "Epoch [6471/20000] Train-Loss: 0.5583537250836085 Test-Loss: 0.16871546649861124 Train-f1: 0.45011252042542677 Test-f1: 0.4144537815126051 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [6472/20000] Train-Loss: 0.5568860120464347 Test-Loss: 0.16940380862937146 Train-f1: 0.4563154121863799 Test-f1: 0.4093581780538303 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6473/20000] Train-Loss: 0.5580330448190284 Test-Loss: 0.1698115966917856 Train-f1: 0.46127427267438337 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6474/20000] Train-Loss: 0.5568589913406301 Test-Loss: 0.16946742970000978 Train-f1: 0.472632503524952 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [6475/20000] Train-Loss: 0.5576815545677078 Test-Loss: 0.1687315721478097 Train-f1: 0.4593834309256315 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6476/20000] Train-Loss: 0.5614571320338174 Test-Loss: 0.16881084403763147 Train-f1: 0.4695595815777443 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6477/20000] Train-Loss: 0.557285498788582 Test-Loss: 0.1696062529722099 Train-f1: 0.4614185814185814 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6478/20000] Train-Loss: 0.557613531167321 Test-Loss: 0.16946400211779084 Train-f1: 0.4614895821342236 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6479/20000] Train-Loss: 0.5607809968051339 Test-Loss: 0.16844780788510022 Train-f1: 0.46590441932168547 Test-f1: 0.4144537815126051 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6480/20000] Train-Loss: 0.5559169743155405 Test-Loss: 0.1700864156962205 Train-f1: 0.45838716691566883 Test-f1: 0.4602863322690288 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6481/20000] Train-Loss: 0.5607423396616544 Test-Loss: 0.16984683232899409 Train-f1: 0.4606963645673323 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6482/20000] Train-Loss: 0.5559777312877835 Test-Loss: 0.1710897211774208 Train-f1: 0.47674308514235475 Test-f1: 0.45103407755581665 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [6483/20000] Train-Loss: 0.5579624568162522 Test-Loss: 0.16915016476352446 Train-f1: 0.4585940855228216 Test-f1: 0.4031772027513921 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6484/20000] Train-Loss: 0.5567119564032459 Test-Loss: 0.17246980078142943 Train-f1: 0.45735302110698434 Test-f1: 0.42939958592132504 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6485/20000] Train-Loss: 0.5576652003749971 Test-Loss: 0.16844371553368645 Train-f1: 0.4583259631799642 Test-f1: 0.4253968253968254 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6486/20000] Train-Loss: 0.5588056106670326 Test-Loss: 0.16847194001913426 Train-f1: 0.4608470252975339 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6487/20000] Train-Loss: 0.563128751270647 Test-Loss: 0.1686707255328362 Train-f1: 0.4584748485972067 Test-f1: 0.43628155628155635 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6488/20000] Train-Loss: 0.5566965182441833 Test-Loss: 0.16966990480015032 Train-f1: 0.45267515678504094 Test-f1: 0.4253968253968254 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6489/20000] Train-Loss: 0.5544126239762149 Test-Loss: 0.16955335267847152 Train-f1: 0.46138951034728226 Test-f1: 0.4253968253968254 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6490/20000] Train-Loss: 0.5565627186656381 Test-Loss: 0.168920067276475 Train-f1: 0.46053252821306734 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6491/20000] Train-Loss: 0.5586466882080525 Test-Loss: 0.17059364782239814 Train-f1: 0.46894320943426726 Test-f1: 0.4545938580861545 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6492/20000] Train-Loss: 0.5575464321905033 Test-Loss: 0.16882414107098448 Train-f1: 0.45288806175902946 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6493/20000] Train-Loss: 0.5578518614675003 Test-Loss: 0.17069336113096106 Train-f1: 0.4539047765546738 Test-f1: 0.41948862966890543 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6494/20000] Train-Loss: 0.5574507641367825 Test-Loss: 0.16831386384537161 Train-f1: 0.46050614827287095 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6495/20000] Train-Loss: 0.5589309938560472 Test-Loss: 0.16826909909114904 Train-f1: 0.4548138880200188 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6496/20000] Train-Loss: 0.5563938633846917 Test-Loss: 0.17443720295386497 Train-f1: 0.4622136294880777 Test-f1: 0.41754215933320415 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6497/20000] Train-Loss: 0.5584863296097841 Test-Loss: 0.1702428367346071 Train-f1: 0.4597566391331379 Test-f1: 0.43485919098822323 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6498/20000] Train-Loss: 0.5580372802061995 Test-Loss: 0.17300314319389365 Train-f1: 0.4585294903456635 Test-f1: 0.42247442455242973 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6499/20000] Train-Loss: 0.5519502899735322 Test-Loss: 0.1729168446539623 Train-f1: 0.4620361950789034 Test-f1: 0.4037866173296331 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6500/20000] Train-Loss: 0.5578596385847283 Test-Loss: 0.1687209817728687 Train-f1: 0.46403605439347945 Test-f1: 0.43628155628155635 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6501/20000] Train-Loss: 0.5583247711536261 Test-Loss: 0.1696377220378785 Train-f1: 0.4660592096075967 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6502/20000] Train-Loss: 0.5542100810506277 Test-Loss: 0.17051670618376297 Train-f1: 0.4539998204145367 Test-f1: 0.41626928471248253 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6503/20000] Train-Loss: 0.5574528663112187 Test-Loss: 0.17016624041746772 Train-f1: 0.4564603955932999 Test-f1: 0.4302232667450059 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6504/20000] Train-Loss: 0.5572848418382109 Test-Loss: 0.16811023183478577 Train-f1: 0.4601692688402469 Test-f1: 0.43628155628155635 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6505/20000] Train-Loss: 0.5594216798819607 Test-Loss: 0.17065289776514825 Train-f1: 0.46488106875203644 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6506/20000] Train-Loss: 0.5490519450796463 Test-Loss: 0.1708650010420514 Train-f1: 0.4628569466801956 Test-f1: 0.4419047619047619 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6507/20000] Train-Loss: 0.5533239988411423 Test-Loss: 0.16836731633406263 Train-f1: 0.4620760959470636 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6508/20000] Train-Loss: 0.5608670743535437 Test-Loss: 0.16902044125457258 Train-f1: 0.4585294903456635 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6509/20000] Train-Loss: 0.5556267322597056 Test-Loss: 0.17011081728877847 Train-f1: 0.465506148272871 Test-f1: 0.45103407755581665 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6510/20000] Train-Loss: 0.5574406110830971 Test-Loss: 0.16949085048429663 Train-f1: 0.4617329458491245 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6511/20000] Train-Loss: 0.5554422395216291 Test-Loss: 0.16882934222378967 Train-f1: 0.45532978391822354 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6512/20000] Train-Loss: 0.5537425104226125 Test-Loss: 0.16826400003416223 Train-f1: 0.4637986272349369 Test-f1: 0.4028833182321555 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6513/20000] Train-Loss: 0.5552683080370765 Test-Loss: 0.1709137014120483 Train-f1: 0.45767103087716166 Test-f1: 0.45103407755581665 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6514/20000] Train-Loss: 0.556207453507422 Test-Loss: 0.17057136483244492 Train-f1: 0.45571622790755156 Test-f1: 0.41987577639751555 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6515/20000] Train-Loss: 0.5579745199129109 Test-Loss: 0.17026529010952374 Train-f1: 0.46217229821541805 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6516/20000] Train-Loss: 0.5561789663419074 Test-Loss: 0.1731570248965789 Train-f1: 0.4576907169454444 Test-f1: 0.4447942921400597 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6517/20000] Train-Loss: 0.5540511930806287 Test-Loss: 0.16880484462055623 Train-f1: 0.46286304118876825 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6518/20000] Train-Loss: 0.5576750904673431 Test-Loss: 0.17064535430262426 Train-f1: 0.4609895756863896 Test-f1: 0.397364953886693 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [6519/20000] Train-Loss: 0.5599123578779928 Test-Loss: 0.16847703017036392 Train-f1: 0.46053252821306734 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6520/20000] Train-Loss: 0.5571915927895252 Test-Loss: 0.16822947205882843 Train-f1: 0.46302334162433534 Test-f1: 0.4253968253968254 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6521/20000] Train-Loss: 0.5565098081492444 Test-Loss: 0.16957804266035706 Train-f1: 0.4592939214588805 Test-f1: 0.43405844155844153 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6522/20000] Train-Loss: 0.5560772189482215 Test-Loss: 0.16937516521029372 Train-f1: 0.45555966986499163 Test-f1: 0.42430685593476286 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6523/20000] Train-Loss: 0.5550157709325796 Test-Loss: 0.17074253276450754 Train-f1: 0.4560260678364226 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6524/20000] Train-Loss: 0.5539511043092104 Test-Loss: 0.16808330121406617 Train-f1: 0.46341112786163646 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6525/20000] Train-Loss: 0.5534026634654541 Test-Loss: 0.1803132816839592 Train-f1: 0.4572983600697082 Test-f1: 0.3858589405911618 Train-acc: 0.7781456953642384 Test-acc: 0.6710526315789473\n",
      "Epoch [6526/20000] Train-Loss: 0.5581100445104132 Test-Loss: 0.17287565534508195 Train-f1: 0.4655120348165912 Test-f1: 0.43426704014939316 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6527/20000] Train-Loss: 0.5561804116539928 Test-Loss: 0.17106292183620347 Train-f1: 0.46422032967583454 Test-f1: 0.41948862966890543 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6528/20000] Train-Loss: 0.5548332322924953 Test-Loss: 0.16820319567818953 Train-f1: 0.45839217308898705 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6529/20000] Train-Loss: 0.556658480502844 Test-Loss: 0.16842562889984347 Train-f1: 0.4646674104566614 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6530/20000] Train-Loss: 0.5556559962835617 Test-Loss: 0.1688480985133634 Train-f1: 0.4608470252975339 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6531/20000] Train-Loss: 0.5525238043780561 Test-Loss: 0.1687480247375841 Train-f1: 0.448014784922327 Test-f1: 0.4093581780538303 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [6532/20000] Train-Loss: 0.5565658651289384 Test-Loss: 0.16810387660022585 Train-f1: 0.4549928142261008 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6533/20000] Train-Loss: 0.5563079162608269 Test-Loss: 0.16946895635278733 Train-f1: 0.47262580912713437 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [6534/20000] Train-Loss: 0.5510394373459664 Test-Loss: 0.16835449831329707 Train-f1: 0.46086606125528373 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6535/20000] Train-Loss: 0.5544382341845989 Test-Loss: 0.17369697586330332 Train-f1: 0.4675723619075187 Test-f1: 0.41754215933320415 Train-acc: 0.7913907284768212 Test-acc: 0.7105263157894737\n",
      "Epoch [6536/20000] Train-Loss: 0.5563508866767591 Test-Loss: 0.16838926420502773 Train-f1: 0.4533309276280847 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6537/20000] Train-Loss: 0.5541792523423282 Test-Loss: 0.1681173401589833 Train-f1: 0.4632673620140032 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6538/20000] Train-Loss: 0.5535002682961584 Test-Loss: 0.17093685272256318 Train-f1: 0.45099804493754386 Test-f1: 0.43574229691876754 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6539/20000] Train-Loss: 0.5508749537093168 Test-Loss: 0.1718557721228799 Train-f1: 0.4595343849144287 Test-f1: 0.4128560719640181 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6540/20000] Train-Loss: 0.5594072456605559 Test-Loss: 0.17261589852035808 Train-f1: 0.4549758229881733 Test-f1: 0.41363211951447243 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6541/20000] Train-Loss: 0.5526542817587378 Test-Loss: 0.17374874886532615 Train-f1: 0.46252388788019994 Test-f1: 0.42822708164603346 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6542/20000] Train-Loss: 0.5532405568695067 Test-Loss: 0.168302612090722 Train-f1: 0.45887527835419173 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6543/20000] Train-Loss: 0.5592974675863756 Test-Loss: 0.16865474527024962 Train-f1: 0.4636361389197874 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6544/20000] Train-Loss: 0.5595198584790377 Test-Loss: 0.1769472579782634 Train-f1: 0.4529920926834895 Test-f1: 0.39702267881372355 Train-acc: 0.7715231788079471 Test-acc: 0.6842105263157895\n",
      "Epoch [6545/20000] Train-Loss: 0.5542124356938287 Test-Loss: 0.171519416279697 Train-f1: 0.47083873274051563 Test-f1: 0.45103407755581665 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6546/20000] Train-Loss: 0.5558845683406971 Test-Loss: 0.16946675646169823 Train-f1: 0.45819370999826503 Test-f1: 0.397364953886693 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6547/20000] Train-Loss: 0.5544402993523568 Test-Loss: 0.17247437692081327 Train-f1: 0.4676085869272496 Test-f1: 0.40272034395153417 Train-acc: 0.7913907284768212 Test-acc: 0.6973684210526315\n",
      "Epoch [6548/20000] Train-Loss: 0.5555413491844287 Test-Loss: 0.16824993567489968 Train-f1: 0.45783922171018937 Test-f1: 0.42564459930313586 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6549/20000] Train-Loss: 0.5555716665018174 Test-Loss: 0.16882586034851604 Train-f1: 0.46138951034728226 Test-f1: 0.43628155628155635 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6550/20000] Train-Loss: 0.5551211852789223 Test-Loss: 0.16781096254409664 Train-f1: 0.4813934466532164 Test-f1: 0.436390977443609 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [6551/20000] Train-Loss: 0.5559837945706259 Test-Loss: 0.16797617183023453 Train-f1: 0.45568645834146615 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6552/20000] Train-Loss: 0.5564459526270088 Test-Loss: 0.17139300792806014 Train-f1: 0.4597566391331379 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6553/20000] Train-Loss: 0.5536078846197898 Test-Loss: 0.175876036068023 Train-f1: 0.4535357923514243 Test-f1: 0.39702267881372355 Train-acc: 0.7715231788079471 Test-acc: 0.6842105263157895\n",
      "Epoch [6554/20000] Train-Loss: 0.5556541747866531 Test-Loss: 0.17046576777607128 Train-f1: 0.47054976339783783 Test-f1: 0.45103407755581665 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6555/20000] Train-Loss: 0.554254754155533 Test-Loss: 0.17216284708747567 Train-f1: 0.45543456754082934 Test-f1: 0.4545938580861545 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6556/20000] Train-Loss: 0.5577057610330939 Test-Loss: 0.17010003366137652 Train-f1: 0.4615200978290215 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6557/20000] Train-Loss: 0.555089707823994 Test-Loss: 0.16845641699577207 Train-f1: 0.46363588686901336 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6558/20000] Train-Loss: 0.5549777556776272 Test-Loss: 0.1725208944030882 Train-f1: 0.45808840460787864 Test-f1: 0.4447942921400597 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6559/20000] Train-Loss: 0.55679545581732 Test-Loss: 0.1681044930915253 Train-f1: 0.4605808346953012 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6560/20000] Train-Loss: 0.5538557336421943 Test-Loss: 0.1676079890238534 Train-f1: 0.46586978864113676 Test-f1: 0.42564459930313586 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6561/20000] Train-Loss: 0.5522205618737813 Test-Loss: 0.17202943374995752 Train-f1: 0.45640380358149796 Test-f1: 0.42939958592132504 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6562/20000] Train-Loss: 0.5555624079445226 Test-Loss: 0.1708523216571752 Train-f1: 0.45609846427493766 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6563/20000] Train-Loss: 0.552488585915698 Test-Loss: 0.17308240624998777 Train-f1: 0.467767908160615 Test-f1: 0.4386138804049252 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6564/20000] Train-Loss: 0.5545670778909956 Test-Loss: 0.16970228365311021 Train-f1: 0.47079117673867465 Test-f1: 0.45103407755581665 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6565/20000] Train-Loss: 0.5538529815395747 Test-Loss: 0.16769356775528152 Train-f1: 0.4642496458438057 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6566/20000] Train-Loss: 0.5517467910514099 Test-Loss: 0.17741500836850616 Train-f1: 0.4566001960022069 Test-f1: 0.42620365912373614 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6567/20000] Train-Loss: 0.5545665719845737 Test-Loss: 0.1679433105524877 Train-f1: 0.45153978727845756 Test-f1: 0.4028833182321555 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [6568/20000] Train-Loss: 0.5504093552597988 Test-Loss: 0.168702519942187 Train-f1: 0.47110580728374674 Test-f1: 0.42564459930313586 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [6569/20000] Train-Loss: 0.5576773357379208 Test-Loss: 0.16943347240456985 Train-f1: 0.45706962094058873 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6570/20000] Train-Loss: 0.5555413120123177 Test-Loss: 0.1685383674214461 Train-f1: 0.4605808346953012 Test-f1: 0.4253968253968254 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6571/20000] Train-Loss: 0.5487010801170548 Test-Loss: 0.1693386301101436 Train-f1: 0.4649335264344169 Test-f1: 0.44472049689440996 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6572/20000] Train-Loss: 0.556497045257416 Test-Loss: 0.1680894769449741 Train-f1: 0.44841554872812617 Test-f1: 0.4576190476190477 Train-acc: 0.7649006622516556 Test-acc: 0.7763157894736842\n",
      "Epoch [6573/20000] Train-Loss: 0.5526242834540224 Test-Loss: 0.17102560252026444 Train-f1: 0.4718011501926659 Test-f1: 0.397364953886693 Train-acc: 0.7947019867549668 Test-acc: 0.6973684210526315\n",
      "Epoch [6574/20000] Train-Loss: 0.5500390034747119 Test-Loss: 0.16794856923079554 Train-f1: 0.4696039558836934 Test-f1: 0.4028833182321555 Train-acc: 0.7913907284768212 Test-acc: 0.7105263157894737\n",
      "Epoch [6575/20000] Train-Loss: 0.5547977890100853 Test-Loss: 0.1677117327502271 Train-f1: 0.4520942635014631 Test-f1: 0.43628155628155635 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6576/20000] Train-Loss: 0.5541383185942869 Test-Loss: 0.17038086361461077 Train-f1: 0.46217229821541805 Test-f1: 0.4403558383409921 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6577/20000] Train-Loss: 0.5542818409240561 Test-Loss: 0.16790691843931946 Train-f1: 0.4582968730688419 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6578/20000] Train-Loss: 0.5533475318420089 Test-Loss: 0.17093422111168993 Train-f1: 0.462225774935607 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6579/20000] Train-Loss: 0.5537372054993055 Test-Loss: 0.1682269228450297 Train-f1: 0.4704920699367158 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6580/20000] Train-Loss: 0.5539900394350115 Test-Loss: 0.16759186193306788 Train-f1: 0.45487381553699374 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6581/20000] Train-Loss: 0.5543290816951777 Test-Loss: 0.1682667098461517 Train-f1: 0.4520121139815537 Test-f1: 0.4028833182321555 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [6582/20000] Train-Loss: 0.5540443745162379 Test-Loss: 0.16862002433086032 Train-f1: 0.4519955099749987 Test-f1: 0.4028833182321555 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6583/20000] Train-Loss: 0.5557672063322969 Test-Loss: 0.16731547872912447 Train-f1: 0.45997490525189877 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6584/20000] Train-Loss: 0.5513814660146943 Test-Loss: 0.16729173960799393 Train-f1: 0.46493001188056143 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6585/20000] Train-Loss: 0.5551170963276271 Test-Loss: 0.16735744456829452 Train-f1: 0.46335861247307886 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6586/20000] Train-Loss: 0.5537795787367751 Test-Loss: 0.17195310620145685 Train-f1: 0.4601233280552016 Test-f1: 0.40272034395153417 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [6587/20000] Train-Loss: 0.5517033654243162 Test-Loss: 0.16838103413435093 Train-f1: 0.4637106641217638 Test-f1: 0.4028833182321555 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6588/20000] Train-Loss: 0.5521503083590561 Test-Loss: 0.16749882822392886 Train-f1: 0.4589954055830452 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6589/20000] Train-Loss: 0.5533142143307999 Test-Loss: 0.16766707896505806 Train-f1: 0.46019843501032726 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6590/20000] Train-Loss: 0.5488036102985715 Test-Loss: 0.17487839112874412 Train-f1: 0.45789136495073846 Test-f1: 0.40651253536240367 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6591/20000] Train-Loss: 0.5501016426102793 Test-Loss: 0.17058698075304404 Train-f1: 0.45919660763549663 Test-f1: 0.397364953886693 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6592/20000] Train-Loss: 0.5515799021815609 Test-Loss: 0.17078714772071 Train-f1: 0.4625949384860385 Test-f1: 0.42430685593476286 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6593/20000] Train-Loss: 0.5556342107885205 Test-Loss: 0.16953405359877655 Train-f1: 0.4495411908581028 Test-f1: 0.42430685593476286 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [6594/20000] Train-Loss: 0.5547544355628723 Test-Loss: 0.16899053910044315 Train-f1: 0.465975230425739 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6595/20000] Train-Loss: 0.554901691537528 Test-Loss: 0.1702862198826271 Train-f1: 0.4680049611950876 Test-f1: 0.45103407755581665 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6596/20000] Train-Loss: 0.5529755459699252 Test-Loss: 0.16742344015933344 Train-f1: 0.4584748485972067 Test-f1: 0.42564459930313586 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6597/20000] Train-Loss: 0.5523085242287888 Test-Loss: 0.1696332305123253 Train-f1: 0.4641528865261204 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6598/20000] Train-Loss: 0.5532840323326567 Test-Loss: 0.1685924457367347 Train-f1: 0.45913428811681634 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6599/20000] Train-Loss: 0.5568601721454531 Test-Loss: 0.16738598605096272 Train-f1: 0.45440133356105183 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6600/20000] Train-Loss: 0.5502848908663784 Test-Loss: 0.16762688176300589 Train-f1: 0.4557788427445642 Test-f1: 0.4028833182321555 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6601/20000] Train-Loss: 0.5542403741191237 Test-Loss: 0.16915471190224785 Train-f1: 0.46899172391410177 Test-f1: 0.45103407755581665 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6602/20000] Train-Loss: 0.551652344076452 Test-Loss: 0.16964362684170478 Train-f1: 0.4491586305423746 Test-f1: 0.45103407755581665 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [6603/20000] Train-Loss: 0.5560459905481456 Test-Loss: 0.16792910861089624 Train-f1: 0.46927935574535784 Test-f1: 0.43628155628155635 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6604/20000] Train-Loss: 0.5520507629820955 Test-Loss: 0.16710987825114604 Train-f1: 0.45232941597032933 Test-f1: 0.42564459930313586 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6605/20000] Train-Loss: 0.5544817844089237 Test-Loss: 0.16842877007898463 Train-f1: 0.4603379928795893 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6606/20000] Train-Loss: 0.5515789054172855 Test-Loss: 0.16728974295685092 Train-f1: 0.47109587380742124 Test-f1: 0.42564459930313586 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [6607/20000] Train-Loss: 0.5522308530588587 Test-Loss: 0.16729664712732575 Train-f1: 0.4576692688402469 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6608/20000] Train-Loss: 0.5517650275856083 Test-Loss: 0.17249823346340046 Train-f1: 0.4502015787900183 Test-f1: 0.4447942921400597 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [6609/20000] Train-Loss: 0.5574790679298254 Test-Loss: 0.17588534381443557 Train-f1: 0.46335861247307886 Test-f1: 0.40746844925949405 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [6610/20000] Train-Loss: 0.5533475989654048 Test-Loss: 0.17236698502939288 Train-f1: 0.45296019947967353 Test-f1: 0.4447942921400597 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6611/20000] Train-Loss: 0.5533002916732187 Test-Loss: 0.16736087116938017 Train-f1: 0.46937881747582066 Test-f1: 0.4028833182321555 Train-acc: 0.7913907284768212 Test-acc: 0.7105263157894737\n",
      "Epoch [6612/20000] Train-Loss: 0.5525180681865098 Test-Loss: 0.17074049238018507 Train-f1: 0.45884134797425224 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6613/20000] Train-Loss: 0.5549250480430833 Test-Loss: 0.1675342398895731 Train-f1: 0.46259497118246884 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6614/20000] Train-Loss: 0.551262956707427 Test-Loss: 0.1670926073075206 Train-f1: 0.45295760721789635 Test-f1: 0.4144537815126051 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6615/20000] Train-Loss: 0.5540078453751237 Test-Loss: 0.16711179256361247 Train-f1: 0.47300661291783674 Test-f1: 0.42564459930313586 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [6616/20000] Train-Loss: 0.5512580699840244 Test-Loss: 0.16715129072198054 Train-f1: 0.4608108417724418 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6617/20000] Train-Loss: 0.5511556807401111 Test-Loss: 0.16892636463883332 Train-f1: 0.4532174533137229 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6618/20000] Train-Loss: 0.556335872771937 Test-Loss: 0.16795220727373997 Train-f1: 0.45645254725531065 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6619/20000] Train-Loss: 0.5525539267288109 Test-Loss: 0.17124085000433462 Train-f1: 0.46512444702622985 Test-f1: 0.4403558383409921 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6620/20000] Train-Loss: 0.549387511003791 Test-Loss: 0.17077629808924416 Train-f1: 0.4541334527541424 Test-f1: 0.41987577639751555 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6621/20000] Train-Loss: 0.5515510021880325 Test-Loss: 0.16829174141099462 Train-f1: 0.44665863054237465 Test-f1: 0.446829268292683 Train-acc: 0.7649006622516556 Test-acc: 0.7631578947368421\n",
      "Epoch [6622/20000] Train-Loss: 0.549501823600349 Test-Loss: 0.17207520194668155 Train-f1: 0.46283922171018943 Test-f1: 0.43426704014939316 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6623/20000] Train-Loss: 0.5493584576287428 Test-Loss: 0.16713746797527382 Train-f1: 0.45864707033384244 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6624/20000] Train-Loss: 0.553673263504423 Test-Loss: 0.1672872175036957 Train-f1: 0.4635235966612288 Test-f1: 0.4028833182321555 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6625/20000] Train-Loss: 0.5534567426022762 Test-Loss: 0.168419051598878 Train-f1: 0.4656963645673323 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6626/20000] Train-Loss: 0.5491363968648155 Test-Loss: 0.17062879707504502 Train-f1: 0.4731345212506374 Test-f1: 0.4545938580861545 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [6627/20000] Train-Loss: 0.5520755889236877 Test-Loss: 0.16731631173257358 Train-f1: 0.45534934574554076 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6628/20000] Train-Loss: 0.5503021616353998 Test-Loss: 0.1676556196807267 Train-f1: 0.4576692688402469 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6629/20000] Train-Loss: 0.549548112498985 Test-Loss: 0.16766197607914224 Train-f1: 0.4715473507186297 Test-f1: 0.4144537815126051 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6630/20000] Train-Loss: 0.5524982198464051 Test-Loss: 0.16708663550378824 Train-f1: 0.4619484518075511 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6631/20000] Train-Loss: 0.5506148706923174 Test-Loss: 0.17011821734811594 Train-f1: 0.4503239591949269 Test-f1: 0.4088146077440076 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [6632/20000] Train-Loss: 0.550397433436672 Test-Loss: 0.1699986527926811 Train-f1: 0.47332114612623355 Test-f1: 0.45103407755581665 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [6633/20000] Train-Loss: 0.5513301909669339 Test-Loss: 0.16867661638922363 Train-f1: 0.45656269196641863 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6634/20000] Train-Loss: 0.5526527537909156 Test-Loss: 0.16788382832934212 Train-f1: 0.46679334354171437 Test-f1: 0.43574229691876754 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6635/20000] Train-Loss: 0.5511963547121299 Test-Loss: 0.17015857946665297 Train-f1: 0.46050614827287095 Test-f1: 0.42430685593476286 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6636/20000] Train-Loss: 0.5501003663707535 Test-Loss: 0.16908768824713377 Train-f1: 0.4714924718050494 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6637/20000] Train-Loss: 0.5499887950548918 Test-Loss: 0.17216303650134607 Train-f1: 0.4654971178336833 Test-f1: 0.42247442455242973 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6638/20000] Train-Loss: 0.55377449566787 Test-Loss: 0.17588535104637987 Train-f1: 0.4458999034538603 Test-f1: 0.3858589405911618 Train-acc: 0.7615894039735099 Test-acc: 0.6710526315789473\n",
      "best score_dx!!\n",
      "Epoch [6639/20000] Train-Loss: 0.549643688655916 Test-Loss: 0.16699803456567955 Train-f1: 0.4576907169454444 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6640/20000] Train-Loss: 0.5534859375015648 Test-Loss: 0.17007704740074955 Train-f1: 0.4661335129866554 Test-f1: 0.397364953886693 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [6641/20000] Train-Loss: 0.5482686880932942 Test-Loss: 0.1691702722316463 Train-f1: 0.46484891566515574 Test-f1: 0.4545938580861545 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6642/20000] Train-Loss: 0.5506088330404033 Test-Loss: 0.1672682854100561 Train-f1: 0.4540167865707433 Test-f1: 0.4028833182321555 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [6643/20000] Train-Loss: 0.5510209843344226 Test-Loss: 0.1729062661824023 Train-f1: 0.46485295403238663 Test-f1: 0.4386138804049252 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6644/20000] Train-Loss: 0.5517035057556445 Test-Loss: 0.16767355452920318 Train-f1: 0.4681759672564114 Test-f1: 0.43628155628155635 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6645/20000] Train-Loss: 0.5514914902424686 Test-Loss: 0.16879449333791038 Train-f1: 0.4606978927236118 Test-f1: 0.4088146077440076 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6646/20000] Train-Loss: 0.5525378178508593 Test-Loss: 0.16746721268540443 Train-f1: 0.4601652011723954 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6647/20000] Train-Loss: 0.5514493293152887 Test-Loss: 0.16861153328675285 Train-f1: 0.46208110237579003 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6648/20000] Train-Loss: 0.550480856582548 Test-Loss: 0.16941978385597206 Train-f1: 0.46616973800638634 Test-f1: 0.4249795283327874 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6649/20000] Train-Loss: 0.5515173765448481 Test-Loss: 0.16843824732572146 Train-f1: 0.4617141898563819 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6650/20000] Train-Loss: 0.5535244190803263 Test-Loss: 0.16965741011442417 Train-f1: 0.4615622568462604 Test-f1: 0.41987577639751555 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6651/20000] Train-Loss: 0.5518035050973426 Test-Loss: 0.16771788250863007 Train-f1: 0.4628224649292788 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6652/20000] Train-Loss: 0.5485412482171675 Test-Loss: 0.17305087048369006 Train-f1: 0.4553209746784669 Test-f1: 0.40746844925949405 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [6653/20000] Train-Loss: 0.5482083690367818 Test-Loss: 0.1784334348726158 Train-f1: 0.46291591138732757 Test-f1: 0.4178951396861845 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6654/20000] Train-Loss: 0.5523274529552391 Test-Loss: 0.17166416528548695 Train-f1: 0.4601692688402469 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6655/20000] Train-Loss: 0.5502802803068713 Test-Loss: 0.1676043498138054 Train-f1: 0.463006148272871 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6656/20000] Train-Loss: 0.5493212029948048 Test-Loss: 0.1678020373321637 Train-f1: 0.45638951034728226 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6657/20000] Train-Loss: 0.5505358781891364 Test-Loss: 0.16869314277503641 Train-f1: 0.4711902249467134 Test-f1: 0.43405844155844153 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [6658/20000] Train-Loss: 0.5528540667096051 Test-Loss: 0.16952321380183175 Train-f1: 0.45614850844052873 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6659/20000] Train-Loss: 0.5485035562582825 Test-Loss: 0.17502478776171565 Train-f1: 0.4577353254158645 Test-f1: 0.39702267881372355 Train-acc: 0.7781456953642384 Test-acc: 0.6842105263157895\n",
      "Epoch [6660/20000] Train-Loss: 0.5495128941457551 Test-Loss: 0.16853454649969268 Train-f1: 0.4558139385354706 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6661/20000] Train-Loss: 0.5529523308771148 Test-Loss: 0.17402551896778085 Train-f1: 0.4574820788530466 Test-f1: 0.42822708164603346 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6662/20000] Train-Loss: 0.5512710520283732 Test-Loss: 0.17083365031096218 Train-f1: 0.45885343116235483 Test-f1: 0.397364953886693 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6663/20000] Train-Loss: 0.5524160380515047 Test-Loss: 0.17450590366391627 Train-f1: 0.45949386336483106 Test-f1: 0.4386138804049252 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6664/20000] Train-Loss: 0.5542944389768799 Test-Loss: 0.16706025998354654 Train-f1: 0.46837208150971354 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6665/20000] Train-Loss: 0.552466633733689 Test-Loss: 0.16681277540951317 Train-f1: 0.45822994210090984 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6666/20000] Train-Loss: 0.551388115986342 Test-Loss: 0.16771005484945964 Train-f1: 0.45803306399466415 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6667/20000] Train-Loss: 0.5512751767059095 Test-Loss: 0.1692600713277634 Train-f1: 0.4484656471960745 Test-f1: 0.41987577639751555 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [6668/20000] Train-Loss: 0.5508056940308473 Test-Loss: 0.1669898550684252 Train-f1: 0.4675723619075187 Test-f1: 0.4028833182321555 Train-acc: 0.7913907284768212 Test-acc: 0.7105263157894737\n",
      "Epoch [6669/20000] Train-Loss: 0.5497717620351258 Test-Loss: 0.1724428185721108 Train-f1: 0.47006143557041763 Test-f1: 0.4386138804049252 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [6670/20000] Train-Loss: 0.5488842157867427 Test-Loss: 0.167642106755265 Train-f1: 0.4518994907595572 Test-f1: 0.4028833182321555 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6671/20000] Train-Loss: 0.5511907631790239 Test-Loss: 0.16651536654592855 Train-f1: 0.4515321076472155 Test-f1: 0.42564459930313586 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6672/20000] Train-Loss: 0.5490051495837992 Test-Loss: 0.16797705535383717 Train-f1: 0.46902010000428573 Test-f1: 0.42564459930313586 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6673/20000] Train-Loss: 0.5506428944161446 Test-Loss: 0.16666939256287824 Train-f1: 0.466386060396049 Test-f1: 0.4144537815126051 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6674/20000] Train-Loss: 0.5448373963126462 Test-Loss: 0.17218054100812524 Train-f1: 0.46563965851780986 Test-f1: 0.4419047619047619 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6675/20000] Train-Loss: 0.5524246888717476 Test-Loss: 0.17019951259441937 Train-f1: 0.4563981417891771 Test-f1: 0.4403558383409921 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6676/20000] Train-Loss: 0.5472984654448115 Test-Loss: 0.17011280109463287 Train-f1: 0.4698341824637808 Test-f1: 0.42519841269841263 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6677/20000] Train-Loss: 0.550544432923797 Test-Loss: 0.1668474468014885 Train-f1: 0.46341112786163646 Test-f1: 0.436390977443609 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6678/20000] Train-Loss: 0.5495771384325249 Test-Loss: 0.17024680009831475 Train-f1: 0.4599820788530466 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6679/20000] Train-Loss: 0.5523122404729383 Test-Loss: 0.1693448434939489 Train-f1: 0.4568777491375883 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6680/20000] Train-Loss: 0.5512658470535814 Test-Loss: 0.1698245170952833 Train-f1: 0.4558671663622997 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6681/20000] Train-Loss: 0.5499131644813401 Test-Loss: 0.16698532213437112 Train-f1: 0.47711486309159296 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [6682/20000] Train-Loss: 0.5487776135707924 Test-Loss: 0.16909983429730274 Train-f1: 0.46112689294306614 Test-f1: 0.43017074458722054 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6683/20000] Train-Loss: 0.5445621449918904 Test-Loss: 0.17918171243887948 Train-f1: 0.4598189256081765 Test-f1: 0.4356868717636308 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6684/20000] Train-Loss: 0.5521211795129438 Test-Loss: 0.16647246684663558 Train-f1: 0.45742016065706254 Test-f1: 0.42564459930313586 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6685/20000] Train-Loss: 0.5489989841908549 Test-Loss: 0.1691173173963362 Train-f1: 0.4628449181517773 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6686/20000] Train-Loss: 0.5479086978036866 Test-Loss: 0.17453910250262214 Train-f1: 0.4661363902508566 Test-f1: 0.4386138804049252 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6687/20000] Train-Loss: 0.5477164616947708 Test-Loss: 0.16942334963770014 Train-f1: 0.46491617496461507 Test-f1: 0.4545938580861545 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6688/20000] Train-Loss: 0.5513599311570083 Test-Loss: 0.1668225756803427 Train-f1: 0.4564131996814423 Test-f1: 0.4028833182321555 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6689/20000] Train-Loss: 0.549439170394177 Test-Loss: 0.16957053976169506 Train-f1: 0.4628224649292788 Test-f1: 0.41987577639751555 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6690/20000] Train-Loss: 0.5498037141975868 Test-Loss: 0.1714903179747351 Train-f1: 0.4646526538783215 Test-f1: 0.4447942921400597 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6691/20000] Train-Loss: 0.5500105432801046 Test-Loss: 0.16826099122799432 Train-f1: 0.47084302770549513 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6692/20000] Train-Loss: 0.5502814062487471 Test-Loss: 0.16637253138865946 Train-f1: 0.4611132615243612 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6693/20000] Train-Loss: 0.549083018628658 Test-Loss: 0.17016868474275443 Train-f1: 0.47067862893949847 Test-f1: 0.4403558383409921 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [6694/20000] Train-Loss: 0.5512686086285508 Test-Loss: 0.16757995944566462 Train-f1: 0.46050761835389514 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6695/20000] Train-Loss: 0.5506609919909158 Test-Loss: 0.16888672641609434 Train-f1: 0.45864707033384244 Test-f1: 0.4088146077440076 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6696/20000] Train-Loss: 0.5547215068188257 Test-Loss: 0.1664825831289067 Train-f1: 0.46263997497654047 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6697/20000] Train-Loss: 0.552685533137964 Test-Loss: 0.16637464112461034 Train-f1: 0.4644013941119228 Test-f1: 0.4028833182321555 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6698/20000] Train-Loss: 0.5479431075582907 Test-Loss: 0.16647419997315135 Train-f1: 0.4661335129866554 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6699/20000] Train-Loss: 0.546671668729434 Test-Loss: 0.1724120839498499 Train-f1: 0.4634260002422934 Test-f1: 0.4181515499425948 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [6700/20000] Train-Loss: 0.5479349729034426 Test-Loss: 0.1684587149535 Train-f1: 0.4571537346992617 Test-f1: 0.4593650793650793 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6701/20000] Train-Loss: 0.5528453134520394 Test-Loss: 0.17031724709423438 Train-f1: 0.4578030569175233 Test-f1: 0.45103407755581665 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6702/20000] Train-Loss: 0.5520590799195668 Test-Loss: 0.16608773848131558 Train-f1: 0.4585465708105863 Test-f1: 0.43650793650793657 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6703/20000] Train-Loss: 0.5495868142689788 Test-Loss: 0.16630074692255492 Train-f1: 0.45687894129141127 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6704/20000] Train-Loss: 0.5481470926097292 Test-Loss: 0.1722849444379526 Train-f1: 0.4721624420970675 Test-f1: 0.4415686274509804 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [6705/20000] Train-Loss: 0.5514331341282115 Test-Loss: 0.16862347667728353 Train-f1: 0.46622697952994213 Test-f1: 0.45103407755581665 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6706/20000] Train-Loss: 0.5479011825289833 Test-Loss: 0.166369680469694 Train-f1: 0.4710031125125034 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6707/20000] Train-Loss: 0.5510567039706205 Test-Loss: 0.17089636657563864 Train-f1: 0.46488106875203644 Test-f1: 0.4403558383409921 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6708/20000] Train-Loss: 0.54974069873539 Test-Loss: 0.1705346218841224 Train-f1: 0.46263433697486456 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6709/20000] Train-Loss: 0.5501063699405445 Test-Loss: 0.17125744078677196 Train-f1: 0.4635313639628412 Test-f1: 0.4447942921400597 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6710/20000] Train-Loss: 0.550416466210016 Test-Loss: 0.1732607678336141 Train-f1: 0.4581863904650015 Test-f1: 0.40746844925949405 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6711/20000] Train-Loss: 0.5485101124676711 Test-Loss: 0.16619098179820485 Train-f1: 0.4658613765798904 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6712/20000] Train-Loss: 0.5515583938685198 Test-Loss: 0.16899952682713543 Train-f1: 0.46283922171018943 Test-f1: 0.397364953886693 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [6713/20000] Train-Loss: 0.54619816650538 Test-Loss: 0.185182109814268 Train-f1: 0.46694212057233314 Test-f1: 0.4217161217161217 Train-acc: 0.7913907284768212 Test-acc: 0.7105263157894737\n",
      "Epoch [6714/20000] Train-Loss: 0.5484452549210027 Test-Loss: 0.17000483131416033 Train-f1: 0.45885343116235483 Test-f1: 0.45103407755581665 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6715/20000] Train-Loss: 0.5480534154714307 Test-Loss: 0.16816592914693226 Train-f1: 0.4449820788530466 Test-f1: 0.45103407755581665 Train-acc: 0.7615894039735099 Test-acc: 0.7631578947368421\n",
      "Epoch [6716/20000] Train-Loss: 0.5477850213226348 Test-Loss: 0.17245844858635268 Train-f1: 0.4615539264907194 Test-f1: 0.4178951396861845 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6717/20000] Train-Loss: 0.5514257216962206 Test-Loss: 0.1681913558421393 Train-f1: 0.4603043990510403 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6718/20000] Train-Loss: 0.5488951393593302 Test-Loss: 0.16660036634469286 Train-f1: 0.4662740615809208 Test-f1: 0.436390977443609 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6719/20000] Train-Loss: 0.5496429825656838 Test-Loss: 0.16667532709934668 Train-f1: 0.4642162006646715 Test-f1: 0.4031772027513921 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [6720/20000] Train-Loss: 0.5468989852433546 Test-Loss: 0.1659114003611478 Train-f1: 0.46191198210833556 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6721/20000] Train-Loss: 0.5483603490958765 Test-Loss: 0.17049681417179188 Train-f1: 0.45628041544157816 Test-f1: 0.4088146077440076 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6722/20000] Train-Loss: 0.5507193585136108 Test-Loss: 0.16821124488388625 Train-f1: 0.467767908160615 Test-f1: 0.45103407755581665 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6723/20000] Train-Loss: 0.5426448949264662 Test-Loss: 0.1732165658393683 Train-f1: 0.4511436950146628 Test-f1: 0.43052100840336127 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6724/20000] Train-Loss: 0.547311309540209 Test-Loss: 0.16658571615759524 Train-f1: 0.45913428811681634 Test-f1: 0.42564459930313586 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6725/20000] Train-Loss: 0.5490639075122775 Test-Loss: 0.16645830689060653 Train-f1: 0.4568548970686367 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6726/20000] Train-Loss: 0.5516435116612859 Test-Loss: 0.16601159462195816 Train-f1: 0.45800088264344707 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6727/20000] Train-Loss: 0.5494036961214844 Test-Loss: 0.16688760546108028 Train-f1: 0.4644924648529498 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6728/20000] Train-Loss: 0.5463335729152123 Test-Loss: 0.1676832758907296 Train-f1: 0.4615965581473359 Test-f1: 0.41987577639751555 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [6729/20000] Train-Loss: 0.5495019215366167 Test-Loss: 0.16660835721788098 Train-f1: 0.4673690362708053 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6730/20000] Train-Loss: 0.5464023045555951 Test-Loss: 0.16646844395404672 Train-f1: 0.46245691109710363 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6731/20000] Train-Loss: 0.5466732801041825 Test-Loss: 0.16604759025251553 Train-f1: 0.46616974855129156 Test-f1: 0.42564459930313586 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6732/20000] Train-Loss: 0.5518695733912317 Test-Loss: 0.16670041052277978 Train-f1: 0.46535510611273095 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6733/20000] Train-Loss: 0.5484615341288944 Test-Loss: 0.17665309450141434 Train-f1: 0.46964874551971325 Test-f1: 0.4078136449535269 Train-acc: 0.7913907284768212 Test-acc: 0.6973684210526315\n",
      "Epoch [6734/20000] Train-Loss: 0.5488988045567491 Test-Loss: 0.1716780371356526 Train-f1: 0.4651034727773336 Test-f1: 0.3914438502673797 Train-acc: 0.7880794701986755 Test-acc: 0.6842105263157895\n",
      "Epoch [6735/20000] Train-Loss: 0.5460371643667048 Test-Loss: 0.1686510553793199 Train-f1: 0.46271768438258143 Test-f1: 0.4545938580861545 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6736/20000] Train-Loss: 0.5482909606839739 Test-Loss: 0.1671074053767854 Train-f1: 0.4567950644910308 Test-f1: 0.4028833182321555 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6737/20000] Train-Loss: 0.5473891066504295 Test-Loss: 0.16607585474199998 Train-f1: 0.4681686429512516 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6738/20000] Train-Loss: 0.5498507266319129 Test-Loss: 0.17026217939768892 Train-f1: 0.46335861247307886 Test-f1: 0.4302232667450059 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6739/20000] Train-Loss: 0.5482546449359582 Test-Loss: 0.16618789606068565 Train-f1: 0.46970174264031483 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6740/20000] Train-Loss: 0.5480911254081732 Test-Loss: 0.1681482367866476 Train-f1: 0.46112689294306614 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6741/20000] Train-Loss: 0.547022670506857 Test-Loss: 0.166848218471398 Train-f1: 0.4634954435949295 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6742/20000] Train-Loss: 0.5487837735773426 Test-Loss: 0.1662487639398051 Train-f1: 0.4551749936750042 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6743/20000] Train-Loss: 0.5458513158422715 Test-Loss: 0.16667938126412585 Train-f1: 0.45959243676567973 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6744/20000] Train-Loss: 0.5461901704782935 Test-Loss: 0.1657812114355968 Train-f1: 0.453602948909808 Test-f1: 0.4028833182321555 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [6745/20000] Train-Loss: 0.5477994380928206 Test-Loss: 0.1732133287491632 Train-f1: 0.4593336094176868 Test-f1: 0.4386138804049252 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6746/20000] Train-Loss: 0.5447604077770364 Test-Loss: 0.16828599270368327 Train-f1: 0.4631504550770325 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6747/20000] Train-Loss: 0.5478002268574431 Test-Loss: 0.1673568727708292 Train-f1: 0.4548138880200188 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6748/20000] Train-Loss: 0.5486759983364577 Test-Loss: 0.16676379857028006 Train-f1: 0.4495411908581028 Test-f1: 0.446829268292683 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [6749/20000] Train-Loss: 0.5506335908098672 Test-Loss: 0.16830128701888863 Train-f1: 0.4531973678941819 Test-f1: 0.41987577639751555 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6750/20000] Train-Loss: 0.5486654616241825 Test-Loss: 0.17086335992420526 Train-f1: 0.4646526538783215 Test-f1: 0.43455108359133127 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6751/20000] Train-Loss: 0.5475754552634241 Test-Loss: 0.16775837940145946 Train-f1: 0.4576907169454444 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6752/20000] Train-Loss: 0.5492127596281662 Test-Loss: 0.16669999141572195 Train-f1: 0.4555862371525669 Test-f1: 0.43574229691876754 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6753/20000] Train-Loss: 0.5388325067414999 Test-Loss: 0.17036144317546442 Train-f1: 0.4540167865707433 Test-f1: 0.4401731261066299 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6754/20000] Train-Loss: 0.550957500080845 Test-Loss: 0.17077781527149677 Train-f1: 0.4457643122643035 Test-f1: 0.397364953886693 Train-acc: 0.7649006622516556 Test-acc: 0.6973684210526315\n",
      "Epoch [6755/20000] Train-Loss: 0.5453338460915311 Test-Loss: 0.1670232023484812 Train-f1: 0.47297400582208027 Test-f1: 0.43574229691876754 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [6756/20000] Train-Loss: 0.5453111069644808 Test-Loss: 0.16586530058185298 Train-f1: 0.46053252821306734 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6757/20000] Train-Loss: 0.5456447796916759 Test-Loss: 0.16744608782480236 Train-f1: 0.45829683446776476 Test-f1: 0.43405844155844153 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6758/20000] Train-Loss: 0.5464509169923641 Test-Loss: 0.16731725583455997 Train-f1: 0.4533435340808568 Test-f1: 0.41987577639751555 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6759/20000] Train-Loss: 0.5482507185720408 Test-Loss: 0.16681403921447585 Train-f1: 0.4635886195502197 Test-f1: 0.4124675324675325 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6760/20000] Train-Loss: 0.5476996836621687 Test-Loss: 0.16561095014741092 Train-f1: 0.4655120348165912 Test-f1: 0.4028833182321555 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6761/20000] Train-Loss: 0.5445863945155471 Test-Loss: 0.17025750987356555 Train-f1: 0.46184613380235817 Test-f1: 0.42766798418972324 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6762/20000] Train-Loss: 0.5463434347499043 Test-Loss: 0.16562060176816684 Train-f1: 0.4648923272765848 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6763/20000] Train-Loss: 0.5483446455162863 Test-Loss: 0.1658596984831921 Train-f1: 0.4654971178336833 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6764/20000] Train-Loss: 0.5462922971895893 Test-Loss: 0.16632803947339864 Train-f1: 0.4635313639628412 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6765/20000] Train-Loss: 0.5459496078253017 Test-Loss: 0.16547694706452257 Train-f1: 0.46286304118876825 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6766/20000] Train-Loss: 0.5471487284808787 Test-Loss: 0.16678042651110628 Train-f1: 0.45767103087716166 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6767/20000] Train-Loss: 0.5463610346933382 Test-Loss: 0.1655145473050409 Train-f1: 0.468876337460681 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6768/20000] Train-Loss: 0.5477528105328779 Test-Loss: 0.1664233637928296 Train-f1: 0.4677831897567156 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6769/20000] Train-Loss: 0.5456436820163121 Test-Loss: 0.16593398906260415 Train-f1: 0.46053252821306734 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6770/20000] Train-Loss: 0.5442973394662429 Test-Loss: 0.1659911160230442 Train-f1: 0.4585698690650023 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6771/20000] Train-Loss: 0.548993886623015 Test-Loss: 0.16878913792965586 Train-f1: 0.4724213230571612 Test-f1: 0.4403558383409921 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [6772/20000] Train-Loss: 0.5472200137710791 Test-Loss: 0.16669237425969655 Train-f1: 0.46388695085267234 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6773/20000] Train-Loss: 0.5475653405973162 Test-Loss: 0.16570039159992517 Train-f1: 0.46341383195592056 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6774/20000] Train-Loss: 0.5472630167363928 Test-Loss: 0.16537034983085183 Train-f1: 0.4602064402158847 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6775/20000] Train-Loss: 0.5477952906557417 Test-Loss: 0.16596990378648094 Train-f1: 0.46247508618157535 Test-f1: 0.436390977443609 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6776/20000] Train-Loss: 0.5461917229670802 Test-Loss: 0.1663239532723953 Train-f1: 0.46050761835389514 Test-f1: 0.4028833182321555 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6777/20000] Train-Loss: 0.5477115102617904 Test-Loss: 0.1667283921130321 Train-f1: 0.46050761835389514 Test-f1: 0.4028833182321555 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6778/20000] Train-Loss: 0.5478948502838877 Test-Loss: 0.16816336645630114 Train-f1: 0.46530890379281437 Test-f1: 0.41987577639751555 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6779/20000] Train-Loss: 0.5466330451183334 Test-Loss: 0.1669709672398621 Train-f1: 0.45953881325333434 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6780/20000] Train-Loss: 0.5468282815494511 Test-Loss: 0.17014534182381505 Train-f1: 0.4538725482698318 Test-f1: 0.397364953886693 Train-acc: 0.7715231788079471 Test-acc: 0.6973684210526315\n",
      "Epoch [6781/20000] Train-Loss: 0.5492343670167148 Test-Loss: 0.1730459119860646 Train-f1: 0.46739188240677076 Test-f1: 0.4386138804049252 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6782/20000] Train-Loss: 0.5463010700803689 Test-Loss: 0.16577591016563842 Train-f1: 0.4679709002866573 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6783/20000] Train-Loss: 0.5459409327944017 Test-Loss: 0.17224457497159423 Train-f1: 0.4502860546063003 Test-f1: 0.4386138804049252 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [6784/20000] Train-Loss: 0.5485294401571994 Test-Loss: 0.16956143077284774 Train-f1: 0.46172623398607315 Test-f1: 0.42939958592132504 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6785/20000] Train-Loss: 0.5468691285246887 Test-Loss: 0.1680559496109913 Train-f1: 0.46141413756412086 Test-f1: 0.44069706037669415 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6786/20000] Train-Loss: 0.5456383332913648 Test-Loss: 0.16742644048801542 Train-f1: 0.4698484317392738 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6787/20000] Train-Loss: 0.5446598061017642 Test-Loss: 0.16554963905431494 Train-f1: 0.4555862371525669 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6788/20000] Train-Loss: 0.5466900638015115 Test-Loss: 0.16875863717362333 Train-f1: 0.46263433697486456 Test-f1: 0.4305997407800165 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6789/20000] Train-Loss: 0.5433336438552597 Test-Loss: 0.16539229098209499 Train-f1: 0.46669483109226484 Test-f1: 0.43650793650793657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6790/20000] Train-Loss: 0.5499035525042508 Test-Loss: 0.1696525642235807 Train-f1: 0.4647062109348665 Test-f1: 0.45103407755581665 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6791/20000] Train-Loss: 0.5464030673638967 Test-Loss: 0.1739512689731586 Train-f1: 0.47378089029028114 Test-f1: 0.4386138804049252 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [6792/20000] Train-Loss: 0.5465799012163451 Test-Loss: 0.1664648775841893 Train-f1: 0.4659063831737158 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6793/20000] Train-Loss: 0.546233649854206 Test-Loss: 0.16892480576988728 Train-f1: 0.465506148272871 Test-f1: 0.45103407755581665 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6794/20000] Train-Loss: 0.5462693747412934 Test-Loss: 0.16652534532542038 Train-f1: 0.46694212057233314 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6795/20000] Train-Loss: 0.5462267984788384 Test-Loss: 0.1677346408023981 Train-f1: 0.4614774032114239 Test-f1: 0.43017074458722054 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6796/20000] Train-Loss: 0.5448101117251167 Test-Loss: 0.16585099894291827 Train-f1: 0.46566024398597106 Test-f1: 0.4028833182321555 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6797/20000] Train-Loss: 0.5464593897868453 Test-Loss: 0.17081419313906426 Train-f1: 0.4548138880200188 Test-f1: 0.43426704014939316 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6798/20000] Train-Loss: 0.5469346013268951 Test-Loss: 0.16939843787554132 Train-f1: 0.45947275317155445 Test-f1: 0.44069706037669415 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6799/20000] Train-Loss: 0.5465725608955635 Test-Loss: 0.16775570216514885 Train-f1: 0.4528010491753364 Test-f1: 0.45103407755581665 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6800/20000] Train-Loss: 0.545152931097748 Test-Loss: 0.16511021250779673 Train-f1: 0.45768418060283456 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6801/20000] Train-Loss: 0.5461355278667643 Test-Loss: 0.16561360707907052 Train-f1: 0.45803306399466415 Test-f1: 0.42564459930313586 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6802/20000] Train-Loss: 0.5459370262409746 Test-Loss: 0.1653002324849545 Train-f1: 0.4686585793770931 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6803/20000] Train-Loss: 0.5473038974104665 Test-Loss: 0.1650935009913036 Train-f1: 0.46607608875621975 Test-f1: 0.43650793650793657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6804/20000] Train-Loss: 0.5457486021259477 Test-Loss: 0.16837232946539432 Train-f1: 0.4711902249467134 Test-f1: 0.397364953886693 Train-acc: 0.7947019867549668 Test-acc: 0.6973684210526315\n",
      "Epoch [6805/20000] Train-Loss: 0.5473128296965517 Test-Loss: 0.16691297474101358 Train-f1: 0.4696182903301419 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [6806/20000] Train-Loss: 0.5448789065156635 Test-Loss: 0.16597359451960142 Train-f1: 0.4667763460547805 Test-f1: 0.43650793650793657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6807/20000] Train-Loss: 0.5467088635639464 Test-Loss: 0.16574019705823084 Train-f1: 0.4647062109348665 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6808/20000] Train-Loss: 0.5432808671881172 Test-Loss: 0.1707552138754878 Train-f1: 0.4530849974398361 Test-f1: 0.42413273001508306 Train-acc: 0.7715231788079471 Test-acc: 0.7236842105263158\n",
      "Epoch [6809/20000] Train-Loss: 0.5462998191280986 Test-Loss: 0.16667694932942123 Train-f1: 0.4581391013911661 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [6810/20000] Train-Loss: 0.5457197481972018 Test-Loss: 0.16495680746383604 Train-f1: 0.4605165090432708 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6811/20000] Train-Loss: 0.5473288460917087 Test-Loss: 0.16629604807416584 Train-f1: 0.4501597448156124 Test-f1: 0.42564459930313586 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [6812/20000] Train-Loss: 0.5422912380743009 Test-Loss: 0.17076494813486776 Train-f1: 0.4685585236804749 Test-f1: 0.4386138804049252 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6813/20000] Train-Loss: 0.5481007547373333 Test-Loss: 0.1691644679536962 Train-f1: 0.4632010003036019 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6814/20000] Train-Loss: 0.5462297323321516 Test-Loss: 0.1681251031183112 Train-f1: 0.4552036798462442 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6815/20000] Train-Loss: 0.5473414236676677 Test-Loss: 0.16511723384091115 Train-f1: 0.45920840795921986 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6816/20000] Train-Loss: 0.5484761470681473 Test-Loss: 0.16544044114618567 Train-f1: 0.47560516211032466 Test-f1: 0.44709236709236705 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [6817/20000] Train-Loss: 0.5446875260298131 Test-Loss: 0.1684339520104267 Train-f1: 0.47064033261385846 Test-f1: 0.45103407755581665 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6818/20000] Train-Loss: 0.5444934035116992 Test-Loss: 0.16971105201819275 Train-f1: 0.4733874128441068 Test-f1: 0.43017074458722054 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [6819/20000] Train-Loss: 0.546797366977754 Test-Loss: 0.16751015328525223 Train-f1: 0.4601692688402469 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6820/20000] Train-Loss: 0.5451860854356353 Test-Loss: 0.1650014793299708 Train-f1: 0.4656504550770325 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6821/20000] Train-Loss: 0.5474123697310885 Test-Loss: 0.16513488221217948 Train-f1: 0.4557947704915845 Test-f1: 0.42564459930313586 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6822/20000] Train-Loss: 0.5440071666169101 Test-Loss: 0.1689945585619936 Train-f1: 0.46263997497654047 Test-f1: 0.397364953886693 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [6823/20000] Train-Loss: 0.5461225030605126 Test-Loss: 0.1677675160875014 Train-f1: 0.4551749936750042 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6824/20000] Train-Loss: 0.546168678330315 Test-Loss: 0.1693911746479923 Train-f1: 0.4660464397016827 Test-f1: 0.44069706037669415 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6825/20000] Train-Loss: 0.5486301118649054 Test-Loss: 0.16937881368554022 Train-f1: 0.4666063449978607 Test-f1: 0.41987577639751555 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6826/20000] Train-Loss: 0.5422972938069758 Test-Loss: 0.17293241126837072 Train-f1: 0.47273204567061794 Test-f1: 0.42485053722981736 Train-acc: 0.7980132450331126 Test-acc: 0.7236842105263158\n",
      "Epoch [6827/20000] Train-Loss: 0.5456408266927998 Test-Loss: 0.1651184569082447 Train-f1: 0.44819754350440266 Test-f1: 0.4576190476190477 Train-acc: 0.7649006622516556 Test-acc: 0.7763157894736842\n",
      "Epoch [6828/20000] Train-Loss: 0.5458605045175428 Test-Loss: 0.16654857287788077 Train-f1: 0.45752887780891893 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6829/20000] Train-Loss: 0.544447992506575 Test-Loss: 0.1652074533635457 Train-f1: 0.46815045507703257 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6830/20000] Train-Loss: 0.5413448296074289 Test-Loss: 0.17122866980141419 Train-f1: 0.46263433697486456 Test-f1: 0.4474745605920444 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6831/20000] Train-Loss: 0.546172698419345 Test-Loss: 0.1651924685287573 Train-f1: 0.45901089901089903 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6832/20000] Train-Loss: 0.5407114441017901 Test-Loss: 0.16590510755176457 Train-f1: 0.4617317121700829 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6833/20000] Train-Loss: 0.5452963621865663 Test-Loss: 0.1686042400127493 Train-f1: 0.4670653300358949 Test-f1: 0.397364953886693 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [6834/20000] Train-Loss: 0.5472942052572841 Test-Loss: 0.16679202903814627 Train-f1: 0.46119422274317473 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6835/20000] Train-Loss: 0.5420760942415472 Test-Loss: 0.1650541255484447 Train-f1: 0.46879717719159403 Test-f1: 0.42564459930313586 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6836/20000] Train-Loss: 0.5427258151584299 Test-Loss: 0.16736232189168193 Train-f1: 0.4698484317392738 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6837/20000] Train-Loss: 0.5441399152253886 Test-Loss: 0.16566852896131928 Train-f1: 0.4485983851643816 Test-f1: 0.4576190476190477 Train-acc: 0.7649006622516556 Test-acc: 0.7763157894736842\n",
      "Epoch [6838/20000] Train-Loss: 0.5466549662296943 Test-Loss: 0.17032020638132775 Train-f1: 0.4603641208709778 Test-f1: 0.4447942921400597 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6839/20000] Train-Loss: 0.5450247278685771 Test-Loss: 0.17233806437135224 Train-f1: 0.4681846348206948 Test-f1: 0.42846358707106313 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [6840/20000] Train-Loss: 0.5420698488364535 Test-Loss: 0.16905641187493645 Train-f1: 0.4505653302932421 Test-f1: 0.397364953886693 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [6841/20000] Train-Loss: 0.5442923700846919 Test-Loss: 0.16643546251817692 Train-f1: 0.45498207885304653 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6842/20000] Train-Loss: 0.5456401486992518 Test-Loss: 0.16774561266263308 Train-f1: 0.47064033261385846 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6843/20000] Train-Loss: 0.5495701278561498 Test-Loss: 0.16515059019860176 Train-f1: 0.45755011512112526 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6844/20000] Train-Loss: 0.5465546937600143 Test-Loss: 0.16528877426662741 Train-f1: 0.45962180200222474 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6845/20000] Train-Loss: 0.5434286979811799 Test-Loss: 0.16623827799686872 Train-f1: 0.4598358927460021 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6846/20000] Train-Loss: 0.5453702049438709 Test-Loss: 0.168078776956639 Train-f1: 0.4527656813541209 Test-f1: 0.4305997407800165 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6847/20000] Train-Loss: 0.5431432895527327 Test-Loss: 0.1712694746286974 Train-f1: 0.45822994210090984 Test-f1: 0.4078136449535269 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [6848/20000] Train-Loss: 0.5449717302470034 Test-Loss: 0.16889539670656123 Train-f1: 0.4712727980651973 Test-f1: 0.4419047619047619 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6849/20000] Train-Loss: 0.5440443431021486 Test-Loss: 0.1662419808018188 Train-f1: 0.45364883649420984 Test-f1: 0.43650793650793657 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6850/20000] Train-Loss: 0.5470543919418082 Test-Loss: 0.16626629901742482 Train-f1: 0.45561306483697467 Test-f1: 0.4001638504864311 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [6851/20000] Train-Loss: 0.5427132238332569 Test-Loss: 0.16646170530712973 Train-f1: 0.4572998842856896 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6852/20000] Train-Loss: 0.542444307206482 Test-Loss: 0.16661286952673626 Train-f1: 0.4597746050978337 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6853/20000] Train-Loss: 0.5435907411607235 Test-Loss: 0.16536087034673 Train-f1: 0.4747118085827763 Test-f1: 0.42564459930313586 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [6854/20000] Train-Loss: 0.5465849825216255 Test-Loss: 0.1697471470505553 Train-f1: 0.465419789290757 Test-f1: 0.4403558383409921 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6855/20000] Train-Loss: 0.5432153942149898 Test-Loss: 0.16755138945577705 Train-f1: 0.4782654751099093 Test-f1: 0.43574229691876754 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [6856/20000] Train-Loss: 0.5428754239245345 Test-Loss: 0.16827536596775336 Train-f1: 0.46477248870900406 Test-f1: 0.44069706037669415 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6857/20000] Train-Loss: 0.5472184900273156 Test-Loss: 0.1658535223866195 Train-f1: 0.4597538720157509 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6858/20000] Train-Loss: 0.5432067614263347 Test-Loss: 0.16472903055467647 Train-f1: 0.45733238940762283 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6859/20000] Train-Loss: 0.5420026334228205 Test-Loss: 0.16610981129347493 Train-f1: 0.4659616026565576 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6860/20000] Train-Loss: 0.5449355327775234 Test-Loss: 0.16518762143204144 Train-f1: 0.4523492317257306 Test-f1: 0.44709236709236705 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [6861/20000] Train-Loss: 0.5426399820010738 Test-Loss: 0.16749347726925773 Train-f1: 0.46679334354171437 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6862/20000] Train-Loss: 0.5454557566837277 Test-Loss: 0.1653237939275754 Train-f1: 0.46139580468218966 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6863/20000] Train-Loss: 0.5471180208236752 Test-Loss: 0.16629985890614887 Train-f1: 0.46126921637306084 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6864/20000] Train-Loss: 0.5420376833069862 Test-Loss: 0.1655026534955098 Train-f1: 0.47368859098211724 Test-f1: 0.4028833182321555 Train-acc: 0.7980132450331126 Test-acc: 0.7105263157894737\n",
      "Epoch [6865/20000] Train-Loss: 0.5447391689543654 Test-Loss: 0.16527237212756554 Train-f1: 0.4677831897567156 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6866/20000] Train-Loss: 0.5471980417960348 Test-Loss: 0.1667896678602732 Train-f1: 0.46033922171018943 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6867/20000] Train-Loss: 0.5414202531771245 Test-Loss: 0.1678427556046659 Train-f1: 0.4479357054896623 Test-f1: 0.41987577639751555 Train-acc: 0.7649006622516556 Test-acc: 0.7236842105263158\n",
      "Epoch [6868/20000] Train-Loss: 0.5436380377567149 Test-Loss: 0.1650371296263898 Train-f1: 0.46053252821306734 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6869/20000] Train-Loss: 0.5459961523359746 Test-Loss: 0.16678477001223657 Train-f1: 0.46266926884024695 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6870/20000] Train-Loss: 0.5457875157835721 Test-Loss: 0.16619071922446593 Train-f1: 0.46057606989284006 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6871/20000] Train-Loss: 0.5422189827215328 Test-Loss: 0.1683051305524908 Train-f1: 0.46328193186627536 Test-f1: 0.4403558383409921 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6872/20000] Train-Loss: 0.5442628459823404 Test-Loss: 0.1702857875092368 Train-f1: 0.4632673620140032 Test-f1: 0.42939958592132504 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6873/20000] Train-Loss: 0.5429139195088785 Test-Loss: 0.16642153063025875 Train-f1: 0.46332973101027014 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6874/20000] Train-Loss: 0.5424544752978434 Test-Loss: 0.16515009435772116 Train-f1: 0.4583259631799642 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6875/20000] Train-Loss: 0.5419330086804084 Test-Loss: 0.16455503103082328 Train-f1: 0.4585940855228216 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6876/20000] Train-Loss: 0.5419273309817397 Test-Loss: 0.16652194761059677 Train-f1: 0.4628335536685819 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6877/20000] Train-Loss: 0.5454099951876341 Test-Loss: 0.16678930835358022 Train-f1: 0.458371731412503 Test-f1: 0.41987577639751555 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6878/20000] Train-Loss: 0.543519036374935 Test-Loss: 0.1651392483702818 Train-f1: 0.4568553212911133 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6879/20000] Train-Loss: 0.5419896640366659 Test-Loss: 0.17621637205048096 Train-f1: 0.46493001188056143 Test-f1: 0.41754215933320415 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6880/20000] Train-Loss: 0.5454266642740992 Test-Loss: 0.16639668680453334 Train-f1: 0.4628335536685819 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6881/20000] Train-Loss: 0.5418403712486748 Test-Loss: 0.1742166804771057 Train-f1: 0.4588693627323301 Test-f1: 0.42822708164603346 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [6882/20000] Train-Loss: 0.5446411816662536 Test-Loss: 0.16432747868465786 Train-f1: 0.4609895756863896 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6883/20000] Train-Loss: 0.5485130411740808 Test-Loss: 0.1648221371641096 Train-f1: 0.4602064402158847 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6884/20000] Train-Loss: 0.5419733028773192 Test-Loss: 0.16440412192063972 Train-f1: 0.46306427974390785 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6885/20000] Train-Loss: 0.541494573507893 Test-Loss: 0.16539201128770628 Train-f1: 0.4706691068703842 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6886/20000] Train-Loss: 0.5439997927610646 Test-Loss: 0.17380635337110456 Train-f1: 0.46639475960944254 Test-f1: 0.4386138804049252 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6887/20000] Train-Loss: 0.542251430651624 Test-Loss: 0.16516567637143947 Train-f1: 0.4644119855299979 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6888/20000] Train-Loss: 0.5370485361229562 Test-Loss: 0.17358234240715664 Train-f1: 0.47520359071653767 Test-f1: 0.4372125579001544 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [6889/20000] Train-Loss: 0.5467795196001853 Test-Loss: 0.166351294573613 Train-f1: 0.448851155100764 Test-f1: 0.4305997407800165 Train-acc: 0.7649006622516556 Test-acc: 0.7368421052631579\n",
      "Epoch [6890/20000] Train-Loss: 0.5435723061318407 Test-Loss: 0.16528107872436928 Train-f1: 0.4682976765585461 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6891/20000] Train-Loss: 0.5400714358468021 Test-Loss: 0.16524567167684662 Train-f1: 0.4661269338074729 Test-f1: 0.42564459930313586 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [6892/20000] Train-Loss: 0.5437504774780799 Test-Loss: 0.16726461516761876 Train-f1: 0.46328193186627536 Test-f1: 0.4302232667450059 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6893/20000] Train-Loss: 0.5433444729665774 Test-Loss: 0.1658404824094274 Train-f1: 0.46341112786163646 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6894/20000] Train-Loss: 0.5407257369284252 Test-Loss: 0.16523017148573857 Train-f1: 0.46379746900061763 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6895/20000] Train-Loss: 0.5407611551767566 Test-Loss: 0.16616538628040095 Train-f1: 0.46105629517089214 Test-f1: 0.4088146077440076 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [6896/20000] Train-Loss: 0.542184994420926 Test-Loss: 0.17222388837704253 Train-f1: 0.45783159464788775 Test-f1: 0.3858589405911618 Train-acc: 0.7781456953642384 Test-acc: 0.6710526315789473\n",
      "Epoch [6897/20000] Train-Loss: 0.5422261141717948 Test-Loss: 0.16761619999219052 Train-f1: 0.46101619464640714 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6898/20000] Train-Loss: 0.5416007714269974 Test-Loss: 0.16482384928285435 Train-f1: 0.46937881747582066 Test-f1: 0.43628155628155635 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6899/20000] Train-Loss: 0.5448831562885351 Test-Loss: 0.16646779171551923 Train-f1: 0.47117286771167705 Test-f1: 0.4305997407800165 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [6900/20000] Train-Loss: 0.5390335307664448 Test-Loss: 0.1655256080157301 Train-f1: 0.4647694334650856 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6901/20000] Train-Loss: 0.5444624623632465 Test-Loss: 0.1651747594761286 Train-f1: 0.4555243020437761 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6902/20000] Train-Loss: 0.5386256277825295 Test-Loss: 0.16551706354459167 Train-f1: 0.4697340187908138 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6903/20000] Train-Loss: 0.5439951284325294 Test-Loss: 0.16515751258301042 Train-f1: 0.4534494576524211 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6904/20000] Train-Loss: 0.5466025651934857 Test-Loss: 0.16593623445479852 Train-f1: 0.4572983600697082 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6905/20000] Train-Loss: 0.5443902091269388 Test-Loss: 0.16643486934596288 Train-f1: 0.4735699807438937 Test-f1: 0.44069706037669415 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [6906/20000] Train-Loss: 0.5435627297942406 Test-Loss: 0.16477833289492871 Train-f1: 0.45483238940762283 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6907/20000] Train-Loss: 0.5405413471131326 Test-Loss: 0.17702254382393923 Train-f1: 0.45483238940762283 Test-f1: 0.4385387501666571 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6908/20000] Train-Loss: 0.5435047781279809 Test-Loss: 0.16447846290871945 Train-f1: 0.4684574467831739 Test-f1: 0.43628155628155635 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6909/20000] Train-Loss: 0.5435498542474743 Test-Loss: 0.16461020696365913 Train-f1: 0.46053252821306734 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6910/20000] Train-Loss: 0.542349350848466 Test-Loss: 0.17107996715032614 Train-f1: 0.4633430277054952 Test-f1: 0.42846358707106313 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [6911/20000] Train-Loss: 0.537775209546266 Test-Loss: 0.16734269957852946 Train-f1: 0.46369521476686204 Test-f1: 0.4483006535947712 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6912/20000] Train-Loss: 0.545583524547522 Test-Loss: 0.16562520300242314 Train-f1: 0.46908610901075243 Test-f1: 0.43574229691876754 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6913/20000] Train-Loss: 0.5429663720410932 Test-Loss: 0.16459834099173215 Train-f1: 0.45628041544157816 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [6914/20000] Train-Loss: 0.544005827924326 Test-Loss: 0.16509493774005382 Train-f1: 0.4612437602408261 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6915/20000] Train-Loss: 0.5397506412524452 Test-Loss: 0.16414811794835596 Train-f1: 0.45047627945880764 Test-f1: 0.4028833182321555 Train-acc: 0.7682119205298014 Test-acc: 0.7105263157894737\n",
      "Epoch [6916/20000] Train-Loss: 0.5403175153822661 Test-Loss: 0.16479329021199857 Train-f1: 0.46246218723455373 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6917/20000] Train-Loss: 0.544082216854207 Test-Loss: 0.16548543298997256 Train-f1: 0.46559581935230787 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6918/20000] Train-Loss: 0.5405343149368692 Test-Loss: 0.17063627328383268 Train-f1: 0.4651034727773336 Test-f1: 0.4386138804049252 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6919/20000] Train-Loss: 0.5427157373544879 Test-Loss: 0.16395208829243202 Train-f1: 0.4583765536834129 Test-f1: 0.43628155628155635 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6920/20000] Train-Loss: 0.5434095944314508 Test-Loss: 0.16549628747890205 Train-f1: 0.4563706464547238 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [6921/20000] Train-Loss: 0.5420821768214958 Test-Loss: 0.16531722247458439 Train-f1: 0.45314297324256836 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6922/20000] Train-Loss: 0.5411292552234517 Test-Loss: 0.16521411792316582 Train-f1: 0.46815045507703257 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6923/20000] Train-Loss: 0.5399399186947955 Test-Loss: 0.16967225757187002 Train-f1: 0.46850937335719456 Test-f1: 0.45103407755581665 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6924/20000] Train-Loss: 0.5402594713717752 Test-Loss: 0.16436599090377382 Train-f1: 0.46879717719159403 Test-f1: 0.42564459930313586 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [6925/20000] Train-Loss: 0.5416634206636529 Test-Loss: 0.16910678256125153 Train-f1: 0.4491937129043654 Test-f1: 0.4403558383409921 Train-acc: 0.7682119205298014 Test-acc: 0.75\n",
      "Epoch [6926/20000] Train-Loss: 0.5404624746996176 Test-Loss: 0.16434666814943946 Train-f1: 0.4560808042307876 Test-f1: 0.43650793650793657 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6927/20000] Train-Loss: 0.5426647319795054 Test-Loss: 0.16426723369132204 Train-f1: 0.4597538720157509 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [6928/20000] Train-Loss: 0.5419175602319164 Test-Loss: 0.1638991321871977 Train-f1: 0.4586415473258899 Test-f1: 0.42564459930313586 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [6929/20000] Train-Loss: 0.539413141761376 Test-Loss: 0.16493519327774453 Train-f1: 0.448035081974581 Test-f1: 0.446829268292683 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [6930/20000] Train-Loss: 0.5425455972153157 Test-Loss: 0.16582337592599658 Train-f1: 0.46393956057092306 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6931/20000] Train-Loss: 0.5409738734960582 Test-Loss: 0.16670362634257757 Train-f1: 0.46800614827287107 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [6932/20000] Train-Loss: 0.5409303530900168 Test-Loss: 0.16862310710300482 Train-f1: 0.4605808346953012 Test-f1: 0.46554019457245266 Train-acc: 0.7814569536423841 Test-acc: 0.7894736842105263\n",
      "Epoch [6933/20000] Train-Loss: 0.5422224967763055 Test-Loss: 0.1678154026368979 Train-f1: 0.4555243020437761 Test-f1: 0.43574229691876754 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [6934/20000] Train-Loss: 0.5381827851790117 Test-Loss: 0.1658073685611653 Train-f1: 0.4578030569175233 Test-f1: 0.43628155628155635 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [6935/20000] Train-Loss: 0.5397812223815275 Test-Loss: 0.16520351778261402 Train-f1: 0.45879775410461326 Test-f1: 0.4028833182321555 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [6936/20000] Train-Loss: 0.5452990385519444 Test-Loss: 0.16464560567543704 Train-f1: 0.458360586682636 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [6937/20000] Train-Loss: 0.5418557060014539 Test-Loss: 0.16569400817070146 Train-f1: 0.4653119656141994 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6938/20000] Train-Loss: 0.542233718906276 Test-Loss: 0.16771641343791566 Train-f1: 0.4555243020437761 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6939/20000] Train-Loss: 0.5399194423905502 Test-Loss: 0.16423014018860868 Train-f1: 0.46887146417832337 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6940/20000] Train-Loss: 0.5418316659368172 Test-Loss: 0.16553241041063005 Train-f1: 0.46658856719343955 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [6941/20000] Train-Loss: 0.5410858631445956 Test-Loss: 0.16453800744619823 Train-f1: 0.47084302770549513 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6942/20000] Train-Loss: 0.5375290054264895 Test-Loss: 0.16735328047257167 Train-f1: 0.46183840886727134 Test-f1: 0.397364953886693 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [6943/20000] Train-Loss: 0.5397730268518985 Test-Loss: 0.1680555055726192 Train-f1: 0.4736446591519055 Test-f1: 0.41987577639751555 Train-acc: 0.7980132450331126 Test-acc: 0.7236842105263158\n",
      "Epoch [6944/20000] Train-Loss: 0.539612515016153 Test-Loss: 0.16746778073230448 Train-f1: 0.47006161772596766 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [6945/20000] Train-Loss: 0.5410541517158208 Test-Loss: 0.16650550022194177 Train-f1: 0.45843189419276376 Test-f1: 0.44678185391418274 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6946/20000] Train-Loss: 0.5460820402766356 Test-Loss: 0.166020714104488 Train-f1: 0.4533839124391056 Test-f1: 0.43574229691876754 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [6947/20000] Train-Loss: 0.5354504835055441 Test-Loss: 0.16903159278907168 Train-f1: 0.4734673564854832 Test-f1: 0.4419047619047619 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [6948/20000] Train-Loss: 0.5410633214865735 Test-Loss: 0.16489843975707313 Train-f1: 0.45037374568060484 Test-f1: 0.446829268292683 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [6949/20000] Train-Loss: 0.54194020722105 Test-Loss: 0.16432561817762764 Train-f1: 0.4558275370009575 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [6950/20000] Train-Loss: 0.5362111830396821 Test-Loss: 0.1722347026465634 Train-f1: 0.4665947919869685 Test-f1: 0.3863921499223252 Train-acc: 0.7880794701986755 Test-acc: 0.6710526315789473\n",
      "Epoch [6951/20000] Train-Loss: 0.5431395775352098 Test-Loss: 0.16567076467021014 Train-f1: 0.45155136934629 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [6952/20000] Train-Loss: 0.5378883448409286 Test-Loss: 0.1664858294391751 Train-f1: 0.45669505806986643 Test-f1: 0.397364953886693 Train-acc: 0.7748344370860927 Test-acc: 0.6973684210526315\n",
      "Epoch [6953/20000] Train-Loss: 0.5392670169837619 Test-Loss: 0.16497321733893594 Train-f1: 0.4669072552686856 Test-f1: 0.43574229691876754 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [6954/20000] Train-Loss: 0.5420650406910287 Test-Loss: 0.16615389466598793 Train-f1: 0.4666063449978607 Test-f1: 0.4477903287732132 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6955/20000] Train-Loss: 0.5442196806576376 Test-Loss: 0.16596611746723838 Train-f1: 0.4635713588782181 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6956/20000] Train-Loss: 0.5393096374459556 Test-Loss: 0.16537147062742943 Train-f1: 0.4647438682061248 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6957/20000] Train-Loss: 0.5378043058413219 Test-Loss: 0.16391492497504126 Train-f1: 0.45059996529677926 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "Epoch [6958/20000] Train-Loss: 0.5451799483375158 Test-Loss: 0.16670720839640832 Train-f1: 0.45538918766779873 Test-f1: 0.45103407755581665 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [6959/20000] Train-Loss: 0.5426645875715387 Test-Loss: 0.16450302227380154 Train-f1: 0.4712723478920662 Test-f1: 0.43650793650793657 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [6960/20000] Train-Loss: 0.5412515381850718 Test-Loss: 0.1646124098910688 Train-f1: 0.46302334162433534 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [6961/20000] Train-Loss: 0.5413366274843638 Test-Loss: 0.1642454159559867 Train-f1: 0.4635869782837922 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [6962/20000] Train-Loss: 0.5394803229059358 Test-Loss: 0.17267181949533933 Train-f1: 0.45393518903943963 Test-f1: 0.4386138804049252 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [6963/20000] Train-Loss: 0.5400409447818373 Test-Loss: 0.16678948415449746 Train-f1: 0.4661826560100632 Test-f1: 0.44069706037669415 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6964/20000] Train-Loss: 0.5407746711392247 Test-Loss: 0.1673093695978818 Train-f1: 0.46491617496461507 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [6965/20000] Train-Loss: 0.541678052249839 Test-Loss: 0.16745995376448009 Train-f1: 0.4702809097135782 Test-f1: 0.453416149068323 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [6966/20000] Train-Loss: 0.5400190276024714 Test-Loss: 0.17015904023305592 Train-f1: 0.4611842481499696 Test-f1: 0.4386138804049252 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6967/20000] Train-Loss: 0.5410147295321364 Test-Loss: 0.1668530978570486 Train-f1: 0.47363927319997623 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [6968/20000] Train-Loss: 0.5374010247430894 Test-Loss: 0.16735699012870792 Train-f1: 0.4654971178336833 Test-f1: 0.45103407755581665 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [6969/20000] Train-Loss: 0.5412358184955725 Test-Loss: 0.16394695098421855 Train-f1: 0.4645689714012695 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [6970/20000] Train-Loss: 0.540265757277601 Test-Loss: 0.16363079009812176 Train-f1: 0.4578549814574659 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6971/20000] Train-Loss: 0.5447208606943211 Test-Loss: 0.1650758212708183 Train-f1: 0.4630126457839939 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6972/20000] Train-Loss: 0.5419496092249314 Test-Loss: 0.16496157885663007 Train-f1: 0.4632029819669025 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6973/20000] Train-Loss: 0.5380459469611054 Test-Loss: 0.17198331340864487 Train-f1: 0.4697295536005213 Test-f1: 0.41754215933320415 Train-acc: 0.7947019867549668 Test-acc: 0.7105263157894737\n",
      "Epoch [6974/20000] Train-Loss: 0.5401108092412833 Test-Loss: 0.16634703158100256 Train-f1: 0.4643733829287887 Test-f1: 0.4302232667450059 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [6975/20000] Train-Loss: 0.5406981013134906 Test-Loss: 0.16388323369600666 Train-f1: 0.46154317207593387 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6976/20000] Train-Loss: 0.5402774851792098 Test-Loss: 0.16357327735529292 Train-f1: 0.45839217308898705 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6977/20000] Train-Loss: 0.5387675849747356 Test-Loss: 0.16903649689745615 Train-f1: 0.46240022163350825 Test-f1: 0.44069706037669415 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [6978/20000] Train-Loss: 0.5377161539250253 Test-Loss: 0.16558825578791322 Train-f1: 0.4886855088277221 Test-f1: 0.453416149068323 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [6979/20000] Train-Loss: 0.5414383680220101 Test-Loss: 0.16707669051972718 Train-f1: 0.4581391013911661 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6980/20000] Train-Loss: 0.5369707145884195 Test-Loss: 0.16481893499981334 Train-f1: 0.460175546029802 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6981/20000] Train-Loss: 0.5415496080207207 Test-Loss: 0.16411301553964358 Train-f1: 0.45949531116794545 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [6982/20000] Train-Loss: 0.5405432564130059 Test-Loss: 0.16881947885367063 Train-f1: 0.46053252821306734 Test-f1: 0.4305997407800165 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [6983/20000] Train-Loss: 0.5362329667369051 Test-Loss: 0.16439961227788008 Train-f1: 0.47044003608912643 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [6984/20000] Train-Loss: 0.5366688135712342 Test-Loss: 0.17336358464416673 Train-f1: 0.46650132993763976 Test-f1: 0.4178951396861845 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [6985/20000] Train-Loss: 0.5403651785010899 Test-Loss: 0.16553700214717937 Train-f1: 0.4681846348206948 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6986/20000] Train-Loss: 0.5387033413086759 Test-Loss: 0.16801150351141134 Train-f1: 0.4592939214588805 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6987/20000] Train-Loss: 0.5385579729948673 Test-Loss: 0.16584480012090821 Train-f1: 0.45999061413188597 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [6988/20000] Train-Loss: 0.5340968098000515 Test-Loss: 0.1641059308225205 Train-f1: 0.4670158526016488 Test-f1: 0.4156933389191454 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [6989/20000] Train-Loss: 0.5378250998113026 Test-Loss: 0.16450152193217013 Train-f1: 0.4598189256081765 Test-f1: 0.43628155628155635 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [6990/20000] Train-Loss: 0.5399287523212708 Test-Loss: 0.16394995004872293 Train-f1: 0.4617432479719035 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [6991/20000] Train-Loss: 0.541572188051601 Test-Loss: 0.16323064927598377 Train-f1: 0.458481545447267 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6992/20000] Train-Loss: 0.5407375349765331 Test-Loss: 0.16741694212688826 Train-f1: 0.45742639686446085 Test-f1: 0.45103407755581665 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [6993/20000] Train-Loss: 0.5320196610352687 Test-Loss: 0.16612233670333515 Train-f1: 0.46868416095149357 Test-f1: 0.4593650793650793 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [6994/20000] Train-Loss: 0.535512005163773 Test-Loss: 0.1633671616235943 Train-f1: 0.4740097094299083 Test-f1: 0.43650793650793657 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [6995/20000] Train-Loss: 0.5384832071980608 Test-Loss: 0.1849693946744216 Train-f1: 0.46518434450300716 Test-f1: 0.41128205128205125 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "best score_dx!!\n",
      "Epoch [6996/20000] Train-Loss: 0.543916155472822 Test-Loss: 0.16320672147174142 Train-f1: 0.45465827679011406 Test-f1: 0.42564459930313586 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [6997/20000] Train-Loss: 0.537672101470604 Test-Loss: 0.16863046751165425 Train-f1: 0.4611842481499696 Test-f1: 0.397364953886693 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [6998/20000] Train-Loss: 0.5369106562068495 Test-Loss: 0.1634781251485084 Train-f1: 0.4721279043018173 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [6999/20000] Train-Loss: 0.5381742716679703 Test-Loss: 0.16781133207746568 Train-f1: 0.4641528865261204 Test-f1: 0.4403558383409921 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7000/20000] Train-Loss: 0.5366469130373762 Test-Loss: 0.16732904356628195 Train-f1: 0.46715818063728876 Test-f1: 0.4302232667450059 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7001/20000] Train-Loss: 0.5398392064730455 Test-Loss: 0.16336827319714292 Train-f1: 0.46105629517089214 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7002/20000] Train-Loss: 0.5371927946830524 Test-Loss: 0.16767664596274787 Train-f1: 0.46929959169022306 Test-f1: 0.4419047619047619 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7003/20000] Train-Loss: 0.5369367326110736 Test-Loss: 0.16384534795695 Train-f1: 0.4658708498708499 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7004/20000] Train-Loss: 0.5423063689855784 Test-Loss: 0.16354687435696486 Train-f1: 0.4672761800249132 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7005/20000] Train-Loss: 0.534661331098137 Test-Loss: 0.16683919753852738 Train-f1: 0.47138094715292256 Test-f1: 0.4545938580861545 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7006/20000] Train-Loss: 0.5401396525356748 Test-Loss: 0.16593559069552125 Train-f1: 0.45901796401098827 Test-f1: 0.4305997407800165 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [7007/20000] Train-Loss: 0.5372082690072866 Test-Loss: 0.16314870864977862 Train-f1: 0.4602064402158847 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7008/20000] Train-Loss: 0.5384017471837246 Test-Loss: 0.16335945899504625 Train-f1: 0.4643981339608761 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7009/20000] Train-Loss: 0.5385355431880856 Test-Loss: 0.16459179750098396 Train-f1: 0.46106568920736146 Test-f1: 0.43628155628155635 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7010/20000] Train-Loss: 0.5387505814955065 Test-Loss: 0.163417082242695 Train-f1: 0.45680444446128715 Test-f1: 0.43650793650793657 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [7011/20000] Train-Loss: 0.5359237998231166 Test-Loss: 0.16682341728779126 Train-f1: 0.46931689382290254 Test-f1: 0.4545938580861545 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7012/20000] Train-Loss: 0.5413536435343681 Test-Loss: 0.16314425774673996 Train-f1: 0.4560154913864739 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7013/20000] Train-Loss: 0.5392723730427941 Test-Loss: 0.16576411936807714 Train-f1: 0.4734673564854832 Test-f1: 0.44069706037669415 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7014/20000] Train-Loss: 0.5387672698430434 Test-Loss: 0.1644280742760057 Train-f1: 0.46716437199690597 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7015/20000] Train-Loss: 0.5359289020303217 Test-Loss: 0.17415500785250113 Train-f1: 0.46531405395424646 Test-f1: 0.4177200577200577 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [7016/20000] Train-Loss: 0.5373913649138162 Test-Loss: 0.16323711872253313 Train-f1: 0.4609596023156305 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7017/20000] Train-Loss: 0.5358753346127896 Test-Loss: 0.1715359023791126 Train-f1: 0.4692037475952633 Test-f1: 0.4078136449535269 Train-acc: 0.7913907284768212 Test-acc: 0.6973684210526315\n",
      "Epoch [7018/20000] Train-Loss: 0.5374919223545084 Test-Loss: 0.16462176741693865 Train-f1: 0.4631761255609524 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7019/20000] Train-Loss: 0.5379384440571278 Test-Loss: 0.16859282447492044 Train-f1: 0.46263433697486456 Test-f1: 0.46554019457245266 Train-acc: 0.7847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [7020/20000] Train-Loss: 0.538384701270467 Test-Loss: 0.16384494309749725 Train-f1: 0.46106568920736146 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7021/20000] Train-Loss: 0.5378208366018873 Test-Loss: 0.16577273788105673 Train-f1: 0.46034731791832806 Test-f1: 0.44069706037669415 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7022/20000] Train-Loss: 0.5386187367085581 Test-Loss: 0.17301620787912872 Train-f1: 0.47031159043596793 Test-f1: 0.4386138804049252 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7023/20000] Train-Loss: 0.5385498100622509 Test-Loss: 0.16574032617131168 Train-f1: 0.4553392217101894 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7024/20000] Train-Loss: 0.5384347026256633 Test-Loss: 0.16412435657086413 Train-f1: 0.45609846427493766 Test-f1: 0.4028833182321555 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [7025/20000] Train-Loss: 0.5391156263320864 Test-Loss: 0.16679481963495016 Train-f1: 0.4673701626946764 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7026/20000] Train-Loss: 0.5338882570186495 Test-Loss: 0.1630153899164459 Train-f1: 0.46566024398597106 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7027/20000] Train-Loss: 0.540410952027593 Test-Loss: 0.1645739565697115 Train-f1: 0.46937881747582066 Test-f1: 0.42564459930313586 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7028/20000] Train-Loss: 0.5367153487403931 Test-Loss: 0.16525844383683702 Train-f1: 0.46644372450362004 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7029/20000] Train-Loss: 0.5374784637045806 Test-Loss: 0.16293232202431882 Train-f1: 0.46109587241931493 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7030/20000] Train-Loss: 0.5378060390069864 Test-Loss: 0.1640986578449691 Train-f1: 0.4620029243800377 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7031/20000] Train-Loss: 0.531873322171789 Test-Loss: 0.17751128547139802 Train-f1: 0.468577860612732 Test-f1: 0.4282828282828284 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7032/20000] Train-Loss: 0.5367704139225091 Test-Loss: 0.16334905243309986 Train-f1: 0.45785262024808715 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7033/20000] Train-Loss: 0.5365750441110232 Test-Loss: 0.16486123130067273 Train-f1: 0.4597566391331379 Test-f1: 0.42983458771847644 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7034/20000] Train-Loss: 0.5366954840759324 Test-Loss: 0.16495902146501434 Train-f1: 0.4663642666768443 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7035/20000] Train-Loss: 0.5380982727607708 Test-Loss: 0.16489611380990465 Train-f1: 0.46263997497654047 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7036/20000] Train-Loss: 0.5367438412788623 Test-Loss: 0.16358780983760754 Train-f1: 0.46106568920736146 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7037/20000] Train-Loss: 0.5377650122683545 Test-Loss: 0.1645472812580007 Train-f1: 0.4624909874903175 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7038/20000] Train-Loss: 0.5361738556313315 Test-Loss: 0.1669283095087473 Train-f1: 0.46784403171499944 Test-f1: 0.4403558383409921 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7039/20000] Train-Loss: 0.5409425435424292 Test-Loss: 0.1643727419074555 Train-f1: 0.4692375738159562 Test-f1: 0.42564459930313586 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7040/20000] Train-Loss: 0.5330628889538425 Test-Loss: 0.16690699781611854 Train-f1: 0.46539132847836007 Test-f1: 0.4545938580861545 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7041/20000] Train-Loss: 0.5392610710827919 Test-Loss: 0.16382209864084585 Train-f1: 0.46658856719343955 Test-f1: 0.43650793650793657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7042/20000] Train-Loss: 0.5374493472513433 Test-Loss: 0.1633191018052643 Train-f1: 0.4606303980258648 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7043/20000] Train-Loss: 0.5376247468755876 Test-Loss: 0.16441731815420935 Train-f1: 0.46664777679798936 Test-f1: 0.4144537815126051 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7044/20000] Train-Loss: 0.5383166665562417 Test-Loss: 0.16398481447144805 Train-f1: 0.46397915760937014 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7045/20000] Train-Loss: 0.5377125702469988 Test-Loss: 0.1639443213503047 Train-f1: 0.460780612309364 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7046/20000] Train-Loss: 0.5390289021150411 Test-Loss: 0.16802766213714465 Train-f1: 0.44958396675787976 Test-f1: 0.397364953886693 Train-acc: 0.7682119205298014 Test-acc: 0.6973684210526315\n",
      "Epoch [7047/20000] Train-Loss: 0.5412375620311463 Test-Loss: 0.16641297230426907 Train-f1: 0.4651034727773336 Test-f1: 0.44069706037669415 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7048/20000] Train-Loss: 0.5396471495380932 Test-Loss: 0.16439837294982954 Train-f1: 0.4713778223528843 Test-f1: 0.42564459930313586 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7049/20000] Train-Loss: 0.5358605829299357 Test-Loss: 0.1634515009815656 Train-f1: 0.4676679841897233 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7050/20000] Train-Loss: 0.5359561593853637 Test-Loss: 0.16485578742228105 Train-f1: 0.45692568926225474 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [7051/20000] Train-Loss: 0.5373609227233352 Test-Loss: 0.16389039376320969 Train-f1: 0.4684574467831739 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7052/20000] Train-Loss: 0.5404149634138966 Test-Loss: 0.16299766844068783 Train-f1: 0.4506576824278098 Test-f1: 0.4469172932330827 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [7053/20000] Train-Loss: 0.5377797220266516 Test-Loss: 0.1633630771297479 Train-f1: 0.4612437602408261 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7054/20000] Train-Loss: 0.5358441770385595 Test-Loss: 0.16699150176763874 Train-f1: 0.4655120348165912 Test-f1: 0.4403558383409921 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7055/20000] Train-Loss: 0.5370053986583715 Test-Loss: 0.16331911553641054 Train-f1: 0.4555862371525669 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7056/20000] Train-Loss: 0.5375094555861187 Test-Loss: 0.1648442672530593 Train-f1: 0.4676010110632676 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7057/20000] Train-Loss: 0.5411645516625571 Test-Loss: 0.1635604560498283 Train-f1: 0.4539553320424692 Test-f1: 0.44709236709236705 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [7058/20000] Train-Loss: 0.536470246739789 Test-Loss: 0.16498929086476266 Train-f1: 0.4600166522601311 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7059/20000] Train-Loss: 0.535841048483702 Test-Loss: 0.16269888037161004 Train-f1: 0.4658040057713051 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7060/20000] Train-Loss: 0.5401231218521919 Test-Loss: 0.16393419536434334 Train-f1: 0.4594081763766706 Test-f1: 0.42564459930313586 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [7061/20000] Train-Loss: 0.5347305349601216 Test-Loss: 0.16413401970636196 Train-f1: 0.4695595815777443 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7062/20000] Train-Loss: 0.5377154624383202 Test-Loss: 0.16658665454938507 Train-f1: 0.45785262024808715 Test-f1: 0.45103407755581665 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7063/20000] Train-Loss: 0.539466684126353 Test-Loss: 0.16460883976165866 Train-f1: 0.4502015787900183 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "Epoch [7064/20000] Train-Loss: 0.5349661662893233 Test-Loss: 0.16665216940302752 Train-f1: 0.46779643766388884 Test-f1: 0.44069706037669415 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7065/20000] Train-Loss: 0.5346745553833946 Test-Loss: 0.16322602039320946 Train-f1: 0.4673690362708053 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7066/20000] Train-Loss: 0.5345247619489011 Test-Loss: 0.16539785794590747 Train-f1: 0.4534494576524211 Test-f1: 0.4068599033816426 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [7067/20000] Train-Loss: 0.5388659195375821 Test-Loss: 0.16351369931056178 Train-f1: 0.45622577548473053 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7068/20000] Train-Loss: 0.5342161572498364 Test-Loss: 0.17568871811539655 Train-f1: 0.4683559663032263 Test-f1: 0.43462006503481165 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7069/20000] Train-Loss: 0.5356684589994003 Test-Loss: 0.16421146732009342 Train-f1: 0.4551727281013581 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7070/20000] Train-Loss: 0.5360080348907174 Test-Loss: 0.16306761680452064 Train-f1: 0.45343179760827096 Test-f1: 0.4469172932330827 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [7071/20000] Train-Loss: 0.5373256471464434 Test-Loss: 0.16426400600163865 Train-f1: 0.4663285667600441 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7072/20000] Train-Loss: 0.5363924538815286 Test-Loss: 0.1649536777884984 Train-f1: 0.4681963645673323 Test-f1: 0.46554019457245266 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [7073/20000] Train-Loss: 0.5335627689498602 Test-Loss: 0.16260071807517396 Train-f1: 0.45561306483697467 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7074/20000] Train-Loss: 0.5370336827531696 Test-Loss: 0.16306476388360774 Train-f1: 0.4735169175591608 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7075/20000] Train-Loss: 0.5371271827572074 Test-Loss: 0.16385188153612285 Train-f1: 0.4516724266398122 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [7076/20000] Train-Loss: 0.5357440439306923 Test-Loss: 0.16405453253509386 Train-f1: 0.4615200978290215 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7077/20000] Train-Loss: 0.5365478116965303 Test-Loss: 0.16628231813227973 Train-f1: 0.4638343499597354 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7078/20000] Train-Loss: 0.5324518232804791 Test-Loss: 0.1645062377452308 Train-f1: 0.47596661127347045 Test-f1: 0.43574229691876754 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [7079/20000] Train-Loss: 0.5400976863109662 Test-Loss: 0.16351743564536675 Train-f1: 0.4635313639628412 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7080/20000] Train-Loss: 0.536789891360563 Test-Loss: 0.1636641988604401 Train-f1: 0.46240022163350825 Test-f1: 0.43628155628155635 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7081/20000] Train-Loss: 0.5350515971958982 Test-Loss: 0.1624491435246787 Train-f1: 0.45498207885304653 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7082/20000] Train-Loss: 0.5355154449030203 Test-Loss: 0.1679409120892954 Train-f1: 0.47066645888525266 Test-f1: 0.42939958592132504 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7083/20000] Train-Loss: 0.5353290776827199 Test-Loss: 0.16296008391902253 Train-f1: 0.4631620435954851 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7084/20000] Train-Loss: 0.5398141027345423 Test-Loss: 0.16250322696513833 Train-f1: 0.46393956057092306 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7085/20000] Train-Loss: 0.5327717058717738 Test-Loss: 0.1676059148045503 Train-f1: 0.4514961646491956 Test-f1: 0.42939958592132504 Train-acc: 0.7715231788079471 Test-acc: 0.7368421052631579\n",
      "Epoch [7086/20000] Train-Loss: 0.5372944313050673 Test-Loss: 0.16350846516489592 Train-f1: 0.4615813212717422 Test-f1: 0.4028833182321555 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [7087/20000] Train-Loss: 0.5374321147157531 Test-Loss: 0.16374435399502713 Train-f1: 0.4600166522601311 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7088/20000] Train-Loss: 0.5376574319020572 Test-Loss: 0.1636188336074385 Train-f1: 0.46044858583811604 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7089/20000] Train-Loss: 0.5320257956463478 Test-Loss: 0.17155092281855164 Train-f1: 0.4789326961369972 Test-f1: 0.43453699245706956 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [7090/20000] Train-Loss: 0.5400142954651196 Test-Loss: 0.1637322372757522 Train-f1: 0.4634112554112554 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7091/20000] Train-Loss: 0.5362201076948289 Test-Loss: 0.16550418090825567 Train-f1: 0.453493896609678 Test-f1: 0.4088146077440076 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [7092/20000] Train-Loss: 0.5375270848022807 Test-Loss: 0.16900871388970207 Train-f1: 0.4597538720157509 Test-f1: 0.42474323062558367 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7093/20000] Train-Loss: 0.5403165138580489 Test-Loss: 0.1626862876177763 Train-f1: 0.45874076888973325 Test-f1: 0.43650793650793657 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [7094/20000] Train-Loss: 0.5349373978480626 Test-Loss: 0.16448889494696992 Train-f1: 0.4691649413388544 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7095/20000] Train-Loss: 0.5369261737548934 Test-Loss: 0.16353881624828395 Train-f1: 0.4586415473258899 Test-f1: 0.43628155628155635 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [7096/20000] Train-Loss: 0.5320437830926376 Test-Loss: 0.16497810960536866 Train-f1: 0.46571672357404215 Test-f1: 0.41987577639751555 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7097/20000] Train-Loss: 0.5340352640723841 Test-Loss: 0.1626764569403764 Train-f1: 0.4734888933240544 Test-f1: 0.43650793650793657 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7098/20000] Train-Loss: 0.536022712609619 Test-Loss: 0.16449787869818197 Train-f1: 0.46251665226013117 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7099/20000] Train-Loss: 0.5331406964768657 Test-Loss: 0.16217179299361728 Train-f1: 0.47207503152263247 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7100/20000] Train-Loss: 0.5355595450419877 Test-Loss: 0.16561867503046396 Train-f1: 0.46549767569045664 Test-f1: 0.44069706037669415 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7101/20000] Train-Loss: 0.5385887779766443 Test-Loss: 0.1673391990851529 Train-f1: 0.45539937536376873 Test-f1: 0.4088146077440076 Train-acc: 0.7748344370860927 Test-acc: 0.7105263157894737\n",
      "Epoch [7102/20000] Train-Loss: 0.5352488614386245 Test-Loss: 0.16432415308101364 Train-f1: 0.46015550292685103 Test-f1: 0.4028833182321555 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [7103/20000] Train-Loss: 0.5343042418528947 Test-Loss: 0.16442802698626724 Train-f1: 0.47175024358991474 Test-f1: 0.397364953886693 Train-acc: 0.7947019867549668 Test-acc: 0.6973684210526315\n",
      "Epoch [7104/20000] Train-Loss: 0.5381936932777345 Test-Loss: 0.16395991223808307 Train-f1: 0.4631620435954851 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7105/20000] Train-Loss: 0.5379096915527558 Test-Loss: 0.16305903185234163 Train-f1: 0.46756039820051126 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7106/20000] Train-Loss: 0.5335790549114087 Test-Loss: 0.16353060159232988 Train-f1: 0.4653069369789682 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7107/20000] Train-Loss: 0.537137699099808 Test-Loss: 0.16294298293644424 Train-f1: 0.4629819524200164 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7108/20000] Train-Loss: 0.5350654575986213 Test-Loss: 0.16620013450280294 Train-f1: 0.47175024358991474 Test-f1: 0.4406970603766943 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7109/20000] Train-Loss: 0.532727521190431 Test-Loss: 0.16534033711897206 Train-f1: 0.4778859009506767 Test-f1: 0.4545938580861545 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [7110/20000] Train-Loss: 0.5360768673450625 Test-Loss: 0.1655436154035044 Train-f1: 0.4686585793770931 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7111/20000] Train-Loss: 0.5330247109398684 Test-Loss: 0.16665868610888418 Train-f1: 0.4679815898833728 Test-f1: 0.46554019457245266 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [7112/20000] Train-Loss: 0.5384593515500987 Test-Loss: 0.16615032456206774 Train-f1: 0.4586415473258899 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7113/20000] Train-Loss: 0.5366230019254917 Test-Loss: 0.16233305692128625 Train-f1: 0.46613308877680043 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7114/20000] Train-Loss: 0.5350042541099306 Test-Loss: 0.16437457532288224 Train-f1: 0.470247860536403 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7115/20000] Train-Loss: 0.5308503933527097 Test-Loss: 0.16327206943381878 Train-f1: 0.4675723619075187 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7116/20000] Train-Loss: 0.5337345466788691 Test-Loss: 0.1652487529520206 Train-f1: 0.47050614827287107 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7117/20000] Train-Loss: 0.5363910334320383 Test-Loss: 0.1647926174732978 Train-f1: 0.46622697952994213 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7118/20000] Train-Loss: 0.5365139851615218 Test-Loss: 0.16563988872884142 Train-f1: 0.46815045507703257 Test-f1: 0.4406970603766943 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7119/20000] Train-Loss: 0.533251802355636 Test-Loss: 0.1667809870260728 Train-f1: 0.47220522036076373 Test-f1: 0.43574229691876754 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7120/20000] Train-Loss: 0.5338643967800346 Test-Loss: 0.16313470005725755 Train-f1: 0.4527267763292608 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [7121/20000] Train-Loss: 0.5321623891785384 Test-Loss: 0.1630774390463536 Train-f1: 0.46141413756412086 Test-f1: 0.4028833182321555 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [7122/20000] Train-Loss: 0.5339451103434202 Test-Loss: 0.16866942615610536 Train-f1: 0.46019105093408663 Test-f1: 0.43426704014939316 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7123/20000] Train-Loss: 0.5363857483456591 Test-Loss: 0.16601184798312837 Train-f1: 0.4703799696556453 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7124/20000] Train-Loss: 0.5365602712710095 Test-Loss: 0.16216087225174777 Train-f1: 0.46129055394840657 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7125/20000] Train-Loss: 0.537150058617643 Test-Loss: 0.16366484166027148 Train-f1: 0.4605808346953012 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7126/20000] Train-Loss: 0.5358217075468108 Test-Loss: 0.16667823292845485 Train-f1: 0.4634122067895297 Test-f1: 0.4406970603766943 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7127/20000] Train-Loss: 0.5372086472987347 Test-Loss: 0.16573328312131308 Train-f1: 0.45452199038384666 Test-f1: 0.43574229691876754 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [7128/20000] Train-Loss: 0.5356129857046359 Test-Loss: 0.16341327653843055 Train-f1: 0.4683559663032263 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7129/20000] Train-Loss: 0.5365438425148046 Test-Loss: 0.16440060086599173 Train-f1: 0.45768845137179837 Test-f1: 0.4088146077440076 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [7130/20000] Train-Loss: 0.5325199267844908 Test-Loss: 0.16354560554291867 Train-f1: 0.4695595815777443 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7131/20000] Train-Loss: 0.5358067012403614 Test-Loss: 0.16433669400208387 Train-f1: 0.46382848818797895 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7132/20000] Train-Loss: 0.5350109155483808 Test-Loss: 0.16400084810535948 Train-f1: 0.46116558311990746 Test-f1: 0.4028833182321555 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [7133/20000] Train-Loss: 0.5320874846573398 Test-Loss: 0.164598023631282 Train-f1: 0.461218834780031 Test-f1: 0.4457303936373703 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7134/20000] Train-Loss: 0.5347642719210169 Test-Loss: 0.16289230479824754 Train-f1: 0.46073305576411167 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7135/20000] Train-Loss: 0.5338400740746332 Test-Loss: 0.1667257882906955 Train-f1: 0.4646674104566614 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7136/20000] Train-Loss: 0.5311907612331476 Test-Loss: 0.17336806075518094 Train-f1: 0.46855350742447516 Test-f1: 0.4282828282828284 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7137/20000] Train-Loss: 0.5397563860436124 Test-Loss: 0.16378052549514543 Train-f1: 0.46858234932769094 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7138/20000] Train-Loss: 0.5327832509572593 Test-Loss: 0.163100994069959 Train-f1: 0.4526812416145997 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [7139/20000] Train-Loss: 0.5361779771500198 Test-Loss: 0.16258220550002253 Train-f1: 0.4631445207155309 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7140/20000] Train-Loss: 0.5334887297166767 Test-Loss: 0.17168771425177395 Train-f1: 0.46344292998266584 Test-f1: 0.4282828282828284 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [7141/20000] Train-Loss: 0.534114764667071 Test-Loss: 0.16390538938327281 Train-f1: 0.4709990549990549 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7142/20000] Train-Loss: 0.5329523613605799 Test-Loss: 0.1620562842137682 Train-f1: 0.460780612309364 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7143/20000] Train-Loss: 0.5334239868962092 Test-Loss: 0.16184384339432742 Train-f1: 0.460780612309364 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7144/20000] Train-Loss: 0.5361070323403616 Test-Loss: 0.16306757475389447 Train-f1: 0.46220785974581224 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7145/20000] Train-Loss: 0.5351031717932561 Test-Loss: 0.16255170702186722 Train-f1: 0.46286304118876825 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7146/20000] Train-Loss: 0.5337733128034073 Test-Loss: 0.17012729133202975 Train-f1: 0.46634960022056793 Test-f1: 0.4178951396861845 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [7147/20000] Train-Loss: 0.5367960804496176 Test-Loss: 0.16772667558269153 Train-f1: 0.45785262024808715 Test-f1: 0.42939958592132504 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [7148/20000] Train-Loss: 0.5324564692345971 Test-Loss: 0.16263700329079772 Train-f1: 0.4679709002866573 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7149/20000] Train-Loss: 0.5350581930196171 Test-Loss: 0.16408694004749683 Train-f1: 0.4654971178336833 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7150/20000] Train-Loss: 0.5320864244070225 Test-Loss: 0.16221672134061405 Train-f1: 0.47557954610819114 Test-f1: 0.4576190476190477 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [7151/20000] Train-Loss: 0.533471204856965 Test-Loss: 0.16223184103043736 Train-f1: 0.4602064402158847 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7152/20000] Train-Loss: 0.5339539573045571 Test-Loss: 0.1659016741885581 Train-f1: 0.4632956201316783 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7153/20000] Train-Loss: 0.5327419356804377 Test-Loss: 0.16219420623692504 Train-f1: 0.45364883649420984 Test-f1: 0.4028833182321555 Train-acc: 0.7715231788079471 Test-acc: 0.7105263157894737\n",
      "Epoch [7154/20000] Train-Loss: 0.535054464796653 Test-Loss: 0.1693957695006799 Train-f1: 0.45769606536992624 Test-f1: 0.42846358707106313 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [7155/20000] Train-Loss: 0.5316206734552573 Test-Loss: 0.16196001713258493 Train-f1: 0.47272689334661155 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7156/20000] Train-Loss: 0.5341272664566808 Test-Loss: 0.1647510324573614 Train-f1: 0.47031159043596793 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7157/20000] Train-Loss: 0.5350164049773743 Test-Loss: 0.16385266751203484 Train-f1: 0.45539937536376873 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [7158/20000] Train-Loss: 0.5312942864059398 Test-Loss: 0.16258664699752137 Train-f1: 0.4559925407033735 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7159/20000] Train-Loss: 0.5300917434862009 Test-Loss: 0.1677712896593286 Train-f1: 0.4693373301531715 Test-f1: 0.4545938580861545 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7160/20000] Train-Loss: 0.5309803974160348 Test-Loss: 0.16210209841283035 Train-f1: 0.4678120078120078 Test-f1: 0.4156933389191454 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7161/20000] Train-Loss: 0.5329319410912705 Test-Loss: 0.16297468525751505 Train-f1: 0.45938887820836366 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7162/20000] Train-Loss: 0.5325931554036356 Test-Loss: 0.16623142990088663 Train-f1: 0.4555243020437761 Test-f1: 0.44069706037669415 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [7163/20000] Train-Loss: 0.5371696899054297 Test-Loss: 0.1646462327205153 Train-f1: 0.4638666189704634 Test-f1: 0.4406970603766943 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7164/20000] Train-Loss: 0.5342352702883209 Test-Loss: 0.16176243183518452 Train-f1: 0.468471466942883 Test-f1: 0.41766233766233773 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7165/20000] Train-Loss: 0.5349428854005865 Test-Loss: 0.16523761058712608 Train-f1: 0.4693820736441568 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7166/20000] Train-Loss: 0.5293110105564234 Test-Loss: 0.16477081743900698 Train-f1: 0.47031159043596793 Test-f1: 0.453416149068323 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7167/20000] Train-Loss: 0.5308351987455937 Test-Loss: 0.16235113954050254 Train-f1: 0.4586399108138238 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [7168/20000] Train-Loss: 0.5324754902849709 Test-Loss: 0.1633843018975189 Train-f1: 0.4636463343686518 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7169/20000] Train-Loss: 0.529336499110847 Test-Loss: 0.16466123441365452 Train-f1: 0.47595466857882657 Test-f1: 0.41741553465691406 Train-acc: 0.8013245033112583 Test-acc: 0.7236842105263158\n",
      "Epoch [7170/20000] Train-Loss: 0.5360667217530914 Test-Loss: 0.1622073919844544 Train-f1: 0.45715341237543594 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7171/20000] Train-Loss: 0.5344768937962858 Test-Loss: 0.16290886524016018 Train-f1: 0.4761237102967503 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7172/20000] Train-Loss: 0.5345547422444694 Test-Loss: 0.1630502447245957 Train-f1: 0.46286304118876825 Test-f1: 0.43628155628155635 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7173/20000] Train-Loss: 0.5352380878244503 Test-Loss: 0.16813321523017768 Train-f1: 0.45687894129141127 Test-f1: 0.43426704014939316 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [7174/20000] Train-Loss: 0.530674172751027 Test-Loss: 0.16647475455095084 Train-f1: 0.45544786476463495 Test-f1: 0.43574229691876754 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [7175/20000] Train-Loss: 0.5341725396000874 Test-Loss: 0.1646133784911673 Train-f1: 0.46569368916510534 Test-f1: 0.4406970603766943 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7176/20000] Train-Loss: 0.5313293145658369 Test-Loss: 0.16761123947282386 Train-f1: 0.47096735648548327 Test-f1: 0.4403558383409921 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7177/20000] Train-Loss: 0.5326991377528354 Test-Loss: 0.1622428851849655 Train-f1: 0.46742615292726236 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7178/20000] Train-Loss: 0.532911151824544 Test-Loss: 0.1637052792447132 Train-f1: 0.4661796868165514 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7179/20000] Train-Loss: 0.5324003447491715 Test-Loss: 0.16172964112177005 Train-f1: 0.47207528889813694 Test-f1: 0.42564459930313586 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7180/20000] Train-Loss: 0.5354129184866858 Test-Loss: 0.1627455786095545 Train-f1: 0.47273583286021026 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7181/20000] Train-Loss: 0.532564041966298 Test-Loss: 0.1676526927803953 Train-f1: 0.46739188240677076 Test-f1: 0.4419047619047619 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7182/20000] Train-Loss: 0.5343809647405903 Test-Loss: 0.16342329691663998 Train-f1: 0.46129055394840657 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7183/20000] Train-Loss: 0.5325206044885853 Test-Loss: 0.16447257307032911 Train-f1: 0.46842536345641933 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7184/20000] Train-Loss: 0.5328611045859617 Test-Loss: 0.16156017565864395 Train-f1: 0.47541480768669986 Test-f1: 0.44709236709236705 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7185/20000] Train-Loss: 0.5319960630210175 Test-Loss: 0.16406911438053678 Train-f1: 0.46756039820051126 Test-f1: 0.4457303936373703 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7186/20000] Train-Loss: 0.5328320041882719 Test-Loss: 0.1629518486071516 Train-f1: 0.4540479890830523 Test-f1: 0.43628155628155635 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [7187/20000] Train-Loss: 0.5339218815182073 Test-Loss: 0.16214759395713915 Train-f1: 0.45483238940762283 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7188/20000] Train-Loss: 0.5335200810186173 Test-Loss: 0.1726322411973778 Train-f1: 0.4586833855799373 Test-f1: 0.41795397584871274 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [7189/20000] Train-Loss: 0.5330336827810008 Test-Loss: 0.16443078458584484 Train-f1: 0.45921176118528706 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7190/20000] Train-Loss: 0.5297559365152336 Test-Loss: 0.16273754100609866 Train-f1: 0.45845960231563054 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7191/20000] Train-Loss: 0.533804247189449 Test-Loss: 0.16456399673633812 Train-f1: 0.45267203427662395 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [7192/20000] Train-Loss: 0.529906557078309 Test-Loss: 0.16602281882048622 Train-f1: 0.47493966650645547 Test-f1: 0.45103407755581665 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7193/20000] Train-Loss: 0.533631775449041 Test-Loss: 0.16151857713756781 Train-f1: 0.46291591138732757 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7194/20000] Train-Loss: 0.5287283211785416 Test-Loss: 0.16138499137418047 Train-f1: 0.46422813709470423 Test-f1: 0.4144537815126051 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [7195/20000] Train-Loss: 0.5350671035765887 Test-Loss: 0.16319459085513752 Train-f1: 0.4744561234464994 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7196/20000] Train-Loss: 0.5308664205059314 Test-Loss: 0.16324945384547346 Train-f1: 0.4638141144481523 Test-f1: 0.4469172932330827 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7197/20000] Train-Loss: 0.5321425007913103 Test-Loss: 0.16389467734387628 Train-f1: 0.45816688231385727 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7198/20000] Train-Loss: 0.5312967600005609 Test-Loss: 0.16281140084557397 Train-f1: 0.46344292998266584 Test-f1: 0.4144537815126051 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [7199/20000] Train-Loss: 0.533396355825393 Test-Loss: 0.16122514959281328 Train-f1: 0.4608470252975339 Test-f1: 0.43628155628155635 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7200/20000] Train-Loss: 0.5318873306744155 Test-Loss: 0.1637917695581579 Train-f1: 0.4641528865261204 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7201/20000] Train-Loss: 0.5331594858373722 Test-Loss: 0.16668575528907545 Train-f1: 0.46571672357404215 Test-f1: 0.4545938580861545 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7202/20000] Train-Loss: 0.5298016804334388 Test-Loss: 0.1650157527106352 Train-f1: 0.47253309398095916 Test-f1: 0.4088146077440076 Train-acc: 0.7947019867549668 Test-acc: 0.7105263157894737\n",
      "Epoch [7203/20000] Train-Loss: 0.5343680488107914 Test-Loss: 0.16468325826894484 Train-f1: 0.46566024398597106 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7204/20000] Train-Loss: 0.5304275973704207 Test-Loss: 0.16322965891547825 Train-f1: 0.46388695085267234 Test-f1: 0.4144537815126051 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [7205/20000] Train-Loss: 0.5308810122405425 Test-Loss: 0.16435724177747696 Train-f1: 0.46531405395424646 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7206/20000] Train-Loss: 0.5308502005764323 Test-Loss: 0.1612127772833751 Train-f1: 0.4698181818181818 Test-f1: 0.42564459930313586 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7207/20000] Train-Loss: 0.5308922149502104 Test-Loss: 0.16538111885159548 Train-f1: 0.4651034727773336 Test-f1: 0.4457303936373703 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7208/20000] Train-Loss: 0.5322039345030476 Test-Loss: 0.16307781433479945 Train-f1: 0.4686567339175102 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7209/20000] Train-Loss: 0.5312490566025125 Test-Loss: 0.16248417056492456 Train-f1: 0.4615622568462604 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7210/20000] Train-Loss: 0.5320129541861981 Test-Loss: 0.16331694547876396 Train-f1: 0.4679613915376316 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7211/20000] Train-Loss: 0.5273030674183217 Test-Loss: 0.17377067618403588 Train-f1: 0.46629207289323055 Test-f1: 0.4375069149917021 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7212/20000] Train-Loss: 0.5282367094196392 Test-Loss: 0.16152434666099272 Train-f1: 0.46707327243091623 Test-f1: 0.4059136212624585 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "Epoch [7213/20000] Train-Loss: 0.5361487464045694 Test-Loss: 0.16161974097024892 Train-f1: 0.4571997508516805 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7214/20000] Train-Loss: 0.5310580470000994 Test-Loss: 0.16163681062485516 Train-f1: 0.46369521476686204 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7215/20000] Train-Loss: 0.532355667802972 Test-Loss: 0.16360462972659962 Train-f1: 0.4581391013911661 Test-f1: 0.397364953886693 Train-acc: 0.7781456953642384 Test-acc: 0.6973684210526315\n",
      "Epoch [7216/20000] Train-Loss: 0.533631655814557 Test-Loss: 0.16686429310484743 Train-f1: 0.4706504550770326 Test-f1: 0.41987577639751555 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7217/20000] Train-Loss: 0.5293518545835109 Test-Loss: 0.16299047413581927 Train-f1: 0.4651470233332063 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7218/20000] Train-Loss: 0.5346781750884296 Test-Loss: 0.16129623867545193 Train-f1: 0.46116182152131235 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7219/20000] Train-Loss: 0.5316356400046208 Test-Loss: 0.16467786798311415 Train-f1: 0.4663743671902084 Test-f1: 0.45564935064935064 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7220/20000] Train-Loss: 0.5324427522283217 Test-Loss: 0.16317657612374312 Train-f1: 0.4619203112003749 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7221/20000] Train-Loss: 0.5287136352607014 Test-Loss: 0.16265562703019912 Train-f1: 0.45624179861779374 Test-f1: 0.4576190476190477 Train-acc: 0.7748344370860927 Test-acc: 0.7763157894736842\n",
      "Epoch [7222/20000] Train-Loss: 0.529578090820935 Test-Loss: 0.16235759649507556 Train-f1: 0.46858234932769094 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7223/20000] Train-Loss: 0.5331970184420578 Test-Loss: 0.1712981950682339 Train-f1: 0.4626943131270308 Test-f1: 0.4177200577200577 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [7224/20000] Train-Loss: 0.5330105674728745 Test-Loss: 0.1729368356331742 Train-f1: 0.46141413756412086 Test-f1: 0.4177200577200577 Train-acc: 0.7814569536423841 Test-acc: 0.7105263157894737\n",
      "Epoch [7225/20000] Train-Loss: 0.531626652066621 Test-Loss: 0.16136048705821954 Train-f1: 0.46386792229289986 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7226/20000] Train-Loss: 0.5340958438903799 Test-Loss: 0.16099016165837943 Train-f1: 0.46344292998266584 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7227/20000] Train-Loss: 0.5354121593884721 Test-Loss: 0.16263943863816505 Train-f1: 0.4576907169454444 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7228/20000] Train-Loss: 0.5324857456876022 Test-Loss: 0.1636302708749685 Train-f1: 0.46363588686901336 Test-f1: 0.397364953886693 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [7229/20000] Train-Loss: 0.5291842437291727 Test-Loss: 0.1630359734577277 Train-f1: 0.4555243020437761 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7230/20000] Train-Loss: 0.5316305845446655 Test-Loss: 0.16751600885821563 Train-f1: 0.4680049611950876 Test-f1: 0.4305997407800165 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7231/20000] Train-Loss: 0.5328764092146708 Test-Loss: 0.1609939795806194 Train-f1: 0.46497172033322265 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7232/20000] Train-Loss: 0.5289765534412052 Test-Loss: 0.172805137815825 Train-f1: 0.46897540477073285 Test-f1: 0.44621212121212117 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7233/20000] Train-Loss: 0.5343085295535117 Test-Loss: 0.16217181821750767 Train-f1: 0.45609846427493766 Test-f1: 0.43628155628155635 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [7234/20000] Train-Loss: 0.5320037838044472 Test-Loss: 0.1638291870214196 Train-f1: 0.4651470233332063 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7235/20000] Train-Loss: 0.5284406923831618 Test-Loss: 0.16268963442516152 Train-f1: 0.47468966798310425 Test-f1: 0.4028833182321555 Train-acc: 0.7980132450331126 Test-acc: 0.7105263157894737\n",
      "Epoch [7236/20000] Train-Loss: 0.5318360368435305 Test-Loss: 0.16161720123637544 Train-f1: 0.4554692370738268 Test-f1: 0.4124675324675325 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7237/20000] Train-Loss: 0.5315219349284395 Test-Loss: 0.16116845577866448 Train-f1: 0.46593982137326284 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7238/20000] Train-Loss: 0.5350125861972188 Test-Loss: 0.16206965664392595 Train-f1: 0.4646674104566614 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7239/20000] Train-Loss: 0.530926520629976 Test-Loss: 0.16195768615720296 Train-f1: 0.46344292998266584 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7240/20000] Train-Loss: 0.5288550578496464 Test-Loss: 0.16115701627231335 Train-f1: 0.4608470252975339 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7241/20000] Train-Loss: 0.5317598845585907 Test-Loss: 0.16254567246509571 Train-f1: 0.4602041746422387 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7242/20000] Train-Loss: 0.5298605181741053 Test-Loss: 0.1608927795486243 Train-f1: 0.45839217308898705 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7243/20000] Train-Loss: 0.5296990166153486 Test-Loss: 0.1637745935852649 Train-f1: 0.47236714827102233 Test-f1: 0.46554019457245266 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [7244/20000] Train-Loss: 0.5316935250196184 Test-Loss: 0.16364322017813915 Train-f1: 0.474026614736505 Test-f1: 0.453416149068323 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7245/20000] Train-Loss: 0.5342296379423097 Test-Loss: 0.16248824668769035 Train-f1: 0.4662740615809208 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7246/20000] Train-Loss: 0.5322360079313887 Test-Loss: 0.16140181594123684 Train-f1: 0.46381458974242407 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7247/20000] Train-Loss: 0.531473976623841 Test-Loss: 0.16142985347312402 Train-f1: 0.4601692688402469 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7248/20000] Train-Loss: 0.531872438252319 Test-Loss: 0.16454453986962758 Train-f1: 0.4661363902508566 Test-f1: 0.4305997407800165 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7249/20000] Train-Loss: 0.5333024374316262 Test-Loss: 0.16421111266651175 Train-f1: 0.4642496458438057 Test-f1: 0.397364953886693 Train-acc: 0.7847682119205298 Test-acc: 0.6973684210526315\n",
      "Epoch [7250/20000] Train-Loss: 0.5313073798519808 Test-Loss: 0.16626312659586448 Train-f1: 0.4677831897567156 Test-f1: 0.4305997407800165 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7251/20000] Train-Loss: 0.5338510084105549 Test-Loss: 0.163583172134436 Train-f1: 0.45713063243749164 Test-f1: 0.4088146077440076 Train-acc: 0.7781456953642384 Test-acc: 0.7105263157894737\n",
      "Epoch [7252/20000] Train-Loss: 0.5288405097153298 Test-Loss: 0.16735857663774067 Train-f1: 0.47871003810448565 Test-f1: 0.42939958592132504 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [7253/20000] Train-Loss: 0.5307694559657931 Test-Loss: 0.16080508611837613 Train-f1: 0.45529087889336334 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7254/20000] Train-Loss: 0.5313450000725287 Test-Loss: 0.16070133974185516 Train-f1: 0.46044436127724725 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7255/20000] Train-Loss: 0.5308415126951465 Test-Loss: 0.16088203044126018 Train-f1: 0.46077435515955545 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7256/20000] Train-Loss: 0.5315078888742377 Test-Loss: 0.1612763176217141 Train-f1: 0.4628458498023715 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7257/20000] Train-Loss: 0.526673213596531 Test-Loss: 0.16090026118276715 Train-f1: 0.47165418085760535 Test-f1: 0.43650793650793657 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7258/20000] Train-Loss: 0.530878520306076 Test-Loss: 0.16087459704218748 Train-f1: 0.45999061413188597 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7259/20000] Train-Loss: 0.5300727968555414 Test-Loss: 0.16171115331347544 Train-f1: 0.46263433697486456 Test-f1: 0.43628155628155635 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7260/20000] Train-Loss: 0.530398026673044 Test-Loss: 0.16288692761229698 Train-f1: 0.45783922171018937 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7261/20000] Train-Loss: 0.5305666784395475 Test-Loss: 0.16892992626756528 Train-f1: 0.4712319316521304 Test-f1: 0.44458402965865657 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7262/20000] Train-Loss: 0.5330675803065851 Test-Loss: 0.16176187637018863 Train-f1: 0.4581391013911661 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7263/20000] Train-Loss: 0.5284410536043681 Test-Loss: 0.16401310893354284 Train-f1: 0.45771673595986223 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7264/20000] Train-Loss: 0.5296690316286352 Test-Loss: 0.16451354610380095 Train-f1: 0.46282749356451003 Test-f1: 0.453416149068323 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7265/20000] Train-Loss: 0.530604764626661 Test-Loss: 0.161031718103979 Train-f1: 0.4739279531986404 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7266/20000] Train-Loss: 0.531941852036288 Test-Loss: 0.16165004175798073 Train-f1: 0.4578483976005375 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7267/20000] Train-Loss: 0.5297159626705263 Test-Loss: 0.16332266725263483 Train-f1: 0.4631156383804246 Test-f1: 0.4457303936373703 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7268/20000] Train-Loss: 0.5302471928928592 Test-Loss: 0.16593447233216801 Train-f1: 0.46386357590727717 Test-f1: 0.44069706037669415 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7269/20000] Train-Loss: 0.5278881249313181 Test-Loss: 0.16614935301973013 Train-f1: 0.462543397558286 Test-f1: 0.42939958592132504 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7270/20000] Train-Loss: 0.5310831364154464 Test-Loss: 0.1647630626146486 Train-f1: 0.45760648803992954 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7271/20000] Train-Loss: 0.5278127869689988 Test-Loss: 0.16428037855986335 Train-f1: 0.46635903031405174 Test-f1: 0.397364953886693 Train-acc: 0.7880794701986755 Test-acc: 0.6973684210526315\n",
      "Epoch [7272/20000] Train-Loss: 0.5290836361175982 Test-Loss: 0.1610764012108248 Train-f1: 0.45183238321694824 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [7273/20000] Train-Loss: 0.5331551153780403 Test-Loss: 0.16151820196929864 Train-f1: 0.46301153220857927 Test-f1: 0.43628155628155635 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7274/20000] Train-Loss: 0.5310202151037989 Test-Loss: 0.16115020911647454 Train-f1: 0.468471466942883 Test-f1: 0.43650793650793657 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7275/20000] Train-Loss: 0.5270469079501475 Test-Loss: 0.16895750030233245 Train-f1: 0.46397915760937014 Test-f1: 0.4341216991963261 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7276/20000] Train-Loss: 0.5237394256940003 Test-Loss: 0.16403920948982648 Train-f1: 0.459460261988785 Test-f1: 0.4477903287732132 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7277/20000] Train-Loss: 0.5306261212818972 Test-Loss: 0.1609428676181371 Train-f1: 0.47914314393119317 Test-f1: 0.4144537815126051 Train-acc: 0.804635761589404 Test-acc: 0.7236842105263158\n",
      "Epoch [7278/20000] Train-Loss: 0.5277225736258222 Test-Loss: 0.16286606726898548 Train-f1: 0.46531405395424646 Test-f1: 0.45564935064935064 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7279/20000] Train-Loss: 0.5289116336266152 Test-Loss: 0.1623243639303998 Train-f1: 0.4635713588782181 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7280/20000] Train-Loss: 0.530176651963165 Test-Loss: 0.16148277226721594 Train-f1: 0.4651444901291809 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7281/20000] Train-Loss: 0.5292000693245381 Test-Loss: 0.16618812274533182 Train-f1: 0.45796873834263396 Test-f1: 0.4305997407800165 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [7282/20000] Train-Loss: 0.5272208969117658 Test-Loss: 0.1607714628188668 Train-f1: 0.46287800494979264 Test-f1: 0.4144537815126051 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [7283/20000] Train-Loss: 0.5324004904480394 Test-Loss: 0.16524702990050083 Train-f1: 0.4789326961369972 Test-f1: 0.45564935064935064 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [7284/20000] Train-Loss: 0.5290350753330397 Test-Loss: 0.16315898019907937 Train-f1: 0.470633317096513 Test-f1: 0.45564935064935064 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7285/20000] Train-Loss: 0.5298543987915508 Test-Loss: 0.17189691241120972 Train-f1: 0.455452164323132 Test-f1: 0.4282828282828284 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7286/20000] Train-Loss: 0.531427545088441 Test-Loss: 0.16140430732005867 Train-f1: 0.46353568617939783 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7287/20000] Train-Loss: 0.5285970204547693 Test-Loss: 0.1611968143674457 Train-f1: 0.46429512516469035 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7288/20000] Train-Loss: 0.5302207293269782 Test-Loss: 0.1628537134864834 Train-f1: 0.4759673564854833 Test-f1: 0.4469172932330827 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7289/20000] Train-Loss: 0.5310390516577461 Test-Loss: 0.1613950779871586 Train-f1: 0.46824912313963407 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7290/20000] Train-Loss: 0.530891274109624 Test-Loss: 0.16224713381624728 Train-f1: 0.46335861247307886 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7291/20000] Train-Loss: 0.5294291072058492 Test-Loss: 0.1611224470266244 Train-f1: 0.4679613915376316 Test-f1: 0.4144537815126051 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7292/20000] Train-Loss: 0.5295595664023527 Test-Loss: 0.16070779995771947 Train-f1: 0.4676010110632676 Test-f1: 0.43650793650793657 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7293/20000] Train-Loss: 0.5311534066720602 Test-Loss: 0.16102716338994996 Train-f1: 0.47273583286021026 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7294/20000] Train-Loss: 0.5294253189319537 Test-Loss: 0.16063989339528215 Train-f1: 0.4558671663622997 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7295/20000] Train-Loss: 0.5283793272639078 Test-Loss: 0.1672568884596393 Train-f1: 0.46569368916510534 Test-f1: 0.39187165775401067 Train-acc: 0.7880794701986755 Test-acc: 0.6842105263157895\n",
      "Epoch [7296/20000] Train-Loss: 0.5310606432070127 Test-Loss: 0.16397985248742533 Train-f1: 0.45713063243749164 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7297/20000] Train-Loss: 0.5296768244027098 Test-Loss: 0.16745564267940644 Train-f1: 0.45713063243749164 Test-f1: 0.44888010540184453 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7298/20000] Train-Loss: 0.5342667465711407 Test-Loss: 0.16299572119738906 Train-f1: 0.46044436127724725 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7299/20000] Train-Loss: 0.5320101132356702 Test-Loss: 0.16147713874064662 Train-f1: 0.4631620435954851 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7300/20000] Train-Loss: 0.5286249046503867 Test-Loss: 0.16569831683965117 Train-f1: 0.46033355366858186 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7301/20000] Train-Loss: 0.526843637895833 Test-Loss: 0.1642284826619627 Train-f1: 0.463148173463963 Test-f1: 0.4545938580861545 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7302/20000] Train-Loss: 0.5275857775896802 Test-Loss: 0.16300465071562711 Train-f1: 0.4596179263794051 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7303/20000] Train-Loss: 0.5283426842418047 Test-Loss: 0.16465442384257103 Train-f1: 0.46719367588932803 Test-f1: 0.4457303936373703 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7304/20000] Train-Loss: 0.5317087711912132 Test-Loss: 0.1629744837381424 Train-f1: 0.4578270509977827 Test-f1: 0.46554019457245266 Train-acc: 0.7781456953642384 Test-acc: 0.7894736842105263\n",
      "Epoch [7305/20000] Train-Loss: 0.5322817829950075 Test-Loss: 0.16245823442789278 Train-f1: 0.4629819524200164 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7306/20000] Train-Loss: 0.5299936620662544 Test-Loss: 0.160440406326711 Train-f1: 0.464305656111471 Test-f1: 0.43628155628155635 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7307/20000] Train-Loss: 0.5280400609538874 Test-Loss: 0.16390249644079713 Train-f1: 0.4651444901291809 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7308/20000] Train-Loss: 0.5279154324622365 Test-Loss: 0.161107434518118 Train-f1: 0.4625404040404041 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7309/20000] Train-Loss: 0.5260862027728831 Test-Loss: 0.16157352809044118 Train-f1: 0.46251665226013117 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7310/20000] Train-Loss: 0.5292281597755129 Test-Loss: 0.16138805333661907 Train-f1: 0.46531405395424646 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7311/20000] Train-Loss: 0.525200743684304 Test-Loss: 0.1699182877296883 Train-f1: 0.46639699901729104 Test-f1: 0.43675048355899426 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7312/20000] Train-Loss: 0.5269268563468118 Test-Loss: 0.16283758385941483 Train-f1: 0.4733069000091149 Test-f1: 0.43574229691876754 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7313/20000] Train-Loss: 0.5302350731273363 Test-Loss: 0.16131862363645688 Train-f1: 0.4561784603680673 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7314/20000] Train-Loss: 0.5317178983559162 Test-Loss: 0.16355798222079512 Train-f1: 0.46323850732861416 Test-f1: 0.4068599033816426 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [7315/20000] Train-Loss: 0.5293234019197445 Test-Loss: 0.16280451545165986 Train-f1: 0.46462453767639733 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7316/20000] Train-Loss: 0.5304463869601547 Test-Loss: 0.16107392626208214 Train-f1: 0.4659616026565576 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7317/20000] Train-Loss: 0.5288698623978632 Test-Loss: 0.1609529059613594 Train-f1: 0.4680133989608969 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7318/20000] Train-Loss: 0.5299887365183501 Test-Loss: 0.16168639319088654 Train-f1: 0.45296019947967353 Test-f1: 0.4469172932330827 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [7319/20000] Train-Loss: 0.5254748835351735 Test-Loss: 0.1628326280667726 Train-f1: 0.45959976823996074 Test-f1: 0.46554019457245266 Train-acc: 0.7814569536423841 Test-acc: 0.7894736842105263\n",
      "Epoch [7320/20000] Train-Loss: 0.5275468020883946 Test-Loss: 0.16942977229581363 Train-f1: 0.4548138880200188 Test-f1: 0.4426916799402199 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [7321/20000] Train-Loss: 0.5239219775709671 Test-Loss: 0.16134116637623663 Train-f1: 0.46670568117043515 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7322/20000] Train-Loss: 0.5258008664250668 Test-Loss: 0.16768093362794623 Train-f1: 0.47600461659449456 Test-f1: 0.4344337363247945 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [7323/20000] Train-Loss: 0.5259318467028934 Test-Loss: 0.16024031076360165 Train-f1: 0.46312860539593803 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7324/20000] Train-Loss: 0.5345655861957523 Test-Loss: 0.1602891672595777 Train-f1: 0.468471466942883 Test-f1: 0.43650793650793657 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7325/20000] Train-Loss: 0.528702469963903 Test-Loss: 0.16051846445032364 Train-f1: 0.4638958417549808 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7326/20000] Train-Loss: 0.5255354723463064 Test-Loss: 0.16431182666193456 Train-f1: 0.4689906092614201 Test-f1: 0.4088146077440076 Train-acc: 0.7913907284768212 Test-acc: 0.7105263157894737\n",
      "Epoch [7327/20000] Train-Loss: 0.5241373697610197 Test-Loss: 0.16197723575626508 Train-f1: 0.4651340208540332 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7328/20000] Train-Loss: 0.5312235842214403 Test-Loss: 0.16262810625812313 Train-f1: 0.4704755751494882 Test-f1: 0.46554019457245266 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [7329/20000] Train-Loss: 0.529078923632031 Test-Loss: 0.16033411735371803 Train-f1: 0.45364883649420984 Test-f1: 0.43628155628155635 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7330/20000] Train-Loss: 0.5305026522851554 Test-Loss: 0.16021330325148953 Train-f1: 0.468471466942883 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7331/20000] Train-Loss: 0.5296547931225984 Test-Loss: 0.16213529373328026 Train-f1: 0.4676010110632676 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7332/20000] Train-Loss: 0.52738526231121 Test-Loss: 0.1609291242472035 Train-f1: 0.460780612309364 Test-f1: 0.43628155628155635 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7333/20000] Train-Loss: 0.5302835490763358 Test-Loss: 0.16331805575993355 Train-f1: 0.4631620435954851 Test-f1: 0.4305997407800165 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7334/20000] Train-Loss: 0.526112818107314 Test-Loss: 0.17079550083897874 Train-f1: 0.46673864547120597 Test-f1: 0.43675048355899426 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7335/20000] Train-Loss: 0.5325516546178614 Test-Loss: 0.16125373087285663 Train-f1: 0.4556738509807102 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7336/20000] Train-Loss: 0.524139731526682 Test-Loss: 0.1612083083169488 Train-f1: 0.47175024358991474 Test-f1: 0.41766233766233773 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7337/20000] Train-Loss: 0.5284472615765882 Test-Loss: 0.1636458786651001 Train-f1: 0.45738116955508257 Test-f1: 0.4305997407800165 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [7338/20000] Train-Loss: 0.5262991698959116 Test-Loss: 0.1626915417347484 Train-f1: 0.4623275887207301 Test-f1: 0.43628155628155635 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7339/20000] Train-Loss: 0.5293567241305237 Test-Loss: 0.16007919011808744 Train-f1: 0.4601783723522853 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7340/20000] Train-Loss: 0.5304822293406851 Test-Loss: 0.1612034513588699 Train-f1: 0.468471466942883 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7341/20000] Train-Loss: 0.5287174745558008 Test-Loss: 0.16262120956161744 Train-f1: 0.467183919884241 Test-f1: 0.43574229691876754 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7342/20000] Train-Loss: 0.528354388938384 Test-Loss: 0.16028572841461328 Train-f1: 0.4506945274583403 Test-f1: 0.41766233766233773 Train-acc: 0.7682119205298014 Test-acc: 0.7236842105263158\n",
      "Epoch [7343/20000] Train-Loss: 0.5298896064291816 Test-Loss: 0.16135712189863438 Train-f1: 0.4474946577850442 Test-f1: 0.44709236709236705 Train-acc: 0.7649006622516556 Test-acc: 0.7631578947368421\n",
      "Epoch [7344/20000] Train-Loss: 0.5273918617008941 Test-Loss: 0.16821170321770507 Train-f1: 0.4598358927460021 Test-f1: 0.4344337363247945 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7345/20000] Train-Loss: 0.5280130701620105 Test-Loss: 0.16096188131409342 Train-f1: 0.46077435515955545 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7346/20000] Train-Loss: 0.5315613861345458 Test-Loss: 0.16185869054919946 Train-f1: 0.4575166522601311 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7347/20000] Train-Loss: 0.527670219643034 Test-Loss: 0.16108424820896214 Train-f1: 0.4571537346992617 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7348/20000] Train-Loss: 0.5306966076571586 Test-Loss: 0.16066735214734104 Train-f1: 0.45825371982360597 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [7349/20000] Train-Loss: 0.5292551893464219 Test-Loss: 0.16233374703099024 Train-f1: 0.46852877358788964 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7350/20000] Train-Loss: 0.5301644189365687 Test-Loss: 0.16105337694728258 Train-f1: 0.4733232047902051 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7351/20000] Train-Loss: 0.5243250995029956 Test-Loss: 0.1620007916340043 Train-f1: 0.4574942908292689 Test-f1: 0.4503312334891282 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7352/20000] Train-Loss: 0.528305283074093 Test-Loss: 0.16090805711282918 Train-f1: 0.4759673564854833 Test-f1: 0.4469172932330827 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7353/20000] Train-Loss: 0.5266081718405855 Test-Loss: 0.16153793231230584 Train-f1: 0.48113373564139145 Test-f1: 0.446829268292683 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [7354/20000] Train-Loss: 0.5257995957703039 Test-Loss: 0.16163851156089562 Train-f1: 0.4688708133971291 Test-f1: 0.43650793650793657 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7355/20000] Train-Loss: 0.5297665168319643 Test-Loss: 0.16079532020054293 Train-f1: 0.4620479319533313 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7356/20000] Train-Loss: 0.5282796951704571 Test-Loss: 0.1624422269111804 Train-f1: 0.46323850732861416 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7357/20000] Train-Loss: 0.5293124251173611 Test-Loss: 0.16139422088993982 Train-f1: 0.4575166522601311 Test-f1: 0.43650793650793657 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [7358/20000] Train-Loss: 0.5296151928086831 Test-Loss: 0.16085696425455942 Train-f1: 0.4673690362708053 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7359/20000] Train-Loss: 0.5249632966641915 Test-Loss: 0.16795467389331836 Train-f1: 0.47044003608912643 Test-f1: 0.44458402965865657 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7360/20000] Train-Loss: 0.5311758266945403 Test-Loss: 0.16075640436169247 Train-f1: 0.4554972644453711 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7361/20000] Train-Loss: 0.524314850557896 Test-Loss: 0.1656386514979563 Train-f1: 0.4712679940468497 Test-f1: 0.44069706037669415 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7362/20000] Train-Loss: 0.5282618162401028 Test-Loss: 0.1648908299007093 Train-f1: 0.46742615292726236 Test-f1: 0.44069706037669415 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7363/20000] Train-Loss: 0.5272883165395521 Test-Loss: 0.16096343724145865 Train-f1: 0.4580119673287375 Test-f1: 0.43650793650793657 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [7364/20000] Train-Loss: 0.5278995177485044 Test-Loss: 0.16074987964316395 Train-f1: 0.4644976167144688 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7365/20000] Train-Loss: 0.5295605875986582 Test-Loss: 0.16156389891968004 Train-f1: 0.46107844474416676 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7366/20000] Train-Loss: 0.5268851530276129 Test-Loss: 0.16166679335822154 Train-f1: 0.4635886195502197 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7367/20000] Train-Loss: 0.527833336092431 Test-Loss: 0.16005342735250955 Train-f1: 0.4664635963649405 Test-f1: 0.43650793650793657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7368/20000] Train-Loss: 0.5289975125997521 Test-Loss: 0.1608537873308188 Train-f1: 0.470772777946691 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7369/20000] Train-Loss: 0.525903043421048 Test-Loss: 0.16435203841121468 Train-f1: 0.46322299855501037 Test-f1: 0.45564935064935064 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7370/20000] Train-Loss: 0.5178912332780872 Test-Loss: 0.16257388619931465 Train-f1: 0.4874988002687398 Test-f1: 0.41930363543266774 Train-acc: 0.8145695364238411 Test-acc: 0.7236842105263158\n",
      "Epoch [7371/20000] Train-Loss: 0.5264222656181408 Test-Loss: 0.16265359947588087 Train-f1: 0.45879901316233623 Test-f1: 0.46554019457245266 Train-acc: 0.7814569536423841 Test-acc: 0.7894736842105263\n",
      "Epoch [7372/20000] Train-Loss: 0.5278133705592593 Test-Loss: 0.17414408484899846 Train-f1: 0.4660464397016827 Test-f1: 0.43675048355899426 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7373/20000] Train-Loss: 0.5191911308610184 Test-Loss: 0.16015548629184312 Train-f1: 0.45984072657615727 Test-f1: 0.4144537815126051 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [7374/20000] Train-Loss: 0.5276138180777913 Test-Loss: 0.16187476459793854 Train-f1: 0.4698161248310132 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7375/20000] Train-Loss: 0.5270824424820679 Test-Loss: 0.16348867427897543 Train-f1: 0.4682754534682344 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7376/20000] Train-Loss: 0.5244463791169407 Test-Loss: 0.15999294109176415 Train-f1: 0.4673683771728709 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7377/20000] Train-Loss: 0.5261414120093613 Test-Loss: 0.16161725806805324 Train-f1: 0.4651470233332063 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7378/20000] Train-Loss: 0.5273870177717538 Test-Loss: 0.16892525273571488 Train-f1: 0.4712679940468497 Test-f1: 0.4239074205353209 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7379/20000] Train-Loss: 0.5252312929380182 Test-Loss: 0.17062456522145913 Train-f1: 0.46341383195592056 Test-f1: 0.4177724714310081 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [7380/20000] Train-Loss: 0.5283283736576228 Test-Loss: 0.16083944213701232 Train-f1: 0.46282749356451003 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7381/20000] Train-Loss: 0.5289040114987106 Test-Loss: 0.16143709504240078 Train-f1: 0.46090266985077644 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7382/20000] Train-Loss: 0.5255597979079105 Test-Loss: 0.1622383887504806 Train-f1: 0.4707782929460823 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7383/20000] Train-Loss: 0.5267845817421025 Test-Loss: 0.1618528264270136 Train-f1: 0.46549767569045664 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7384/20000] Train-Loss: 0.5266723801484894 Test-Loss: 0.16222341148233121 Train-f1: 0.4653119656141994 Test-f1: 0.4457303936373703 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7385/20000] Train-Loss: 0.5257467522094252 Test-Loss: 0.16389786037382356 Train-f1: 0.4609147984336392 Test-f1: 0.45103407755581665 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7386/20000] Train-Loss: 0.5224157019700484 Test-Loss: 0.16122936926139456 Train-f1: 0.4560049618701501 Test-f1: 0.4124675324675325 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7387/20000] Train-Loss: 0.527520167567654 Test-Loss: 0.16042085074046672 Train-f1: 0.46673864547120597 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7388/20000] Train-Loss: 0.5262526531419903 Test-Loss: 0.16034308421018578 Train-f1: 0.47409891078825855 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7389/20000] Train-Loss: 0.5263783026771038 Test-Loss: 0.16398044401731354 Train-f1: 0.46053252821306734 Test-f1: 0.4305997407800165 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7390/20000] Train-Loss: 0.528172835707614 Test-Loss: 0.1605462608441893 Train-f1: 0.45914680168966376 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7391/20000] Train-Loss: 0.528353178669846 Test-Loss: 0.159699242030174 Train-f1: 0.4679755751494882 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7392/20000] Train-Loss: 0.5263941747313461 Test-Loss: 0.16474870778977155 Train-f1: 0.47845724394991773 Test-f1: 0.45103407755581665 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [7393/20000] Train-Loss: 0.527126942838462 Test-Loss: 0.16020705200935537 Train-f1: 0.4598431166710407 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7394/20000] Train-Loss: 0.5222334249147255 Test-Loss: 0.16693520290539282 Train-f1: 0.4615200978290215 Test-f1: 0.41623529411764704 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7395/20000] Train-Loss: 0.5309265955592323 Test-Loss: 0.1625333687383826 Train-f1: 0.46121735475643566 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7396/20000] Train-Loss: 0.5290720494858286 Test-Loss: 0.1610413045547169 Train-f1: 0.46129055394840657 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7397/20000] Train-Loss: 0.5249793201546575 Test-Loss: 0.16697832962324452 Train-f1: 0.4610077198786876 Test-f1: 0.4239074205353209 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7398/20000] Train-Loss: 0.5267919375789087 Test-Loss: 0.16524344039514036 Train-f1: 0.46084786605909017 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7399/20000] Train-Loss: 0.5269506444287441 Test-Loss: 0.1619889837170711 Train-f1: 0.4712679940468497 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7400/20000] Train-Loss: 0.5249143222202126 Test-Loss: 0.1601429798147312 Train-f1: 0.46215481892477195 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7401/20000] Train-Loss: 0.5258716942215744 Test-Loss: 0.16352640050848882 Train-f1: 0.46830179846785464 Test-f1: 0.4457303936373703 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7402/20000] Train-Loss: 0.5274899948116212 Test-Loss: 0.16071648451583426 Train-f1: 0.46881765253518976 Test-f1: 0.43650793650793657 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7403/20000] Train-Loss: 0.5236267164426628 Test-Loss: 0.16044775507862885 Train-f1: 0.46278455708854427 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7404/20000] Train-Loss: 0.5263086800518489 Test-Loss: 0.16260393039167637 Train-f1: 0.45452199038384666 Test-f1: 0.46554019457245266 Train-acc: 0.7748344370860927 Test-acc: 0.7894736842105263\n",
      "Epoch [7405/20000] Train-Loss: 0.5255112819196898 Test-Loss: 0.15991941338513058 Train-f1: 0.460976076555024 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7406/20000] Train-Loss: 0.5252474342533012 Test-Loss: 0.16326532651644565 Train-f1: 0.4598811695550826 Test-f1: 0.4305997407800165 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7407/20000] Train-Loss: 0.5255020734456043 Test-Loss: 0.160753496608891 Train-f1: 0.4602041746422387 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7408/20000] Train-Loss: 0.525149109822361 Test-Loss: 0.1609029267881169 Train-f1: 0.46335861247307886 Test-f1: 0.4469172932330827 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7409/20000] Train-Loss: 0.5273555073794929 Test-Loss: 0.16010948444065942 Train-f1: 0.45503439185068506 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7410/20000] Train-Loss: 0.5258659612959519 Test-Loss: 0.15991551292303355 Train-f1: 0.47295053398275166 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7411/20000] Train-Loss: 0.5270388143369135 Test-Loss: 0.1598009526278796 Train-f1: 0.4606552112662655 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7412/20000] Train-Loss: 0.5275579313459279 Test-Loss: 0.15967029911561417 Train-f1: 0.45512569080903925 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7413/20000] Train-Loss: 0.5219689779874136 Test-Loss: 0.1654264414046668 Train-f1: 0.47045894892514895 Test-f1: 0.4457303936373703 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7414/20000] Train-Loss: 0.5202138936154627 Test-Loss: 0.15978247685572974 Train-f1: 0.46119690924061063 Test-f1: 0.41766233766233773 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7415/20000] Train-Loss: 0.5230655912858023 Test-Loss: 0.16536459665215505 Train-f1: 0.46127427267438337 Test-f1: 0.45564935064935064 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7416/20000] Train-Loss: 0.5255041518188736 Test-Loss: 0.161982447205081 Train-f1: 0.4611842481499696 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7417/20000] Train-Loss: 0.5250912324814712 Test-Loss: 0.16012581108622723 Train-f1: 0.4577767830158736 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7418/20000] Train-Loss: 0.5229857311780134 Test-Loss: 0.16470181885534335 Train-f1: 0.46518356894547985 Test-f1: 0.4457303936373703 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7419/20000] Train-Loss: 0.5276277006470861 Test-Loss: 0.16216881708185354 Train-f1: 0.46903980177263804 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7420/20000] Train-Loss: 0.5246744148743561 Test-Loss: 0.15986825588453446 Train-f1: 0.460040404040404 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7421/20000] Train-Loss: 0.5235581557109685 Test-Loss: 0.1647895151447541 Train-f1: 0.47978547002991334 Test-f1: 0.4545938580861545 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [7422/20000] Train-Loss: 0.5243548439565694 Test-Loss: 0.15980575197568803 Train-f1: 0.45655163571925444 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7423/20000] Train-Loss: 0.5268745364131935 Test-Loss: 0.16181960165505957 Train-f1: 0.4669072552686856 Test-f1: 0.45564935064935064 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7424/20000] Train-Loss: 0.5259510551102845 Test-Loss: 0.16043075390980455 Train-f1: 0.4718865804384727 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7425/20000] Train-Loss: 0.523615124256948 Test-Loss: 0.15963153523116924 Train-f1: 0.4659063831737158 Test-f1: 0.43650793650793657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7426/20000] Train-Loss: 0.5234960323823646 Test-Loss: 0.1595229261834934 Train-f1: 0.4628366159973186 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7427/20000] Train-Loss: 0.5227160507965752 Test-Loss: 0.16092865784424495 Train-f1: 0.47231567473294084 Test-f1: 0.460952380952381 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7428/20000] Train-Loss: 0.5322400451922845 Test-Loss: 0.16067580799174755 Train-f1: 0.458923163535944 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7429/20000] Train-Loss: 0.5245458000741355 Test-Loss: 0.16487831065875674 Train-f1: 0.4632029819669025 Test-f1: 0.45103407755581665 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7430/20000] Train-Loss: 0.5260937833040168 Test-Loss: 0.16257203540805493 Train-f1: 0.45483238940762283 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7431/20000] Train-Loss: 0.5250952753849631 Test-Loss: 0.16012086022105038 Train-f1: 0.4715182656269681 Test-f1: 0.43650793650793657 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7432/20000] Train-Loss: 0.5251788742357509 Test-Loss: 0.1601384153620651 Train-f1: 0.460175546029802 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7433/20000] Train-Loss: 0.5270765015550838 Test-Loss: 0.16039957714438627 Train-f1: 0.46429512516469035 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7434/20000] Train-Loss: 0.5256961947496611 Test-Loss: 0.16115552170103242 Train-f1: 0.4569787498888592 Test-f1: 0.43574229691876754 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "Epoch [7435/20000] Train-Loss: 0.5266665550345451 Test-Loss: 0.16144089528080613 Train-f1: 0.4609739562808155 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7436/20000] Train-Loss: 0.5248215142277478 Test-Loss: 0.16088996965194133 Train-f1: 0.47347019652913713 Test-f1: 0.43574229691876754 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7437/20000] Train-Loss: 0.5261427349978188 Test-Loss: 0.16033825828130094 Train-f1: 0.4686753246753247 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7438/20000] Train-Loss: 0.5270101472973212 Test-Loss: 0.1627001131757758 Train-f1: 0.4679815898833728 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7439/20000] Train-Loss: 0.5276398934023349 Test-Loss: 0.1616976384838883 Train-f1: 0.46335861247307886 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7440/20000] Train-Loss: 0.525957376942255 Test-Loss: 0.1660411711685254 Train-f1: 0.46263433697486456 Test-f1: 0.42933359390267734 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7441/20000] Train-Loss: 0.5279716025081571 Test-Loss: 0.16273091935180828 Train-f1: 0.46397149198433113 Test-f1: 0.4088146077440076 Train-acc: 0.7847682119205298 Test-acc: 0.7105263157894737\n",
      "Epoch [7442/20000] Train-Loss: 0.519777599067546 Test-Loss: 0.16507384814707485 Train-f1: 0.4695254880433003 Test-f1: 0.453416149068323 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7443/20000] Train-Loss: 0.5258530014076589 Test-Loss: 0.1599220170570256 Train-f1: 0.4611264798443141 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7444/20000] Train-Loss: 0.5241390398047869 Test-Loss: 0.15937163637442783 Train-f1: 0.46301153220857927 Test-f1: 0.44000000000000006 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7445/20000] Train-Loss: 0.5250823942846264 Test-Loss: 0.165014387126483 Train-f1: 0.46657471883455803 Test-f1: 0.45564935064935064 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7446/20000] Train-Loss: 0.5230891061227689 Test-Loss: 0.15986867159491633 Train-f1: 0.474375760882194 Test-f1: 0.4144537815126051 Train-acc: 0.7980132450331126 Test-acc: 0.7236842105263158\n",
      "Epoch [7447/20000] Train-Loss: 0.5248345227748143 Test-Loss: 0.16376445865362343 Train-f1: 0.4643074997546517 Test-f1: 0.44069706037669415 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7448/20000] Train-Loss: 0.5259510801870836 Test-Loss: 0.16727052595893227 Train-f1: 0.46057606989284006 Test-f1: 0.4239074205353209 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7449/20000] Train-Loss: 0.5253116345636308 Test-Loss: 0.1597726224424659 Train-f1: 0.47337373737373734 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7450/20000] Train-Loss: 0.5222556968726138 Test-Loss: 0.16752376745329095 Train-f1: 0.463006148272871 Test-f1: 0.4341972308997648 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7451/20000] Train-Loss: 0.5242967425453681 Test-Loss: 0.15963691838129296 Train-f1: 0.47250064211621157 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7452/20000] Train-Loss: 0.522345387598708 Test-Loss: 0.16212948045909872 Train-f1: 0.4732074137972919 Test-f1: 0.46554019457245266 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [7453/20000] Train-Loss: 0.5251818003712091 Test-Loss: 0.16307964409811643 Train-f1: 0.4706504550770326 Test-f1: 0.45564935064935064 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7454/20000] Train-Loss: 0.5220696553339943 Test-Loss: 0.16196287753010138 Train-f1: 0.45112337465385693 Test-f1: 0.46554019457245266 Train-acc: 0.7682119205298014 Test-acc: 0.7894736842105263\n",
      "Epoch [7455/20000] Train-Loss: 0.5227820601953523 Test-Loss: 0.16030214486268743 Train-f1: 0.4636766589835181 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7456/20000] Train-Loss: 0.5239032387505044 Test-Loss: 0.16068931465452382 Train-f1: 0.47417156947842865 Test-f1: 0.43650793650793657 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7457/20000] Train-Loss: 0.5226029392786421 Test-Loss: 0.15953414359482834 Train-f1: 0.4712679940468497 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7458/20000] Train-Loss: 0.5242534692963402 Test-Loss: 0.1613180342923571 Train-f1: 0.47097317005361405 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7459/20000] Train-Loss: 0.5225598659205599 Test-Loss: 0.15992834332493072 Train-f1: 0.4670137344669719 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7460/20000] Train-Loss: 0.5223054822486856 Test-Loss: 0.16008684057628764 Train-f1: 0.4716396158839367 Test-f1: 0.4144537815126051 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7461/20000] Train-Loss: 0.5246285683597243 Test-Loss: 0.15941570177243372 Train-f1: 0.4589723320158103 Test-f1: 0.44000000000000006 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7462/20000] Train-Loss: 0.5233608329689734 Test-Loss: 0.16231466514597992 Train-f1: 0.46229714142140904 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7463/20000] Train-Loss: 0.525465512102718 Test-Loss: 0.16465047784515272 Train-f1: 0.467183919884241 Test-f1: 0.43574229691876754 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7464/20000] Train-Loss: 0.523914623556599 Test-Loss: 0.16194507286525128 Train-f1: 0.4574942908292689 Test-f1: 0.4457303936373703 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7465/20000] Train-Loss: 0.5241052789193782 Test-Loss: 0.1596020675711116 Train-f1: 0.4637408589292814 Test-f1: 0.4469172932330827 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7466/20000] Train-Loss: 0.5216979521203966 Test-Loss: 0.16290380288575154 Train-f1: 0.4693373301531715 Test-f1: 0.453416149068323 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7467/20000] Train-Loss: 0.5271530259100999 Test-Loss: 0.16472242592644595 Train-f1: 0.4530058197872767 Test-f1: 0.4401493930905696 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [7468/20000] Train-Loss: 0.5239279661746057 Test-Loss: 0.1598212225523664 Train-f1: 0.46569368916510534 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7469/20000] Train-Loss: 0.525439613804606 Test-Loss: 0.17178790649753534 Train-f1: 0.4788018193916974 Test-f1: 0.4282828282828284 Train-acc: 0.804635761589404 Test-acc: 0.7236842105263158\n",
      "Epoch [7470/20000] Train-Loss: 0.5268410026554148 Test-Loss: 0.15974345471229343 Train-f1: 0.46271989791267887 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7471/20000] Train-Loss: 0.5228962554732864 Test-Loss: 0.1604202533560086 Train-f1: 0.4651340208540332 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7472/20000] Train-Loss: 0.5228577599349844 Test-Loss: 0.16076648880724226 Train-f1: 0.4652356211831192 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7473/20000] Train-Loss: 0.5252847611939694 Test-Loss: 0.16119452216429503 Train-f1: 0.46858234932769094 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7474/20000] Train-Loss: 0.5255324503282096 Test-Loss: 0.1600868523767749 Train-f1: 0.4712679940468497 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7475/20000] Train-Loss: 0.5253322564862042 Test-Loss: 0.1598305123566001 Train-f1: 0.4731094684163276 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7476/20000] Train-Loss: 0.523590236834268 Test-Loss: 0.16404784430923544 Train-f1: 0.46549767569045664 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7477/20000] Train-Loss: 0.5235583701894964 Test-Loss: 0.1604841685309865 Train-f1: 0.4609174210253234 Test-f1: 0.43574229691876754 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7478/20000] Train-Loss: 0.5222012982255477 Test-Loss: 0.1646304429750694 Train-f1: 0.4665393577720943 Test-f1: 0.4088146077440076 Train-acc: 0.7880794701986755 Test-acc: 0.7105263157894737\n",
      "best score_dx!!\n",
      "Epoch [7479/20000] Train-Loss: 0.5290772907272592 Test-Loss: 0.15931049949045714 Train-f1: 0.4700113862977625 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7480/20000] Train-Loss: 0.5239917560701143 Test-Loss: 0.1596529814981045 Train-f1: 0.46127257176770503 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7481/20000] Train-Loss: 0.5226950504487528 Test-Loss: 0.16492718470333514 Train-f1: 0.46429512516469035 Test-f1: 0.4401493930905696 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7482/20000] Train-Loss: 0.5236349658948697 Test-Loss: 0.16114420406668906 Train-f1: 0.4786921197408452 Test-f1: 0.45564935064935064 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [7483/20000] Train-Loss: 0.5246755328934484 Test-Loss: 0.16003891559380332 Train-f1: 0.46644616868979244 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7484/20000] Train-Loss: 0.5258066535600392 Test-Loss: 0.16045793918161705 Train-f1: 0.4672354130263473 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7485/20000] Train-Loss: 0.5237911238331826 Test-Loss: 0.17406688204269863 Train-f1: 0.4687338511153941 Test-f1: 0.4282828282828284 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7486/20000] Train-Loss: 0.5279101082642808 Test-Loss: 0.15939398388675505 Train-f1: 0.4647846889952153 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7487/20000] Train-Loss: 0.5237410950201872 Test-Loss: 0.16228032700776993 Train-f1: 0.4651034727773336 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7488/20000] Train-Loss: 0.5210692579960339 Test-Loss: 0.15891063972702188 Train-f1: 0.47921130106263343 Test-f1: 0.4469172932330827 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [7489/20000] Train-Loss: 0.5251323888678535 Test-Loss: 0.17003215702064958 Train-f1: 0.470633317096513 Test-f1: 0.4385387501666571 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7490/20000] Train-Loss: 0.5209531128893844 Test-Loss: 0.16159348404708057 Train-f1: 0.46194822630952925 Test-f1: 0.4457303936373703 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7491/20000] Train-Loss: 0.52345948526688 Test-Loss: 0.16204383299929437 Train-f1: 0.4556738509807102 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7492/20000] Train-Loss: 0.5233526668624947 Test-Loss: 0.15906479980268198 Train-f1: 0.47690049081353425 Test-f1: 0.44709236709236705 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7493/20000] Train-Loss: 0.5191600808274953 Test-Loss: 0.16306266918181667 Train-f1: 0.47450885668276965 Test-f1: 0.4545938580861545 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [7494/20000] Train-Loss: 0.5251640925781408 Test-Loss: 0.15963697783034508 Train-f1: 0.4666705934511368 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7495/20000] Train-Loss: 0.5247479933932139 Test-Loss: 0.1632047859832146 Train-f1: 0.4710031125125034 Test-f1: 0.45564935064935064 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7496/20000] Train-Loss: 0.5231254588495083 Test-Loss: 0.16094502232131103 Train-f1: 0.46909517531230893 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7497/20000] Train-Loss: 0.5246824268953096 Test-Loss: 0.16545463499873964 Train-f1: 0.46760315665650226 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7498/20000] Train-Loss: 0.5260232541346602 Test-Loss: 0.1594085751749926 Train-f1: 0.4660070325467684 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7499/20000] Train-Loss: 0.5239858496989264 Test-Loss: 0.1640174474237334 Train-f1: 0.46107844474416676 Test-f1: 0.44027149321266973 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7500/20000] Train-Loss: 0.5211145668879378 Test-Loss: 0.15901500745166022 Train-f1: 0.4569891488717827 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7501/20000] Train-Loss: 0.5228472369887258 Test-Loss: 0.15914963209220612 Train-f1: 0.468892298948565 Test-f1: 0.4144537815126051 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7502/20000] Train-Loss: 0.5239655282776275 Test-Loss: 0.1591481385459247 Train-f1: 0.4671978559361861 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7503/20000] Train-Loss: 0.5250102481778767 Test-Loss: 0.1597122639137067 Train-f1: 0.4735689545164524 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7504/20000] Train-Loss: 0.5232603370660308 Test-Loss: 0.1611919199909003 Train-f1: 0.4761870439006131 Test-f1: 0.43650793650793657 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [7505/20000] Train-Loss: 0.5224527352570061 Test-Loss: 0.1594945447326176 Train-f1: 0.45785262024808715 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7506/20000] Train-Loss: 0.522445272311575 Test-Loss: 0.15956783159695942 Train-f1: 0.4679755751494882 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7507/20000] Train-Loss: 0.5216439904092073 Test-Loss: 0.1604969455754105 Train-f1: 0.46220785974581224 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7508/20000] Train-Loss: 0.5237042861719617 Test-Loss: 0.159714504321775 Train-f1: 0.4660639601733571 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7509/20000] Train-Loss: 0.5217266800747392 Test-Loss: 0.16762986213787162 Train-f1: 0.4601783723522853 Test-f1: 0.44745098039215686 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7510/20000] Train-Loss: 0.5189415491886575 Test-Loss: 0.1595642228639541 Train-f1: 0.46627791986359757 Test-f1: 0.41766233766233773 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7511/20000] Train-Loss: 0.5240903457377245 Test-Loss: 0.16393078633065983 Train-f1: 0.45879901316233623 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7512/20000] Train-Loss: 0.5220159684305551 Test-Loss: 0.16096849334557528 Train-f1: 0.4551896203796836 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7513/20000] Train-Loss: 0.5231538535867911 Test-Loss: 0.16370580048354708 Train-f1: 0.46874675857416576 Test-f1: 0.45067664508123617 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7514/20000] Train-Loss: 0.5199254671726968 Test-Loss: 0.16040181313382224 Train-f1: 0.46947341150479394 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7515/20000] Train-Loss: 0.5197418554324407 Test-Loss: 0.16670697890258457 Train-f1: 0.45803306399466415 Test-f1: 0.4341972308997648 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [7516/20000] Train-Loss: 0.5229538732716325 Test-Loss: 0.16048766794925098 Train-f1: 0.46547557514948823 Test-f1: 0.43650793650793657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7517/20000] Train-Loss: 0.5245137686660678 Test-Loss: 0.16070078599653284 Train-f1: 0.46102490232750915 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7518/20000] Train-Loss: 0.5208455899635804 Test-Loss: 0.16201466766562836 Train-f1: 0.4657507526675394 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7519/20000] Train-Loss: 0.5243641023542968 Test-Loss: 0.15998320986791356 Train-f1: 0.46779168657429526 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7520/20000] Train-Loss: 0.5231265301824652 Test-Loss: 0.16083010326648361 Train-f1: 0.45906723368095204 Test-f1: 0.4457303936373703 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7521/20000] Train-Loss: 0.5197441202904435 Test-Loss: 0.16368885692386909 Train-f1: 0.46544755695694795 Test-f1: 0.45067664508123617 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7522/20000] Train-Loss: 0.5210643012036572 Test-Loss: 0.16165898083592142 Train-f1: 0.46944381478483094 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7523/20000] Train-Loss: 0.5230787998981925 Test-Loss: 0.16064008966947324 Train-f1: 0.4609739562808155 Test-f1: 0.4144537815126051 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7524/20000] Train-Loss: 0.5269239346190903 Test-Loss: 0.15900453639263892 Train-f1: 0.4659616026565576 Test-f1: 0.41766233766233773 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7525/20000] Train-Loss: 0.5246546836205481 Test-Loss: 0.15973158098745122 Train-f1: 0.46779643766388884 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7526/20000] Train-Loss: 0.5230101955018364 Test-Loss: 0.15856265233149505 Train-f1: 0.47272689334661155 Test-f1: 0.43650793650793657 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7527/20000] Train-Loss: 0.52195329113798 Test-Loss: 0.16006793574864914 Train-f1: 0.4644976167144688 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7528/20000] Train-Loss: 0.5186738939678314 Test-Loss: 0.16034406998531683 Train-f1: 0.47877367065775883 Test-f1: 0.446829268292683 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [7529/20000] Train-Loss: 0.5235019488815383 Test-Loss: 0.16031007472437245 Train-f1: 0.46349796366469515 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7530/20000] Train-Loss: 0.5188087496094624 Test-Loss: 0.16207164728575585 Train-f1: 0.4691267338020141 Test-f1: 0.4088146077440076 Train-acc: 0.7913907284768212 Test-acc: 0.7105263157894737\n",
      "Epoch [7531/20000] Train-Loss: 0.5232540549451615 Test-Loss: 0.16116622065251965 Train-f1: 0.4676289621788703 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7532/20000] Train-Loss: 0.519843117715722 Test-Loss: 0.1630757467677191 Train-f1: 0.47475200406368556 Test-f1: 0.46554019457245266 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [7533/20000] Train-Loss: 0.5190135290050466 Test-Loss: 0.16053265360188768 Train-f1: 0.46382848818797895 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7534/20000] Train-Loss: 0.5236382539151635 Test-Loss: 0.16927806498019918 Train-f1: 0.46341112786163646 Test-f1: 0.43675048355899426 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7535/20000] Train-Loss: 0.5250899544878547 Test-Loss: 0.1629214200978418 Train-f1: 0.4602529115189241 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7536/20000] Train-Loss: 0.5202131107809984 Test-Loss: 0.16608721417640596 Train-f1: 0.47428862250645326 Test-f1: 0.4344337363247945 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [7537/20000] Train-Loss: 0.5219197615549909 Test-Loss: 0.16154727059563395 Train-f1: 0.45223718905348226 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [7538/20000] Train-Loss: 0.5198358467386284 Test-Loss: 0.16004604398564187 Train-f1: 0.45624864504148926 Test-f1: 0.4144537815126051 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7539/20000] Train-Loss: 0.5209559345889752 Test-Loss: 0.16184654405159146 Train-f1: 0.46531818181818174 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7540/20000] Train-Loss: 0.5225822240281112 Test-Loss: 0.17145917960540405 Train-f1: 0.46815045507703257 Test-f1: 0.4282828282828284 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7541/20000] Train-Loss: 0.525951166483608 Test-Loss: 0.1598231565982945 Train-f1: 0.4635589656152316 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7542/20000] Train-Loss: 0.5180443898405298 Test-Loss: 0.16374101415911183 Train-f1: 0.46779387280634166 Test-f1: 0.46554019457245266 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [7543/20000] Train-Loss: 0.5231004385216468 Test-Loss: 0.1596803150759914 Train-f1: 0.4559772662688264 Test-f1: 0.446829268292683 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7544/20000] Train-Loss: 0.5145106721399437 Test-Loss: 0.1668596429872132 Train-f1: 0.47469550269697275 Test-f1: 0.4419047619047619 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7545/20000] Train-Loss: 0.5250155398042857 Test-Loss: 0.1651336426366509 Train-f1: 0.45317895747502296 Test-f1: 0.44558629776021075 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7546/20000] Train-Loss: 0.5222006694827542 Test-Loss: 0.15828231714788696 Train-f1: 0.4613723478371018 Test-f1: 0.43650793650793657 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7547/20000] Train-Loss: 0.5250727810737196 Test-Loss: 0.15901766204675374 Train-f1: 0.4598668467135093 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7548/20000] Train-Loss: 0.5243890122780528 Test-Loss: 0.16086601540740403 Train-f1: 0.47088910257420746 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7549/20000] Train-Loss: 0.5227783591879122 Test-Loss: 0.1670728044560033 Train-f1: 0.45556915606521675 Test-f1: 0.44458402965865657 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [7550/20000] Train-Loss: 0.523460814118785 Test-Loss: 0.16000036048535393 Train-f1: 0.47068102211643054 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7551/20000] Train-Loss: 0.5208651643409137 Test-Loss: 0.1622794700813745 Train-f1: 0.470772777946691 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7552/20000] Train-Loss: 0.5212173022048849 Test-Loss: 0.16054187037952242 Train-f1: 0.4608511224210086 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7553/20000] Train-Loss: 0.5212612548600019 Test-Loss: 0.16050328061222333 Train-f1: 0.47045815392041046 Test-f1: 0.43574229691876754 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7554/20000] Train-Loss: 0.5161017556835362 Test-Loss: 0.15874854545496825 Train-f1: 0.4720512791331409 Test-f1: 0.4156933389191454 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7555/20000] Train-Loss: 0.5210384579757785 Test-Loss: 0.15994792467941762 Train-f1: 0.46831988919932177 Test-f1: 0.4144537815126051 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7556/20000] Train-Loss: 0.5219601455270878 Test-Loss: 0.15899343398164795 Train-f1: 0.4606552112662655 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7557/20000] Train-Loss: 0.5233220857574038 Test-Loss: 0.1589214333765975 Train-f1: 0.470247860536403 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7558/20000] Train-Loss: 0.5209220481062822 Test-Loss: 0.15972541512302516 Train-f1: 0.460479886638959 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7559/20000] Train-Loss: 0.5230742579519571 Test-Loss: 0.16231609796357022 Train-f1: 0.4676010110632676 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7560/20000] Train-Loss: 0.5190163452771078 Test-Loss: 0.1608677802447136 Train-f1: 0.4629819524200164 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7561/20000] Train-Loss: 0.51890430200972 Test-Loss: 0.16058600508360166 Train-f1: 0.46286304118876825 Test-f1: 0.4457303936373703 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7562/20000] Train-Loss: 0.5222108357834286 Test-Loss: 0.16042766918426016 Train-f1: 0.4659419026319447 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7563/20000] Train-Loss: 0.5209174819012945 Test-Loss: 0.16073694304447825 Train-f1: 0.4659616026565576 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7564/20000] Train-Loss: 0.522642728755084 Test-Loss: 0.16114769761271702 Train-f1: 0.4695445095445095 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7565/20000] Train-Loss: 0.5213625982563349 Test-Loss: 0.16019204921206742 Train-f1: 0.4730959595959595 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7566/20000] Train-Loss: 0.5219440928150594 Test-Loss: 0.15928135168951563 Train-f1: 0.46879717719159403 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7567/20000] Train-Loss: 0.521323486009025 Test-Loss: 0.1612147197169923 Train-f1: 0.4679755751494882 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7568/20000] Train-Loss: 0.5205466026532959 Test-Loss: 0.15820276152274873 Train-f1: 0.46933024893413566 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7569/20000] Train-Loss: 0.5216973502553834 Test-Loss: 0.15867931727312878 Train-f1: 0.46531818181818174 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7570/20000] Train-Loss: 0.5227263367807437 Test-Loss: 0.1591780987736061 Train-f1: 0.4572283368805108 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7571/20000] Train-Loss: 0.5213939185862022 Test-Loss: 0.1608279045956644 Train-f1: 0.4633245197623458 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7572/20000] Train-Loss: 0.520302951126209 Test-Loss: 0.16155641156460707 Train-f1: 0.45878505751548493 Test-f1: 0.46554019457245266 Train-acc: 0.7814569536423841 Test-acc: 0.7894736842105263\n",
      "Epoch [7573/20000] Train-Loss: 0.520200383718038 Test-Loss: 0.1710364718103017 Train-f1: 0.46897676428362345 Test-f1: 0.4282828282828284 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7574/20000] Train-Loss: 0.5218160653442956 Test-Loss: 0.15822492847321493 Train-f1: 0.4628366159973186 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7575/20000] Train-Loss: 0.5167711720735462 Test-Loss: 0.16022195542058712 Train-f1: 0.4723423655094686 Test-f1: 0.45564935064935064 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7576/20000] Train-Loss: 0.5211270755953242 Test-Loss: 0.15966072129327716 Train-f1: 0.46690586221272135 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7577/20000] Train-Loss: 0.5182541085533499 Test-Loss: 0.15858807474284667 Train-f1: 0.4658040057713051 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7578/20000] Train-Loss: 0.51753389937152 Test-Loss: 0.15845860623697622 Train-f1: 0.4814139765560258 Test-f1: 0.429005943840951 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [7579/20000] Train-Loss: 0.5231439737120845 Test-Loss: 0.16347658485786815 Train-f1: 0.4669072552686856 Test-f1: 0.4356787406570312 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7580/20000] Train-Loss: 0.52288359604289 Test-Loss: 0.16711342705107163 Train-f1: 0.46325623507066815 Test-f1: 0.4426916799402199 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7581/20000] Train-Loss: 0.5214819111017497 Test-Loss: 0.16271525265408968 Train-f1: 0.4626600180794915 Test-f1: 0.45564935064935064 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7582/20000] Train-Loss: 0.5236814152087657 Test-Loss: 0.1621225868373529 Train-f1: 0.4609739562808155 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7583/20000] Train-Loss: 0.5216149781041588 Test-Loss: 0.1587982637626839 Train-f1: 0.4706266832278542 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7584/20000] Train-Loss: 0.520704813207777 Test-Loss: 0.15928289207929425 Train-f1: 0.4711464518917935 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7585/20000] Train-Loss: 0.52099907637296 Test-Loss: 0.16036860861268648 Train-f1: 0.4558986636265754 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7586/20000] Train-Loss: 0.5206395077498905 Test-Loss: 0.16613849022901228 Train-f1: 0.4568548970686367 Test-f1: 0.4235123085869355 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [7587/20000] Train-Loss: 0.5174046760189718 Test-Loss: 0.16583943830001138 Train-f1: 0.463605645987189 Test-f1: 0.4235123085869355 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [7588/20000] Train-Loss: 0.5205076385790697 Test-Loss: 0.1589688980116946 Train-f1: 0.45713063243749164 Test-f1: 0.4576190476190477 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7589/20000] Train-Loss: 0.5205622264104234 Test-Loss: 0.16401921944104844 Train-f1: 0.4679755751494882 Test-f1: 0.4401493930905696 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7590/20000] Train-Loss: 0.520661268918967 Test-Loss: 0.15846735756185698 Train-f1: 0.46642542268767073 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7591/20000] Train-Loss: 0.5203031976346673 Test-Loss: 0.15935723936432353 Train-f1: 0.4628335536685819 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7592/20000] Train-Loss: 0.5217408391640459 Test-Loss: 0.16343647119484148 Train-f1: 0.45719750400459036 Test-f1: 0.41951447245564893 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7593/20000] Train-Loss: 0.5164276092322577 Test-Loss: 0.16050743534557732 Train-f1: 0.46760315665650226 Test-f1: 0.45564935064935064 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7594/20000] Train-Loss: 0.5226696535642876 Test-Loss: 0.17177723697487896 Train-f1: 0.46396321303960003 Test-f1: 0.4385387501666571 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [7595/20000] Train-Loss: 0.5189943732709728 Test-Loss: 0.15810786561657422 Train-f1: 0.4660182467635884 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7596/20000] Train-Loss: 0.519902089690904 Test-Loss: 0.1623792318022586 Train-f1: 0.46569368916510534 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7597/20000] Train-Loss: 0.5190359924426541 Test-Loss: 0.15939829607440723 Train-f1: 0.46835177647194204 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7598/20000] Train-Loss: 0.5161511713155129 Test-Loss: 0.1605603097037593 Train-f1: 0.4629819524200164 Test-f1: 0.4457303936373703 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7599/20000] Train-Loss: 0.5241923985309261 Test-Loss: 0.15971795959539606 Train-f1: 0.46344852501841116 Test-f1: 0.4469172932330827 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7600/20000] Train-Loss: 0.5215853745438228 Test-Loss: 0.1592644318994635 Train-f1: 0.46779643766388884 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7601/20000] Train-Loss: 0.5200248334654984 Test-Loss: 0.15855992356783952 Train-f1: 0.46835177647194204 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7602/20000] Train-Loss: 0.5213864950842968 Test-Loss: 0.15881817146327445 Train-f1: 0.4651444901291809 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7603/20000] Train-Loss: 0.517958198857638 Test-Loss: 0.1598258883622273 Train-f1: 0.4634954435949295 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7604/20000] Train-Loss: 0.5197050383770054 Test-Loss: 0.16411969624133763 Train-f1: 0.4655120348165912 Test-f1: 0.4419047619047619 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7605/20000] Train-Loss: 0.519683229830777 Test-Loss: 0.16004756275721224 Train-f1: 0.4631401724569426 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7606/20000] Train-Loss: 0.5196234311555198 Test-Loss: 0.15847040165573037 Train-f1: 0.46897676428362345 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7607/20000] Train-Loss: 0.5190865192343301 Test-Loss: 0.15923221023882903 Train-f1: 0.46531405395424646 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7608/20000] Train-Loss: 0.5161188270851762 Test-Loss: 0.16294403791051035 Train-f1: 0.47207528889813694 Test-f1: 0.44027149321266973 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7609/20000] Train-Loss: 0.5229903896969149 Test-Loss: 0.16234664116912356 Train-f1: 0.4558648291634628 Test-f1: 0.44683117900509206 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7610/20000] Train-Loss: 0.5207543256850947 Test-Loss: 0.1581301514729997 Train-f1: 0.4698739309471017 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7611/20000] Train-Loss: 0.5191534182173013 Test-Loss: 0.15996081184818856 Train-f1: 0.46779643766388884 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7612/20000] Train-Loss: 0.519041315071977 Test-Loss: 0.15796237280664321 Train-f1: 0.4665393577720943 Test-f1: 0.41766233766233773 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7613/20000] Train-Loss: 0.5207311673403535 Test-Loss: 0.16085732755307341 Train-f1: 0.4698914323086985 Test-f1: 0.44951827242524917 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7614/20000] Train-Loss: 0.5150474385412733 Test-Loss: 0.15878586340665127 Train-f1: 0.466487070190395 Test-f1: 0.4156933389191454 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7615/20000] Train-Loss: 0.5205943481992032 Test-Loss: 0.1654245827598293 Train-f1: 0.4666107715331494 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [7616/20000] Train-Loss: 0.5151667297853108 Test-Loss: 0.16224826867475822 Train-f1: 0.4659419026319447 Test-f1: 0.453416149068323 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7617/20000] Train-Loss: 0.520511270678133 Test-Loss: 0.15780881603163419 Train-f1: 0.459634069289681 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7618/20000] Train-Loss: 0.5189048475082908 Test-Loss: 0.158016641243998 Train-f1: 0.459460261988785 Test-f1: 0.41766233766233773 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7619/20000] Train-Loss: 0.521219707813822 Test-Loss: 0.15900817495135708 Train-f1: 0.4681109599574141 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7620/20000] Train-Loss: 0.5179382424578638 Test-Loss: 0.15811062777659604 Train-f1: 0.47093226821707257 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7621/20000] Train-Loss: 0.518487993253592 Test-Loss: 0.16030334989099365 Train-f1: 0.47154589371980676 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7622/20000] Train-Loss: 0.5207363542317089 Test-Loss: 0.1590664410282447 Train-f1: 0.46713307396537207 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7623/20000] Train-Loss: 0.5205125351836857 Test-Loss: 0.16317656744395403 Train-f1: 0.46703417623061405 Test-f1: 0.45067664508123617 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7624/20000] Train-Loss: 0.5192259742830552 Test-Loss: 0.15889351865978288 Train-f1: 0.4677831897567156 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7625/20000] Train-Loss: 0.520304195178617 Test-Loss: 0.15912273745237981 Train-f1: 0.4661826560100632 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7626/20000] Train-Loss: 0.5208074152039617 Test-Loss: 0.15876545296402275 Train-f1: 0.46850937335719456 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7627/20000] Train-Loss: 0.5177389327833802 Test-Loss: 0.1578437005350583 Train-f1: 0.4605165090432708 Test-f1: 0.44000000000000006 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7628/20000] Train-Loss: 0.5198665005880231 Test-Loss: 0.15994163707798686 Train-f1: 0.4569787498888592 Test-f1: 0.4503312334891282 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7629/20000] Train-Loss: 0.5175569241869704 Test-Loss: 0.15826739299948114 Train-f1: 0.47182848818797896 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7630/20000] Train-Loss: 0.5215483602646422 Test-Loss: 0.1578120364405868 Train-f1: 0.4602041746422387 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7631/20000] Train-Loss: 0.5209083907359463 Test-Loss: 0.1583772054431949 Train-f1: 0.4615326770576975 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7632/20000] Train-Loss: 0.5232497527117238 Test-Loss: 0.15933012987056688 Train-f1: 0.46229714142140904 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7633/20000] Train-Loss: 0.5195854392249877 Test-Loss: 0.1578819335462315 Train-f1: 0.4713108611382683 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7634/20000] Train-Loss: 0.5177366856598038 Test-Loss: 0.1583520111969411 Train-f1: 0.471574166881026 Test-f1: 0.42564459930313586 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [7635/20000] Train-Loss: 0.5194341999940107 Test-Loss: 0.1577554169309883 Train-f1: 0.475553899522572 Test-f1: 0.4469172932330827 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7636/20000] Train-Loss: 0.5162614768487531 Test-Loss: 0.15774655368037407 Train-f1: 0.471574166881026 Test-f1: 0.41766233766233773 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7637/20000] Train-Loss: 0.5196004325239532 Test-Loss: 0.1591751038822459 Train-f1: 0.4558635759072772 Test-f1: 0.4124675324675325 Train-acc: 0.7748344370860927 Test-acc: 0.7236842105263158\n",
      "Epoch [7638/20000] Train-Loss: 0.5210688168942191 Test-Loss: 0.15788749484151204 Train-f1: 0.47519602914908077 Test-f1: 0.41766233766233773 Train-acc: 0.8013245033112583 Test-acc: 0.7236842105263158\n",
      "Epoch [7639/20000] Train-Loss: 0.5208111331631171 Test-Loss: 0.16373019920436516 Train-f1: 0.4651470233332063 Test-f1: 0.4404334365325077 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7640/20000] Train-Loss: 0.5167641849484365 Test-Loss: 0.15810049948100152 Train-f1: 0.47921130106263343 Test-f1: 0.44709236709236705 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [7641/20000] Train-Loss: 0.5189272105679608 Test-Loss: 0.16131183344696376 Train-f1: 0.45266217566773725 Test-f1: 0.4576190476190477 Train-acc: 0.7715231788079471 Test-acc: 0.7763157894736842\n",
      "Epoch [7642/20000] Train-Loss: 0.5170605903716473 Test-Loss: 0.15815662871731095 Train-f1: 0.4577767830158736 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7643/20000] Train-Loss: 0.5203472038820836 Test-Loss: 0.16409374679565356 Train-f1: 0.46053252821306734 Test-f1: 0.44027149321266973 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7644/20000] Train-Loss: 0.5198113516789875 Test-Loss: 0.16380490726756208 Train-f1: 0.47064033261385846 Test-f1: 0.4401493930905696 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7645/20000] Train-Loss: 0.5239184768482713 Test-Loss: 0.1579382963775392 Train-f1: 0.46835177647194204 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7646/20000] Train-Loss: 0.51836673302014 Test-Loss: 0.1624113306272849 Train-f1: 0.46291591138732757 Test-f1: 0.4404334365325077 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7647/20000] Train-Loss: 0.5209951179461637 Test-Loss: 0.15780544528063106 Train-f1: 0.468803212089597 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7648/20000] Train-Loss: 0.5186623771500634 Test-Loss: 0.1592721871807982 Train-f1: 0.4606552112662655 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7649/20000] Train-Loss: 0.5168652079043933 Test-Loss: 0.15918008562719957 Train-f1: 0.4745564413828185 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7650/20000] Train-Loss: 0.5164607826078375 Test-Loss: 0.16895783281524276 Train-f1: 0.45822994210090984 Test-f1: 0.4280288816874183 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [7651/20000] Train-Loss: 0.5202693453875559 Test-Loss: 0.15755505724012897 Train-f1: 0.46864230546327834 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7652/20000] Train-Loss: 0.5183147952157591 Test-Loss: 0.1576153706489651 Train-f1: 0.4729091718915955 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7653/20000] Train-Loss: 0.5179179788010504 Test-Loss: 0.1584076438308586 Train-f1: 0.46939796470482387 Test-f1: 0.4124675324675325 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7654/20000] Train-Loss: 0.5174449084705249 Test-Loss: 0.1576935369818167 Train-f1: 0.4659419026319447 Test-f1: 0.429005943840951 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7655/20000] Train-Loss: 0.5167544552130546 Test-Loss: 0.16058390008989631 Train-f1: 0.46006583839156556 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7656/20000] Train-Loss: 0.5203670281863287 Test-Loss: 0.15911987867383645 Train-f1: 0.4733069000091149 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7657/20000] Train-Loss: 0.5183681595784484 Test-Loss: 0.16207171006228033 Train-f1: 0.4632029819669025 Test-f1: 0.45564935064935064 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7658/20000] Train-Loss: 0.5218125867553892 Test-Loss: 0.1637017415183669 Train-f1: 0.4603842658177074 Test-f1: 0.4401493930905696 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7659/20000] Train-Loss: 0.5160163268680039 Test-Loss: 0.165501369750552 Train-f1: 0.45808840460787864 Test-f1: 0.4341216991963261 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [7660/20000] Train-Loss: 0.5183231114693078 Test-Loss: 0.16002019330134074 Train-f1: 0.46821117978169846 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7661/20000] Train-Loss: 0.5161086519660745 Test-Loss: 0.16225985686739175 Train-f1: 0.47250898652339035 Test-f1: 0.4356787406570312 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7662/20000] Train-Loss: 0.5193082938695875 Test-Loss: 0.16346279812289236 Train-f1: 0.45271318073569644 Test-f1: 0.4404334365325077 Train-acc: 0.7715231788079471 Test-acc: 0.75\n",
      "Epoch [7663/20000] Train-Loss: 0.5250763222079001 Test-Loss: 0.16273094198244664 Train-f1: 0.46297557514948817 Test-f1: 0.44558629776021075 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7664/20000] Train-Loss: 0.52039927855444 Test-Loss: 0.1664881084447698 Train-f1: 0.4699634278764712 Test-f1: 0.4593201178892971 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7665/20000] Train-Loss: 0.5178765537294877 Test-Loss: 0.15963945714192662 Train-f1: 0.4633475171721545 Test-f1: 0.460952380952381 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7666/20000] Train-Loss: 0.5195186326594072 Test-Loss: 0.15757580686203687 Train-f1: 0.47027615862398464 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7667/20000] Train-Loss: 0.5210292623683307 Test-Loss: 0.15760654336930605 Train-f1: 0.45738116955508257 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7668/20000] Train-Loss: 0.515823617102304 Test-Loss: 0.1597068025130981 Train-f1: 0.46703417623061405 Test-f1: 0.45564935064935064 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7669/20000] Train-Loss: 0.5199523702909952 Test-Loss: 0.16223040053001325 Train-f1: 0.46344852501841116 Test-f1: 0.44558629776021075 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7670/20000] Train-Loss: 0.5130483022853594 Test-Loss: 0.15996016916646671 Train-f1: 0.46874675857416576 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7671/20000] Train-Loss: 0.5170700094920425 Test-Loss: 0.1578332284772271 Train-f1: 0.46566340908106146 Test-f1: 0.43650793650793657 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7672/20000] Train-Loss: 0.5217443824852241 Test-Loss: 0.1575817673777698 Train-f1: 0.4651340208540332 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7673/20000] Train-Loss: 0.5177330589751852 Test-Loss: 0.15884745468848271 Train-f1: 0.47116259613014044 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7674/20000] Train-Loss: 0.5186330297041168 Test-Loss: 0.15811208152710993 Train-f1: 0.465333553668582 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7675/20000] Train-Loss: 0.5168139622622625 Test-Loss: 0.16099172271061085 Train-f1: 0.4681759672564114 Test-f1: 0.45564935064935064 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7676/20000] Train-Loss: 0.5204691698730568 Test-Loss: 0.15781868525005455 Train-f1: 0.4692375738159562 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7677/20000] Train-Loss: 0.5126597013095552 Test-Loss: 0.15941113592967082 Train-f1: 0.4696518436662475 Test-f1: 0.4457303936373703 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7678/20000] Train-Loss: 0.5192016007043292 Test-Loss: 0.1579025765853776 Train-f1: 0.4654096505194129 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7679/20000] Train-Loss: 0.5155030685150361 Test-Loss: 0.1585703160291292 Train-f1: 0.47134730803735003 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7680/20000] Train-Loss: 0.5161059747009231 Test-Loss: 0.16040340159195934 Train-f1: 0.46642542268767073 Test-f1: 0.4144537815126051 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7681/20000] Train-Loss: 0.5195957276396235 Test-Loss: 0.15945189905858778 Train-f1: 0.46531405395424646 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7682/20000] Train-Loss: 0.5170400687200514 Test-Loss: 0.16156649766471876 Train-f1: 0.4749407114624506 Test-f1: 0.453416149068323 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7683/20000] Train-Loss: 0.5163956301655903 Test-Loss: 0.15751981078598276 Train-f1: 0.47146886677572597 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7684/20000] Train-Loss: 0.5191201071132399 Test-Loss: 0.16091842024822192 Train-f1: 0.46335861247307886 Test-f1: 0.45726405090137856 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7685/20000] Train-Loss: 0.5123343977914112 Test-Loss: 0.16445348608717636 Train-f1: 0.4732627649957717 Test-f1: 0.4532716620413197 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7686/20000] Train-Loss: 0.518243297887161 Test-Loss: 0.15990985362477353 Train-f1: 0.4631620435954851 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7687/20000] Train-Loss: 0.5174997913647211 Test-Loss: 0.16552133148630407 Train-f1: 0.45959243676567973 Test-f1: 0.4532716620413197 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [7688/20000] Train-Loss: 0.5134347956138445 Test-Loss: 0.15741018168787876 Train-f1: 0.46635903031405174 Test-f1: 0.42564459930313586 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7689/20000] Train-Loss: 0.5181776912707208 Test-Loss: 0.1609761496420967 Train-f1: 0.4681488450468235 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7690/20000] Train-Loss: 0.5172198812251939 Test-Loss: 0.15958367686692637 Train-f1: 0.46353568617939783 Test-f1: 0.4469172932330827 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7691/20000] Train-Loss: 0.5176664209592359 Test-Loss: 0.15894563909611184 Train-f1: 0.4651444901291809 Test-f1: 0.42564459930313586 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7692/20000] Train-Loss: 0.5180290639440952 Test-Loss: 0.16268541801973513 Train-f1: 0.4653119656141994 Test-f1: 0.44027149321266973 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7693/20000] Train-Loss: 0.5191065831758658 Test-Loss: 0.15726920441890876 Train-f1: 0.4561784603680673 Test-f1: 0.4503312334891282 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7694/20000] Train-Loss: 0.5156139018027114 Test-Loss: 0.16220759979393254 Train-f1: 0.4712979536794967 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7695/20000] Train-Loss: 0.5100366060238637 Test-Loss: 0.1620990644234801 Train-f1: 0.47587163286306405 Test-f1: 0.4545938580861545 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [7696/20000] Train-Loss: 0.5219623371207321 Test-Loss: 0.16128204003340252 Train-f1: 0.4611264798443141 Test-f1: 0.45564935064935064 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7697/20000] Train-Loss: 0.514691348308502 Test-Loss: 0.16207713515781338 Train-f1: 0.4741733447001989 Test-f1: 0.44027149321266973 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [7698/20000] Train-Loss: 0.5167851297954018 Test-Loss: 0.15951409757669086 Train-f1: 0.4680748233478198 Test-f1: 0.43650793650793657 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7699/20000] Train-Loss: 0.5157222791379986 Test-Loss: 0.15994838732567462 Train-f1: 0.47643164289127016 Test-f1: 0.42564459930313586 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [7700/20000] Train-Loss: 0.5175125281276614 Test-Loss: 0.16047294986492472 Train-f1: 0.46228988047606345 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7701/20000] Train-Loss: 0.5171239844020641 Test-Loss: 0.15920063975305712 Train-f1: 0.4634112554112554 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7702/20000] Train-Loss: 0.5182707996797108 Test-Loss: 0.16636505863706819 Train-f1: 0.4647438682061248 Test-f1: 0.4341972308997648 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7703/20000] Train-Loss: 0.5185216949869171 Test-Loss: 0.1605740756673684 Train-f1: 0.4698161248310132 Test-f1: 0.45564935064935064 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7704/20000] Train-Loss: 0.5170059428949274 Test-Loss: 0.16238795331547012 Train-f1: 0.45825371982360597 Test-f1: 0.4466252587991718 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7705/20000] Train-Loss: 0.5151674596298199 Test-Loss: 0.15763709735949138 Train-f1: 0.46267837235228537 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7706/20000] Train-Loss: 0.5186970169929539 Test-Loss: 0.16252252662753056 Train-f1: 0.46858234932769094 Test-f1: 0.4404334365325077 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7707/20000] Train-Loss: 0.5160839333938324 Test-Loss: 0.15956566246999807 Train-f1: 0.4651444901291809 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7708/20000] Train-Loss: 0.5166421375617671 Test-Loss: 0.15750642984282925 Train-f1: 0.46646006713534743 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7709/20000] Train-Loss: 0.516462471030262 Test-Loss: 0.1603634585540974 Train-f1: 0.4579634779278713 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7710/20000] Train-Loss: 0.514369964145619 Test-Loss: 0.16829084694598445 Train-f1: 0.4678285626051991 Test-f1: 0.4426916799402199 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7711/20000] Train-Loss: 0.5175828892158897 Test-Loss: 0.16005260306997632 Train-f1: 0.45810744056562863 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7712/20000] Train-Loss: 0.5168994896208962 Test-Loss: 0.16108139846707983 Train-f1: 0.46070320395526865 Test-f1: 0.45726405090137856 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7713/20000] Train-Loss: 0.5178044507202159 Test-Loss: 0.1624661692854344 Train-f1: 0.4597538720157509 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7714/20000] Train-Loss: 0.5169027018199507 Test-Loss: 0.1620911819033671 Train-f1: 0.460040404040404 Test-f1: 0.4356787406570312 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7715/20000] Train-Loss: 0.5193463100213943 Test-Loss: 0.1572442474448012 Train-f1: 0.47947167477853403 Test-f1: 0.44709236709236705 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [7716/20000] Train-Loss: 0.5148535721595217 Test-Loss: 0.16872481569357572 Train-f1: 0.4631866437601867 Test-f1: 0.4385387501666571 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7717/20000] Train-Loss: 0.5168836258184941 Test-Loss: 0.17195929394048587 Train-f1: 0.4602064402158847 Test-f1: 0.4282828282828284 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [7718/20000] Train-Loss: 0.517264348078417 Test-Loss: 0.15995050726685656 Train-f1: 0.46159152118000446 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7719/20000] Train-Loss: 0.51658829758375 Test-Loss: 0.15956195990930933 Train-f1: 0.4629677378582488 Test-f1: 0.45564935064935064 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7720/20000] Train-Loss: 0.5160780535138869 Test-Loss: 0.15801998020842487 Train-f1: 0.46664777679798936 Test-f1: 0.4470923670923671 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7721/20000] Train-Loss: 0.5185061587928008 Test-Loss: 0.16089976430964956 Train-f1: 0.4647438682061248 Test-f1: 0.4356787406570312 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7722/20000] Train-Loss: 0.5204664162353647 Test-Loss: 0.1570017463072716 Train-f1: 0.46118254639925993 Test-f1: 0.429005943840951 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7723/20000] Train-Loss: 0.5171791489188092 Test-Loss: 0.15905403435742974 Train-f1: 0.46091027697185305 Test-f1: 0.45564935064935064 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7724/20000] Train-Loss: 0.5171889991184138 Test-Loss: 0.15742575121483304 Train-f1: 0.47557954610819114 Test-f1: 0.4576190476190477 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [7725/20000] Train-Loss: 0.5153804556299869 Test-Loss: 0.15939231921933097 Train-f1: 0.4729855072463768 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7726/20000] Train-Loss: 0.5171228603719332 Test-Loss: 0.16096428939058463 Train-f1: 0.4608511224210086 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7727/20000] Train-Loss: 0.5133813711846904 Test-Loss: 0.1620217845484044 Train-f1: 0.47576928450761474 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [7728/20000] Train-Loss: 0.5167272261102331 Test-Loss: 0.15787024817149947 Train-f1: 0.45842943812165027 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7729/20000] Train-Loss: 0.5162979968236516 Test-Loss: 0.16333903884468484 Train-f1: 0.46301153220857927 Test-f1: 0.4401493930905696 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7730/20000] Train-Loss: 0.5123757548475412 Test-Loss: 0.15676360369349182 Train-f1: 0.46553962954539463 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7731/20000] Train-Loss: 0.5171208192141431 Test-Loss: 0.16401478967005065 Train-f1: 0.46271989791267887 Test-f1: 0.44027149321266973 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7732/20000] Train-Loss: 0.5141508346858665 Test-Loss: 0.15739806454576824 Train-f1: 0.4585627705627705 Test-f1: 0.4156933389191454 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [7733/20000] Train-Loss: 0.5173992953281794 Test-Loss: 0.1612345434470342 Train-f1: 0.47248845100626324 Test-f1: 0.45564935064935064 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7734/20000] Train-Loss: 0.5166950976792123 Test-Loss: 0.15731873545857003 Train-f1: 0.4661123427496626 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7735/20000] Train-Loss: 0.5150898569223319 Test-Loss: 0.1586299969690663 Train-f1: 0.4660070325467684 Test-f1: 0.42564459930313586 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7736/20000] Train-Loss: 0.5091850257742286 Test-Loss: 0.17111289440625774 Train-f1: 0.4645377585377585 Test-f1: 0.4308206864835487 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7737/20000] Train-Loss: 0.5149295692083679 Test-Loss: 0.16210898273991609 Train-f1: 0.4587133679180996 Test-f1: 0.44678185391418274 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7738/20000] Train-Loss: 0.5160665425895234 Test-Loss: 0.15756325324197368 Train-f1: 0.47272689334661155 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7739/20000] Train-Loss: 0.5142422245193682 Test-Loss: 0.15703020338486331 Train-f1: 0.4471357432376541 Test-f1: 0.4503312334891282 Train-acc: 0.7649006622516556 Test-acc: 0.7631578947368421\n",
      "Epoch [7740/20000] Train-Loss: 0.5183831470935323 Test-Loss: 0.15706191788528354 Train-f1: 0.46493001188056143 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7741/20000] Train-Loss: 0.5156683017099293 Test-Loss: 0.15766553829464622 Train-f1: 0.468471466942883 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7742/20000] Train-Loss: 0.5142990366936026 Test-Loss: 0.16219351803887216 Train-f1: 0.45971280893020017 Test-f1: 0.44027149321266973 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [7743/20000] Train-Loss: 0.5150137603051941 Test-Loss: 0.15692413686018 Train-f1: 0.467183919884241 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7744/20000] Train-Loss: 0.5156431290750267 Test-Loss: 0.1571534019078567 Train-f1: 0.46271989791267887 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7745/20000] Train-Loss: 0.5144819419831609 Test-Loss: 0.15863248599227528 Train-f1: 0.4639594551789674 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7746/20000] Train-Loss: 0.5181722817357189 Test-Loss: 0.16081206517692534 Train-f1: 0.4577767830158736 Test-f1: 0.44683117900509206 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7747/20000] Train-Loss: 0.5151903011023523 Test-Loss: 0.15856288441663116 Train-f1: 0.46559595959595956 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7748/20000] Train-Loss: 0.5191772703101553 Test-Loss: 0.16247893910205738 Train-f1: 0.4625937185781037 Test-f1: 0.44027149321266973 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7749/20000] Train-Loss: 0.5191064755981711 Test-Loss: 0.15693639307546228 Train-f1: 0.46137707282809337 Test-f1: 0.42564459930313586 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7750/20000] Train-Loss: 0.5192265766303577 Test-Loss: 0.15746174498452356 Train-f1: 0.46881765253518976 Test-f1: 0.42564459930313586 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7751/20000] Train-Loss: 0.509908009772003 Test-Loss: 0.1639679205790757 Train-f1: 0.46569368916510534 Test-f1: 0.44027149321266973 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7752/20000] Train-Loss: 0.5182147404824263 Test-Loss: 0.15743878111662296 Train-f1: 0.46247508618157535 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7753/20000] Train-Loss: 0.5190336084609871 Test-Loss: 0.1589589279570463 Train-f1: 0.47363927319997623 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7754/20000] Train-Loss: 0.5147393923172858 Test-Loss: 0.1593045258861215 Train-f1: 0.4660070325467684 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7755/20000] Train-Loss: 0.5178331057923738 Test-Loss: 0.15739961293185184 Train-f1: 0.47402702249843875 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7756/20000] Train-Loss: 0.5142110337550992 Test-Loss: 0.16049682956306185 Train-f1: 0.47496619292147957 Test-f1: 0.45564935064935064 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [7757/20000] Train-Loss: 0.5177070046891303 Test-Loss: 0.15722262247405694 Train-f1: 0.46656385056385047 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7758/20000] Train-Loss: 0.5167003287312002 Test-Loss: 0.15735875752983497 Train-f1: 0.46267837235228537 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7759/20000] Train-Loss: 0.5140791373892535 Test-Loss: 0.15852435074458593 Train-f1: 0.46897676428362345 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7760/20000] Train-Loss: 0.5176762895715044 Test-Loss: 0.1576541805766294 Train-f1: 0.46719367588932803 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7761/20000] Train-Loss: 0.5146994046120169 Test-Loss: 0.1584917814800495 Train-f1: 0.45943273761892056 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7762/20000] Train-Loss: 0.5170571972645427 Test-Loss: 0.16069443324302626 Train-f1: 0.4650404040404041 Test-f1: 0.45564935064935064 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7763/20000] Train-Loss: 0.5202595504501402 Test-Loss: 0.15904887039020563 Train-f1: 0.4633475171721545 Test-f1: 0.43574229691876754 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7764/20000] Train-Loss: 0.5156096470368415 Test-Loss: 0.15732334830355565 Train-f1: 0.46497172033322265 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7765/20000] Train-Loss: 0.5141273346500689 Test-Loss: 0.15861921338044285 Train-f1: 0.4734673564854832 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7766/20000] Train-Loss: 0.5158409967510583 Test-Loss: 0.15700581903390906 Train-f1: 0.46909517531230893 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7767/20000] Train-Loss: 0.5139863492652764 Test-Loss: 0.15681147214669722 Train-f1: 0.4609861104974703 Test-f1: 0.45069597069597067 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7768/20000] Train-Loss: 0.5133052872326538 Test-Loss: 0.16071685216582524 Train-f1: 0.4704755751494882 Test-f1: 0.4509610604805302 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7769/20000] Train-Loss: 0.511087064300575 Test-Loss: 0.17763935091140434 Train-f1: 0.4712492447206609 Test-f1: 0.43675048355899426 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7770/20000] Train-Loss: 0.5181029087639398 Test-Loss: 0.1621359413367483 Train-f1: 0.4576066333527752 Test-f1: 0.4554347826086957 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7771/20000] Train-Loss: 0.511649824189647 Test-Loss: 0.15716576163794269 Train-f1: 0.47161812121999114 Test-f1: 0.4156933389191454 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7772/20000] Train-Loss: 0.5169470053374076 Test-Loss: 0.15702113039242385 Train-f1: 0.4700722924589714 Test-f1: 0.41766233766233773 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7773/20000] Train-Loss: 0.5145963245700771 Test-Loss: 0.1572281416650552 Train-f1: 0.4550749877831549 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7774/20000] Train-Loss: 0.5133303584991115 Test-Loss: 0.15718167012848813 Train-f1: 0.46228988047606345 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7775/20000] Train-Loss: 0.5131106395730194 Test-Loss: 0.1568546067398512 Train-f1: 0.4659281623334449 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7776/20000] Train-Loss: 0.5137604683281316 Test-Loss: 0.15726805949834757 Train-f1: 0.46547557514948823 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7777/20000] Train-Loss: 0.5149067358252954 Test-Loss: 0.15691951741364796 Train-f1: 0.47134730803735003 Test-f1: 0.429005943840951 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7778/20000] Train-Loss: 0.5165041420124312 Test-Loss: 0.15723198697477306 Train-f1: 0.4789222157884578 Test-f1: 0.4156933389191454 Train-acc: 0.804635761589404 Test-acc: 0.7236842105263158\n",
      "Epoch [7779/20000] Train-Loss: 0.5173025484701551 Test-Loss: 0.16006614348120624 Train-f1: 0.4667171383088686 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7780/20000] Train-Loss: 0.5140279545881734 Test-Loss: 0.16057710462679242 Train-f1: 0.476160474171225 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7781/20000] Train-Loss: 0.5184574415674917 Test-Loss: 0.1586885021788107 Train-f1: 0.4659419026319447 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7782/20000] Train-Loss: 0.5162095051147648 Test-Loss: 0.15792772102499186 Train-f1: 0.4647662772238782 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7783/20000] Train-Loss: 0.5133955960314778 Test-Loss: 0.15791661154216677 Train-f1: 0.46306427974390785 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7784/20000] Train-Loss: 0.5155959410505124 Test-Loss: 0.15694613646260747 Train-f1: 0.4707782929460823 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7785/20000] Train-Loss: 0.5199053902192191 Test-Loss: 0.15666799062746065 Train-f1: 0.4615326770576975 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7786/20000] Train-Loss: 0.51672714918838 Test-Loss: 0.1597594417614934 Train-f1: 0.460479886638959 Test-f1: 0.45726405090137856 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7787/20000] Train-Loss: 0.5144980314641636 Test-Loss: 0.16291715157283373 Train-f1: 0.4713778223528843 Test-f1: 0.45296803652968043 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7788/20000] Train-Loss: 0.510568743224014 Test-Loss: 0.1569564194698467 Train-f1: 0.46644616868979244 Test-f1: 0.4277159468438538 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7789/20000] Train-Loss: 0.5158927005162682 Test-Loss: 0.15854609875020778 Train-f1: 0.4615721300367147 Test-f1: 0.44951827242524917 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7790/20000] Train-Loss: 0.5170853201349642 Test-Loss: 0.15873347974879345 Train-f1: 0.4609739562808155 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7791/20000] Train-Loss: 0.5134222571455999 Test-Loss: 0.1571964352651596 Train-f1: 0.4706266832278542 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7792/20000] Train-Loss: 0.5137583836081249 Test-Loss: 0.1570675715181374 Train-f1: 0.4733853039175095 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7793/20000] Train-Loss: 0.512051843423398 Test-Loss: 0.1589335432236611 Train-f1: 0.4697089771542327 Test-f1: 0.45119841858166543 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7794/20000] Train-Loss: 0.5116824655480542 Test-Loss: 0.16437909855090646 Train-f1: 0.4628728895109789 Test-f1: 0.4010579513499695 Train-acc: 0.7814569536423841 Test-acc: 0.6973684210526315\n",
      "Epoch [7795/20000] Train-Loss: 0.513691530929917 Test-Loss: 0.16072246581137203 Train-f1: 0.4729393155676943 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7796/20000] Train-Loss: 0.5142920332567236 Test-Loss: 0.15957387209902196 Train-f1: 0.46531818181818174 Test-f1: 0.43574229691876754 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7797/20000] Train-Loss: 0.5128577104212247 Test-Loss: 0.1598182943832315 Train-f1: 0.46306427974390785 Test-f1: 0.46554019457245266 Train-acc: 0.7847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [7798/20000] Train-Loss: 0.5154436988583205 Test-Loss: 0.1572216182692229 Train-f1: 0.45844686579679833 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7799/20000] Train-Loss: 0.5117718733824306 Test-Loss: 0.15743192007765958 Train-f1: 0.4598280056189399 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7800/20000] Train-Loss: 0.5142651617060296 Test-Loss: 0.15667456968407645 Train-f1: 0.4626699244920157 Test-f1: 0.4469172932330827 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7801/20000] Train-Loss: 0.5147350264169605 Test-Loss: 0.15971895182157625 Train-f1: 0.46517837235228543 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7802/20000] Train-Loss: 0.5165672455385976 Test-Loss: 0.15820709366141328 Train-f1: 0.4631401724569426 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7803/20000] Train-Loss: 0.5174053021113798 Test-Loss: 0.16927585654970217 Train-f1: 0.4738675403271676 Test-f1: 0.4278787878787879 Train-acc: 0.7980132450331126 Test-acc: 0.7236842105263158\n",
      "Epoch [7804/20000] Train-Loss: 0.5103551561097207 Test-Loss: 0.16084914072948725 Train-f1: 0.4774114175740591 Test-f1: 0.46554019457245266 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [7805/20000] Train-Loss: 0.5171606347904627 Test-Loss: 0.1568053383602511 Train-f1: 0.46353568617939783 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7806/20000] Train-Loss: 0.5138480567669196 Test-Loss: 0.1567330888990687 Train-f1: 0.46569368916510534 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7807/20000] Train-Loss: 0.5133072134622997 Test-Loss: 0.16379510540577433 Train-f1: 0.4658040057713051 Test-f1: 0.4344337363247945 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7808/20000] Train-Loss: 0.5147607704704202 Test-Loss: 0.16595804067956704 Train-f1: 0.470445863587321 Test-f1: 0.4341216991963261 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7809/20000] Train-Loss: 0.5071745220803068 Test-Loss: 0.16309873194209903 Train-f1: 0.47592428275658094 Test-f1: 0.40299282806745496 Train-acc: 0.7980132450331126 Test-acc: 0.6973684210526315\n",
      "Epoch [7810/20000] Train-Loss: 0.5138263605513953 Test-Loss: 0.15800229075929817 Train-f1: 0.46858234932769094 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7811/20000] Train-Loss: 0.5158671735974496 Test-Loss: 0.15873833773050752 Train-f1: 0.4681488450468235 Test-f1: 0.4576190476190477 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7812/20000] Train-Loss: 0.5138887001244651 Test-Loss: 0.15970300808098375 Train-f1: 0.46118301079505564 Test-f1: 0.44683117900509206 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7813/20000] Train-Loss: 0.514541241803896 Test-Loss: 0.15721476734034434 Train-f1: 0.4751700981944884 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7814/20000] Train-Loss: 0.5145502188563238 Test-Loss: 0.1574185147851741 Train-f1: 0.4603842658177074 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [7815/20000] Train-Loss: 0.5157943940881031 Test-Loss: 0.15679422003618682 Train-f1: 0.46874675857416576 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7816/20000] Train-Loss: 0.5127783973154697 Test-Loss: 0.15636147595191913 Train-f1: 0.45785262024808715 Test-f1: 0.4503312334891282 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7817/20000] Train-Loss: 0.5160527830313323 Test-Loss: 0.15949397105891325 Train-f1: 0.4691157090226071 Test-f1: 0.4252857311181807 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7818/20000] Train-Loss: 0.5142544620653392 Test-Loss: 0.1616796108059068 Train-f1: 0.4671978559361861 Test-f1: 0.44027149321266973 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [7819/20000] Train-Loss: 0.5114569541580324 Test-Loss: 0.1628619118687329 Train-f1: 0.4626600180794915 Test-f1: 0.4404334365325077 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7820/20000] Train-Loss: 0.5147871963887293 Test-Loss: 0.15896747267581762 Train-f1: 0.4625404040404041 Test-f1: 0.44683117900509206 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7821/20000] Train-Loss: 0.515064580967707 Test-Loss: 0.15909305589992198 Train-f1: 0.47590102718794985 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7822/20000] Train-Loss: 0.5165222484212009 Test-Loss: 0.15611682124606777 Train-f1: 0.4711464518917935 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7823/20000] Train-Loss: 0.5158857526624773 Test-Loss: 0.1614955065333552 Train-f1: 0.4679815898833728 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7824/20000] Train-Loss: 0.5117953546305111 Test-Loss: 0.15771260219579702 Train-f1: 0.48167126567126567 Test-f1: 0.446829268292683 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [7825/20000] Train-Loss: 0.5175840966456459 Test-Loss: 0.1564354044586445 Train-f1: 0.4507758566090317 Test-f1: 0.4469172932330827 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [7826/20000] Train-Loss: 0.5131490517696421 Test-Loss: 0.16040631581602002 Train-f1: 0.45898949981054393 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7827/20000] Train-Loss: 0.514441445473629 Test-Loss: 0.15706092969115537 Train-f1: 0.4679755751494882 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7828/20000] Train-Loss: 0.511352017684479 Test-Loss: 0.1563248628713137 Train-f1: 0.4606629534137653 Test-f1: 0.429005943840951 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [7829/20000] Train-Loss: 0.5168124453966415 Test-Loss: 0.15766611188217572 Train-f1: 0.4734888933240544 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7830/20000] Train-Loss: 0.5116602301731278 Test-Loss: 0.16539217441644619 Train-f1: 0.470445863587321 Test-f1: 0.4344337363247945 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7831/20000] Train-Loss: 0.5139436090172838 Test-Loss: 0.15674990659684132 Train-f1: 0.4660120743454077 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7832/20000] Train-Loss: 0.5114617446398083 Test-Loss: 0.1594857000489113 Train-f1: 0.4661363902508566 Test-f1: 0.44649416182596957 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7833/20000] Train-Loss: 0.5147681350490808 Test-Loss: 0.1584490315937096 Train-f1: 0.464623954727241 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7834/20000] Train-Loss: 0.5140941438064153 Test-Loss: 0.15903263351806932 Train-f1: 0.47821518712668887 Test-f1: 0.446829268292683 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [7835/20000] Train-Loss: 0.5157443966096242 Test-Loss: 0.1587903531991848 Train-f1: 0.4651444901291809 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7836/20000] Train-Loss: 0.5096376308011273 Test-Loss: 0.15640842551243753 Train-f1: 0.46665605574272045 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7837/20000] Train-Loss: 0.5129555006335241 Test-Loss: 0.1590668285951885 Train-f1: 0.46447217437757365 Test-f1: 0.4457303936373703 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7838/20000] Train-Loss: 0.5121464041685594 Test-Loss: 0.15686156699697204 Train-f1: 0.4766970214883842 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7839/20000] Train-Loss: 0.511613770178134 Test-Loss: 0.15861059499340271 Train-f1: 0.4733853039175095 Test-f1: 0.45564935064935064 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7840/20000] Train-Loss: 0.515714036484345 Test-Loss: 0.15989510065573873 Train-f1: 0.46308608540756974 Test-f1: 0.4466252587991718 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7841/20000] Train-Loss: 0.5137412400427803 Test-Loss: 0.1582817052546867 Train-f1: 0.4814139765560258 Test-f1: 0.446829268292683 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [7842/20000] Train-Loss: 0.5119379076191164 Test-Loss: 0.1567589175035834 Train-f1: 0.47643164289127016 Test-f1: 0.4469172932330827 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7843/20000] Train-Loss: 0.5135967483831455 Test-Loss: 0.1632967016745767 Train-f1: 0.465333553668582 Test-f1: 0.4404334365325077 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7844/20000] Train-Loss: 0.5135494690961979 Test-Loss: 0.16400788092864416 Train-f1: 0.4700922266139657 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7845/20000] Train-Loss: 0.5124851636972615 Test-Loss: 0.1584532724464906 Train-f1: 0.46850880749473517 Test-f1: 0.45564935064935064 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7846/20000] Train-Loss: 0.5152593921562912 Test-Loss: 0.16545821172443337 Train-f1: 0.44958396675787976 Test-f1: 0.43313627938737687 Train-acc: 0.7682119205298014 Test-acc: 0.7368421052631579\n",
      "Epoch [7847/20000] Train-Loss: 0.5145017666227757 Test-Loss: 0.16210917906008868 Train-f1: 0.468892298948565 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7848/20000] Train-Loss: 0.514186885732285 Test-Loss: 0.15607394349820905 Train-f1: 0.4763035868456165 Test-f1: 0.4469172932330827 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7849/20000] Train-Loss: 0.5135521386958244 Test-Loss: 0.15652485757514048 Train-f1: 0.4660070325467684 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7850/20000] Train-Loss: 0.5139029231762615 Test-Loss: 0.1569036694769499 Train-f1: 0.47835503626807974 Test-f1: 0.4469172932330827 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [7851/20000] Train-Loss: 0.5158324887516982 Test-Loss: 0.157053420099814 Train-f1: 0.4706504550770326 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7852/20000] Train-Loss: 0.5054008118252993 Test-Loss: 0.15897242970425937 Train-f1: 0.47033613347633346 Test-f1: 0.41257656885842797 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7853/20000] Train-Loss: 0.5187838221143076 Test-Loss: 0.15875453960659608 Train-f1: 0.46743879318505704 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7854/20000] Train-Loss: 0.5122973675884042 Test-Loss: 0.1561200509914285 Train-f1: 0.47044003608912643 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7855/20000] Train-Loss: 0.5105808774787198 Test-Loss: 0.1689765169527257 Train-f1: 0.46961791831357047 Test-f1: 0.4282828282828284 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7856/20000] Train-Loss: 0.5117871169865992 Test-Loss: 0.16329841914679305 Train-f1: 0.4716396158839367 Test-f1: 0.44027149321266973 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7857/20000] Train-Loss: 0.5147001669661248 Test-Loss: 0.1569016452124445 Train-f1: 0.46743490655685777 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7858/20000] Train-Loss: 0.5132137534851896 Test-Loss: 0.15802103582310564 Train-f1: 0.4710031125125034 Test-f1: 0.44951827242524917 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7859/20000] Train-Loss: 0.5135824848316757 Test-Loss: 0.15618364591498113 Train-f1: 0.45013253557440613 Test-f1: 0.4503312334891282 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [7860/20000] Train-Loss: 0.5158550123319207 Test-Loss: 0.15618227886421118 Train-f1: 0.4631620435954851 Test-f1: 0.429005943840951 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7861/20000] Train-Loss: 0.5126322728074205 Test-Loss: 0.1574170981926174 Train-f1: 0.46850937335719456 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7862/20000] Train-Loss: 0.5108942990663318 Test-Loss: 0.1600898655399802 Train-f1: 0.470445863587321 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7863/20000] Train-Loss: 0.5123663816823116 Test-Loss: 0.16065621923141815 Train-f1: 0.47643164289127016 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [7864/20000] Train-Loss: 0.5137866171096778 Test-Loss: 0.15857163814111694 Train-f1: 0.4653523392677589 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7865/20000] Train-Loss: 0.5075197172041682 Test-Loss: 0.15634543132038367 Train-f1: 0.4715066780335322 Test-f1: 0.429005943840951 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7866/20000] Train-Loss: 0.510347739884596 Test-Loss: 0.16440025657043314 Train-f1: 0.4698161248310132 Test-f1: 0.4532716620413197 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7867/20000] Train-Loss: 0.5119302465039993 Test-Loss: 0.16479522056034798 Train-f1: 0.46762259333572376 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [7868/20000] Train-Loss: 0.5105551583588178 Test-Loss: 0.17149511573436016 Train-f1: 0.4706504550770326 Test-f1: 0.44621212121212117 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7869/20000] Train-Loss: 0.5157819212454257 Test-Loss: 0.15691971317049094 Train-f1: 0.475952393713448 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7870/20000] Train-Loss: 0.51229120661404 Test-Loss: 0.16187988017908495 Train-f1: 0.4631156383804246 Test-f1: 0.4404334365325077 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [7871/20000] Train-Loss: 0.5115408734525461 Test-Loss: 0.15599567497685324 Train-f1: 0.462510011727403 Test-f1: 0.4469172932330827 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7872/20000] Train-Loss: 0.5094728340871832 Test-Loss: 0.1691460331428849 Train-f1: 0.46945124330061266 Test-f1: 0.4522149888853605 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7873/20000] Train-Loss: 0.5192844566240856 Test-Loss: 0.16202261613181473 Train-f1: 0.469127953127953 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7874/20000] Train-Loss: 0.5100802929768131 Test-Loss: 0.16067978608229969 Train-f1: 0.4778859009506767 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [7875/20000] Train-Loss: 0.5122186530869576 Test-Loss: 0.1637209807458447 Train-f1: 0.4762376163003214 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [7876/20000] Train-Loss: 0.5142330510819413 Test-Loss: 0.1567130912593494 Train-f1: 0.4654096505194129 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7877/20000] Train-Loss: 0.5120643585680527 Test-Loss: 0.15846845373487004 Train-f1: 0.4632673065193712 Test-f1: 0.4457303936373703 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7878/20000] Train-Loss: 0.5094944426372426 Test-Loss: 0.15682733726610804 Train-f1: 0.46561773097719 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7879/20000] Train-Loss: 0.5109502556397426 Test-Loss: 0.15641860235038088 Train-f1: 0.46613308877680043 Test-f1: 0.4144537815126051 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "best score_dx!!\n",
      "Epoch [7880/20000] Train-Loss: 0.5057369446189756 Test-Loss: 0.15591314525123337 Train-f1: 0.4665541574663683 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7881/20000] Train-Loss: 0.5144864305811234 Test-Loss: 0.1562455170457291 Train-f1: 0.4545417432129006 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7882/20000] Train-Loss: 0.5136212674951643 Test-Loss: 0.15631478902308615 Train-f1: 0.46554317777819954 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7883/20000] Train-Loss: 0.5114302392781986 Test-Loss: 0.15723293122016915 Train-f1: 0.47027615862398464 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7884/20000] Train-Loss: 0.5122066705550246 Test-Loss: 0.16142021400033704 Train-f1: 0.45918129348388936 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7885/20000] Train-Loss: 0.5114500340434179 Test-Loss: 0.15916166465069176 Train-f1: 0.47293033563701037 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7886/20000] Train-Loss: 0.5101504311426321 Test-Loss: 0.1587028964531374 Train-f1: 0.46866414290247993 Test-f1: 0.45726405090137856 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7887/20000] Train-Loss: 0.5113094947106013 Test-Loss: 0.15609904121326804 Train-f1: 0.4711464518917935 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7888/20000] Train-Loss: 0.5093272168985213 Test-Loss: 0.15572221196595964 Train-f1: 0.47635466157570505 Test-f1: 0.4469172932330827 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7889/20000] Train-Loss: 0.5115273322972403 Test-Loss: 0.1598913703042461 Train-f1: 0.4683430277054951 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7890/20000] Train-Loss: 0.5085807434525859 Test-Loss: 0.15711125136349455 Train-f1: 0.46325623507066815 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7891/20000] Train-Loss: 0.515553488662747 Test-Loss: 0.15975085992826593 Train-f1: 0.45534881987597337 Test-f1: 0.44683117900509206 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7892/20000] Train-Loss: 0.5130987901254025 Test-Loss: 0.158209614143475 Train-f1: 0.46379746900061763 Test-f1: 0.45564935064935064 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7893/20000] Train-Loss: 0.5104905234678824 Test-Loss: 0.15778723716687026 Train-f1: 0.46554317777819954 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7894/20000] Train-Loss: 0.5119968230399384 Test-Loss: 0.16429632906961594 Train-f1: 0.47251646903820815 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7895/20000] Train-Loss: 0.5130476897669173 Test-Loss: 0.157764069809732 Train-f1: 0.4679412960890332 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7896/20000] Train-Loss: 0.5180060347605467 Test-Loss: 0.1588016581886207 Train-f1: 0.4731177891462025 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7897/20000] Train-Loss: 0.5113853629004926 Test-Loss: 0.17032192769454368 Train-f1: 0.4732627649957717 Test-f1: 0.4358787878787879 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [7898/20000] Train-Loss: 0.5095932178507254 Test-Loss: 0.1565792225754267 Train-f1: 0.4732031943212068 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7899/20000] Train-Loss: 0.5118238459668106 Test-Loss: 0.15592300382128632 Train-f1: 0.47044003608912643 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7900/20000] Train-Loss: 0.5138104845376025 Test-Loss: 0.15622166657167005 Train-f1: 0.4580779096066614 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7901/20000] Train-Loss: 0.5090008318128562 Test-Loss: 0.16470671325523942 Train-f1: 0.4802635046113307 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [7902/20000] Train-Loss: 0.5062535789864213 Test-Loss: 0.16013109511769189 Train-f1: 0.4703181818181818 Test-f1: 0.453416149068323 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7903/20000] Train-Loss: 0.5120843529703468 Test-Loss: 0.1576292525631958 Train-f1: 0.4658040057713051 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7904/20000] Train-Loss: 0.5118209993966423 Test-Loss: 0.15560825228191605 Train-f1: 0.47093226821707257 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7905/20000] Train-Loss: 0.5110313620615533 Test-Loss: 0.15796364250381803 Train-f1: 0.47251646903820815 Test-f1: 0.45564935064935064 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7906/20000] Train-Loss: 0.5108370083292999 Test-Loss: 0.15704931114765208 Train-f1: 0.4678181818181818 Test-f1: 0.4457303936373703 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7907/20000] Train-Loss: 0.5158881688312185 Test-Loss: 0.15662545312505186 Train-f1: 0.460780612309364 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7908/20000] Train-Loss: 0.5113900204524968 Test-Loss: 0.15731765525895527 Train-f1: 0.470633317096513 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7909/20000] Train-Loss: 0.5082462961167145 Test-Loss: 0.15565756403256883 Train-f1: 0.46821117978169846 Test-f1: 0.429005943840951 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7910/20000] Train-Loss: 0.5128304697960845 Test-Loss: 0.1569777201123784 Train-f1: 0.4647448954939971 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7911/20000] Train-Loss: 0.5070400809381304 Test-Loss: 0.1597836023992537 Train-f1: 0.47273583286021026 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7912/20000] Train-Loss: 0.5125733713589153 Test-Loss: 0.16263651872739626 Train-f1: 0.46306427974390785 Test-f1: 0.4404334365325077 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7913/20000] Train-Loss: 0.5111665998448884 Test-Loss: 0.16234532467100582 Train-f1: 0.46457776369971493 Test-f1: 0.4404334365325077 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7914/20000] Train-Loss: 0.5074934163556786 Test-Loss: 0.15593244621812002 Train-f1: 0.4598732480893311 Test-f1: 0.4277159468438538 Train-acc: 0.7781456953642384 Test-acc: 0.7368421052631579\n",
      "Epoch [7915/20000] Train-Loss: 0.5106745779492928 Test-Loss: 0.15784648952376581 Train-f1: 0.4646526538783215 Test-f1: 0.4457303936373703 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7916/20000] Train-Loss: 0.5082296280509562 Test-Loss: 0.15545801322747616 Train-f1: 0.47415304921120793 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7917/20000] Train-Loss: 0.5118139473829703 Test-Loss: 0.15670080557807398 Train-f1: 0.46013827892239545 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7918/20000] Train-Loss: 0.5110661825049676 Test-Loss: 0.1564656776279296 Train-f1: 0.47079117673867465 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7919/20000] Train-Loss: 0.5088748080477384 Test-Loss: 0.1561782404044093 Train-f1: 0.46366354077863026 Test-f1: 0.4470923670923671 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7920/20000] Train-Loss: 0.5084982072066683 Test-Loss: 0.16020983069042324 Train-f1: 0.4651340208540332 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7921/20000] Train-Loss: 0.5088455502064495 Test-Loss: 0.15653934895253102 Train-f1: 0.4678181818181818 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7922/20000] Train-Loss: 0.5130133313653589 Test-Loss: 0.1564187842640951 Train-f1: 0.45855143184768715 Test-f1: 0.446829268292683 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7923/20000] Train-Loss: 0.5070756859547427 Test-Loss: 0.1574139384421557 Train-f1: 0.4738675403271676 Test-f1: 0.4124675324675325 Train-acc: 0.7980132450331126 Test-acc: 0.7236842105263158\n",
      "Epoch [7924/20000] Train-Loss: 0.5102145253749312 Test-Loss: 0.15551003980709943 Train-f1: 0.47079117673867465 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7925/20000] Train-Loss: 0.5106577032332189 Test-Loss: 0.16248811823057877 Train-f1: 0.46229714142140904 Test-f1: 0.44027149321266973 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [7926/20000] Train-Loss: 0.5069962748698993 Test-Loss: 0.15572947813424876 Train-f1: 0.47097317005361405 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7927/20000] Train-Loss: 0.5123821074320778 Test-Loss: 0.15791839788723688 Train-f1: 0.4716396158839367 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7928/20000] Train-Loss: 0.5124261896529889 Test-Loss: 0.15580972596619638 Train-f1: 0.4658708498708499 Test-f1: 0.41766233766233773 Train-acc: 0.7880794701986755 Test-acc: 0.7236842105263158\n",
      "Epoch [7929/20000] Train-Loss: 0.5113012135152782 Test-Loss: 0.15737210915956792 Train-f1: 0.45738116955508257 Test-f1: 0.4469172932330827 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7930/20000] Train-Loss: 0.5101318888980251 Test-Loss: 0.1582376026749413 Train-f1: 0.46229714142140904 Test-f1: 0.4457303936373703 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7931/20000] Train-Loss: 0.5121991209573629 Test-Loss: 0.15758216300172562 Train-f1: 0.45842943812165027 Test-f1: 0.44649416182596957 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [7932/20000] Train-Loss: 0.5081481728095993 Test-Loss: 0.1666971636712436 Train-f1: 0.4645892734946383 Test-f1: 0.4426916799402199 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7933/20000] Train-Loss: 0.510992499170623 Test-Loss: 0.15641291973164373 Train-f1: 0.4629423206456874 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7934/20000] Train-Loss: 0.5100583692305986 Test-Loss: 0.15783768341908433 Train-f1: 0.4581665586626194 Test-f1: 0.45564935064935064 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7935/20000] Train-Loss: 0.5083542938129225 Test-Loss: 0.15593427432759321 Train-f1: 0.4727606306736741 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7936/20000] Train-Loss: 0.5090337011792232 Test-Loss: 0.1566566109967167 Train-f1: 0.4711352376749736 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7937/20000] Train-Loss: 0.5091367463634192 Test-Loss: 0.16127394434863346 Train-f1: 0.4713108611382683 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7938/20000] Train-Loss: 0.5090127102937919 Test-Loss: 0.1555091563642909 Train-f1: 0.46916629717913627 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7939/20000] Train-Loss: 0.5087989355141684 Test-Loss: 0.16138821812678045 Train-f1: 0.48210264187085966 Test-f1: 0.4652641421224871 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [7940/20000] Train-Loss: 0.5128126933635616 Test-Loss: 0.16309608280641938 Train-f1: 0.4636463343686518 Test-f1: 0.4344337363247945 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7941/20000] Train-Loss: 0.510440664442848 Test-Loss: 0.16203600671205248 Train-f1: 0.470772777946691 Test-f1: 0.4404334365325077 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [7942/20000] Train-Loss: 0.5104069538670379 Test-Loss: 0.15633653658866495 Train-f1: 0.46943797689121425 Test-f1: 0.4576190476190477 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7943/20000] Train-Loss: 0.5110961268398896 Test-Loss: 0.15762635275369222 Train-f1: 0.46531818181818174 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7944/20000] Train-Loss: 0.5122202400693486 Test-Loss: 0.1556892484330971 Train-f1: 0.4814139765560258 Test-f1: 0.45069597069597067 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [7945/20000] Train-Loss: 0.5100930053541308 Test-Loss: 0.15625144341597547 Train-f1: 0.4755578334708769 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7946/20000] Train-Loss: 0.511716454474367 Test-Loss: 0.15864219842031108 Train-f1: 0.4644275918728473 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7947/20000] Train-Loss: 0.5099519736276508 Test-Loss: 0.15801087251548418 Train-f1: 0.47427686958372883 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7948/20000] Train-Loss: 0.5113549830424491 Test-Loss: 0.15657253241333755 Train-f1: 0.46821117978169846 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7949/20000] Train-Loss: 0.5099958879088923 Test-Loss: 0.15566611074544026 Train-f1: 0.47161812121999114 Test-f1: 0.4470923670923671 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7950/20000] Train-Loss: 0.5133785154171333 Test-Loss: 0.15660486378946592 Train-f1: 0.4598431166710407 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7951/20000] Train-Loss: 0.5098398540393131 Test-Loss: 0.1565579678078357 Train-f1: 0.4644924648529498 Test-f1: 0.45 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7952/20000] Train-Loss: 0.5094967196758744 Test-Loss: 0.15805328443821406 Train-f1: 0.46393956057092306 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7953/20000] Train-Loss: 0.5145417283051258 Test-Loss: 0.16338081815175282 Train-f1: 0.4647662772238782 Test-f1: 0.4341972308997648 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [7954/20000] Train-Loss: 0.5119845579168408 Test-Loss: 0.15842080696543295 Train-f1: 0.46510944184854397 Test-f1: 0.45564935064935064 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7955/20000] Train-Loss: 0.5093117509348141 Test-Loss: 0.15574983760520011 Train-f1: 0.45590242522517066 Test-f1: 0.44709236709236705 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [7956/20000] Train-Loss: 0.5099442420347434 Test-Loss: 0.15866110601173886 Train-f1: 0.4681759672564114 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7957/20000] Train-Loss: 0.5092504785596473 Test-Loss: 0.15912007689504679 Train-f1: 0.4598431166710407 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7958/20000] Train-Loss: 0.5121741282835542 Test-Loss: 0.15921089283119605 Train-f1: 0.468471466942883 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7959/20000] Train-Loss: 0.5114781347712486 Test-Loss: 0.15514100809267695 Train-f1: 0.46635903031405174 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7960/20000] Train-Loss: 0.5086192086843206 Test-Loss: 0.15615256694170349 Train-f1: 0.4718534533390478 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [7961/20000] Train-Loss: 0.5108364664706354 Test-Loss: 0.1585189436661817 Train-f1: 0.4679613915376316 Test-f1: 0.45726405090137856 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7962/20000] Train-Loss: 0.5108944692726166 Test-Loss: 0.1559941326765047 Train-f1: 0.4789222157884578 Test-f1: 0.4469172932330827 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [7963/20000] Train-Loss: 0.5075083872348022 Test-Loss: 0.15569908903791413 Train-f1: 0.4640586531453178 Test-f1: 0.4277159468438538 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7964/20000] Train-Loss: 0.510780506221829 Test-Loss: 0.1568176317507849 Train-f1: 0.4740817788872708 Test-f1: 0.4457303936373703 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [7965/20000] Train-Loss: 0.5105438571363939 Test-Loss: 0.15722681725117166 Train-f1: 0.46632010797495804 Test-f1: 0.44649416182596957 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [7966/20000] Train-Loss: 0.5109725171523323 Test-Loss: 0.1590139335151362 Train-f1: 0.4631396799779566 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7967/20000] Train-Loss: 0.5090314245026764 Test-Loss: 0.1611317836591904 Train-f1: 0.4719078176725235 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [7968/20000] Train-Loss: 0.5120753723048536 Test-Loss: 0.1611074922254878 Train-f1: 0.4711464518917935 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7969/20000] Train-Loss: 0.5111443273213157 Test-Loss: 0.15622751518730016 Train-f1: 0.460040404040404 Test-f1: 0.446829268292683 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7970/20000] Train-Loss: 0.5077675684436772 Test-Loss: 0.15690232189469377 Train-f1: 0.463924978687127 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7971/20000] Train-Loss: 0.5081579727706511 Test-Loss: 0.15790058557386927 Train-f1: 0.4695268652915711 Test-f1: 0.45564935064935064 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [7972/20000] Train-Loss: 0.5089867704646895 Test-Loss: 0.161280271409455 Train-f1: 0.4645892734946383 Test-f1: 0.4532716620413197 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7973/20000] Train-Loss: 0.5084117486360108 Test-Loss: 0.1562678513240657 Train-f1: 0.46218896079003524 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7974/20000] Train-Loss: 0.5081132988026928 Test-Loss: 0.15636375462471563 Train-f1: 0.4625181774671397 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [7975/20000] Train-Loss: 0.5065481969030134 Test-Loss: 0.15943768201742176 Train-f1: 0.46897676428362345 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7976/20000] Train-Loss: 0.512432505436445 Test-Loss: 0.15613466646179264 Train-f1: 0.4733853039175095 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7977/20000] Train-Loss: 0.508925959909366 Test-Loss: 0.15564513372669467 Train-f1: 0.4727868464767484 Test-f1: 0.4277159468438538 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [7978/20000] Train-Loss: 0.5088612356774705 Test-Loss: 0.15670132425507372 Train-f1: 0.4713750522981292 Test-f1: 0.4457303936373703 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [7979/20000] Train-Loss: 0.5084316616272996 Test-Loss: 0.1564232570298465 Train-f1: 0.4688708133971292 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7980/20000] Train-Loss: 0.5074727060681196 Test-Loss: 0.15776861145879376 Train-f1: 0.4667193675889328 Test-f1: 0.44951827242524917 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7981/20000] Train-Loss: 0.5076830953601479 Test-Loss: 0.15701159262854236 Train-f1: 0.46407366546496986 Test-f1: 0.42564459930313586 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [7982/20000] Train-Loss: 0.5038519288049236 Test-Loss: 0.1628800715035341 Train-f1: 0.46736599953991254 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7983/20000] Train-Loss: 0.5131125335853752 Test-Loss: 0.15643580574867985 Train-f1: 0.47645858017280407 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [7984/20000] Train-Loss: 0.5035173455955663 Test-Loss: 0.15504538891579295 Train-f1: 0.4653523392677589 Test-f1: 0.44000000000000006 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [7985/20000] Train-Loss: 0.4991937197813717 Test-Loss: 0.16380275358819946 Train-f1: 0.470247860536403 Test-f1: 0.4652641421224871 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [7986/20000] Train-Loss: 0.5163681767020704 Test-Loss: 0.1607942221744935 Train-f1: 0.4573586446213513 Test-f1: 0.4554347826086957 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [7987/20000] Train-Loss: 0.509240357979462 Test-Loss: 0.1588480794819025 Train-f1: 0.4687304913742031 Test-f1: 0.45726405090137856 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [7988/20000] Train-Loss: 0.510570401525772 Test-Loss: 0.15725070828161059 Train-f1: 0.4709990549990549 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7989/20000] Train-Loss: 0.5080816828861394 Test-Loss: 0.15575010692484664 Train-f1: 0.4715066780335322 Test-f1: 0.41766233766233773 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [7990/20000] Train-Loss: 0.5098976366746559 Test-Loss: 0.15887477033845446 Train-f1: 0.46620197837589145 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7991/20000] Train-Loss: 0.5026714476731253 Test-Loss: 0.15608257975965803 Train-f1: 0.47038038173088365 Test-f1: 0.4156933389191454 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [7992/20000] Train-Loss: 0.5131886983392674 Test-Loss: 0.1575097765581017 Train-f1: 0.4665856888209829 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [7993/20000] Train-Loss: 0.5077038454692433 Test-Loss: 0.16281990018778764 Train-f1: 0.4689667055573393 Test-f1: 0.4341972308997648 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [7994/20000] Train-Loss: 0.5080299995973808 Test-Loss: 0.1580908458143975 Train-f1: 0.4612864649963589 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [7995/20000] Train-Loss: 0.5100805636961063 Test-Loss: 0.15883698021315543 Train-f1: 0.4706266832278542 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [7996/20000] Train-Loss: 0.5093092338669581 Test-Loss: 0.1554914037184482 Train-f1: 0.46349796366469515 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7997/20000] Train-Loss: 0.5092180711471713 Test-Loss: 0.15867860302993117 Train-f1: 0.4638682072017069 Test-f1: 0.45564935064935064 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [7998/20000] Train-Loss: 0.508936545312467 Test-Loss: 0.15592567494205886 Train-f1: 0.4629507203417263 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [7999/20000] Train-Loss: 0.5099741239811155 Test-Loss: 0.15603598909251126 Train-f1: 0.4766970214883842 Test-f1: 0.4469172932330827 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8000/20000] Train-Loss: 0.5080085747321853 Test-Loss: 0.15696638826077294 Train-f1: 0.4602041746422387 Test-f1: 0.44683117900509206 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8001/20000] Train-Loss: 0.5056399029247541 Test-Loss: 0.15883859825735394 Train-f1: 0.46215481892477195 Test-f1: 0.4554347826086957 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8002/20000] Train-Loss: 0.4996932802406422 Test-Loss: 0.15655026137662928 Train-f1: 0.4616643863382993 Test-f1: 0.41257656885842797 Train-acc: 0.7814569536423841 Test-acc: 0.7236842105263158\n",
      "Epoch [8003/20000] Train-Loss: 0.5161217550451379 Test-Loss: 0.15531526709359483 Train-f1: 0.4660452254298407 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8004/20000] Train-Loss: 0.5083313266340526 Test-Loss: 0.1561582585541113 Train-f1: 0.4682754534682344 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8005/20000] Train-Loss: 0.5086638608080665 Test-Loss: 0.1555256386027438 Train-f1: 0.45561306483697467 Test-f1: 0.4469172932330827 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [8006/20000] Train-Loss: 0.5061766214785511 Test-Loss: 0.1577373208176245 Train-f1: 0.4698484317392738 Test-f1: 0.45564935064935064 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8007/20000] Train-Loss: 0.5129989707142226 Test-Loss: 0.15714822565569087 Train-f1: 0.4577767830158736 Test-f1: 0.4466252587991718 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8008/20000] Train-Loss: 0.5081356720291359 Test-Loss: 0.1567219489477219 Train-f1: 0.44993183393183395 Test-f1: 0.4457303936373703 Train-acc: 0.7682119205298014 Test-acc: 0.7631578947368421\n",
      "Epoch [8009/20000] Train-Loss: 0.5063995042263937 Test-Loss: 0.15785292178998422 Train-f1: 0.46215481892477195 Test-f1: 0.45564935064935064 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8010/20000] Train-Loss: 0.5087115119155048 Test-Loss: 0.15743033052578598 Train-f1: 0.4686446053346474 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8011/20000] Train-Loss: 0.5061452426563917 Test-Loss: 0.1551849230551913 Train-f1: 0.4702577575441425 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8012/20000] Train-Loss: 0.508393942772923 Test-Loss: 0.15816660168008517 Train-f1: 0.47537833995793816 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8013/20000] Train-Loss: 0.5059899681063097 Test-Loss: 0.15572110755786084 Train-f1: 0.4705959595959596 Test-f1: 0.45 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8014/20000] Train-Loss: 0.5090954151616885 Test-Loss: 0.15539344469406688 Train-f1: 0.4626600180794915 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8015/20000] Train-Loss: 0.5092069537831726 Test-Loss: 0.15511747485098976 Train-f1: 0.4690080369365683 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8016/20000] Train-Loss: 0.5102718901979294 Test-Loss: 0.15614901846082588 Train-f1: 0.46247508618157535 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8017/20000] Train-Loss: 0.5099519720294169 Test-Loss: 0.15547021319609433 Train-f1: 0.4577767830158736 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8018/20000] Train-Loss: 0.5066285095192145 Test-Loss: 0.16012112100093523 Train-f1: 0.46457776369971493 Test-f1: 0.453416149068323 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8019/20000] Train-Loss: 0.5089355185076794 Test-Loss: 0.15555981699789914 Train-f1: 0.4788842320069069 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8020/20000] Train-Loss: 0.5048220325232301 Test-Loss: 0.15717268400181675 Train-f1: 0.4737105544558961 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8021/20000] Train-Loss: 0.5077198102786424 Test-Loss: 0.1586490784286856 Train-f1: 0.47608467778689273 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8022/20000] Train-Loss: 0.5068690114736439 Test-Loss: 0.15612552216168918 Train-f1: 0.4613820705427732 Test-f1: 0.45 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8023/20000] Train-Loss: 0.5072515631700075 Test-Loss: 0.15631311900430314 Train-f1: 0.46842536345641933 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8024/20000] Train-Loss: 0.5111490989686339 Test-Loss: 0.15677206324568613 Train-f1: 0.4726703579630879 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8025/20000] Train-Loss: 0.5045490204301759 Test-Loss: 0.1549940752951031 Train-f1: 0.46595220412593646 Test-f1: 0.4277159468438538 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [8026/20000] Train-Loss: 0.5098786029315237 Test-Loss: 0.15482077032034736 Train-f1: 0.4790392253340944 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8027/20000] Train-Loss: 0.5096949823782109 Test-Loss: 0.1562752192557606 Train-f1: 0.46944381478483094 Test-f1: 0.446829268292683 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8028/20000] Train-Loss: 0.5080163032121016 Test-Loss: 0.15602322281487482 Train-f1: 0.46531818181818174 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8029/20000] Train-Loss: 0.5081285146915704 Test-Loss: 0.15462384245727565 Train-f1: 0.470899547172579 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8030/20000] Train-Loss: 0.5084165272477488 Test-Loss: 0.15483034243969326 Train-f1: 0.46830179846785464 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8031/20000] Train-Loss: 0.5035592844313521 Test-Loss: 0.16827304162009904 Train-f1: 0.4727310738042446 Test-f1: 0.4341972308997648 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [8032/20000] Train-Loss: 0.50718244217524 Test-Loss: 0.15499901409362077 Train-f1: 0.4658612608923168 Test-f1: 0.4469172932330827 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8033/20000] Train-Loss: 0.5085580338456684 Test-Loss: 0.16047826793649234 Train-f1: 0.4677831897567156 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8034/20000] Train-Loss: 0.5067914289789303 Test-Loss: 0.15917938265863127 Train-f1: 0.4705291222313372 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8035/20000] Train-Loss: 0.5057852290076462 Test-Loss: 0.15543196339696366 Train-f1: 0.46830179846785464 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8036/20000] Train-Loss: 0.5059458867893578 Test-Loss: 0.15497876505184816 Train-f1: 0.4603463593801724 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8037/20000] Train-Loss: 0.5028162236048203 Test-Loss: 0.16201454883928573 Train-f1: 0.4715473507186297 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8038/20000] Train-Loss: 0.5090759319896789 Test-Loss: 0.15489827539309906 Train-f1: 0.47130343776306505 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8039/20000] Train-Loss: 0.5076820849313113 Test-Loss: 0.1546145395498108 Train-f1: 0.4730733614211875 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8040/20000] Train-Loss: 0.5071766240886235 Test-Loss: 0.1558031864970733 Train-f1: 0.4658612608923168 Test-f1: 0.4576190476190477 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8041/20000] Train-Loss: 0.508674000229076 Test-Loss: 0.15591076431896914 Train-f1: 0.476160474171225 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8042/20000] Train-Loss: 0.508238755482649 Test-Loss: 0.15454804997767524 Train-f1: 0.4606040446040446 Test-f1: 0.4469172932330827 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8043/20000] Train-Loss: 0.5101127693676254 Test-Loss: 0.15505362101297393 Train-f1: 0.46199819489234006 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8044/20000] Train-Loss: 0.5036930190433232 Test-Loss: 0.16419027752772028 Train-f1: 0.4763506854167853 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [8045/20000] Train-Loss: 0.5073179262672152 Test-Loss: 0.16295246872709135 Train-f1: 0.46517837235228543 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8046/20000] Train-Loss: 0.5105024065511188 Test-Loss: 0.15583266113630573 Train-f1: 0.45013253557440613 Test-f1: 0.4576190476190477 Train-acc: 0.7682119205298014 Test-acc: 0.7763157894736842\n",
      "Epoch [8047/20000] Train-Loss: 0.5080637464944094 Test-Loss: 0.1584712733130564 Train-f1: 0.46593982137326284 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8048/20000] Train-Loss: 0.5056674021611537 Test-Loss: 0.15448248956500177 Train-f1: 0.46846159974994334 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8049/20000] Train-Loss: 0.5063269813000075 Test-Loss: 0.15607268551340833 Train-f1: 0.4679811706190603 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8050/20000] Train-Loss: 0.5053794365320431 Test-Loss: 0.1624231403430199 Train-f1: 0.46325623507066815 Test-f1: 0.4554347826086957 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8051/20000] Train-Loss: 0.506794039215499 Test-Loss: 0.15505253021592103 Train-f1: 0.4660182467635884 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8052/20000] Train-Loss: 0.5074675266716793 Test-Loss: 0.15551280877587215 Train-f1: 0.470445863587321 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8053/20000] Train-Loss: 0.5094185048991057 Test-Loss: 0.1592450580248764 Train-f1: 0.4691267338020141 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8054/20000] Train-Loss: 0.5107259894410366 Test-Loss: 0.15633519905594517 Train-f1: 0.46287800494979264 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8055/20000] Train-Loss: 0.5051488419929416 Test-Loss: 0.15488158430852728 Train-f1: 0.4732727779466909 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8056/20000] Train-Loss: 0.5085924774380403 Test-Loss: 0.1556605567191817 Train-f1: 0.462510011727403 Test-f1: 0.446829268292683 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8057/20000] Train-Loss: 0.5066704325989559 Test-Loss: 0.15530098907112572 Train-f1: 0.47602780961834645 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8058/20000] Train-Loss: 0.508711637982277 Test-Loss: 0.1579912205543354 Train-f1: 0.4671978559361861 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8059/20000] Train-Loss: 0.5061694488043313 Test-Loss: 0.1546796151413872 Train-f1: 0.4705959595959596 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8060/20000] Train-Loss: 0.5069737519291079 Test-Loss: 0.1626963691204581 Train-f1: 0.4647438682061248 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8061/20000] Train-Loss: 0.5072478277514706 Test-Loss: 0.15605616654028084 Train-f1: 0.4607445887445888 Test-f1: 0.45069597069597067 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8062/20000] Train-Loss: 0.5084760820702737 Test-Loss: 0.15741316496019522 Train-f1: 0.46229714142140904 Test-f1: 0.44683117900509206 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8063/20000] Train-Loss: 0.5056159176708072 Test-Loss: 0.16000276408389544 Train-f1: 0.46879717719159403 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8064/20000] Train-Loss: 0.5081099020551921 Test-Loss: 0.15720032880882961 Train-f1: 0.46370572888999073 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8065/20000] Train-Loss: 0.5066508165037094 Test-Loss: 0.1568331377784908 Train-f1: 0.4783737373737374 Test-f1: 0.45564935064935064 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [8066/20000] Train-Loss: 0.5078842255512224 Test-Loss: 0.1553709666602001 Train-f1: 0.46808740024603157 Test-f1: 0.429005943840951 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [8067/20000] Train-Loss: 0.5038692528461812 Test-Loss: 0.15997557508028024 Train-f1: 0.4723129553373456 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [8068/20000] Train-Loss: 0.5041689121020463 Test-Loss: 0.15439793796256238 Train-f1: 0.4692362376744278 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8069/20000] Train-Loss: 0.5059035504063711 Test-Loss: 0.15490424830449137 Train-f1: 0.47250064211621157 Test-f1: 0.4576190476190477 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8070/20000] Train-Loss: 0.503264338406882 Test-Loss: 0.16898145954684596 Train-f1: 0.4693372601178035 Test-f1: 0.4280288816874183 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [8071/20000] Train-Loss: 0.5089400341827084 Test-Loss: 0.15563008303135134 Train-f1: 0.4751700981944884 Test-f1: 0.44709236709236705 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8072/20000] Train-Loss: 0.5050260676778936 Test-Loss: 0.15707300817719866 Train-f1: 0.47272689334661155 Test-f1: 0.45564935064935064 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8073/20000] Train-Loss: 0.5047075318517324 Test-Loss: 0.1561321868698381 Train-f1: 0.4608511224210086 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8074/20000] Train-Loss: 0.5059156612865837 Test-Loss: 0.1591943826965564 Train-f1: 0.46858234932769094 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8075/20000] Train-Loss: 0.5089872655053628 Test-Loss: 0.1561639811784008 Train-f1: 0.4653072145246059 Test-f1: 0.4457303936373703 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8076/20000] Train-Loss: 0.508757341828486 Test-Loss: 0.15596202888550717 Train-f1: 0.4598431166710407 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8077/20000] Train-Loss: 0.5078937226517393 Test-Loss: 0.1548107205342818 Train-f1: 0.4629819524200164 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8078/20000] Train-Loss: 0.5081851254083779 Test-Loss: 0.1565302672033307 Train-f1: 0.4710604191575099 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8079/20000] Train-Loss: 0.5074688090810626 Test-Loss: 0.15641296826505371 Train-f1: 0.46703417623061405 Test-f1: 0.44951827242524917 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8080/20000] Train-Loss: 0.5060214771520976 Test-Loss: 0.15858218125166143 Train-f1: 0.470054998793329 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8081/20000] Train-Loss: 0.5084907658195424 Test-Loss: 0.15515771986597862 Train-f1: 0.4607445887445888 Test-f1: 0.4576190476190477 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [8082/20000] Train-Loss: 0.5097480173414376 Test-Loss: 0.15492062692845546 Train-f1: 0.47047949293121005 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8083/20000] Train-Loss: 0.5047451833426283 Test-Loss: 0.15526611686257008 Train-f1: 0.47381576474748666 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8084/20000] Train-Loss: 0.5061299590623276 Test-Loss: 0.15758361684928093 Train-f1: 0.46267837235228537 Test-f1: 0.44683117900509206 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8085/20000] Train-Loss: 0.5066574557254699 Test-Loss: 0.1562744006483635 Train-f1: 0.4698739309471017 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8086/20000] Train-Loss: 0.5034333801948565 Test-Loss: 0.15798823796980704 Train-f1: 0.4629819524200164 Test-f1: 0.44683117900509206 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8087/20000] Train-Loss: 0.5076142594905172 Test-Loss: 0.15532559545858016 Train-f1: 0.45676340420865974 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8088/20000] Train-Loss: 0.5085294962310692 Test-Loss: 0.15694303543708282 Train-f1: 0.4705888893714981 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8089/20000] Train-Loss: 0.5047867601692086 Test-Loss: 0.1589592902030078 Train-f1: 0.4723129553373456 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8090/20000] Train-Loss: 0.5049030456481325 Test-Loss: 0.15771366130378273 Train-f1: 0.46613308877680043 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8091/20000] Train-Loss: 0.5062878970074512 Test-Loss: 0.154359612527109 Train-f1: 0.46301153220857927 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8092/20000] Train-Loss: 0.5069189270583297 Test-Loss: 0.16068432149009845 Train-f1: 0.4698801406573237 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8093/20000] Train-Loss: 0.5069066511098743 Test-Loss: 0.16274065688620856 Train-f1: 0.46655370347559694 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8094/20000] Train-Loss: 0.5108588095587798 Test-Loss: 0.1591227001483529 Train-f1: 0.46286304118876825 Test-f1: 0.4554347826086957 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8095/20000] Train-Loss: 0.508872032733803 Test-Loss: 0.15657012323566658 Train-f1: 0.4689048136961764 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8096/20000] Train-Loss: 0.5022975731136832 Test-Loss: 0.15553594919287644 Train-f1: 0.4613909000164259 Test-f1: 0.4277159468438538 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [8097/20000] Train-Loss: 0.5074836901667176 Test-Loss: 0.15819404600597503 Train-f1: 0.4729121416504719 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8098/20000] Train-Loss: 0.5019968276772344 Test-Loss: 0.15614306877629172 Train-f1: 0.46567180877710046 Test-f1: 0.45564935064935064 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8099/20000] Train-Loss: 0.5075558686945543 Test-Loss: 0.15463626030138378 Train-f1: 0.4689691145974745 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8100/20000] Train-Loss: 0.5063021799721004 Test-Loss: 0.15813271551691419 Train-f1: 0.46267837235228537 Test-f1: 0.4466252587991718 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8101/20000] Train-Loss: 0.5060696342432351 Test-Loss: 0.1548015515675236 Train-f1: 0.4730959595959595 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8102/20000] Train-Loss: 0.5001402572284739 Test-Loss: 0.159061423212876 Train-f1: 0.47692020145382175 Test-f1: 0.4252857311181807 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [8103/20000] Train-Loss: 0.5051220028629734 Test-Loss: 0.15459230997977738 Train-f1: 0.4738476199683722 Test-f1: 0.4469172932330827 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8104/20000] Train-Loss: 0.50527989618859 Test-Loss: 0.15435543043083905 Train-f1: 0.47574605132421965 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8105/20000] Train-Loss: 0.5085219105657985 Test-Loss: 0.1556326688405468 Train-f1: 0.47088910257420746 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8106/20000] Train-Loss: 0.5071405973957089 Test-Loss: 0.1555822683645923 Train-f1: 0.47342074814382773 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8107/20000] Train-Loss: 0.5060263243976778 Test-Loss: 0.154467038705783 Train-f1: 0.4631156383804246 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8108/20000] Train-Loss: 0.5047664831428224 Test-Loss: 0.15465779169607083 Train-f1: 0.4692691049868749 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8109/20000] Train-Loss: 0.5074567854284717 Test-Loss: 0.15543306325743242 Train-f1: 0.47377401641441474 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8110/20000] Train-Loss: 0.5058591507665913 Test-Loss: 0.15567955546930493 Train-f1: 0.4677831897567156 Test-f1: 0.43574229691876754 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [8111/20000] Train-Loss: 0.505985183762103 Test-Loss: 0.1577030195916673 Train-f1: 0.4633612574706544 Test-f1: 0.4466252587991718 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8112/20000] Train-Loss: 0.505866984898665 Test-Loss: 0.15591343828443666 Train-f1: 0.470445863587321 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8113/20000] Train-Loss: 0.5100915256084374 Test-Loss: 0.15520928457813693 Train-f1: 0.4578549814574659 Test-f1: 0.44709236709236705 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8114/20000] Train-Loss: 0.5062968566072974 Test-Loss: 0.15516878070877188 Train-f1: 0.46814937654681027 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8115/20000] Train-Loss: 0.5074024033539367 Test-Loss: 0.15789209787810166 Train-f1: 0.45933791751632125 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8116/20000] Train-Loss: 0.5055568896733702 Test-Loss: 0.15990030168438465 Train-f1: 0.4731424064982946 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8117/20000] Train-Loss: 0.507160303552773 Test-Loss: 0.1576045175967659 Train-f1: 0.46267837235228537 Test-f1: 0.44558629776021075 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8118/20000] Train-Loss: 0.5078264766585212 Test-Loss: 0.1574108385602213 Train-f1: 0.4632435985750556 Test-f1: 0.4554347826086957 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8119/20000] Train-Loss: 0.503999649481121 Test-Loss: 0.15546236223145485 Train-f1: 0.4716396158839367 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8120/20000] Train-Loss: 0.5063995113958034 Test-Loss: 0.16091299038245924 Train-f1: 0.4720619686647288 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8121/20000] Train-Loss: 0.5067707815471107 Test-Loss: 0.1572709040427275 Train-f1: 0.47428862250645326 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8122/20000] Train-Loss: 0.5035213609044832 Test-Loss: 0.15964633163458805 Train-f1: 0.47761761499891 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8123/20000] Train-Loss: 0.5029990664264448 Test-Loss: 0.15468531104080882 Train-f1: 0.46381458974242407 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8124/20000] Train-Loss: 0.506049980454955 Test-Loss: 0.15576559681552066 Train-f1: 0.46602546473134704 Test-f1: 0.45 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8125/20000] Train-Loss: 0.5018103280888652 Test-Loss: 0.15921412424974704 Train-f1: 0.4780078872526333 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [8126/20000] Train-Loss: 0.5074712105575195 Test-Loss: 0.15997243385642057 Train-f1: 0.45864393293549305 Test-f1: 0.4554347826086957 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [8127/20000] Train-Loss: 0.5041365507757958 Test-Loss: 0.1545219957115477 Train-f1: 0.4657322497322497 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8128/20000] Train-Loss: 0.5066236180258946 Test-Loss: 0.15438329051212982 Train-f1: 0.4663136563586338 Test-f1: 0.44000000000000006 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [8129/20000] Train-Loss: 0.5067805815536544 Test-Loss: 0.15527441771105513 Train-f1: 0.4632435985750556 Test-f1: 0.43650793650793657 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [8130/20000] Train-Loss: 0.505091867684342 Test-Loss: 0.1583612024692928 Train-f1: 0.464972165197201 Test-f1: 0.44649416182596957 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8131/20000] Train-Loss: 0.5037320110497677 Test-Loss: 0.1558516253116871 Train-f1: 0.4750245006703282 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8132/20000] Train-Loss: 0.5018570663470072 Test-Loss: 0.15877395955121862 Train-f1: 0.4688708133971291 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8133/20000] Train-Loss: 0.5058790102355774 Test-Loss: 0.1540569141432201 Train-f1: 0.4647662772238782 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8134/20000] Train-Loss: 0.5079078857007774 Test-Loss: 0.15587722330314494 Train-f1: 0.4528919764016006 Test-f1: 0.4457303936373703 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [8135/20000] Train-Loss: 0.5044900260929306 Test-Loss: 0.15566418387940376 Train-f1: 0.47165418085760535 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8136/20000] Train-Loss: 0.5038999493422832 Test-Loss: 0.1561913646145039 Train-f1: 0.4606629534137653 Test-f1: 0.4277159468438538 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [8137/20000] Train-Loss: 0.5059886331535473 Test-Loss: 0.15772604719126185 Train-f1: 0.463148173463963 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8138/20000] Train-Loss: 0.5050889454444135 Test-Loss: 0.15857563813751607 Train-f1: 0.4687959769097283 Test-f1: 0.4360203681942812 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [8139/20000] Train-Loss: 0.5089954568001258 Test-Loss: 0.15752868317994465 Train-f1: 0.47554379475506237 Test-f1: 0.45726405090137856 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8140/20000] Train-Loss: 0.5042034208383237 Test-Loss: 0.15829214690337717 Train-f1: 0.46742615292726236 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8141/20000] Train-Loss: 0.504858142876089 Test-Loss: 0.15881504626531515 Train-f1: 0.47377401641441474 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8142/20000] Train-Loss: 0.5041279151213169 Test-Loss: 0.1552801068487169 Train-f1: 0.46820091999400154 Test-f1: 0.45726405090137856 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8143/20000] Train-Loss: 0.5040948278304558 Test-Loss: 0.157532799926865 Train-f1: 0.4764044604044604 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8144/20000] Train-Loss: 0.5025189492592347 Test-Loss: 0.15433458012931303 Train-f1: 0.46379340046868067 Test-f1: 0.4469172932330827 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8145/20000] Train-Loss: 0.5016425585101881 Test-Loss: 0.15384164199886494 Train-f1: 0.46613308877680043 Test-f1: 0.44000000000000006 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [8146/20000] Train-Loss: 0.5042896398751526 Test-Loss: 0.15523880201153742 Train-f1: 0.4738675403271676 Test-f1: 0.42430685593476297 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [8147/20000] Train-Loss: 0.5071782183277882 Test-Loss: 0.15576284296889384 Train-f1: 0.48417084638120256 Test-f1: 0.4466252587991718 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8148/20000] Train-Loss: 0.5058460174736343 Test-Loss: 0.15432046862774412 Train-f1: 0.4632435985750556 Test-f1: 0.4576190476190477 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8149/20000] Train-Loss: 0.5056405536839851 Test-Loss: 0.15426944266084258 Train-f1: 0.4609382835819953 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8150/20000] Train-Loss: 0.5060576922156694 Test-Loss: 0.1588164598151151 Train-f1: 0.4678181818181818 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8151/20000] Train-Loss: 0.5023284108923295 Test-Loss: 0.1556282608288119 Train-f1: 0.47272689334661155 Test-f1: 0.45726405090137856 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8152/20000] Train-Loss: 0.506208764453662 Test-Loss: 0.16158835378865982 Train-f1: 0.4672690840308521 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [8153/20000] Train-Loss: 0.5058488079800357 Test-Loss: 0.15620689243965644 Train-f1: 0.468892298948565 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8154/20000] Train-Loss: 0.5074346311058238 Test-Loss: 0.15456049315818518 Train-f1: 0.46287800494979264 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8155/20000] Train-Loss: 0.5041778902622612 Test-Loss: 0.15471599809590564 Train-f1: 0.4667193675889328 Test-f1: 0.460952380952381 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8156/20000] Train-Loss: 0.5036902499554692 Test-Loss: 0.15398388702244226 Train-f1: 0.4663549783549783 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8157/20000] Train-Loss: 0.5037773819970539 Test-Loss: 0.15406068159545117 Train-f1: 0.4602041746422387 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8158/20000] Train-Loss: 0.5041810255006112 Test-Loss: 0.16062243053199401 Train-f1: 0.4633670915909359 Test-f1: 0.4516339869281046 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8159/20000] Train-Loss: 0.5071068492433284 Test-Loss: 0.1542761555286674 Train-f1: 0.46316814716814714 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8160/20000] Train-Loss: 0.5026766387053722 Test-Loss: 0.1590824504622464 Train-f1: 0.4730959595959595 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [8161/20000] Train-Loss: 0.5058314247578475 Test-Loss: 0.15803759738534895 Train-f1: 0.4687613493721791 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8162/20000] Train-Loss: 0.5051452643978311 Test-Loss: 0.1554688496809444 Train-f1: 0.4680748233478198 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8163/20000] Train-Loss: 0.5006947590399965 Test-Loss: 0.1547367756494971 Train-f1: 0.4742711948313051 Test-f1: 0.4156933389191454 Train-acc: 0.7980132450331126 Test-acc: 0.7236842105263158\n",
      "Epoch [8164/20000] Train-Loss: 0.5049031641127592 Test-Loss: 0.1557587083297586 Train-f1: 0.4689489130564352 Test-f1: 0.45564935064935064 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8165/20000] Train-Loss: 0.5038445438565707 Test-Loss: 0.1546262046273543 Train-f1: 0.4637408589292814 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8166/20000] Train-Loss: 0.5037133604441253 Test-Loss: 0.15384712809619608 Train-f1: 0.47114229652303596 Test-f1: 0.4469172932330827 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8167/20000] Train-Loss: 0.5030278092108004 Test-Loss: 0.15465336311417693 Train-f1: 0.4683844975164357 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8168/20000] Train-Loss: 0.5023498869312293 Test-Loss: 0.15531674372616747 Train-f1: 0.46627791986359757 Test-f1: 0.4360203681942812 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [8169/20000] Train-Loss: 0.5050595716711037 Test-Loss: 0.15552186520407377 Train-f1: 0.460479886638959 Test-f1: 0.4497330282227307 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8170/20000] Train-Loss: 0.5041333593967005 Test-Loss: 0.15580333562439844 Train-f1: 0.4715023923444976 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8171/20000] Train-Loss: 0.5054279291664856 Test-Loss: 0.15540416174015675 Train-f1: 0.46858234932769094 Test-f1: 0.4457303936373703 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8172/20000] Train-Loss: 0.4992552830956644 Test-Loss: 0.1568026476460589 Train-f1: 0.46763199770924146 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8173/20000] Train-Loss: 0.505113827998174 Test-Loss: 0.15428727214746335 Train-f1: 0.4681109599574141 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8174/20000] Train-Loss: 0.5047521203511698 Test-Loss: 0.15505145374757082 Train-f1: 0.47362882860160466 Test-f1: 0.43574229691876754 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8175/20000] Train-Loss: 0.506969344736673 Test-Loss: 0.1560158792798186 Train-f1: 0.46944381478483094 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8176/20000] Train-Loss: 0.49735248284988637 Test-Loss: 0.15892427971486894 Train-f1: 0.47183065583065587 Test-f1: 0.4239140347746783 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [8177/20000] Train-Loss: 0.5071694192047652 Test-Loss: 0.16814814745098297 Train-f1: 0.4643981339608761 Test-f1: 0.4426916799402199 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [8178/20000] Train-Loss: 0.5040458340956988 Test-Loss: 0.1554237100765165 Train-f1: 0.46370572888999073 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8179/20000] Train-Loss: 0.5039164141193629 Test-Loss: 0.1546871846194791 Train-f1: 0.4674408561356918 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8180/20000] Train-Loss: 0.5056396821845143 Test-Loss: 0.1607407774301617 Train-f1: 0.4717340254603881 Test-f1: 0.4404334365325077 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8181/20000] Train-Loss: 0.5051474376782354 Test-Loss: 0.1595140120974856 Train-f1: 0.4696229442744849 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8182/20000] Train-Loss: 0.5032875865813147 Test-Loss: 0.15744711675604645 Train-f1: 0.4762658602658602 Test-f1: 0.46554019457245266 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [8183/20000] Train-Loss: 0.5039252061049058 Test-Loss: 0.1554353501706421 Train-f1: 0.464972165197201 Test-f1: 0.446829268292683 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8184/20000] Train-Loss: 0.5058181441124785 Test-Loss: 0.1546020745473069 Train-f1: 0.45314297324256836 Test-f1: 0.446829268292683 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [8185/20000] Train-Loss: 0.5033460083215694 Test-Loss: 0.15686775955861312 Train-f1: 0.46267837235228537 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8186/20000] Train-Loss: 0.5054225785568551 Test-Loss: 0.15793561142139168 Train-f1: 0.4667171383088686 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8187/20000] Train-Loss: 0.504805842559525 Test-Loss: 0.160258706945213 Train-f1: 0.4555979020979022 Test-f1: 0.4652641421224871 Train-acc: 0.7748344370860927 Test-acc: 0.7894736842105263\n",
      "Epoch [8188/20000] Train-Loss: 0.5005105172572812 Test-Loss: 0.15513417020412226 Train-f1: 0.4638979346704958 Test-f1: 0.4163977806231328 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [8189/20000] Train-Loss: 0.5078121082020053 Test-Loss: 0.15642480452908347 Train-f1: 0.4602041746422387 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8190/20000] Train-Loss: 0.5071242775191087 Test-Loss: 0.1542123607648306 Train-f1: 0.46287800494979264 Test-f1: 0.44709236709236705 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8191/20000] Train-Loss: 0.5043957778982395 Test-Loss: 0.15586733813149692 Train-f1: 0.46961791831357047 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8192/20000] Train-Loss: 0.5030351260170332 Test-Loss: 0.15609414792838888 Train-f1: 0.4682754534682344 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8193/20000] Train-Loss: 0.5032469629359366 Test-Loss: 0.15545577953747897 Train-f1: 0.4784746682564448 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8194/20000] Train-Loss: 0.5037928294290654 Test-Loss: 0.15506014285932362 Train-f1: 0.46287800494979264 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8195/20000] Train-Loss: 0.503032898534287 Test-Loss: 0.1611683576933926 Train-f1: 0.4626600180794915 Test-f1: 0.4401493930905696 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [8196/20000] Train-Loss: 0.5029875630886006 Test-Loss: 0.155095265994311 Train-f1: 0.46497172033322265 Test-f1: 0.45726405090137856 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8197/20000] Train-Loss: 0.4986091542761666 Test-Loss: 0.1554105531967542 Train-f1: 0.4551986391986391 Test-f1: 0.4239140347746783 Train-acc: 0.7748344370860927 Test-acc: 0.7368421052631579\n",
      "Epoch [8198/20000] Train-Loss: 0.5051678851043389 Test-Loss: 0.15440103491384266 Train-f1: 0.4731953183991946 Test-f1: 0.43650793650793657 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [8199/20000] Train-Loss: 0.5025224122741475 Test-Loss: 0.15368576926696517 Train-f1: 0.47122525049593766 Test-f1: 0.44709236709236705 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8200/20000] Train-Loss: 0.5022213986842826 Test-Loss: 0.15483130861639688 Train-f1: 0.4670137344669719 Test-f1: 0.45 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8201/20000] Train-Loss: 0.5066435634485582 Test-Loss: 0.15710830287114308 Train-f1: 0.4763125358599014 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8202/20000] Train-Loss: 0.5040948040265694 Test-Loss: 0.15451613500174893 Train-f1: 0.4660639601733571 Test-f1: 0.44649416182596957 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8203/20000] Train-Loss: 0.5023907494542693 Test-Loss: 0.15800546243306826 Train-f1: 0.4634112554112554 Test-f1: 0.44558629776021075 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8204/20000] Train-Loss: 0.5023801016595227 Test-Loss: 0.15474009465722155 Train-f1: 0.46287800494979264 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8205/20000] Train-Loss: 0.5030091924190259 Test-Loss: 0.1569138467802931 Train-f1: 0.46046544446544446 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8206/20000] Train-Loss: 0.5012600601408553 Test-Loss: 0.15845154096348013 Train-f1: 0.4670167880899587 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8207/20000] Train-Loss: 0.5034605852448302 Test-Loss: 0.15622478171059553 Train-f1: 0.47409961889098157 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8208/20000] Train-Loss: 0.5029554266397931 Test-Loss: 0.15413970972374486 Train-f1: 0.475952393713448 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8209/20000] Train-Loss: 0.5037476605035568 Test-Loss: 0.1602086430116459 Train-f1: 0.46821117978169846 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8210/20000] Train-Loss: 0.5031080098432933 Test-Loss: 0.15432072236777955 Train-f1: 0.4606541908809033 Test-f1: 0.44709236709236705 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8211/20000] Train-Loss: 0.5023220885658777 Test-Loss: 0.15500370441141814 Train-f1: 0.45726262626262615 Test-f1: 0.44951827242524917 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8212/20000] Train-Loss: 0.5051022566360237 Test-Loss: 0.16170821281959713 Train-f1: 0.4679709002866573 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8213/20000] Train-Loss: 0.5023146718084569 Test-Loss: 0.1600534364748328 Train-f1: 0.4727606306736741 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8214/20000] Train-Loss: 0.501581789643095 Test-Loss: 0.15695681786048168 Train-f1: 0.4649944837770924 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8215/20000] Train-Loss: 0.5058989177821878 Test-Loss: 0.153879206957782 Train-f1: 0.4661123427496626 Test-f1: 0.44709236709236705 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8216/20000] Train-Loss: 0.5025800015017954 Test-Loss: 0.15457146486005172 Train-f1: 0.4686753246753247 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8217/20000] Train-Loss: 0.499013013711687 Test-Loss: 0.16337149413790625 Train-f1: 0.46960428745044125 Test-f1: 0.4593201178892971 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8218/20000] Train-Loss: 0.5053081966306391 Test-Loss: 0.15420072494205006 Train-f1: 0.46814937654681027 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8219/20000] Train-Loss: 0.50346410262671 Test-Loss: 0.15485921029278207 Train-f1: 0.476160474171225 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8220/20000] Train-Loss: 0.5032802614242694 Test-Loss: 0.15730187222430814 Train-f1: 0.45742639686446085 Test-f1: 0.44558629776021075 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8221/20000] Train-Loss: 0.5063440203629912 Test-Loss: 0.15558044303472238 Train-f1: 0.4609739562808155 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8222/20000] Train-Loss: 0.5031510109206369 Test-Loss: 0.16267351380833536 Train-f1: 0.47984377777876197 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [8223/20000] Train-Loss: 0.5033399199308769 Test-Loss: 0.1545737026107875 Train-f1: 0.47663911416463034 Test-f1: 0.45 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8224/20000] Train-Loss: 0.5010286928799284 Test-Loss: 0.1574772164249078 Train-f1: 0.46213528576457696 Test-f1: 0.4554347826086957 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8225/20000] Train-Loss: 0.5009061876536367 Test-Loss: 0.15532810739842007 Train-f1: 0.47141505513259235 Test-f1: 0.44951827242524917 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8226/20000] Train-Loss: 0.49950308482797334 Test-Loss: 0.15499869957219342 Train-f1: 0.4728755429108789 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8227/20000] Train-Loss: 0.4965837889996072 Test-Loss: 0.16519210694727568 Train-f1: 0.4667171383088686 Test-f1: 0.4532716620413197 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [8228/20000] Train-Loss: 0.5056084614234192 Test-Loss: 0.1535555259770823 Train-f1: 0.4619750445632798 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8229/20000] Train-Loss: 0.5036324094788515 Test-Loss: 0.15679508772474943 Train-f1: 0.4705456570505871 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8230/20000] Train-Loss: 0.5022282732025415 Test-Loss: 0.15549535350867866 Train-f1: 0.4727170200899479 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8231/20000] Train-Loss: 0.49747319048280497 Test-Loss: 0.15790893403231457 Train-f1: 0.482083916083916 Test-f1: 0.4596816087138668 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [8232/20000] Train-Loss: 0.500882757768946 Test-Loss: 0.16454818655955558 Train-f1: 0.4698739309471017 Test-f1: 0.4545727136431784 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8233/20000] Train-Loss: 0.5055335842295704 Test-Loss: 0.15743910070312123 Train-f1: 0.4606629534137653 Test-f1: 0.4554347826086957 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [8234/20000] Train-Loss: 0.49629840277389564 Test-Loss: 0.15910130058917715 Train-f1: 0.47466029844711466 Test-f1: 0.41818181818181815 Train-acc: 0.7980132450331126 Test-acc: 0.7236842105263158\n",
      "Epoch [8235/20000] Train-Loss: 0.49695272079974623 Test-Loss: 0.16982337166374986 Train-f1: 0.4740054740431642 Test-f1: 0.44165702101736215 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [8236/20000] Train-Loss: 0.508093181877214 Test-Loss: 0.15513442916823453 Train-f1: 0.4686446053346474 Test-f1: 0.45564935064935064 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8237/20000] Train-Loss: 0.5021787087015388 Test-Loss: 0.15722240072056165 Train-f1: 0.46850880749473517 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8238/20000] Train-Loss: 0.5042960820430218 Test-Loss: 0.15472754320337773 Train-f1: 0.47027615862398464 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8239/20000] Train-Loss: 0.5019865180971481 Test-Loss: 0.15556855210746456 Train-f1: 0.4737703728508169 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8240/20000] Train-Loss: 0.49669091486002337 Test-Loss: 0.15617830942393315 Train-f1: 0.4629507203417263 Test-f1: 0.4510087552341073 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8241/20000] Train-Loss: 0.500118488966142 Test-Loss: 0.15566505152059112 Train-f1: 0.46913385505204425 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8242/20000] Train-Loss: 0.5038073908073994 Test-Loss: 0.162173610415968 Train-f1: 0.4586415473258899 Test-f1: 0.44458402965865657 Train-acc: 0.7781456953642384 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [8243/20000] Train-Loss: 0.5025587868895743 Test-Loss: 0.15330915956750943 Train-f1: 0.4686446053346474 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8244/20000] Train-Loss: 0.500728496803556 Test-Loss: 0.1540316486513856 Train-f1: 0.46779168657429526 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8245/20000] Train-Loss: 0.501723014277815 Test-Loss: 0.15950035208942595 Train-f1: 0.47535962280487826 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8246/20000] Train-Loss: 0.5009666081391579 Test-Loss: 0.15533120035405934 Train-f1: 0.4711470093207416 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8247/20000] Train-Loss: 0.5065753397987556 Test-Loss: 0.16362851696030017 Train-f1: 0.4683559663032263 Test-f1: 0.4341972308997648 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [8248/20000] Train-Loss: 0.4991359907949133 Test-Loss: 0.15489165300955218 Train-f1: 0.45918129348388936 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8249/20000] Train-Loss: 0.507752572376408 Test-Loss: 0.15325053475008882 Train-f1: 0.4711933712531245 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8250/20000] Train-Loss: 0.5017668189667429 Test-Loss: 0.15428148008377854 Train-f1: 0.4633419913419914 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8251/20000] Train-Loss: 0.5083064716829292 Test-Loss: 0.1542052830032714 Train-f1: 0.47293033563701037 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8252/20000] Train-Loss: 0.497828227043127 Test-Loss: 0.1551787304887469 Train-f1: 0.4650404040404041 Test-f1: 0.45726405090137856 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8253/20000] Train-Loss: 0.5006630739565424 Test-Loss: 0.156097475327137 Train-f1: 0.47377401641441474 Test-f1: 0.4360203681942812 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8254/20000] Train-Loss: 0.5043801404889264 Test-Loss: 0.153969755479012 Train-f1: 0.4575622720372232 Test-f1: 0.460952380952381 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [8255/20000] Train-Loss: 0.5055251704671695 Test-Loss: 0.15381321179344845 Train-f1: 0.46830179846785464 Test-f1: 0.4469172932330827 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8256/20000] Train-Loss: 0.5032880669310794 Test-Loss: 0.1536937578038771 Train-f1: 0.47141505513259235 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8257/20000] Train-Loss: 0.50167359805095 Test-Loss: 0.15625916401145945 Train-f1: 0.4546486190866831 Test-f1: 0.44649416182596957 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [8258/20000] Train-Loss: 0.5021345854361203 Test-Loss: 0.15681848177198873 Train-f1: 0.4727170200899479 Test-f1: 0.44951827242524917 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8259/20000] Train-Loss: 0.4999334753843001 Test-Loss: 0.1537009643478731 Train-f1: 0.48230535307459543 Test-f1: 0.4469172932330827 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8260/20000] Train-Loss: 0.5013593224517083 Test-Loss: 0.15971668078885723 Train-f1: 0.4702577575441425 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8261/20000] Train-Loss: 0.502362183130465 Test-Loss: 0.15467854826287686 Train-f1: 0.4680748233478198 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8262/20000] Train-Loss: 0.5030598606808433 Test-Loss: 0.15611063368687858 Train-f1: 0.47363927319997623 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8263/20000] Train-Loss: 0.5028914863023491 Test-Loss: 0.15647226113211724 Train-f1: 0.4761870439006131 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8264/20000] Train-Loss: 0.5014497605242528 Test-Loss: 0.15769159820303247 Train-f1: 0.46286304118876825 Test-f1: 0.4596816087138668 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8265/20000] Train-Loss: 0.5047517970521083 Test-Loss: 0.15712997974779053 Train-f1: 0.4658612608923168 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8266/20000] Train-Loss: 0.5013997026413312 Test-Loss: 0.15562658692274414 Train-f1: 0.47342074814382773 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8267/20000] Train-Loss: 0.5011924675164099 Test-Loss: 0.16021173455061294 Train-f1: 0.46497172033322265 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8268/20000] Train-Loss: 0.5000958804406875 Test-Loss: 0.16090003282482535 Train-f1: 0.470445863587321 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8269/20000] Train-Loss: 0.5052852413451765 Test-Loss: 0.15505962184892774 Train-f1: 0.4653523392677589 Test-f1: 0.44951827242524917 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8270/20000] Train-Loss: 0.5000516284848545 Test-Loss: 0.16266243067875988 Train-f1: 0.46779643766388884 Test-f1: 0.4341972308997648 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [8271/20000] Train-Loss: 0.5043127403163946 Test-Loss: 0.15485750445142232 Train-f1: 0.47043669770953944 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8272/20000] Train-Loss: 0.5009722799972054 Test-Loss: 0.1530148203833152 Train-f1: 0.4626699244920157 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8273/20000] Train-Loss: 0.50450080138295 Test-Loss: 0.15422771087134446 Train-f1: 0.4792034632034632 Test-f1: 0.4277159468438538 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [8274/20000] Train-Loss: 0.5009643965553947 Test-Loss: 0.15782299866173694 Train-f1: 0.4674408561356918 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8275/20000] Train-Loss: 0.5014437524464535 Test-Loss: 0.15621783963293656 Train-f1: 0.470633317096513 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8276/20000] Train-Loss: 0.4987464748147793 Test-Loss: 0.15326155280141543 Train-f1: 0.47207965328582435 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8277/20000] Train-Loss: 0.495755710787066 Test-Loss: 0.1628431832723048 Train-f1: 0.48200865800865794 Test-f1: 0.4545727136431784 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [8278/20000] Train-Loss: 0.5040037265783635 Test-Loss: 0.15321917759935794 Train-f1: 0.4699227569815806 Test-f1: 0.44709236709236705 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8279/20000] Train-Loss: 0.5004423840713352 Test-Loss: 0.1530677813513466 Train-f1: 0.4736533566524468 Test-f1: 0.44000000000000006 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8280/20000] Train-Loss: 0.49963938556611676 Test-Loss: 0.1562730266110603 Train-f1: 0.46830179846785464 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8281/20000] Train-Loss: 0.500007961281456 Test-Loss: 0.1566460941122144 Train-f1: 0.4668674250028725 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8282/20000] Train-Loss: 0.5062271774987057 Test-Loss: 0.16376872846601945 Train-f1: 0.4655952366869737 Test-f1: 0.4341972308997648 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [8283/20000] Train-Loss: 0.5005504106437465 Test-Loss: 0.15413880250096834 Train-f1: 0.4608511224210086 Test-f1: 0.44683117900509206 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8284/20000] Train-Loss: 0.4963658554115907 Test-Loss: 0.15643803564917458 Train-f1: 0.4788624555646705 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8285/20000] Train-Loss: 0.5024426352794553 Test-Loss: 0.1608951169169453 Train-f1: 0.47248845100626324 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [8286/20000] Train-Loss: 0.49906606796738295 Test-Loss: 0.15281765307879472 Train-f1: 0.47692020145382175 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8287/20000] Train-Loss: 0.4982482949058337 Test-Loss: 0.15848305366956392 Train-f1: 0.4679755751494882 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8288/20000] Train-Loss: 0.5014292764902208 Test-Loss: 0.15288001009971486 Train-f1: 0.47401314220143176 Test-f1: 0.44000000000000006 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8289/20000] Train-Loss: 0.5019771664244023 Test-Loss: 0.15423530207894015 Train-f1: 0.47587373737373734 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8290/20000] Train-Loss: 0.5054258904552319 Test-Loss: 0.1549244603261531 Train-f1: 0.4706504550770326 Test-f1: 0.44951827242524917 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8291/20000] Train-Loss: 0.5052227138945602 Test-Loss: 0.15596936237188613 Train-f1: 0.4625404040404041 Test-f1: 0.44558629776021075 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8292/20000] Train-Loss: 0.5000878612738854 Test-Loss: 0.1539476492518781 Train-f1: 0.4685496067233391 Test-f1: 0.4393073593073593 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [8293/20000] Train-Loss: 0.5006743750120707 Test-Loss: 0.15340887109267298 Train-f1: 0.47009328420520635 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8294/20000] Train-Loss: 0.5010271329072966 Test-Loss: 0.15332288585490447 Train-f1: 0.4710894526560255 Test-f1: 0.44000000000000006 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8295/20000] Train-Loss: 0.5043311930816027 Test-Loss: 0.15345647296804157 Train-f1: 0.4649194563302291 Test-f1: 0.460952380952381 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8296/20000] Train-Loss: 0.5020044411893865 Test-Loss: 0.15351972695932548 Train-f1: 0.4721427893648113 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8297/20000] Train-Loss: 0.5037432628785864 Test-Loss: 0.16152697297859098 Train-f1: 0.46850937335719456 Test-f1: 0.4344337363247945 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [8298/20000] Train-Loss: 0.5017719468116244 Test-Loss: 0.15274790232153293 Train-f1: 0.4762658602658602 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8299/20000] Train-Loss: 0.5034578593192093 Test-Loss: 0.1542352474584707 Train-f1: 0.45452199038384666 Test-f1: 0.44683117900509206 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "Epoch [8300/20000] Train-Loss: 0.497551819665387 Test-Loss: 0.1615329084594427 Train-f1: 0.4725414282614405 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8301/20000] Train-Loss: 0.499677858857723 Test-Loss: 0.15383908119710088 Train-f1: 0.4636076555023923 Test-f1: 0.4376874738165061 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [8302/20000] Train-Loss: 0.5028349879050732 Test-Loss: 0.15415966034304998 Train-f1: 0.4668594664326581 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8303/20000] Train-Loss: 0.4952092938553592 Test-Loss: 0.15611035900972314 Train-f1: 0.46670568117043515 Test-f1: 0.43519668737060047 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [8304/20000] Train-Loss: 0.49968277960470786 Test-Loss: 0.15518515659712484 Train-f1: 0.4749903624356179 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8305/20000] Train-Loss: 0.49853908763557553 Test-Loss: 0.15928676910283582 Train-f1: 0.46635903031405174 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8306/20000] Train-Loss: 0.5010656614168307 Test-Loss: 0.15389449099436578 Train-f1: 0.46858234932769094 Test-f1: 0.446829268292683 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8307/20000] Train-Loss: 0.49916092651344235 Test-Loss: 0.15801861485095098 Train-f1: 0.4730733614211875 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8308/20000] Train-Loss: 0.49751203777969 Test-Loss: 0.1539643192379704 Train-f1: 0.4688708133971291 Test-f1: 0.4360203681942812 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [8309/20000] Train-Loss: 0.5004458030127689 Test-Loss: 0.16078862018363163 Train-f1: 0.4729091718915955 Test-f1: 0.4404334365325077 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8310/20000] Train-Loss: 0.4961066748405213 Test-Loss: 0.15686312839750674 Train-f1: 0.47590102718794985 Test-f1: 0.46554019457245266 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [8311/20000] Train-Loss: 0.5005820286715977 Test-Loss: 0.16377421044859058 Train-f1: 0.45881025485513566 Test-f1: 0.4239074205353209 Train-acc: 0.7781456953642384 Test-acc: 0.7236842105263158\n",
      "Epoch [8312/20000] Train-Loss: 0.4993217637400048 Test-Loss: 0.15600455180854014 Train-f1: 0.4647438682061248 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8313/20000] Train-Loss: 0.4979929393593858 Test-Loss: 0.15348811692380973 Train-f1: 0.47377401641441474 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8314/20000] Train-Loss: 0.4992924449410739 Test-Loss: 0.15426694053992654 Train-f1: 0.46287800494979264 Test-f1: 0.4457303936373703 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8315/20000] Train-Loss: 0.5003551445952199 Test-Loss: 0.16179817505389926 Train-f1: 0.46088358457972783 Test-f1: 0.4344337363247945 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [8316/20000] Train-Loss: 0.5019010180740655 Test-Loss: 0.15545949954095645 Train-f1: 0.4539854292049414 Test-f1: 0.4466252587991718 Train-acc: 0.7748344370860927 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8317/20000] Train-Loss: 0.49913472854068164 Test-Loss: 0.15265660950782822 Train-f1: 0.47181688283571804 Test-f1: 0.44000000000000006 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8318/20000] Train-Loss: 0.4980439175938122 Test-Loss: 0.16312348159058215 Train-f1: 0.4798066271548974 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [8319/20000] Train-Loss: 0.4982646311343359 Test-Loss: 0.1529589041337563 Train-f1: 0.4793520493138611 Test-f1: 0.43574229691876754 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [8320/20000] Train-Loss: 0.5032845295200628 Test-Loss: 0.15556079066053166 Train-f1: 0.4670137344669719 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8321/20000] Train-Loss: 0.4995290246326788 Test-Loss: 0.153210475269946 Train-f1: 0.47635466157570505 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8322/20000] Train-Loss: 0.49841953010620077 Test-Loss: 0.1580439658104592 Train-f1: 0.4760699807438938 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8323/20000] Train-Loss: 0.5027172294939344 Test-Loss: 0.1554197877920633 Train-f1: 0.46344852501841116 Test-f1: 0.44683117900509206 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8324/20000] Train-Loss: 0.4954676530338671 Test-Loss: 0.15771897208453414 Train-f1: 0.46821117978169846 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8325/20000] Train-Loss: 0.4975913728672122 Test-Loss: 0.16906510779063535 Train-f1: 0.4627083203629322 Test-f1: 0.4280288816874183 Train-acc: 0.7847682119205298 Test-acc: 0.7236842105263158\n",
      "Epoch [8326/20000] Train-Loss: 0.5047849124116497 Test-Loss: 0.1530176769149306 Train-f1: 0.4785883470527108 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8327/20000] Train-Loss: 0.5032102097365715 Test-Loss: 0.15255746169681192 Train-f1: 0.47068102211643054 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8328/20000] Train-Loss: 0.49931132155362357 Test-Loss: 0.15341502835223403 Train-f1: 0.470735099916341 Test-f1: 0.4360203681942812 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8329/20000] Train-Loss: 0.4986292257677845 Test-Loss: 0.1533199836927933 Train-f1: 0.47841675045839 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8330/20000] Train-Loss: 0.49778027129657904 Test-Loss: 0.1542015582313815 Train-f1: 0.47012399691707846 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8331/20000] Train-Loss: 0.5001965923590358 Test-Loss: 0.15706533424739597 Train-f1: 0.4620384771228885 Test-f1: 0.4554347826086957 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8332/20000] Train-Loss: 0.5015305000189835 Test-Loss: 0.15315277171627817 Train-f1: 0.4733853039175095 Test-f1: 0.44709236709236705 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8333/20000] Train-Loss: 0.4970961300085953 Test-Loss: 0.15282966192472142 Train-f1: 0.4763934185406608 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8334/20000] Train-Loss: 0.4997401307025433 Test-Loss: 0.15404802135964238 Train-f1: 0.4754366977095394 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8335/20000] Train-Loss: 0.4994229420430551 Test-Loss: 0.15381647558857026 Train-f1: 0.4527336943724983 Test-f1: 0.4466252587991718 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [8336/20000] Train-Loss: 0.49045813038772995 Test-Loss: 0.16802042589713917 Train-f1: 0.4696368463380656 Test-f1: 0.43640336134453783 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8337/20000] Train-Loss: 0.5027277077389268 Test-Loss: 0.15458607938813532 Train-f1: 0.4661469702872118 Test-f1: 0.4567457721207795 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8338/20000] Train-Loss: 0.4995428371252873 Test-Loss: 0.1583557339079908 Train-f1: 0.46267837235228537 Test-f1: 0.44558629776021075 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8339/20000] Train-Loss: 0.5011645405831908 Test-Loss: 0.15391878348379073 Train-f1: 0.4841315438348639 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8340/20000] Train-Loss: 0.4976223397311349 Test-Loss: 0.16084341963570373 Train-f1: 0.4647438682061248 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8341/20000] Train-Loss: 0.4983753416819115 Test-Loss: 0.1524177014403761 Train-f1: 0.4741733447001989 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8342/20000] Train-Loss: 0.5017959052718033 Test-Loss: 0.15299877634199108 Train-f1: 0.4711464518917935 Test-f1: 0.43650793650793657 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8343/20000] Train-Loss: 0.5007882123113462 Test-Loss: 0.15458611490768107 Train-f1: 0.4800418574231523 Test-f1: 0.4466252587991718 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8344/20000] Train-Loss: 0.5002339188021555 Test-Loss: 0.157295803410342 Train-f1: 0.47702476750471046 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8345/20000] Train-Loss: 0.5006905892979486 Test-Loss: 0.1540604933986409 Train-f1: 0.46566340908106146 Test-f1: 0.44951827242524917 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8346/20000] Train-Loss: 0.5010759220515718 Test-Loss: 0.1544587503037034 Train-f1: 0.4734928056514369 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8347/20000] Train-Loss: 0.49755992251560477 Test-Loss: 0.15386226241090015 Train-f1: 0.47551093947209055 Test-f1: 0.4457303936373703 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8348/20000] Train-Loss: 0.49987986456824396 Test-Loss: 0.15750186028142224 Train-f1: 0.46531818181818174 Test-f1: 0.4652641421224871 Train-acc: 0.7880794701986755 Test-acc: 0.7894736842105263\n",
      "Epoch [8349/20000] Train-Loss: 0.5024757901668416 Test-Loss: 0.15282842528655727 Train-f1: 0.46613308877680043 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8350/20000] Train-Loss: 0.5003963882256908 Test-Loss: 0.15506828604165349 Train-f1: 0.47697458146227384 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8351/20000] Train-Loss: 0.4974787999533596 Test-Loss: 0.15250885422906366 Train-f1: 0.46644775531893945 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8352/20000] Train-Loss: 0.4979144147420565 Test-Loss: 0.15750827230786227 Train-f1: 0.4618708827404479 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8353/20000] Train-Loss: 0.4990242239502213 Test-Loss: 0.15306197774073202 Train-f1: 0.4732727779466909 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8354/20000] Train-Loss: 0.5019551611526825 Test-Loss: 0.1542920224463153 Train-f1: 0.4710915024993393 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8355/20000] Train-Loss: 0.4980141944490233 Test-Loss: 0.15525644046383807 Train-f1: 0.46944381478483094 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [8356/20000] Train-Loss: 0.5003315195304463 Test-Loss: 0.15238599160680308 Train-f1: 0.4711470093207416 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8357/20000] Train-Loss: 0.5002896814373478 Test-Loss: 0.15433419360920497 Train-f1: 0.46055903481666627 Test-f1: 0.43932367149758456 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [8358/20000] Train-Loss: 0.4997245724401757 Test-Loss: 0.1528650524222204 Train-f1: 0.4729366977095394 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8359/20000] Train-Loss: 0.5006701540654585 Test-Loss: 0.15376646316150838 Train-f1: 0.4595442071622786 Test-f1: 0.45 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8360/20000] Train-Loss: 0.4968586978569038 Test-Loss: 0.15242917302905143 Train-f1: 0.46566340908106146 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8361/20000] Train-Loss: 0.49885198381399626 Test-Loss: 0.15235525316888643 Train-f1: 0.46497172033322265 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8362/20000] Train-Loss: 0.4986199499783051 Test-Loss: 0.15377709090352276 Train-f1: 0.4729755751494881 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8363/20000] Train-Loss: 0.5003715676666041 Test-Loss: 0.15607115217826553 Train-f1: 0.4647662772238782 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8364/20000] Train-Loss: 0.49228016458871615 Test-Loss: 0.1525578704940697 Train-f1: 0.4639715451777163 Test-f1: 0.4393073593073593 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [8365/20000] Train-Loss: 0.500918996578985 Test-Loss: 0.15411337254616259 Train-f1: 0.45644836670388483 Test-f1: 0.44683117900509206 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8366/20000] Train-Loss: 0.4981719146221787 Test-Loss: 0.15301655667999498 Train-f1: 0.47587373737373734 Test-f1: 0.446829268292683 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8367/20000] Train-Loss: 0.5029269636474878 Test-Loss: 0.15275897429699145 Train-f1: 0.46171360171360176 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8368/20000] Train-Loss: 0.4954682978874933 Test-Loss: 0.15379565288458702 Train-f1: 0.46499588216661386 Test-f1: 0.4526190476190476 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8369/20000] Train-Loss: 0.4966399974156143 Test-Loss: 0.1528495884864633 Train-f1: 0.46915284099251214 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8370/20000] Train-Loss: 0.49946021216018116 Test-Loss: 0.15311333517242448 Train-f1: 0.4678680625381991 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8371/20000] Train-Loss: 0.4993348742164283 Test-Loss: 0.15409613406787964 Train-f1: 0.4577767830158736 Test-f1: 0.44683117900509206 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8372/20000] Train-Loss: 0.49733439900316917 Test-Loss: 0.15313087662802902 Train-f1: 0.4644521826224014 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8373/20000] Train-Loss: 0.5015283606112775 Test-Loss: 0.153219516502565 Train-f1: 0.476160474171225 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8374/20000] Train-Loss: 0.4960955661964704 Test-Loss: 0.15790432770955007 Train-f1: 0.4736533566524468 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8375/20000] Train-Loss: 0.4995116429862497 Test-Loss: 0.15407675871710716 Train-f1: 0.47342074814382773 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8376/20000] Train-Loss: 0.4978089607372223 Test-Loss: 0.15221617483429176 Train-f1: 0.46118301079505564 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8377/20000] Train-Loss: 0.4963751761260626 Test-Loss: 0.15459316153033337 Train-f1: 0.47590102718794985 Test-f1: 0.4385580319084156 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [8378/20000] Train-Loss: 0.49739686775468334 Test-Loss: 0.1531362048779882 Train-f1: 0.47105627501765956 Test-f1: 0.4385580319084156 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8379/20000] Train-Loss: 0.5051026562681651 Test-Loss: 0.1559090250305572 Train-f1: 0.4700722924589714 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8380/20000] Train-Loss: 0.4992715032072936 Test-Loss: 0.15296575316933828 Train-f1: 0.47410564009394196 Test-f1: 0.449645003282331 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8381/20000] Train-Loss: 0.5016568480040892 Test-Loss: 0.1522013546378629 Train-f1: 0.470772777946691 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8382/20000] Train-Loss: 0.5002427209121219 Test-Loss: 0.156186178702765 Train-f1: 0.4651444901291809 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8383/20000] Train-Loss: 0.4986869692937511 Test-Loss: 0.15351502558301125 Train-f1: 0.48082983196598217 Test-f1: 0.446829268292683 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8384/20000] Train-Loss: 0.5001617787269248 Test-Loss: 0.1548378991259254 Train-f1: 0.4705456570505871 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8385/20000] Train-Loss: 0.49799641436915476 Test-Loss: 0.15360239163962255 Train-f1: 0.47608467778689273 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8386/20000] Train-Loss: 0.4944240540607443 Test-Loss: 0.17141801787353167 Train-f1: 0.4720464848922825 Test-f1: 0.4443781094527363 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8387/20000] Train-Loss: 0.5004385419312021 Test-Loss: 0.153721351158134 Train-f1: 0.4662102242515062 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8388/20000] Train-Loss: 0.49819912129569344 Test-Loss: 0.15304159017876817 Train-f1: 0.47068102211643054 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8389/20000] Train-Loss: 0.4984821178984536 Test-Loss: 0.15623775230242024 Train-f1: 0.4706504550770326 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8390/20000] Train-Loss: 0.4950201206513105 Test-Loss: 0.15379228289604913 Train-f1: 0.4746225858569016 Test-f1: 0.4385580319084156 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8391/20000] Train-Loss: 0.49281291974298314 Test-Loss: 0.15220320499716963 Train-f1: 0.470899547172579 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8392/20000] Train-Loss: 0.4994318258801759 Test-Loss: 0.15288263651066814 Train-f1: 0.4662740615809208 Test-f1: 0.43909729127120434 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [8393/20000] Train-Loss: 0.49848883570905456 Test-Loss: 0.15300672378480346 Train-f1: 0.4676010110632676 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8394/20000] Train-Loss: 0.49774106533645157 Test-Loss: 0.15268516890998754 Train-f1: 0.47587373737373734 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8395/20000] Train-Loss: 0.49981458074339963 Test-Loss: 0.15404055992010834 Train-f1: 0.46531818181818174 Test-f1: 0.45726405090137856 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8396/20000] Train-Loss: 0.49297589806346376 Test-Loss: 0.1557482324186096 Train-f1: 0.47393939393939394 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8397/20000] Train-Loss: 0.5003241793311703 Test-Loss: 0.15216287779803078 Train-f1: 0.47093226821707257 Test-f1: 0.4395848595848596 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8398/20000] Train-Loss: 0.49778515752326713 Test-Loss: 0.15998549394059644 Train-f1: 0.4628335536685819 Test-f1: 0.4466252587991718 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8399/20000] Train-Loss: 0.4981084731276683 Test-Loss: 0.15719977651463446 Train-f1: 0.4609968741119636 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8400/20000] Train-Loss: 0.4951881691641209 Test-Loss: 0.15677113560900288 Train-f1: 0.4763514739683826 Test-f1: 0.46554019457245266 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [8401/20000] Train-Loss: 0.49430309356198826 Test-Loss: 0.15284156597961268 Train-f1: 0.4827855020796196 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [8402/20000] Train-Loss: 0.4969308372032529 Test-Loss: 0.16378720819006154 Train-f1: 0.4819812895462133 Test-f1: 0.4341972308997648 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [8403/20000] Train-Loss: 0.5005672574911186 Test-Loss: 0.15708595374897397 Train-f1: 0.4660070325467684 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8404/20000] Train-Loss: 0.4981819353775318 Test-Loss: 0.15326338810094836 Train-f1: 0.474487070190395 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8405/20000] Train-Loss: 0.49534320614653915 Test-Loss: 0.15754325967136387 Train-f1: 0.4798066271548974 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [8406/20000] Train-Loss: 0.49823968739494495 Test-Loss: 0.15674847338590744 Train-f1: 0.4580779096066614 Test-f1: 0.4554347826086957 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [8407/20000] Train-Loss: 0.49638758037805053 Test-Loss: 0.15470889672141813 Train-f1: 0.4745997910135841 Test-f1: 0.4360203681942812 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8408/20000] Train-Loss: 0.5042341263077149 Test-Loss: 0.1534744810726338 Train-f1: 0.4725624200076755 Test-f1: 0.446829268292683 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8409/20000] Train-Loss: 0.4999309984995368 Test-Loss: 0.15186092448378605 Train-f1: 0.47043669770953944 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8410/20000] Train-Loss: 0.4974738673578813 Test-Loss: 0.15268772337755493 Train-f1: 0.47882996282996276 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8411/20000] Train-Loss: 0.4978480993692361 Test-Loss: 0.1580445280645062 Train-f1: 0.4687666628760597 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8412/20000] Train-Loss: 0.49801573957638307 Test-Loss: 0.15419018276168844 Train-f1: 0.4692990818705104 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8413/20000] Train-Loss: 0.49936195539635847 Test-Loss: 0.15391728876423955 Train-f1: 0.46517807701988423 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8414/20000] Train-Loss: 0.4998593654113372 Test-Loss: 0.15230459762971219 Train-f1: 0.46306427974390785 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8415/20000] Train-Loss: 0.4988125132096944 Test-Loss: 0.1558784168138753 Train-f1: 0.4624843176059753 Test-f1: 0.44558629776021075 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8416/20000] Train-Loss: 0.5009682295132233 Test-Loss: 0.1552969663019847 Train-f1: 0.47537833995793816 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8417/20000] Train-Loss: 0.49465840698717467 Test-Loss: 0.1539548034143725 Train-f1: 0.46684453220484184 Test-f1: 0.460952380952381 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8418/20000] Train-Loss: 0.49795816412013966 Test-Loss: 0.15266621895081633 Train-f1: 0.4729017746948562 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8419/20000] Train-Loss: 0.49775050072607385 Test-Loss: 0.1527758547569544 Train-f1: 0.475952393713448 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8420/20000] Train-Loss: 0.4990494012269417 Test-Loss: 0.15965204944594133 Train-f1: 0.4732727779466909 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8421/20000] Train-Loss: 0.49365935658864474 Test-Loss: 0.15316299871314643 Train-f1: 0.47370175770175765 Test-f1: 0.45 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8422/20000] Train-Loss: 0.4967003775496816 Test-Loss: 0.15360582501728137 Train-f1: 0.47409961889098157 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8423/20000] Train-Loss: 0.49691173376361797 Test-Loss: 0.15289816115492944 Train-f1: 0.47688150692747494 Test-f1: 0.45 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8424/20000] Train-Loss: 0.496304145393466 Test-Loss: 0.15507210849674008 Train-f1: 0.46241945633022913 Test-f1: 0.4526190476190476 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8425/20000] Train-Loss: 0.4966061843099575 Test-Loss: 0.15236544440774225 Train-f1: 0.46613308877680043 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8426/20000] Train-Loss: 0.4965090293495778 Test-Loss: 0.15463582629918796 Train-f1: 0.47782875860734747 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8427/20000] Train-Loss: 0.4996335708650891 Test-Loss: 0.1546345252917744 Train-f1: 0.46306427974390785 Test-f1: 0.4554347826086957 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8428/20000] Train-Loss: 0.4946402966541396 Test-Loss: 0.15783535225519735 Train-f1: 0.4680959595959596 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8429/20000] Train-Loss: 0.4984880321426285 Test-Loss: 0.15388518974215115 Train-f1: 0.46219728097988966 Test-f1: 0.4466252587991718 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8430/20000] Train-Loss: 0.4980230287273138 Test-Loss: 0.15816683001480228 Train-f1: 0.47348003891770957 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8431/20000] Train-Loss: 0.49699041056966997 Test-Loss: 0.15314760167961308 Train-f1: 0.47608467778689273 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8432/20000] Train-Loss: 0.49486194874691214 Test-Loss: 0.15514215017918578 Train-f1: 0.46703417623061405 Test-f1: 0.44937417654808953 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8433/20000] Train-Loss: 0.49554364673387313 Test-Loss: 0.15290992836417883 Train-f1: 0.4714103559946957 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8434/20000] Train-Loss: 0.49381527288970967 Test-Loss: 0.15249719076088952 Train-f1: 0.47093226821707257 Test-f1: 0.4277159468438538 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [8435/20000] Train-Loss: 0.49949538328005416 Test-Loss: 0.15288408897719005 Train-f1: 0.46172062084257204 Test-f1: 0.44990810208201515 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8436/20000] Train-Loss: 0.4966051359728899 Test-Loss: 0.15212939205258272 Train-f1: 0.4669423648247178 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8437/20000] Train-Loss: 0.49289410321141314 Test-Loss: 0.15326558895608952 Train-f1: 0.47161812121999114 Test-f1: 0.4376874738165061 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8438/20000] Train-Loss: 0.4992364066964549 Test-Loss: 0.16226927734707977 Train-f1: 0.46743490655685777 Test-f1: 0.4341972308997648 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [8439/20000] Train-Loss: 0.49837639990239085 Test-Loss: 0.1581326861522067 Train-f1: 0.4683844975164357 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8440/20000] Train-Loss: 0.4961163376249759 Test-Loss: 0.15232181601390196 Train-f1: 0.4635040262743592 Test-f1: 0.4497330282227307 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8441/20000] Train-Loss: 0.498113422913832 Test-Loss: 0.1529183661296222 Train-f1: 0.47537833995793816 Test-f1: 0.45726405090137856 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8442/20000] Train-Loss: 0.49687912933723843 Test-Loss: 0.15795374453880726 Train-f1: 0.46747895582678184 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8443/20000] Train-Loss: 0.4943470551727222 Test-Loss: 0.15652678958151156 Train-f1: 0.4733853039175095 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [8444/20000] Train-Loss: 0.49761216500223265 Test-Loss: 0.15179726790672926 Train-f1: 0.4662392344497608 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8445/20000] Train-Loss: 0.49366311158571213 Test-Loss: 0.15638002105196858 Train-f1: 0.46990999193816096 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8446/20000] Train-Loss: 0.49621996066978336 Test-Loss: 0.1546300689364075 Train-f1: 0.4741733447001989 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8447/20000] Train-Loss: 0.49846841105884065 Test-Loss: 0.15204705892735046 Train-f1: 0.4817237890924836 Test-f1: 0.4393073593073593 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [8448/20000] Train-Loss: 0.49566595363894767 Test-Loss: 0.15542057787553806 Train-f1: 0.47761761499891 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [8449/20000] Train-Loss: 0.49524586237701446 Test-Loss: 0.15425123156837572 Train-f1: 0.4772587607040162 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8450/20000] Train-Loss: 0.49492023333716806 Test-Loss: 0.1531504908723239 Train-f1: 0.4776249955500706 Test-f1: 0.4536974789915966 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8451/20000] Train-Loss: 0.496528082607746 Test-Loss: 0.1553111570315896 Train-f1: 0.47342074814382773 Test-f1: 0.44951827242524917 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8452/20000] Train-Loss: 0.4924604661935325 Test-Loss: 0.15265985227457396 Train-f1: 0.46353568617939783 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8453/20000] Train-Loss: 0.4947759544962886 Test-Loss: 0.1570727943885683 Train-f1: 0.47119230769230763 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8454/20000] Train-Loss: 0.5023956400997353 Test-Loss: 0.15409049899578495 Train-f1: 0.4725624200076755 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8455/20000] Train-Loss: 0.4938926165264278 Test-Loss: 0.15376528313853177 Train-f1: 0.47992803360545294 Test-f1: 0.4359678460364959 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [8456/20000] Train-Loss: 0.49443657415913506 Test-Loss: 0.15673927304680724 Train-f1: 0.4723129553373456 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8457/20000] Train-Loss: 0.4953984521201565 Test-Loss: 0.1533471440675679 Train-f1: 0.4738675403271676 Test-f1: 0.44951827242524917 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8458/20000] Train-Loss: 0.4929852228380792 Test-Loss: 0.157545062560547 Train-f1: 0.47141505513259235 Test-f1: 0.4652641421224871 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [8459/20000] Train-Loss: 0.4938159263617227 Test-Loss: 0.16488766510917527 Train-f1: 0.4704755751494882 Test-f1: 0.4545727136431784 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8460/20000] Train-Loss: 0.4936860937700654 Test-Loss: 0.15390418266660758 Train-f1: 0.469127953127953 Test-f1: 0.42712259092441096 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [8461/20000] Train-Loss: 0.4970154731334284 Test-Loss: 0.1521458256336988 Train-f1: 0.4745564413828185 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8462/20000] Train-Loss: 0.49502913751013145 Test-Loss: 0.15489779285343236 Train-f1: 0.47326674548596126 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8463/20000] Train-Loss: 0.4946966349659115 Test-Loss: 0.15364887107459643 Train-f1: 0.476160474171225 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8464/20000] Train-Loss: 0.4956495954917017 Test-Loss: 0.15238876131397577 Train-f1: 0.47186862108563227 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8465/20000] Train-Loss: 0.4937334163375639 Test-Loss: 0.1554907587394462 Train-f1: 0.47692020145382175 Test-f1: 0.43519668737060047 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [8466/20000] Train-Loss: 0.4963677476962989 Test-Loss: 0.1523801775810969 Train-f1: 0.48122376531046546 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8467/20000] Train-Loss: 0.49807819039677603 Test-Loss: 0.16177577923408237 Train-f1: 0.46271989791267887 Test-f1: 0.4344337363247945 Train-acc: 0.7847682119205298 Test-acc: 0.7368421052631579\n",
      "Epoch [8468/20000] Train-Loss: 0.49592212820140685 Test-Loss: 0.15225446361567863 Train-f1: 0.47272689334661155 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8469/20000] Train-Loss: 0.4934284156363559 Test-Loss: 0.16016915748898522 Train-f1: 0.46622563228189834 Test-f1: 0.44027149321266973 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [8470/20000] Train-Loss: 0.4961614577898121 Test-Loss: 0.15433326147587237 Train-f1: 0.4604113084302918 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8471/20000] Train-Loss: 0.4934180764330735 Test-Loss: 0.1545542235830684 Train-f1: 0.47554379475506237 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8472/20000] Train-Loss: 0.49663146410324543 Test-Loss: 0.15592047482549298 Train-f1: 0.47517100782000926 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8473/20000] Train-Loss: 0.4959198422383405 Test-Loss: 0.15281874295826023 Train-f1: 0.46976521721047265 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8474/20000] Train-Loss: 0.4973688421111783 Test-Loss: 0.15601531688223702 Train-f1: 0.47541480768669986 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8475/20000] Train-Loss: 0.4964690160674 Test-Loss: 0.15383330847354232 Train-f1: 0.47393939393939394 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8476/20000] Train-Loss: 0.49334554214174 Test-Loss: 0.15687382735275318 Train-f1: 0.4718534533390478 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [8477/20000] Train-Loss: 0.49497376312014124 Test-Loss: 0.15430454430632795 Train-f1: 0.46829635229635225 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8478/20000] Train-Loss: 0.49512320704018264 Test-Loss: 0.15397397604267804 Train-f1: 0.4684349524349525 Test-f1: 0.44937417654808953 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8479/20000] Train-Loss: 0.49408381982369926 Test-Loss: 0.15477055025045888 Train-f1: 0.4781953183991946 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8480/20000] Train-Loss: 0.49855401232226726 Test-Loss: 0.15359587142822925 Train-f1: 0.4781640245223235 Test-f1: 0.4567457721207795 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [8481/20000] Train-Loss: 0.4948701019382597 Test-Loss: 0.15392515310893404 Train-f1: 0.46517807701988423 Test-f1: 0.4510087552341073 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8482/20000] Train-Loss: 0.5016694696572649 Test-Loss: 0.1563398720792873 Train-f1: 0.46554317777819954 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8483/20000] Train-Loss: 0.493930706739919 Test-Loss: 0.15413961663409728 Train-f1: 0.47321921895860725 Test-f1: 0.4526190476190476 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8484/20000] Train-Loss: 0.4968216498915448 Test-Loss: 0.1535299260996346 Train-f1: 0.46858234932769094 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8485/20000] Train-Loss: 0.4968356053449014 Test-Loss: 0.15205012760665276 Train-f1: 0.46814937654681027 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8486/20000] Train-Loss: 0.4949463333715529 Test-Loss: 0.1534117795586621 Train-f1: 0.4783737373737374 Test-f1: 0.4466252587991718 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8487/20000] Train-Loss: 0.4924803724375253 Test-Loss: 0.15687069234411238 Train-f1: 0.4755959595959595 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8488/20000] Train-Loss: 0.49537222967025474 Test-Loss: 0.15647620331420672 Train-f1: 0.4795517804011902 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [8489/20000] Train-Loss: 0.4913819413102646 Test-Loss: 0.15145052328908729 Train-f1: 0.46656385056385047 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8490/20000] Train-Loss: 0.4946926373083134 Test-Loss: 0.15388609394232394 Train-f1: 0.47815682560208106 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8491/20000] Train-Loss: 0.49502451059668756 Test-Loss: 0.15243382242543949 Train-f1: 0.47921130106263343 Test-f1: 0.4277159468438538 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [8492/20000] Train-Loss: 0.4968742258475475 Test-Loss: 0.15219631685315174 Train-f1: 0.4668674250028725 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8493/20000] Train-Loss: 0.49652572925780736 Test-Loss: 0.15374065781060253 Train-f1: 0.47643164289127016 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8494/20000] Train-Loss: 0.49695235074147154 Test-Loss: 0.15223022532734798 Train-f1: 0.46314229652303596 Test-f1: 0.44990810208201515 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8495/20000] Train-Loss: 0.4950820637848785 Test-Loss: 0.15435678852891122 Train-f1: 0.4730959595959595 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8496/20000] Train-Loss: 0.4958458229261874 Test-Loss: 0.15149417672396975 Train-f1: 0.46738989277798854 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8497/20000] Train-Loss: 0.49594222979678637 Test-Loss: 0.15147729005196364 Train-f1: 0.4626611727877551 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8498/20000] Train-Loss: 0.49566907342172806 Test-Loss: 0.15242144671126567 Train-f1: 0.4679412960890332 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8499/20000] Train-Loss: 0.4948911256766356 Test-Loss: 0.15311676281937336 Train-f1: 0.47047949293121005 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8500/20000] Train-Loss: 0.49231312611376077 Test-Loss: 0.15562599751829356 Train-f1: 0.46099175888862726 Test-f1: 0.4239140347746783 Train-acc: 0.7814569536423841 Test-acc: 0.7368421052631579\n",
      "Epoch [8501/20000] Train-Loss: 0.4959953248292628 Test-Loss: 0.1532185439393399 Train-f1: 0.4721468028286016 Test-f1: 0.45726405090137856 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8502/20000] Train-Loss: 0.4922398421616787 Test-Loss: 0.1548417307194089 Train-f1: 0.47598783392133537 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8503/20000] Train-Loss: 0.49847846087734593 Test-Loss: 0.15391484908603087 Train-f1: 0.46107844474416676 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8504/20000] Train-Loss: 0.4947184293233819 Test-Loss: 0.15179740417633286 Train-f1: 0.4727170200899479 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8505/20000] Train-Loss: 0.495862775065829 Test-Loss: 0.15769065304973306 Train-f1: 0.46760315665650226 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8506/20000] Train-Loss: 0.49578854078866613 Test-Loss: 0.1515145773304804 Train-f1: 0.46874998430669496 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8507/20000] Train-Loss: 0.4957217320924616 Test-Loss: 0.15321265327430633 Train-f1: 0.46554317777819954 Test-f1: 0.4497330282227307 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8508/20000] Train-Loss: 0.49096863214959435 Test-Loss: 0.154562918620091 Train-f1: 0.4747399171571834 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8509/20000] Train-Loss: 0.4946690117135462 Test-Loss: 0.15211989714490262 Train-f1: 0.47355305254193214 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8510/20000] Train-Loss: 0.49494498364528144 Test-Loss: 0.15701524965080163 Train-f1: 0.47348003891770957 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8511/20000] Train-Loss: 0.4940041954635716 Test-Loss: 0.1519679620285587 Train-f1: 0.46684982131962 Test-f1: 0.4277159468438538 Train-acc: 0.7880794701986755 Test-acc: 0.7368421052631579\n",
      "Epoch [8512/20000] Train-Loss: 0.4931777537821657 Test-Loss: 0.1543802033857467 Train-f1: 0.4698739309471017 Test-f1: 0.4542943938138636 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8513/20000] Train-Loss: 0.4955952299703887 Test-Loss: 0.156455874752981 Train-f1: 0.46973559361388145 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8514/20000] Train-Loss: 0.48566312195482125 Test-Loss: 0.1534766391685229 Train-f1: 0.47964494722443085 Test-f1: 0.4263385678052993 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [8515/20000] Train-Loss: 0.4992321959765254 Test-Loss: 0.1547613440558377 Train-f1: 0.46947341150479394 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [8516/20000] Train-Loss: 0.49175246475623385 Test-Loss: 0.1512688228314533 Train-f1: 0.4709990549990549 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8517/20000] Train-Loss: 0.4895444167595372 Test-Loss: 0.15708381598801902 Train-f1: 0.4669151138716356 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8518/20000] Train-Loss: 0.49371090545026314 Test-Loss: 0.15290060725124185 Train-f1: 0.4709990549990549 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8519/20000] Train-Loss: 0.4974962051920773 Test-Loss: 0.15688979985957877 Train-f1: 0.4730733614211875 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8520/20000] Train-Loss: 0.48715327410448334 Test-Loss: 0.15479465379108798 Train-f1: 0.4691606240661246 Test-f1: 0.45119841858166543 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8521/20000] Train-Loss: 0.4969272333469278 Test-Loss: 0.15295118503457178 Train-f1: 0.45897815317123775 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8522/20000] Train-Loss: 0.5026426764270446 Test-Loss: 0.16049521065875313 Train-f1: 0.47172436553102737 Test-f1: 0.44076216533124873 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8523/20000] Train-Loss: 0.4920539036636288 Test-Loss: 0.16267873234192698 Train-f1: 0.4749939608994255 Test-f1: 0.4495524296675192 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8524/20000] Train-Loss: 0.4951134679519284 Test-Loss: 0.1557391974011435 Train-f1: 0.46028915614205496 Test-f1: 0.44558629776021075 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8525/20000] Train-Loss: 0.49868766905097867 Test-Loss: 0.15391905979741824 Train-f1: 0.4639881923757311 Test-f1: 0.4526190476190476 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8526/20000] Train-Loss: 0.4979985874891987 Test-Loss: 0.1574945848435897 Train-f1: 0.4608511224210086 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8527/20000] Train-Loss: 0.49547548716162193 Test-Loss: 0.15235008131333225 Train-f1: 0.47611715510603475 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8528/20000] Train-Loss: 0.49865889798390794 Test-Loss: 0.15279803499243727 Train-f1: 0.46850937335719456 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8529/20000] Train-Loss: 0.49678116922153626 Test-Loss: 0.15174058328661427 Train-f1: 0.4630557198969486 Test-f1: 0.4497330282227307 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8530/20000] Train-Loss: 0.49135812182657673 Test-Loss: 0.15443327142780322 Train-f1: 0.46549767569045664 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8531/20000] Train-Loss: 0.49158771390673733 Test-Loss: 0.1512967383213941 Train-f1: 0.48186274509803917 Test-f1: 0.4505267472842796 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8532/20000] Train-Loss: 0.4925523427989398 Test-Loss: 0.15183329557616032 Train-f1: 0.4729366977095394 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8533/20000] Train-Loss: 0.4947377498060683 Test-Loss: 0.1520810553299462 Train-f1: 0.46851365074151496 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8534/20000] Train-Loss: 0.49745895528571826 Test-Loss: 0.15377595227955126 Train-f1: 0.4635171748786773 Test-f1: 0.4466252587991718 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8535/20000] Train-Loss: 0.49304636502119104 Test-Loss: 0.15260486443878857 Train-f1: 0.47535962280487826 Test-f1: 0.4536974789915966 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8536/20000] Train-Loss: 0.49637284398588566 Test-Loss: 0.15763012825191952 Train-f1: 0.46834569472168985 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8537/20000] Train-Loss: 0.4914906816522408 Test-Loss: 0.1574207279385833 Train-f1: 0.468892298948565 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8538/20000] Train-Loss: 0.4917180163108255 Test-Loss: 0.15364301803101524 Train-f1: 0.47250064211621157 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8539/20000] Train-Loss: 0.4962730226362005 Test-Loss: 0.15659286861934563 Train-f1: 0.47326674548596126 Test-f1: 0.4567457721207795 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8540/20000] Train-Loss: 0.49465527856838615 Test-Loss: 0.15347926728534572 Train-f1: 0.48129104975541354 Test-f1: 0.44683117900509206 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8541/20000] Train-Loss: 0.4913856474084128 Test-Loss: 0.1522498490170055 Train-f1: 0.4767655502392344 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8542/20000] Train-Loss: 0.4944160464660566 Test-Loss: 0.15412206272228585 Train-f1: 0.46046544446544446 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8543/20000] Train-Loss: 0.49763591580856464 Test-Loss: 0.1516172181596469 Train-f1: 0.4672690840308521 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8544/20000] Train-Loss: 0.4944082897923514 Test-Loss: 0.1535870502559593 Train-f1: 0.47574723704546107 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8545/20000] Train-Loss: 0.49802309501758696 Test-Loss: 0.1550752189144506 Train-f1: 0.4738701298701299 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8546/20000] Train-Loss: 0.49536550249893174 Test-Loss: 0.15149814142917115 Train-f1: 0.46215481892477195 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8547/20000] Train-Loss: 0.4940086174415355 Test-Loss: 0.16033487229115956 Train-f1: 0.4751700981944884 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8548/20000] Train-Loss: 0.49881151817552144 Test-Loss: 0.1526031097667359 Train-f1: 0.46899175888862726 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8549/20000] Train-Loss: 0.4930905908189168 Test-Loss: 0.15984982519299304 Train-f1: 0.45943273761892056 Test-f1: 0.44683117900509206 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8550/20000] Train-Loss: 0.4926867560822103 Test-Loss: 0.15730659960137172 Train-f1: 0.47231567473294084 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8551/20000] Train-Loss: 0.493821299203776 Test-Loss: 0.15365954952335656 Train-f1: 0.48026851294400963 Test-f1: 0.4360203681942812 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [8552/20000] Train-Loss: 0.49358628207735517 Test-Loss: 0.15439111821605705 Train-f1: 0.4649944837770924 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8553/20000] Train-Loss: 0.4955583518167167 Test-Loss: 0.15237138355448426 Train-f1: 0.47105627501765956 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8554/20000] Train-Loss: 0.4936768632731945 Test-Loss: 0.15245336461085074 Train-f1: 0.4745574620027175 Test-f1: 0.4457303936373703 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8555/20000] Train-Loss: 0.4940734442793667 Test-Loss: 0.15157663228494245 Train-f1: 0.4833239803674586 Test-f1: 0.44709236709236705 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8556/20000] Train-Loss: 0.4976923357184625 Test-Loss: 0.15210494313483805 Train-f1: 0.47709164608271576 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8557/20000] Train-Loss: 0.49174952256557225 Test-Loss: 0.15334895365986256 Train-f1: 0.46286304118876825 Test-f1: 0.44937417654808953 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8558/20000] Train-Loss: 0.493393548063012 Test-Loss: 0.15238081425810898 Train-f1: 0.46838960835099286 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8559/20000] Train-Loss: 0.493572024707438 Test-Loss: 0.1594431268554624 Train-f1: 0.46884001136686554 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8560/20000] Train-Loss: 0.4927370670115733 Test-Loss: 0.15201924791784202 Train-f1: 0.4605165090432708 Test-f1: 0.44683117900509206 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8561/20000] Train-Loss: 0.49268240385836126 Test-Loss: 0.15125306688073714 Train-f1: 0.4837301175542823 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8562/20000] Train-Loss: 0.492498927240404 Test-Loss: 0.1514200596421688 Train-f1: 0.4672960814080035 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8563/20000] Train-Loss: 0.4947895127539838 Test-Loss: 0.15218573041464975 Train-f1: 0.47321921895860725 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8564/20000] Train-Loss: 0.4945615716880923 Test-Loss: 0.15245095287507396 Train-f1: 0.4661123427496626 Test-f1: 0.44649416182596957 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8565/20000] Train-Loss: 0.49283270189201706 Test-Loss: 0.15450851689162665 Train-f1: 0.47498020825836457 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8566/20000] Train-Loss: 0.4944310722635613 Test-Loss: 0.15467867302175412 Train-f1: 0.46349796366469515 Test-f1: 0.44649416182596957 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8567/20000] Train-Loss: 0.49527563578868433 Test-Loss: 0.1520951079990401 Train-f1: 0.4733405601490707 Test-f1: 0.45 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8568/20000] Train-Loss: 0.48922487558518796 Test-Loss: 0.15138759083009065 Train-f1: 0.47405936261721227 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8569/20000] Train-Loss: 0.4939993162716926 Test-Loss: 0.1535569208454935 Train-f1: 0.46760315665650226 Test-f1: 0.44937417654808953 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8570/20000] Train-Loss: 0.4937413449008229 Test-Loss: 0.1520462085649006 Train-f1: 0.4684349524349525 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8571/20000] Train-Loss: 0.49219964918831044 Test-Loss: 0.15666542010863904 Train-f1: 0.47256612001137555 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [8572/20000] Train-Loss: 0.49539677568128393 Test-Loss: 0.15408168263193633 Train-f1: 0.4688592308740228 Test-f1: 0.44951827242524917 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8573/20000] Train-Loss: 0.49830891013548206 Test-Loss: 0.15273086134540476 Train-f1: 0.46554317777819954 Test-f1: 0.4526190476190476 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8574/20000] Train-Loss: 0.4944437144877902 Test-Loss: 0.15223863521254755 Train-f1: 0.4761237102967503 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8575/20000] Train-Loss: 0.4891356086930959 Test-Loss: 0.153841115394312 Train-f1: 0.47409961889098157 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8576/20000] Train-Loss: 0.49423710323485076 Test-Loss: 0.15096926610130754 Train-f1: 0.4672960814080035 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8577/20000] Train-Loss: 0.4919943274230334 Test-Loss: 0.15275588290729133 Train-f1: 0.47093226821707257 Test-f1: 0.4385580319084156 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8578/20000] Train-Loss: 0.49597618916754616 Test-Loss: 0.15341825099709 Train-f1: 0.4776091225847323 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8579/20000] Train-Loss: 0.49210658175011984 Test-Loss: 0.1509913484828502 Train-f1: 0.47363692017894987 Test-f1: 0.4505267472842796 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8580/20000] Train-Loss: 0.49487891871050066 Test-Loss: 0.15192242384813645 Train-f1: 0.4591462065914621 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8581/20000] Train-Loss: 0.4954854649109653 Test-Loss: 0.15240434237002262 Train-f1: 0.46779168657429526 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8582/20000] Train-Loss: 0.497517868427036 Test-Loss: 0.15438119099880016 Train-f1: 0.4765085983599307 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8583/20000] Train-Loss: 0.49235254403660456 Test-Loss: 0.1511524220007513 Train-f1: 0.46386711015764914 Test-f1: 0.4395848595848596 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [8584/20000] Train-Loss: 0.49390442748650143 Test-Loss: 0.15906741527836935 Train-f1: 0.47128345782609954 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8585/20000] Train-Loss: 0.4965655187517668 Test-Loss: 0.15612502030509687 Train-f1: 0.4682754534682344 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8586/20000] Train-Loss: 0.4940001220328627 Test-Loss: 0.15179802142171098 Train-f1: 0.4656120504445448 Test-f1: 0.449645003282331 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8587/20000] Train-Loss: 0.49282412915629525 Test-Loss: 0.15630191145521286 Train-f1: 0.47186862108563227 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8588/20000] Train-Loss: 0.4884502184542003 Test-Loss: 0.15163871786717661 Train-f1: 0.47233892075160816 Test-f1: 0.43909729127120434 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8589/20000] Train-Loss: 0.49412378691540637 Test-Loss: 0.15427797658148998 Train-f1: 0.4708684887835429 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8590/20000] Train-Loss: 0.49238448173561483 Test-Loss: 0.15221225418920542 Train-f1: 0.4733853039175095 Test-f1: 0.45726405090137856 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8591/20000] Train-Loss: 0.4918313170192753 Test-Loss: 0.15571323217094796 Train-f1: 0.47536612938053324 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8592/20000] Train-Loss: 0.4939382512912698 Test-Loss: 0.15154937605542085 Train-f1: 0.47199469143994693 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8593/20000] Train-Loss: 0.49323313057284696 Test-Loss: 0.15168031350239572 Train-f1: 0.4733853039175095 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8594/20000] Train-Loss: 0.49647378282540877 Test-Loss: 0.15149867304861916 Train-f1: 0.46301153220857927 Test-f1: 0.4497330282227307 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8595/20000] Train-Loss: 0.4908870196210375 Test-Loss: 0.15302124465386963 Train-f1: 0.4727170200899479 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8596/20000] Train-Loss: 0.4936696440487861 Test-Loss: 0.15128214010846144 Train-f1: 0.4662392344497608 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8597/20000] Train-Loss: 0.49209010944731396 Test-Loss: 0.15616077227413253 Train-f1: 0.4653523392677589 Test-f1: 0.44649416182596957 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8598/20000] Train-Loss: 0.49437897178883117 Test-Loss: 0.15379362922836662 Train-f1: 0.464623954727241 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8599/20000] Train-Loss: 0.49372227543936287 Test-Loss: 0.1511640002606069 Train-f1: 0.46874998430669496 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8600/20000] Train-Loss: 0.4930951029871278 Test-Loss: 0.16401975311493186 Train-f1: 0.46229714142140904 Test-f1: 0.4426916799402199 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [8601/20000] Train-Loss: 0.49357681490070265 Test-Loss: 0.15075903865305082 Train-f1: 0.4763035868456165 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8602/20000] Train-Loss: 0.48987896216563265 Test-Loss: 0.15925273293004957 Train-f1: 0.4755959595959595 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8603/20000] Train-Loss: 0.4927559805131915 Test-Loss: 0.15247958568852418 Train-f1: 0.46228988047606345 Test-f1: 0.44937417654808953 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8604/20000] Train-Loss: 0.49426151370250865 Test-Loss: 0.15283040781095772 Train-f1: 0.46600865800865793 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8605/20000] Train-Loss: 0.49619113485529115 Test-Loss: 0.152553981606886 Train-f1: 0.46531818181818174 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8606/20000] Train-Loss: 0.49113948705175037 Test-Loss: 0.15214716054962005 Train-f1: 0.46830438621944037 Test-f1: 0.460952380952381 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8607/20000] Train-Loss: 0.4928325468252622 Test-Loss: 0.15086949143726128 Train-f1: 0.4687304913742031 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8608/20000] Train-Loss: 0.49291039621010385 Test-Loss: 0.1511252102806759 Train-f1: 0.4678181818181818 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8609/20000] Train-Loss: 0.4892273207768211 Test-Loss: 0.15535154967602513 Train-f1: 0.48383882045605053 Test-f1: 0.4461529397902675 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8610/20000] Train-Loss: 0.49816391469527077 Test-Loss: 0.1518653633037556 Train-f1: 0.47496619292147957 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8611/20000] Train-Loss: 0.49305802314654235 Test-Loss: 0.15100551758279634 Train-f1: 0.47410564009394196 Test-f1: 0.4395848595848596 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8612/20000] Train-Loss: 0.48978183571088935 Test-Loss: 0.15299736087229238 Train-f1: 0.4738927431646241 Test-f1: 0.4132884262094305 Train-acc: 0.7980132450331126 Test-acc: 0.7236842105263158\n",
      "Epoch [8613/20000] Train-Loss: 0.4905876743850843 Test-Loss: 0.15333403440497823 Train-f1: 0.4772299465240641 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8614/20000] Train-Loss: 0.48553959106295885 Test-Loss: 0.17040986216511736 Train-f1: 0.46517807701988423 Test-f1: 0.45624724694987656 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8615/20000] Train-Loss: 0.4875638581661905 Test-Loss: 0.15219405279806203 Train-f1: 0.472389753168342 Test-f1: 0.4376874738165061 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [8616/20000] Train-Loss: 0.4880191131045589 Test-Loss: 0.15058616036110417 Train-f1: 0.45768845137179837 Test-f1: 0.4503312334891282 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8617/20000] Train-Loss: 0.49181408350570355 Test-Loss: 0.15343302256669003 Train-f1: 0.47251646903820815 Test-f1: 0.4526190476190476 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8618/20000] Train-Loss: 0.4950655915617099 Test-Loss: 0.1533116510180542 Train-f1: 0.4658242437363459 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8619/20000] Train-Loss: 0.49129877341528827 Test-Loss: 0.15101992586305021 Train-f1: 0.4709990549990549 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8620/20000] Train-Loss: 0.4933653406051821 Test-Loss: 0.15159526090122566 Train-f1: 0.46767807701988423 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8621/20000] Train-Loss: 0.4930309205114806 Test-Loss: 0.15706972308065387 Train-f1: 0.4711464518917935 Test-f1: 0.4359678460364959 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8622/20000] Train-Loss: 0.49400280524502793 Test-Loss: 0.15381961136764866 Train-f1: 0.4727310738042446 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8623/20000] Train-Loss: 0.4937764266617628 Test-Loss: 0.15122798576437355 Train-f1: 0.4686753246753247 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8624/20000] Train-Loss: 0.4900399269495381 Test-Loss: 0.1509936086753987 Train-f1: 0.4738701298701299 Test-f1: 0.449645003282331 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8625/20000] Train-Loss: 0.4953216339040853 Test-Loss: 0.15340528910631618 Train-f1: 0.46824912313963407 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8626/20000] Train-Loss: 0.4904049902787185 Test-Loss: 0.1533655178411621 Train-f1: 0.47043669770953944 Test-f1: 0.4526190476190476 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8627/20000] Train-Loss: 0.49482241899890467 Test-Loss: 0.15271216623152264 Train-f1: 0.4660070325467684 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8628/20000] Train-Loss: 0.4926893302139053 Test-Loss: 0.1520281459945263 Train-f1: 0.4738927431646241 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8629/20000] Train-Loss: 0.4928224004583273 Test-Loss: 0.15611019610486107 Train-f1: 0.47393939393939394 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8630/20000] Train-Loss: 0.48921779075353317 Test-Loss: 0.1539878402082448 Train-f1: 0.4711464518917935 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8631/20000] Train-Loss: 0.4913724460159336 Test-Loss: 0.15266946937592335 Train-f1: 0.46022963190635213 Test-f1: 0.44937417654808953 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8632/20000] Train-Loss: 0.48950588085592833 Test-Loss: 0.15321161110405718 Train-f1: 0.4709702535122832 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8633/20000] Train-Loss: 0.4895495694912248 Test-Loss: 0.15330499187037264 Train-f1: 0.46779643766388884 Test-f1: 0.44951827242524917 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8634/20000] Train-Loss: 0.4932500733179222 Test-Loss: 0.15753325292688816 Train-f1: 0.46267837235228537 Test-f1: 0.44558629776021075 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8635/20000] Train-Loss: 0.49166585643460387 Test-Loss: 0.15261647859515085 Train-f1: 0.4632283180284123 Test-f1: 0.4526190476190476 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8636/20000] Train-Loss: 0.4904318132042493 Test-Loss: 0.15198553704676018 Train-f1: 0.48129104975541354 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8637/20000] Train-Loss: 0.492767050955244 Test-Loss: 0.15481960549515605 Train-f1: 0.47027615862398464 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8638/20000] Train-Loss: 0.48981197137332017 Test-Loss: 0.15194500505992237 Train-f1: 0.46830438621944037 Test-f1: 0.45 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8639/20000] Train-Loss: 0.49352101978038604 Test-Loss: 0.1576660445452555 Train-f1: 0.46013827892239545 Test-f1: 0.44558629776021075 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8640/20000] Train-Loss: 0.49152683380913925 Test-Loss: 0.15334128854701995 Train-f1: 0.47272689334661155 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8641/20000] Train-Loss: 0.4904909392518168 Test-Loss: 0.15082175994703526 Train-f1: 0.47899082113806346 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8642/20000] Train-Loss: 0.4911707201538273 Test-Loss: 0.15773610905952296 Train-f1: 0.4749903624356179 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8643/20000] Train-Loss: 0.49150309952200727 Test-Loss: 0.15150999768848297 Train-f1: 0.4711470093207416 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8644/20000] Train-Loss: 0.49080132417351197 Test-Loss: 0.15252883895559358 Train-f1: 0.47165418085760535 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8645/20000] Train-Loss: 0.495533061847963 Test-Loss: 0.15199479335908542 Train-f1: 0.4705888893714981 Test-f1: 0.4536974789915966 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8646/20000] Train-Loss: 0.4910723849334974 Test-Loss: 0.1569200272438077 Train-f1: 0.4705547821450787 Test-f1: 0.4652641421224871 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [8647/20000] Train-Loss: 0.48998433166203825 Test-Loss: 0.15281551969994167 Train-f1: 0.4688708133971292 Test-f1: 0.4536974789915966 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8648/20000] Train-Loss: 0.49036793303415566 Test-Loss: 0.15947481364310756 Train-f1: 0.47193837011147133 Test-f1: 0.4235123085869355 Train-acc: 0.7947019867549668 Test-acc: 0.7236842105263158\n",
      "Epoch [8649/20000] Train-Loss: 0.4886063113762294 Test-Loss: 0.15919329251965644 Train-f1: 0.482469831896757 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [8650/20000] Train-Loss: 0.4880139896615578 Test-Loss: 0.1581967711774781 Train-f1: 0.46830438621944037 Test-f1: 0.4628528560711606 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8651/20000] Train-Loss: 0.4937287746849949 Test-Loss: 0.15366968760125294 Train-f1: 0.4743931885573421 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8652/20000] Train-Loss: 0.48735022281750007 Test-Loss: 0.1560132457817575 Train-f1: 0.46632010797495804 Test-f1: 0.4461529397902675 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8653/20000] Train-Loss: 0.49255081769648246 Test-Loss: 0.15189991941036074 Train-f1: 0.4671978559361861 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8654/20000] Train-Loss: 0.48606690807747227 Test-Loss: 0.15078127770505767 Train-f1: 0.4790424251631774 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8655/20000] Train-Loss: 0.49363934618811056 Test-Loss: 0.15524929792449307 Train-f1: 0.4670167880899587 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8656/20000] Train-Loss: 0.49215542115633143 Test-Loss: 0.15108456631967818 Train-f1: 0.4792881207871469 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8657/20000] Train-Loss: 0.48788306233434064 Test-Loss: 0.160074903244287 Train-f1: 0.46850880749473517 Test-f1: 0.4341216991963261 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [8658/20000] Train-Loss: 0.490612839751341 Test-Loss: 0.15874484982930673 Train-f1: 0.47012399691707846 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8659/20000] Train-Loss: 0.4929124131388489 Test-Loss: 0.15251238314675916 Train-f1: 0.4672960814080035 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8660/20000] Train-Loss: 0.49339508533762927 Test-Loss: 0.15373245731691443 Train-f1: 0.46301153220857927 Test-f1: 0.44683117900509206 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8661/20000] Train-Loss: 0.49197224750695046 Test-Loss: 0.150503727346967 Train-f1: 0.47348003891770957 Test-f1: 0.4395848595848596 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8662/20000] Train-Loss: 0.4899931116870142 Test-Loss: 0.1506317126314207 Train-f1: 0.46549767569045664 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8663/20000] Train-Loss: 0.4914369158755285 Test-Loss: 0.1508016175795413 Train-f1: 0.4680748233478198 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8664/20000] Train-Loss: 0.49126286297389865 Test-Loss: 0.15598383588554182 Train-f1: 0.47168082115679777 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [8665/20000] Train-Loss: 0.4881982097792959 Test-Loss: 0.15038115590343995 Train-f1: 0.4620084291635786 Test-f1: 0.4503312334891282 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8666/20000] Train-Loss: 0.4913945749512503 Test-Loss: 0.15036834893408718 Train-f1: 0.4814699864890947 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8667/20000] Train-Loss: 0.4895601677136582 Test-Loss: 0.1509592012083491 Train-f1: 0.46561773097719 Test-f1: 0.44990810208201515 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8668/20000] Train-Loss: 0.4910326627157355 Test-Loss: 0.15092885139162993 Train-f1: 0.4706266832278542 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8669/20000] Train-Loss: 0.49196353066995485 Test-Loss: 0.15347682996882445 Train-f1: 0.47047949293121005 Test-f1: 0.4542943938138636 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8670/20000] Train-Loss: 0.4924797011768313 Test-Loss: 0.15144282299513848 Train-f1: 0.4812966687543575 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8671/20000] Train-Loss: 0.48975266304038845 Test-Loss: 0.1549678803859615 Train-f1: 0.47272689334661155 Test-f1: 0.43639684222929187 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8672/20000] Train-Loss: 0.49458126371632266 Test-Loss: 0.1509679098841093 Train-f1: 0.46939194139194135 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8673/20000] Train-Loss: 0.48872725671862116 Test-Loss: 0.1523862157158071 Train-f1: 0.4762011273530836 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8674/20000] Train-Loss: 0.49052451526800245 Test-Loss: 0.15647777390144244 Train-f1: 0.47423250030495845 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8675/20000] Train-Loss: 0.4913616739400622 Test-Loss: 0.15340703963810298 Train-f1: 0.4730959595959595 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8676/20000] Train-Loss: 0.4907074312961386 Test-Loss: 0.15537243821255603 Train-f1: 0.46549767569045664 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8677/20000] Train-Loss: 0.4932567326054146 Test-Loss: 0.1513176345852483 Train-f1: 0.46767807701988423 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8678/20000] Train-Loss: 0.49207210733287543 Test-Loss: 0.1556403861095884 Train-f1: 0.4774146048598603 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8679/20000] Train-Loss: 0.48953606962816826 Test-Loss: 0.1547394189051202 Train-f1: 0.4649944837770924 Test-f1: 0.44937417654808953 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8680/20000] Train-Loss: 0.49217312266983687 Test-Loss: 0.15313699435120381 Train-f1: 0.46925345834012305 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8681/20000] Train-Loss: 0.492248320582734 Test-Loss: 0.15364671122844337 Train-f1: 0.47635466157570505 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8682/20000] Train-Loss: 0.4912379382105895 Test-Loss: 0.15516355510042518 Train-f1: 0.4573586446213513 Test-f1: 0.44683117900509206 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8683/20000] Train-Loss: 0.4931384512996624 Test-Loss: 0.15420909476378064 Train-f1: 0.4714029862836502 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8684/20000] Train-Loss: 0.48968342058496256 Test-Loss: 0.16750987097423853 Train-f1: 0.47068102211643054 Test-f1: 0.4426916799402199 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8685/20000] Train-Loss: 0.4934326797455284 Test-Loss: 0.15587167497970592 Train-f1: 0.4727606306736741 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [8686/20000] Train-Loss: 0.4916364627146263 Test-Loss: 0.15197668374900647 Train-f1: 0.4715834936824418 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8687/20000] Train-Loss: 0.49349394013368275 Test-Loss: 0.1532066831525239 Train-f1: 0.4762658602658602 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8688/20000] Train-Loss: 0.4928173155745591 Test-Loss: 0.1586553591901486 Train-f1: 0.47253997316936414 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8689/20000] Train-Loss: 0.491429003858474 Test-Loss: 0.15349903185130767 Train-f1: 0.47047949293121005 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8690/20000] Train-Loss: 0.4913324972063645 Test-Loss: 0.15155164612858435 Train-f1: 0.47892676992801037 Test-f1: 0.4466252587991718 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8691/20000] Train-Loss: 0.4931328852305454 Test-Loss: 0.15017749798537658 Train-f1: 0.47326674548596126 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8692/20000] Train-Loss: 0.49100187885001606 Test-Loss: 0.15049392599611794 Train-f1: 0.47590102718794985 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8693/20000] Train-Loss: 0.4890560921089187 Test-Loss: 0.15515386361193503 Train-f1: 0.475553899522572 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8694/20000] Train-Loss: 0.4889208639783782 Test-Loss: 0.15414652515302 Train-f1: 0.4886736642791417 Test-f1: 0.4385580319084156 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [8695/20000] Train-Loss: 0.4896718762013306 Test-Loss: 0.15264444322353957 Train-f1: 0.46640597844977416 Test-f1: 0.44937417654808953 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8696/20000] Train-Loss: 0.48943940211550646 Test-Loss: 0.15063051667396277 Train-f1: 0.4737960159432582 Test-f1: 0.45000000000000007 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8697/20000] Train-Loss: 0.4912547876947871 Test-Loss: 0.1543729683487081 Train-f1: 0.47342074814382773 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8698/20000] Train-Loss: 0.49048255180021383 Test-Loss: 0.15038984625011695 Train-f1: 0.4813658214285265 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8699/20000] Train-Loss: 0.49091980094652826 Test-Loss: 0.15039781294796137 Train-f1: 0.4786921197408452 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8700/20000] Train-Loss: 0.4934366332263462 Test-Loss: 0.1515617413048603 Train-f1: 0.47272689334661155 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8701/20000] Train-Loss: 0.48935881089893596 Test-Loss: 0.1507229725359381 Train-f1: 0.47342074814382773 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8702/20000] Train-Loss: 0.4895323679477374 Test-Loss: 0.1527164922921267 Train-f1: 0.4783737373737374 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8703/20000] Train-Loss: 0.4908803884962979 Test-Loss: 0.15069259867414742 Train-f1: 0.4816198704953846 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8704/20000] Train-Loss: 0.4902472833103177 Test-Loss: 0.15095715923813416 Train-f1: 0.4781953183991946 Test-f1: 0.449645003282331 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8705/20000] Train-Loss: 0.49057354503266476 Test-Loss: 0.15211275304409064 Train-f1: 0.4761870439006131 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8706/20000] Train-Loss: 0.48780552554250795 Test-Loss: 0.16267755594739064 Train-f1: 0.47097317005361405 Test-f1: 0.4397742127153892 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8707/20000] Train-Loss: 0.49028413568038276 Test-Loss: 0.1532685704258564 Train-f1: 0.47604057635207386 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8708/20000] Train-Loss: 0.49144403699676475 Test-Loss: 0.1506851631881423 Train-f1: 0.47804230552056637 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8709/20000] Train-Loss: 0.4892428424087242 Test-Loss: 0.15018352655758835 Train-f1: 0.47172101916627457 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8710/20000] Train-Loss: 0.4897389927458163 Test-Loss: 0.1548883464195322 Train-f1: 0.4740875912408759 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8711/20000] Train-Loss: 0.49026348072214343 Test-Loss: 0.1559363832401655 Train-f1: 0.4711933712531245 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8712/20000] Train-Loss: 0.4922696323781166 Test-Loss: 0.1532054464909506 Train-f1: 0.45738116955508257 Test-f1: 0.4554347826086957 Train-acc: 0.7781456953642384 Test-acc: 0.7763157894736842\n",
      "Epoch [8713/20000] Train-Loss: 0.4900913888896658 Test-Loss: 0.15301515603204285 Train-f1: 0.4578549814574659 Test-f1: 0.4466252587991718 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8714/20000] Train-Loss: 0.4916724284599147 Test-Loss: 0.15075886239939834 Train-f1: 0.4784746682564448 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8715/20000] Train-Loss: 0.49153154104761376 Test-Loss: 0.15565536581372474 Train-f1: 0.4701780770198843 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8716/20000] Train-Loss: 0.4904389692134369 Test-Loss: 0.15767695544786728 Train-f1: 0.47718353869566776 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [8717/20000] Train-Loss: 0.49148073289382105 Test-Loss: 0.15644966413910394 Train-f1: 0.4792881207871469 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [8718/20000] Train-Loss: 0.48972485515052483 Test-Loss: 0.15183994545028898 Train-f1: 0.4681109599574141 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8719/20000] Train-Loss: 0.49025101223281714 Test-Loss: 0.15808923904335104 Train-f1: 0.4731424064982946 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8720/20000] Train-Loss: 0.4928639636356896 Test-Loss: 0.15251486998106437 Train-f1: 0.47333787364937113 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8721/20000] Train-Loss: 0.4898100881758981 Test-Loss: 0.15142969312107704 Train-f1: 0.46830438621944037 Test-f1: 0.45119841858166543 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8722/20000] Train-Loss: 0.49120044809041746 Test-Loss: 0.15023179732415354 Train-f1: 0.4658040057713051 Test-f1: 0.4493236714975846 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8723/20000] Train-Loss: 0.48634169466492966 Test-Loss: 0.15554808051737745 Train-f1: 0.46949724895476097 Test-f1: 0.4594670406732118 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8724/20000] Train-Loss: 0.4942019831596919 Test-Loss: 0.15041114907670916 Train-f1: 0.4692307716619548 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8725/20000] Train-Loss: 0.48917937030312175 Test-Loss: 0.1522079530779065 Train-f1: 0.4697043044354966 Test-f1: 0.45461235883771095 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8726/20000] Train-Loss: 0.4900562623254586 Test-Loss: 0.1530625155593914 Train-f1: 0.46076396126002195 Test-f1: 0.4466252587991718 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8727/20000] Train-Loss: 0.4916182488953697 Test-Loss: 0.15159384781690802 Train-f1: 0.4714103559946957 Test-f1: 0.45119841858166543 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8728/20000] Train-Loss: 0.48648693149433037 Test-Loss: 0.15682223563927297 Train-f1: 0.4676395248673891 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8729/20000] Train-Loss: 0.48950069963101916 Test-Loss: 0.15800954135157427 Train-f1: 0.4736533566524468 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8730/20000] Train-Loss: 0.48664752697049374 Test-Loss: 0.15544123238486626 Train-f1: 0.4761237102967503 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8731/20000] Train-Loss: 0.48911538802857013 Test-Loss: 0.15171216899780318 Train-f1: 0.4689906092614201 Test-f1: 0.4510087552341073 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8732/20000] Train-Loss: 0.48602298969150665 Test-Loss: 0.15061791662640348 Train-f1: 0.4786004010573329 Test-f1: 0.4385580319084156 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [8733/20000] Train-Loss: 0.49157332289376365 Test-Loss: 0.15260116291006687 Train-f1: 0.45983509788844845 Test-f1: 0.44683117900509206 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8734/20000] Train-Loss: 0.4929811498613606 Test-Loss: 0.15500833329118868 Train-f1: 0.4687613493721791 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8735/20000] Train-Loss: 0.48898743569366565 Test-Loss: 0.1516086384253426 Train-f1: 0.47326674548596126 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8736/20000] Train-Loss: 0.489783185586874 Test-Loss: 0.15666961908774935 Train-f1: 0.48245445243658464 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8737/20000] Train-Loss: 0.4862250796353583 Test-Loss: 0.15540587767566794 Train-f1: 0.4885850049793712 Test-f1: 0.4628528560711606 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [8738/20000] Train-Loss: 0.49254080244780435 Test-Loss: 0.15127840183287844 Train-f1: 0.47363927319997623 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8739/20000] Train-Loss: 0.486321478480509 Test-Loss: 0.1552444009086685 Train-f1: 0.46972307176091344 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [8740/20000] Train-Loss: 0.4905401145629053 Test-Loss: 0.1499655599186505 Train-f1: 0.46076924815258413 Test-f1: 0.45000000000000007 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8741/20000] Train-Loss: 0.4897164185236401 Test-Loss: 0.15062998677910688 Train-f1: 0.4681109599574141 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8742/20000] Train-Loss: 0.4886607572106975 Test-Loss: 0.15044994016533542 Train-f1: 0.4755578334708769 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8743/20000] Train-Loss: 0.48987712883740536 Test-Loss: 0.15084775457712393 Train-f1: 0.4762011273530836 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8744/20000] Train-Loss: 0.4895875643689144 Test-Loss: 0.1505246505590184 Train-f1: 0.47047949293121005 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8745/20000] Train-Loss: 0.49244135180938836 Test-Loss: 0.14983078801034616 Train-f1: 0.47635466157570505 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8746/20000] Train-Loss: 0.4904665465026162 Test-Loss: 0.16810312665319813 Train-f1: 0.47270184826646455 Test-f1: 0.4426916799402199 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [8747/20000] Train-Loss: 0.49255517337912885 Test-Loss: 0.15178627201540484 Train-f1: 0.4743549783549783 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8748/20000] Train-Loss: 0.4908925908641249 Test-Loss: 0.15107677218075322 Train-f1: 0.4625937185781037 Test-f1: 0.449645003282331 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8749/20000] Train-Loss: 0.49035874041480215 Test-Loss: 0.15518135457752455 Train-f1: 0.47684001136686555 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8750/20000] Train-Loss: 0.4830914556354642 Test-Loss: 0.15361269892416357 Train-f1: 0.47976665551378694 Test-f1: 0.4385580319084156 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [8751/20000] Train-Loss: 0.4918491347657086 Test-Loss: 0.15247772876770113 Train-f1: 0.4786515151515152 Test-f1: 0.4567457721207795 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [8752/20000] Train-Loss: 0.4870996125152171 Test-Loss: 0.15029337395216788 Train-f1: 0.4707901029487342 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8753/20000] Train-Loss: 0.48771670110900817 Test-Loss: 0.16660684707941006 Train-f1: 0.47663911416463034 Test-f1: 0.4426916799402199 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [8754/20000] Train-Loss: 0.4834757965817263 Test-Loss: 0.1513662254208262 Train-f1: 0.4826767051932615 Test-f1: 0.4376874738165061 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [8755/20000] Train-Loss: 0.48222879544425984 Test-Loss: 0.16331370782777704 Train-f1: 0.46364451235039467 Test-f1: 0.4419994084590358 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8756/20000] Train-Loss: 0.4912078302188638 Test-Loss: 0.15286005974517874 Train-f1: 0.47355305254193214 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8757/20000] Train-Loss: 0.4890021469187822 Test-Loss: 0.15018552047383013 Train-f1: 0.46864678513228836 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8758/20000] Train-Loss: 0.487285417177885 Test-Loss: 0.15211167589596572 Train-f1: 0.4698801406573237 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8759/20000] Train-Loss: 0.4924604255694653 Test-Loss: 0.15124392026834851 Train-f1: 0.476160474171225 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8760/20000] Train-Loss: 0.4878017480099528 Test-Loss: 0.15084746089348539 Train-f1: 0.4830950972070194 Test-f1: 0.45 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8761/20000] Train-Loss: 0.48861233769599666 Test-Loss: 0.15514032986596485 Train-f1: 0.468436886854153 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [8762/20000] Train-Loss: 0.4895104094466467 Test-Loss: 0.1530535300502277 Train-f1: 0.46719775658290813 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8763/20000] Train-Loss: 0.4873269768773639 Test-Loss: 0.1568988194276858 Train-f1: 0.4733853039175095 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8764/20000] Train-Loss: 0.4918046705354009 Test-Loss: 0.1526662049778776 Train-f1: 0.46801401148096683 Test-f1: 0.44937417654808953 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8765/20000] Train-Loss: 0.4829860573559714 Test-Loss: 0.15117544086176285 Train-f1: 0.47662752128639907 Test-f1: 0.43882202304737516 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [8766/20000] Train-Loss: 0.489654060371219 Test-Loss: 0.14998691979808967 Train-f1: 0.470899547172579 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8767/20000] Train-Loss: 0.48964338503416943 Test-Loss: 0.15185237312706126 Train-f1: 0.4755578334708769 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8768/20000] Train-Loss: 0.4877038119343 Test-Loss: 0.15230357439111453 Train-f1: 0.47892676992801037 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8769/20000] Train-Loss: 0.48824792716012755 Test-Loss: 0.14984547521723016 Train-f1: 0.46864678513228836 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8770/20000] Train-Loss: 0.48040927348537904 Test-Loss: 0.16009998362135303 Train-f1: 0.4824984563057031 Test-f1: 0.4419994084590358 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8771/20000] Train-Loss: 0.4876027731819719 Test-Loss: 0.1509380507810186 Train-f1: 0.46797552447552454 Test-f1: 0.4368741765480896 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [8772/20000] Train-Loss: 0.49032719644771716 Test-Loss: 0.15283027755401812 Train-f1: 0.46864069862385416 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8773/20000] Train-Loss: 0.48816763973482985 Test-Loss: 0.15149433276346777 Train-f1: 0.4649194563302291 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8774/20000] Train-Loss: 0.48829921126730197 Test-Loss: 0.1501122276804996 Train-f1: 0.4674879036333919 Test-f1: 0.44990810208201515 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8775/20000] Train-Loss: 0.4883920594280237 Test-Loss: 0.15097936151741534 Train-f1: 0.4653523392677589 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8776/20000] Train-Loss: 0.48824145054480256 Test-Loss: 0.15208099367372058 Train-f1: 0.47535962280487826 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8777/20000] Train-Loss: 0.48678524021434705 Test-Loss: 0.15695918872426773 Train-f1: 0.4711933712531245 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8778/20000] Train-Loss: 0.4880401233452026 Test-Loss: 0.15085511693385606 Train-f1: 0.47663911416463034 Test-f1: 0.449645003282331 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8779/20000] Train-Loss: 0.4874127890962752 Test-Loss: 0.15276798441378994 Train-f1: 0.46215481892477195 Test-f1: 0.4554347826086957 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [8780/20000] Train-Loss: 0.48831627506507524 Test-Loss: 0.152573957595346 Train-f1: 0.4741733447001989 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8781/20000] Train-Loss: 0.49080395235544727 Test-Loss: 0.15180138274777014 Train-f1: 0.4734928056514369 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8782/20000] Train-Loss: 0.48490815284279587 Test-Loss: 0.14977427885343106 Train-f1: 0.4871194373510706 Test-f1: 0.4503312334891282 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [8783/20000] Train-Loss: 0.4937548565650867 Test-Loss: 0.15034986562041933 Train-f1: 0.4706266832278542 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8784/20000] Train-Loss: 0.4869083875319599 Test-Loss: 0.15026576919769186 Train-f1: 0.47870892235670953 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8785/20000] Train-Loss: 0.4886740300367589 Test-Loss: 0.1528480366739461 Train-f1: 0.4790424251631774 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8786/20000] Train-Loss: 0.4871809784221175 Test-Loss: 0.15561477891000716 Train-f1: 0.47321921895860725 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8787/20000] Train-Loss: 0.4866414064495619 Test-Loss: 0.1500212614613755 Train-f1: 0.47098894997782964 Test-f1: 0.4493236714975846 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8788/20000] Train-Loss: 0.4882245049949168 Test-Loss: 0.15346451551764848 Train-f1: 0.4723341283873852 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8789/20000] Train-Loss: 0.48400877973996626 Test-Loss: 0.1508273750783966 Train-f1: 0.4606541908809033 Test-f1: 0.45393483709273186 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8790/20000] Train-Loss: 0.4816273170449841 Test-Loss: 0.1553035852830063 Train-f1: 0.48062596140455033 Test-f1: 0.4624973437035148 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [8791/20000] Train-Loss: 0.49058315533328106 Test-Loss: 0.152388962080793 Train-f1: 0.47688150692747494 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8792/20000] Train-Loss: 0.4878006035422242 Test-Loss: 0.15134525873173038 Train-f1: 0.47012399691707846 Test-f1: 0.4526190476190476 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8793/20000] Train-Loss: 0.48689173559061255 Test-Loss: 0.15177918446350452 Train-f1: 0.4757983910555815 Test-f1: 0.4536974789915966 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8794/20000] Train-Loss: 0.4893015634386812 Test-Loss: 0.15136632664588257 Train-f1: 0.4766060606060606 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8795/20000] Train-Loss: 0.4880276416277138 Test-Loss: 0.14978619336754057 Train-f1: 0.4707034956881671 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8796/20000] Train-Loss: 0.4869603381346057 Test-Loss: 0.15027737462084662 Train-f1: 0.4617117942010028 Test-f1: 0.44990810208201515 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8797/20000] Train-Loss: 0.4864622474153128 Test-Loss: 0.15055912232540553 Train-f1: 0.4761870439006131 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8798/20000] Train-Loss: 0.4875701850829449 Test-Loss: 0.15146589535760344 Train-f1: 0.47068102211643054 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8799/20000] Train-Loss: 0.48519816481537664 Test-Loss: 0.15642010021910793 Train-f1: 0.4698739309471017 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8800/20000] Train-Loss: 0.4871476356227147 Test-Loss: 0.1508340089542746 Train-f1: 0.47538624674454566 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8801/20000] Train-Loss: 0.4893487519851062 Test-Loss: 0.1495839472025726 Train-f1: 0.46553962954539463 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8802/20000] Train-Loss: 0.48699267145133035 Test-Loss: 0.15362663250263184 Train-f1: 0.47124585477415665 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8803/20000] Train-Loss: 0.4857212793741549 Test-Loss: 0.15040966958315655 Train-f1: 0.4865655278903686 Test-f1: 0.44683117900509206 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [8804/20000] Train-Loss: 0.4900600560087241 Test-Loss: 0.1501651793535219 Train-f1: 0.47604057635207386 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8805/20000] Train-Loss: 0.4839141168781451 Test-Loss: 0.1517883332542181 Train-f1: 0.4713757284571464 Test-f1: 0.4536974789915966 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8806/20000] Train-Loss: 0.49476808819802653 Test-Loss: 0.1509547360979602 Train-f1: 0.4733069000091149 Test-f1: 0.449645003282331 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8807/20000] Train-Loss: 0.4879077114071484 Test-Loss: 0.15051001225095392 Train-f1: 0.4769230915203034 Test-f1: 0.449645003282331 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8808/20000] Train-Loss: 0.4832527032521849 Test-Loss: 0.15557598402722517 Train-f1: 0.46814937654681027 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [8809/20000] Train-Loss: 0.4867196019258438 Test-Loss: 0.15927091695091675 Train-f1: 0.4814182685672601 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [8810/20000] Train-Loss: 0.490119034956803 Test-Loss: 0.15430452627198327 Train-f1: 0.4669423648247178 Test-f1: 0.4466252587991718 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8811/20000] Train-Loss: 0.4911449714391339 Test-Loss: 0.1519920888083279 Train-f1: 0.46613308877680043 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8812/20000] Train-Loss: 0.4856469028095481 Test-Loss: 0.15128591646216458 Train-f1: 0.4781953183991946 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8813/20000] Train-Loss: 0.49227739210991756 Test-Loss: 0.15902348517461115 Train-f1: 0.4778992797363893 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [8814/20000] Train-Loss: 0.4890379223786535 Test-Loss: 0.15519640489991898 Train-f1: 0.46829635229635225 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [8815/20000] Train-Loss: 0.4864516170501514 Test-Loss: 0.14973665616704596 Train-f1: 0.4738927431646241 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8816/20000] Train-Loss: 0.4870979024906806 Test-Loss: 0.14961991651750656 Train-f1: 0.4524892862878394 Test-f1: 0.4497330282227307 Train-acc: 0.7715231788079471 Test-acc: 0.7631578947368421\n",
      "Epoch [8817/20000] Train-Loss: 0.488725789304606 Test-Loss: 0.1510314964877131 Train-f1: 0.4833239803674586 Test-f1: 0.4466252587991718 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8818/20000] Train-Loss: 0.48481413027779224 Test-Loss: 0.17101499878773235 Train-f1: 0.47536612938053324 Test-f1: 0.4426916799402199 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [8819/20000] Train-Loss: 0.4891737352393806 Test-Loss: 0.14997271440702895 Train-f1: 0.4734928056514369 Test-f1: 0.449645003282331 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8820/20000] Train-Loss: 0.48396137429483777 Test-Loss: 0.1562810627868778 Train-f1: 0.4709702535122832 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8821/20000] Train-Loss: 0.48899707078106003 Test-Loss: 0.15110972289695937 Train-f1: 0.47168082115679777 Test-f1: 0.4526190476190476 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8822/20000] Train-Loss: 0.4871787719767815 Test-Loss: 0.15523999313885994 Train-f1: 0.4707901029487342 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8823/20000] Train-Loss: 0.48644292873862627 Test-Loss: 0.15401958687171952 Train-f1: 0.47333787364937113 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8824/20000] Train-Loss: 0.48475920260173244 Test-Loss: 0.15258518660830758 Train-f1: 0.4716245665612754 Test-f1: 0.4510087552341073 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8825/20000] Train-Loss: 0.4881776206470014 Test-Loss: 0.15055446564807293 Train-f1: 0.4755578334708769 Test-f1: 0.4533424822739276 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8826/20000] Train-Loss: 0.4860220338594871 Test-Loss: 0.14929926080337175 Train-f1: 0.4657229416843262 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8827/20000] Train-Loss: 0.48617299031014233 Test-Loss: 0.1501224286884067 Train-f1: 0.4711376551376551 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8828/20000] Train-Loss: 0.4881687399043834 Test-Loss: 0.14988556818025423 Train-f1: 0.4730959595959595 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8829/20000] Train-Loss: 0.4830324247367671 Test-Loss: 0.15752134030581721 Train-f1: 0.4655419309372798 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [8830/20000] Train-Loss: 0.4861148458432259 Test-Loss: 0.15007788566828925 Train-f1: 0.468905737383251 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8831/20000] Train-Loss: 0.48789459142154484 Test-Loss: 0.14989907565806054 Train-f1: 0.46517837235228543 Test-f1: 0.44990810208201515 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8832/20000] Train-Loss: 0.48948462058251474 Test-Loss: 0.14992543425141078 Train-f1: 0.4765367965367965 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8833/20000] Train-Loss: 0.48771227087500557 Test-Loss: 0.1495576346533776 Train-f1: 0.4729366977095394 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8834/20000] Train-Loss: 0.4880074228047164 Test-Loss: 0.15184771865969193 Train-f1: 0.4757349422290476 Test-f1: 0.4567457721207795 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8835/20000] Train-Loss: 0.4845687540862244 Test-Loss: 0.15325873248555105 Train-f1: 0.4749903624356179 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8836/20000] Train-Loss: 0.4862301839951009 Test-Loss: 0.15308582823658967 Train-f1: 0.46580896318970255 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8837/20000] Train-Loss: 0.48577849479474433 Test-Loss: 0.14951750153908172 Train-f1: 0.47222341057635175 Test-f1: 0.4505267472842796 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8838/20000] Train-Loss: 0.4846732811154574 Test-Loss: 0.1495618874973175 Train-f1: 0.4836515151515151 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8839/20000] Train-Loss: 0.48799858310010574 Test-Loss: 0.1501799114932102 Train-f1: 0.4708684887835429 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8840/20000] Train-Loss: 0.48503235295812347 Test-Loss: 0.15195016334319103 Train-f1: 0.48377221349595 Test-f1: 0.4542943938138636 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8841/20000] Train-Loss: 0.4890614161840735 Test-Loss: 0.15286459994980797 Train-f1: 0.47894844163797 Test-f1: 0.4466252587991718 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8842/20000] Train-Loss: 0.48827663653349956 Test-Loss: 0.14970797085308044 Train-f1: 0.47663911416463034 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8843/20000] Train-Loss: 0.4894768866460943 Test-Loss: 0.15115297550131093 Train-f1: 0.4698801406573237 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8844/20000] Train-Loss: 0.4867092913249214 Test-Loss: 0.15150504295831704 Train-f1: 0.4819024998721142 Test-f1: 0.449645003282331 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8845/20000] Train-Loss: 0.48727308807446673 Test-Loss: 0.15130409265703534 Train-f1: 0.47949588601244225 Test-f1: 0.4368741765480896 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [8846/20000] Train-Loss: 0.4915934220093585 Test-Loss: 0.14902318830156724 Train-f1: 0.4676241484027372 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8847/20000] Train-Loss: 0.4862319879663587 Test-Loss: 0.15322117942125832 Train-f1: 0.4680133989608969 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8848/20000] Train-Loss: 0.48561198641570147 Test-Loss: 0.15453255075476743 Train-f1: 0.46821117978169846 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8849/20000] Train-Loss: 0.48909385163533253 Test-Loss: 0.14962661442716713 Train-f1: 0.47410564009394196 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8850/20000] Train-Loss: 0.48889224629504313 Test-Loss: 0.1500387093321996 Train-f1: 0.4734928056514369 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8851/20000] Train-Loss: 0.48744104050626375 Test-Loss: 0.1541189227197611 Train-f1: 0.4653523392677589 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8852/20000] Train-Loss: 0.4910173263054362 Test-Loss: 0.15018748174002733 Train-f1: 0.4712034632034632 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8853/20000] Train-Loss: 0.48339166231385505 Test-Loss: 0.16163008852984678 Train-f1: 0.48122376531046546 Test-f1: 0.4341972308997648 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [8854/20000] Train-Loss: 0.4895064109147053 Test-Loss: 0.15182391798104278 Train-f1: 0.4808737373737374 Test-f1: 0.44854612862701837 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8855/20000] Train-Loss: 0.4810183759756909 Test-Loss: 0.15858622882106146 Train-f1: 0.4712817013663913 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [8856/20000] Train-Loss: 0.4844024837283211 Test-Loss: 0.15568885437063604 Train-f1: 0.48200105593637765 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8857/20000] Train-Loss: 0.48764767790433017 Test-Loss: 0.15140089202926957 Train-f1: 0.47681971587787453 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8858/20000] Train-Loss: 0.4813617342788294 Test-Loss: 0.16777267694828468 Train-f1: 0.4673462191393007 Test-f1: 0.4280288816874183 Train-acc: 0.7913907284768212 Test-acc: 0.7236842105263158\n",
      "Epoch [8859/20000] Train-Loss: 0.48840569615982904 Test-Loss: 0.15105003936803807 Train-f1: 0.4721433288257405 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8860/20000] Train-Loss: 0.4842536668618225 Test-Loss: 0.15094740099636345 Train-f1: 0.47333787364937113 Test-f1: 0.4526190476190476 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8861/20000] Train-Loss: 0.4830121524035323 Test-Loss: 0.151328393247815 Train-f1: 0.4847938992574754 Test-f1: 0.4509610604805303 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8862/20000] Train-Loss: 0.4910041524136306 Test-Loss: 0.15057656625630952 Train-f1: 0.4672960814080035 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8863/20000] Train-Loss: 0.4873951773180909 Test-Loss: 0.15054303061559712 Train-f1: 0.47105627501765956 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8864/20000] Train-Loss: 0.4872673444365052 Test-Loss: 0.16244054505785402 Train-f1: 0.48104861050360126 Test-f1: 0.4341972308997648 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [8865/20000] Train-Loss: 0.4871364596932244 Test-Loss: 0.15076723890305965 Train-f1: 0.46561982246105116 Test-f1: 0.4497330282227307 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8866/20000] Train-Loss: 0.4879619598537629 Test-Loss: 0.15021126395511747 Train-f1: 0.4644284006126286 Test-f1: 0.4503312334891282 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8867/20000] Train-Loss: 0.4903549253112643 Test-Loss: 0.15092934851647874 Train-f1: 0.47645858017280407 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8868/20000] Train-Loss: 0.4874496108318829 Test-Loss: 0.15048484717883712 Train-f1: 0.47348003891770957 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8869/20000] Train-Loss: 0.48272650438804904 Test-Loss: 0.16107997441561064 Train-f1: 0.4741231602561985 Test-f1: 0.4545727136431784 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8870/20000] Train-Loss: 0.4870990086828418 Test-Loss: 0.14953995569609527 Train-f1: 0.46399814477673357 Test-f1: 0.44990810208201515 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8871/20000] Train-Loss: 0.48834830957877184 Test-Loss: 0.14924501292555423 Train-f1: 0.4778108623078762 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8872/20000] Train-Loss: 0.4820301819900516 Test-Loss: 0.15465001883990773 Train-f1: 0.47983884728410275 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [8873/20000] Train-Loss: 0.48595503364728904 Test-Loss: 0.15330330192808922 Train-f1: 0.47098894997782964 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8874/20000] Train-Loss: 0.48462731843182716 Test-Loss: 0.1493234628115767 Train-f1: 0.468892298948565 Test-f1: 0.45000000000000007 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8875/20000] Train-Loss: 0.48095403163017136 Test-Loss: 0.15359285524030591 Train-f1: 0.4832987544153188 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [8876/20000] Train-Loss: 0.4850371749018975 Test-Loss: 0.15824707217518927 Train-f1: 0.4678181818181818 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [8877/20000] Train-Loss: 0.48715617814342443 Test-Loss: 0.15698543605864373 Train-f1: 0.47181688283571804 Test-f1: 0.4466252587991718 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8878/20000] Train-Loss: 0.48720860867923244 Test-Loss: 0.1524254517776662 Train-f1: 0.4730959595959595 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8879/20000] Train-Loss: 0.48692139181601274 Test-Loss: 0.15107437771838786 Train-f1: 0.46874998430669496 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8880/20000] Train-Loss: 0.4848372755234934 Test-Loss: 0.15610658870134247 Train-f1: 0.4677397669038127 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8881/20000] Train-Loss: 0.48725358621975756 Test-Loss: 0.15194947101251652 Train-f1: 0.4762376163003214 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8882/20000] Train-Loss: 0.4826706037096146 Test-Loss: 0.1491311666298626 Train-f1: 0.4792881207871469 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8883/20000] Train-Loss: 0.48986958739646774 Test-Loss: 0.14920383951257843 Train-f1: 0.47009328420520635 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8884/20000] Train-Loss: 0.48603359738653307 Test-Loss: 0.1568736123754258 Train-f1: 0.46990999193816096 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8885/20000] Train-Loss: 0.48018867682272304 Test-Loss: 0.15008800174532536 Train-f1: 0.48115151515151516 Test-f1: 0.4508141119786989 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8886/20000] Train-Loss: 0.4870295363763324 Test-Loss: 0.15159291119159166 Train-f1: 0.4721433288257405 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8887/20000] Train-Loss: 0.48987714524371584 Test-Loss: 0.14949254886928248 Train-f1: 0.47012399691707846 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8888/20000] Train-Loss: 0.48771241168445933 Test-Loss: 0.14913776482030272 Train-f1: 0.4707901029487342 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8889/20000] Train-Loss: 0.48305863121265025 Test-Loss: 0.15340542870231366 Train-f1: 0.47025549017425305 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8890/20000] Train-Loss: 0.48384938392497107 Test-Loss: 0.15647209797339942 Train-f1: 0.47645858017280407 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [8891/20000] Train-Loss: 0.4890244414660614 Test-Loss: 0.1512139609407819 Train-f1: 0.4782472765329908 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8892/20000] Train-Loss: 0.48517829147006314 Test-Loss: 0.15043869121856532 Train-f1: 0.4727606306736741 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8893/20000] Train-Loss: 0.4849570778468153 Test-Loss: 0.14977967718117302 Train-f1: 0.4763035868456165 Test-f1: 0.43932367149758456 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [8894/20000] Train-Loss: 0.4849601623958134 Test-Loss: 0.15203724270999283 Train-f1: 0.468767184169096 Test-f1: 0.45675057208237985 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [8895/20000] Train-Loss: 0.4854916109593562 Test-Loss: 0.1511659666382148 Train-f1: 0.47168082115679777 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8896/20000] Train-Loss: 0.48222640463850697 Test-Loss: 0.1515043928518319 Train-f1: 0.47880171886442396 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8897/20000] Train-Loss: 0.48640640628578613 Test-Loss: 0.15127405303068822 Train-f1: 0.47272689334661155 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8898/20000] Train-Loss: 0.485819914291108 Test-Loss: 0.15284468139145652 Train-f1: 0.46635473551250717 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8899/20000] Train-Loss: 0.48575935377294965 Test-Loss: 0.15007866827746938 Train-f1: 0.4672960814080035 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8900/20000] Train-Loss: 0.48546110219355815 Test-Loss: 0.15249838285687187 Train-f1: 0.47983884728410275 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8901/20000] Train-Loss: 0.48659884727004415 Test-Loss: 0.1495644388263155 Train-f1: 0.4811753711480452 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8902/20000] Train-Loss: 0.48610111496167574 Test-Loss: 0.15691050140892687 Train-f1: 0.4754366977095394 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [8903/20000] Train-Loss: 0.4867281067260053 Test-Loss: 0.14995023732964746 Train-f1: 0.47592576690043414 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8904/20000] Train-Loss: 0.48653270676938637 Test-Loss: 0.15079549316741236 Train-f1: 0.4729366977095394 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8905/20000] Train-Loss: 0.48242690631628954 Test-Loss: 0.15447298828341882 Train-f1: 0.46589498469507895 Test-f1: 0.45313466162830646 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8906/20000] Train-Loss: 0.4879319458274228 Test-Loss: 0.1525958494570307 Train-f1: 0.47344885751383126 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8907/20000] Train-Loss: 0.4845266387100469 Test-Loss: 0.16748346288225938 Train-f1: 0.47348003891770957 Test-f1: 0.4495524296675192 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8908/20000] Train-Loss: 0.4882802946354608 Test-Loss: 0.14966622278103134 Train-f1: 0.46874998430669496 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8909/20000] Train-Loss: 0.4822740927990724 Test-Loss: 0.1499275850061268 Train-f1: 0.4787488980537207 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8910/20000] Train-Loss: 0.48162425790808977 Test-Loss: 0.14926574716520363 Train-f1: 0.4818734793187348 Test-f1: 0.4493236714975846 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8911/20000] Train-Loss: 0.4848658228725282 Test-Loss: 0.14951745329638078 Train-f1: 0.48129104975541354 Test-f1: 0.449645003282331 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8912/20000] Train-Loss: 0.4850231019813148 Test-Loss: 0.1502358507127791 Train-f1: 0.47538624674454566 Test-f1: 0.4536974789915966 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8913/20000] Train-Loss: 0.4872224730617507 Test-Loss: 0.14927209213155843 Train-f1: 0.4707901029487342 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8914/20000] Train-Loss: 0.4867560985688354 Test-Loss: 0.15375326019773689 Train-f1: 0.4817168338907469 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [8915/20000] Train-Loss: 0.4875590144996366 Test-Loss: 0.14959993729864043 Train-f1: 0.46830438621944037 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8916/20000] Train-Loss: 0.4870782539796468 Test-Loss: 0.15324944537935964 Train-f1: 0.4700859854393028 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8917/20000] Train-Loss: 0.486170014007066 Test-Loss: 0.1506174125063887 Train-f1: 0.47393939393939394 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8918/20000] Train-Loss: 0.480614281210865 Test-Loss: 0.14918508520523174 Train-f1: 0.466463050821058 Test-f1: 0.449645003282331 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8919/20000] Train-Loss: 0.48662285561370255 Test-Loss: 0.14894739663436965 Train-f1: 0.47716164261790245 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8920/20000] Train-Loss: 0.48575447848682 Test-Loss: 0.14991330436493577 Train-f1: 0.4697043044354966 Test-f1: 0.45984654731457797 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8921/20000] Train-Loss: 0.4821569684374351 Test-Loss: 0.15096390177672875 Train-f1: 0.46670331548948474 Test-f1: 0.4461529397902675 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8922/20000] Train-Loss: 0.4835433974835447 Test-Loss: 0.15799780546705688 Train-f1: 0.4651444901291809 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8923/20000] Train-Loss: 0.4844648920253126 Test-Loss: 0.14918139237255013 Train-f1: 0.47348003891770957 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8924/20000] Train-Loss: 0.4847010469943817 Test-Loss: 0.15124587245036095 Train-f1: 0.4807701111421959 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8925/20000] Train-Loss: 0.4850908892439276 Test-Loss: 0.15015248927537472 Train-f1: 0.4636076555023923 Test-f1: 0.449645003282331 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8926/20000] Train-Loss: 0.48355328617705096 Test-Loss: 0.14872297555849812 Train-f1: 0.4775315568022441 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8927/20000] Train-Loss: 0.48297381324614674 Test-Loss: 0.15195709265505467 Train-f1: 0.4729366977095394 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8928/20000] Train-Loss: 0.4822092925002161 Test-Loss: 0.15287318618145182 Train-f1: 0.47183065583065587 Test-f1: 0.4485461286270184 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8929/20000] Train-Loss: 0.48320591209009706 Test-Loss: 0.15683011817100131 Train-f1: 0.4676781071674263 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8930/20000] Train-Loss: 0.48671134575499847 Test-Loss: 0.15104845145038762 Train-f1: 0.4581548690676168 Test-f1: 0.4466252587991718 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [8931/20000] Train-Loss: 0.4805780108762491 Test-Loss: 0.15312610224237266 Train-f1: 0.4827188377679989 Test-f1: 0.45313466162830646 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8932/20000] Train-Loss: 0.4873921798027549 Test-Loss: 0.15078694105711954 Train-f1: 0.4607445887445888 Test-f1: 0.44649416182596957 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8933/20000] Train-Loss: 0.48483129843201733 Test-Loss: 0.15131351600525686 Train-f1: 0.4697043044354966 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8934/20000] Train-Loss: 0.48338186379310366 Test-Loss: 0.14955328808188514 Train-f1: 0.47343259134764554 Test-f1: 0.449645003282331 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8935/20000] Train-Loss: 0.487547510001098 Test-Loss: 0.1557597163868681 Train-f1: 0.4662392344497608 Test-f1: 0.44649416182596957 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8936/20000] Train-Loss: 0.48606453653809356 Test-Loss: 0.1583270861315286 Train-f1: 0.4711933712531245 Test-f1: 0.44076216533124873 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8937/20000] Train-Loss: 0.48634797016680276 Test-Loss: 0.15056700401018747 Train-f1: 0.4739279531986404 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8938/20000] Train-Loss: 0.4791832913695606 Test-Loss: 0.14991903351157737 Train-f1: 0.47415304921120793 Test-f1: 0.449645003282331 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8939/20000] Train-Loss: 0.484908834082308 Test-Loss: 0.16219726745584187 Train-f1: 0.469843509405905 Test-f1: 0.4532716620413197 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8940/20000] Train-Loss: 0.48767616489486953 Test-Loss: 0.14939359523866308 Train-f1: 0.46849800269822506 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8941/20000] Train-Loss: 0.48321769567785733 Test-Loss: 0.150556843499533 Train-f1: 0.46531818181818174 Test-f1: 0.45233400741489715 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8942/20000] Train-Loss: 0.48352461530515756 Test-Loss: 0.15563064945186397 Train-f1: 0.4774521687462864 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8943/20000] Train-Loss: 0.48594873236311514 Test-Loss: 0.14948142297423328 Train-f1: 0.4729366977095394 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8944/20000] Train-Loss: 0.4842947005855357 Test-Loss: 0.14957915928791798 Train-f1: 0.4684349524349525 Test-f1: 0.4503312334891282 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8945/20000] Train-Loss: 0.48159434266605533 Test-Loss: 0.15391721016365162 Train-f1: 0.4708334440374749 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [8946/20000] Train-Loss: 0.4862904312848546 Test-Loss: 0.1496857221616857 Train-f1: 0.46314229652303596 Test-f1: 0.45 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8947/20000] Train-Loss: 0.4815773875963368 Test-Loss: 0.15305383810785353 Train-f1: 0.4657229416843262 Test-f1: 0.4554347826086957 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8948/20000] Train-Loss: 0.48538491024985503 Test-Loss: 0.15016502074364982 Train-f1: 0.4785194794909556 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8949/20000] Train-Loss: 0.48404895771921275 Test-Loss: 0.15528241814981106 Train-f1: 0.47272689334661155 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [8950/20000] Train-Loss: 0.48443471723835907 Test-Loss: 0.1502644294689105 Train-f1: 0.476087912087912 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8951/20000] Train-Loss: 0.47951946026800424 Test-Loss: 0.15103064964317936 Train-f1: 0.47635466157570505 Test-f1: 0.44602036819428126 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8952/20000] Train-Loss: 0.4845849153168216 Test-Loss: 0.14878480657000143 Train-f1: 0.4747399171571834 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8953/20000] Train-Loss: 0.4823538450695154 Test-Loss: 0.15290463954732172 Train-f1: 0.47342074814382773 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8954/20000] Train-Loss: 0.48534545965112175 Test-Loss: 0.1521260936602096 Train-f1: 0.47801293055753147 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [8955/20000] Train-Loss: 0.4814393477939631 Test-Loss: 0.1513305296474638 Train-f1: 0.47646753246753254 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8956/20000] Train-Loss: 0.4825042076248211 Test-Loss: 0.15938194875965617 Train-f1: 0.47105627501765956 Test-f1: 0.44027149321266973 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [8957/20000] Train-Loss: 0.48217060159934677 Test-Loss: 0.1500934503010119 Train-f1: 0.4738643706655143 Test-f1: 0.45461235883771095 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8958/20000] Train-Loss: 0.48089515583850995 Test-Loss: 0.1540649433954771 Train-f1: 0.4734888933240544 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8959/20000] Train-Loss: 0.4840347897926047 Test-Loss: 0.1509386984039039 Train-f1: 0.4785883470527108 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8960/20000] Train-Loss: 0.4847991839819266 Test-Loss: 0.1517188564930503 Train-f1: 0.46208396931183343 Test-f1: 0.45313466162830646 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [8961/20000] Train-Loss: 0.4812911705507911 Test-Loss: 0.15124075643523 Train-f1: 0.480648496572013 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8962/20000] Train-Loss: 0.48833174198858276 Test-Loss: 0.14953398724132025 Train-f1: 0.4687613493721791 Test-f1: 0.4466252587991718 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8963/20000] Train-Loss: 0.4825423836598755 Test-Loss: 0.1532850755316519 Train-f1: 0.48369109441848546 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8964/20000] Train-Loss: 0.4800562374889786 Test-Loss: 0.15486248075659548 Train-f1: 0.4761237102967503 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [8965/20000] Train-Loss: 0.4851098603943856 Test-Loss: 0.15141795982290734 Train-f1: 0.48405038518691584 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8966/20000] Train-Loss: 0.48264722803772314 Test-Loss: 0.155690287079622 Train-f1: 0.4757349422290476 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8967/20000] Train-Loss: 0.4842761886135818 Test-Loss: 0.1493285322412283 Train-f1: 0.4742711948313051 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8968/20000] Train-Loss: 0.4855005593752112 Test-Loss: 0.1505943305595977 Train-f1: 0.46696819279995205 Test-f1: 0.44990810208201515 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8969/20000] Train-Loss: 0.483912948248359 Test-Loss: 0.1497910138284033 Train-f1: 0.4673462191393007 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8970/20000] Train-Loss: 0.4833490620408828 Test-Loss: 0.14943050832665977 Train-f1: 0.4781953183991946 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8971/20000] Train-Loss: 0.4822784360381779 Test-Loss: 0.15551680666654186 Train-f1: 0.4843846592292955 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8972/20000] Train-Loss: 0.48748400782712403 Test-Loss: 0.1557856289198404 Train-f1: 0.4729366977095394 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8973/20000] Train-Loss: 0.4844994346617237 Test-Loss: 0.15111876687396397 Train-f1: 0.46821117978169846 Test-f1: 0.44937417654808953 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8974/20000] Train-Loss: 0.477306322426408 Test-Loss: 0.14990339902445918 Train-f1: 0.4669423648247178 Test-f1: 0.45984654731457797 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [8975/20000] Train-Loss: 0.48301245827895795 Test-Loss: 0.1498938340852892 Train-f1: 0.46742615292726236 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8976/20000] Train-Loss: 0.48429316743207246 Test-Loss: 0.1518873734374189 Train-f1: 0.4848893297885004 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [8977/20000] Train-Loss: 0.4795158844184924 Test-Loss: 0.14958031114970555 Train-f1: 0.46670331548948474 Test-f1: 0.4485461286270184 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8978/20000] Train-Loss: 0.4841903726879589 Test-Loss: 0.15112254607223505 Train-f1: 0.4765357886869266 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8979/20000] Train-Loss: 0.4852996686188848 Test-Loss: 0.15220597804474836 Train-f1: 0.47093226821707257 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8980/20000] Train-Loss: 0.48388889726912443 Test-Loss: 0.15122613991580727 Train-f1: 0.47604057635207386 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8981/20000] Train-Loss: 0.4836640297792327 Test-Loss: 0.15064858994444297 Train-f1: 0.45987579356851593 Test-f1: 0.44937417654808953 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [8982/20000] Train-Loss: 0.4797340286102268 Test-Loss: 0.1650963966138014 Train-f1: 0.48048484848484846 Test-f1: 0.4344337363247945 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [8983/20000] Train-Loss: 0.48611819147209157 Test-Loss: 0.1507637939064323 Train-f1: 0.4680873231886629 Test-f1: 0.45119841858166543 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8984/20000] Train-Loss: 0.48446700804509896 Test-Loss: 0.15131318853643905 Train-f1: 0.4687304913742031 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [8985/20000] Train-Loss: 0.4840771477845983 Test-Loss: 0.15071528946168952 Train-f1: 0.4728904870024092 Test-f1: 0.4526190476190476 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [8986/20000] Train-Loss: 0.48028837062843005 Test-Loss: 0.14955058464501833 Train-f1: 0.4711470093207416 Test-f1: 0.449645003282331 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [8987/20000] Train-Loss: 0.48671423007322095 Test-Loss: 0.14839406142856806 Train-f1: 0.46351370930342173 Test-f1: 0.4497330282227307 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [8988/20000] Train-Loss: 0.48449568151132033 Test-Loss: 0.15074123145951313 Train-f1: 0.4701780770198843 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8989/20000] Train-Loss: 0.48513517551742175 Test-Loss: 0.14870746307593505 Train-f1: 0.48099039152688505 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [8990/20000] Train-Loss: 0.4821883130496883 Test-Loss: 0.1566884646797169 Train-f1: 0.47665904024730893 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [8991/20000] Train-Loss: 0.48509900242617593 Test-Loss: 0.1492003541098163 Train-f1: 0.47662752128639907 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8992/20000] Train-Loss: 0.48114875121592343 Test-Loss: 0.15186116179279677 Train-f1: 0.47967040580518977 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8993/20000] Train-Loss: 0.4861097548708967 Test-Loss: 0.15073175572689704 Train-f1: 0.47804230552056637 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [8994/20000] Train-Loss: 0.4824078962427603 Test-Loss: 0.15030726827751958 Train-f1: 0.4749939608994255 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [8995/20000] Train-Loss: 0.4810990977468858 Test-Loss: 0.15443911141385278 Train-f1: 0.47631381877450335 Test-f1: 0.4594670406732118 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [8996/20000] Train-Loss: 0.4829675793900349 Test-Loss: 0.15410411079431252 Train-f1: 0.4818008658008658 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [8997/20000] Train-Loss: 0.48402571446114034 Test-Loss: 0.14945627184167615 Train-f1: 0.47269122013647563 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8998/20000] Train-Loss: 0.48315457275181334 Test-Loss: 0.15158425807481882 Train-f1: 0.4688151198802849 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [8999/20000] Train-Loss: 0.4817604875395598 Test-Loss: 0.14928600300861483 Train-f1: 0.4638268398268398 Test-f1: 0.44990810208201515 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9000/20000] Train-Loss: 0.48299098162033777 Test-Loss: 0.1516035041118181 Train-f1: 0.46306427974390785 Test-f1: 0.43639684222929187 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [9001/20000] Train-Loss: 0.48086574594977255 Test-Loss: 0.14970224607003835 Train-f1: 0.4818008658008658 Test-f1: 0.4510087552341073 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9002/20000] Train-Loss: 0.48898598261628157 Test-Loss: 0.15337835757919965 Train-f1: 0.4685079952425003 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9003/20000] Train-Loss: 0.48230678421927053 Test-Loss: 0.14976084437087875 Train-f1: 0.4729366977095394 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9004/20000] Train-Loss: 0.48210363789247557 Test-Loss: 0.14925506545394324 Train-f1: 0.4711933712531245 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9005/20000] Train-Loss: 0.48262184235363426 Test-Loss: 0.1494843038948096 Train-f1: 0.46767807701988423 Test-f1: 0.43932367149758456 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [9006/20000] Train-Loss: 0.4815546665441379 Test-Loss: 0.14831779265198103 Train-f1: 0.4830076784529339 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9007/20000] Train-Loss: 0.4807680827257511 Test-Loss: 0.15049219264800437 Train-f1: 0.4774146048598603 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9008/20000] Train-Loss: 0.4800498600912367 Test-Loss: 0.14824389282215988 Train-f1: 0.4742857142857142 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9009/20000] Train-Loss: 0.4833590014429154 Test-Loss: 0.14892321324338031 Train-f1: 0.48290528983211906 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9010/20000] Train-Loss: 0.48409281592054904 Test-Loss: 0.15008097221993968 Train-f1: 0.4738927431646241 Test-f1: 0.4536974789915966 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9011/20000] Train-Loss: 0.48097555457251623 Test-Loss: 0.14839696606943248 Train-f1: 0.45741945633022907 Test-f1: 0.4497330282227307 Train-acc: 0.7781456953642384 Test-acc: 0.7631578947368421\n",
      "Epoch [9012/20000] Train-Loss: 0.48454277261955925 Test-Loss: 0.14837625808964236 Train-f1: 0.47707662574292475 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9013/20000] Train-Loss: 0.48244652038928265 Test-Loss: 0.15558026161904503 Train-f1: 0.46704326122469964 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9014/20000] Train-Loss: 0.4839649188683005 Test-Loss: 0.14877919339547543 Train-f1: 0.460976076555024 Test-f1: 0.45119841858166543 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [9015/20000] Train-Loss: 0.48461681820306374 Test-Loss: 0.1520739162047486 Train-f1: 0.4709990549990549 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9016/20000] Train-Loss: 0.4800600333882678 Test-Loss: 0.14955687588293323 Train-f1: 0.48729186602870805 Test-f1: 0.45984654731457797 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9017/20000] Train-Loss: 0.48398217477745453 Test-Loss: 0.1511075922097161 Train-f1: 0.474808450452559 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9018/20000] Train-Loss: 0.48155026378293786 Test-Loss: 0.15120291367684713 Train-f1: 0.46686893445443145 Test-f1: 0.45119841858166543 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9019/20000] Train-Loss: 0.48395661701450965 Test-Loss: 0.15161754954251014 Train-f1: 0.46468175302957915 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9020/20000] Train-Loss: 0.480667001870059 Test-Loss: 0.15481426650967225 Train-f1: 0.4781640245223235 Test-f1: 0.4624973437035148 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9021/20000] Train-Loss: 0.4872580283521602 Test-Loss: 0.14869922296416693 Train-f1: 0.46376651885508646 Test-f1: 0.44990810208201515 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9022/20000] Train-Loss: 0.4823820427569221 Test-Loss: 0.1577614102795788 Train-f1: 0.47362882860160466 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9023/20000] Train-Loss: 0.48470926785032115 Test-Loss: 0.1488122803486477 Train-f1: 0.47047949293121005 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9024/20000] Train-Loss: 0.4785756292082988 Test-Loss: 0.1536714014593767 Train-f1: 0.4813658214285265 Test-f1: 0.44683117900509206 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9025/20000] Train-Loss: 0.48621286181731943 Test-Loss: 0.15102846148663415 Train-f1: 0.47882996282996276 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9026/20000] Train-Loss: 0.4842451145673497 Test-Loss: 0.15149399186934345 Train-f1: 0.4678680625381991 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9027/20000] Train-Loss: 0.47569390759377334 Test-Loss: 0.1623065358630182 Train-f1: 0.4757349422290476 Test-f1: 0.4397742127153892 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9028/20000] Train-Loss: 0.485238655314522 Test-Loss: 0.15019240909803527 Train-f1: 0.47880171886442396 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9029/20000] Train-Loss: 0.485268821950485 Test-Loss: 0.1500782672914703 Train-f1: 0.47602780961834645 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9030/20000] Train-Loss: 0.48174990826749026 Test-Loss: 0.1491209024304463 Train-f1: 0.4787488980537207 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9031/20000] Train-Loss: 0.48402075448502596 Test-Loss: 0.14904292661478477 Train-f1: 0.4678680625381991 Test-f1: 0.43932367149758456 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [9032/20000] Train-Loss: 0.48109451642005563 Test-Loss: 0.15202996687552311 Train-f1: 0.4786921197408452 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9033/20000] Train-Loss: 0.48182164024677343 Test-Loss: 0.15120387293122245 Train-f1: 0.48124056792632974 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9034/20000] Train-Loss: 0.47964152187616294 Test-Loss: 0.15831436239753227 Train-f1: 0.4780959595959596 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9035/20000] Train-Loss: 0.48306014068877784 Test-Loss: 0.15167388922676828 Train-f1: 0.48101568485268853 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9036/20000] Train-Loss: 0.4817624378992721 Test-Loss: 0.15400670471652883 Train-f1: 0.48383882045605053 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9037/20000] Train-Loss: 0.48271087161267057 Test-Loss: 0.14803219754133085 Train-f1: 0.4711470093207416 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9038/20000] Train-Loss: 0.47587344414529326 Test-Loss: 0.1498415898833401 Train-f1: 0.4791387011969598 Test-f1: 0.4623826291079812 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9039/20000] Train-Loss: 0.4810038557481973 Test-Loss: 0.15355831291416097 Train-f1: 0.4644275918728473 Test-f1: 0.4594670406732118 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [9040/20000] Train-Loss: 0.4883769619931938 Test-Loss: 0.15005264582462977 Train-f1: 0.47161812121999114 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9041/20000] Train-Loss: 0.48093071174867075 Test-Loss: 0.1496226423437079 Train-f1: 0.4789702535122832 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9042/20000] Train-Loss: 0.4778422503390057 Test-Loss: 0.15166793307766016 Train-f1: 0.4864636347828837 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9043/20000] Train-Loss: 0.48106896809838745 Test-Loss: 0.14910777459687005 Train-f1: 0.47899082113806346 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9044/20000] Train-Loss: 0.484567982739081 Test-Loss: 0.14854953835392706 Train-f1: 0.47782875860734747 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9045/20000] Train-Loss: 0.4820071624905167 Test-Loss: 0.15010946691753177 Train-f1: 0.47645858017280407 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9046/20000] Train-Loss: 0.4805116163930161 Test-Loss: 0.14975363341775455 Train-f1: 0.47012399691707846 Test-f1: 0.4385580319084156 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [9047/20000] Train-Loss: 0.4812211092513306 Test-Loss: 0.14949176916831533 Train-f1: 0.47336508960709517 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9048/20000] Train-Loss: 0.47979396818937803 Test-Loss: 0.14889823740046557 Train-f1: 0.4666501265006645 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9049/20000] Train-Loss: 0.4825797791498877 Test-Loss: 0.14983095676328528 Train-f1: 0.475952393713448 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9050/20000] Train-Loss: 0.48287525874618675 Test-Loss: 0.15129383149704312 Train-f1: 0.47272689334661155 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9051/20000] Train-Loss: 0.4778302582051972 Test-Loss: 0.16383889947549282 Train-f1: 0.4737960159432582 Test-f1: 0.43313627938737687 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [9052/20000] Train-Loss: 0.4818203096566633 Test-Loss: 0.14826047674868176 Train-f1: 0.4627612774759724 Test-f1: 0.449645003282331 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9053/20000] Train-Loss: 0.48374348654309923 Test-Loss: 0.1509533388777254 Train-f1: 0.46801401148096683 Test-f1: 0.44649416182596957 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9054/20000] Train-Loss: 0.48099443460116414 Test-Loss: 0.14802937521517415 Train-f1: 0.46838960835099286 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9055/20000] Train-Loss: 0.4811275560969878 Test-Loss: 0.148648806474851 Train-f1: 0.47381576474748666 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9056/20000] Train-Loss: 0.4797569379032419 Test-Loss: 0.15108065923858013 Train-f1: 0.4641596452328159 Test-f1: 0.44937417654808953 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9057/20000] Train-Loss: 0.4792313794805213 Test-Loss: 0.15218830981789983 Train-f1: 0.4844773750473054 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9058/20000] Train-Loss: 0.48202871359068755 Test-Loss: 0.15495918453913984 Train-f1: 0.4785883470527108 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9059/20000] Train-Loss: 0.47660221524261387 Test-Loss: 0.14789931194628267 Train-f1: 0.48252282996808543 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9060/20000] Train-Loss: 0.4853055352733738 Test-Loss: 0.14930756271185414 Train-f1: 0.47359206472713017 Test-f1: 0.44854612862701837 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9061/20000] Train-Loss: 0.4788884213737085 Test-Loss: 0.1483848912981804 Train-f1: 0.47348003891770957 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9062/20000] Train-Loss: 0.48205994365151106 Test-Loss: 0.15423053004077886 Train-f1: 0.4764685436218283 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9063/20000] Train-Loss: 0.48067347547133604 Test-Loss: 0.15273883290742224 Train-f1: 0.47535962280487826 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9064/20000] Train-Loss: 0.48248300424350926 Test-Loss: 0.15106867948282654 Train-f1: 0.48377221349595 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9065/20000] Train-Loss: 0.47967309301753347 Test-Loss: 0.1515778732893837 Train-f1: 0.4708684887835429 Test-f1: 0.45119841858166543 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9066/20000] Train-Loss: 0.48221330064058837 Test-Loss: 0.1485412536015768 Train-f1: 0.483893902105551 Test-f1: 0.44990810208201515 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9067/20000] Train-Loss: 0.48526225134326945 Test-Loss: 0.1507177722147817 Train-f1: 0.4657322497322497 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9068/20000] Train-Loss: 0.4803978682302166 Test-Loss: 0.15157036765436457 Train-f1: 0.480648496572013 Test-f1: 0.4466252587991718 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9069/20000] Train-Loss: 0.47925284008769214 Test-Loss: 0.14828966658902062 Train-f1: 0.4737960159432582 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9070/20000] Train-Loss: 0.4790422278107875 Test-Loss: 0.15026832785701325 Train-f1: 0.4749939608994255 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9071/20000] Train-Loss: 0.4844839091786807 Test-Loss: 0.14828206680471714 Train-f1: 0.48166649595540767 Test-f1: 0.45984654731457797 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9072/20000] Train-Loss: 0.4828344582983269 Test-Loss: 0.14794876168417087 Train-f1: 0.46613161468663267 Test-f1: 0.4497330282227307 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9073/20000] Train-Loss: 0.48054229748760685 Test-Loss: 0.1486809939859614 Train-f1: 0.46561982246105116 Test-f1: 0.4497330282227307 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9074/20000] Train-Loss: 0.4828983376914771 Test-Loss: 0.15099800543468553 Train-f1: 0.4707901029487342 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9075/20000] Train-Loss: 0.4806507046035214 Test-Loss: 0.1507340655998881 Train-f1: 0.4729366977095394 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9076/20000] Train-Loss: 0.4701260641506972 Test-Loss: 0.16225111613750218 Train-f1: 0.4783395791661296 Test-f1: 0.4381270903010034 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9077/20000] Train-Loss: 0.48439143104519955 Test-Loss: 0.14841877331868125 Train-f1: 0.47426712784250763 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9078/20000] Train-Loss: 0.4776439995137098 Test-Loss: 0.15226100764777342 Train-f1: 0.4736470681798298 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9079/20000] Train-Loss: 0.4798095104752905 Test-Loss: 0.1520472383778462 Train-f1: 0.48908929385939015 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9080/20000] Train-Loss: 0.48241444087027463 Test-Loss: 0.15117973626628095 Train-f1: 0.4684349524349525 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9081/20000] Train-Loss: 0.4759438459946518 Test-Loss: 0.14895339554783693 Train-f1: 0.4733493896063895 Test-f1: 0.449645003282331 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9082/20000] Train-Loss: 0.4771547553025837 Test-Loss: 0.1497615535603042 Train-f1: 0.4814699864890947 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9083/20000] Train-Loss: 0.48000056036013494 Test-Loss: 0.1549700332979912 Train-f1: 0.48857086999022475 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9084/20000] Train-Loss: 0.48413957647514255 Test-Loss: 0.14913410057620405 Train-f1: 0.47880171886442396 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9085/20000] Train-Loss: 0.48404704228038653 Test-Loss: 0.14953030634425957 Train-f1: 0.46838960835099286 Test-f1: 0.4385580319084156 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [9086/20000] Train-Loss: 0.47943866869821944 Test-Loss: 0.15228595227624492 Train-f1: 0.47517100782000926 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9087/20000] Train-Loss: 0.4777413176234832 Test-Loss: 0.1612588860012821 Train-f1: 0.4729366977095394 Test-f1: 0.4495524296675192 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9088/20000] Train-Loss: 0.47535654263865307 Test-Loss: 0.16981672223100605 Train-f1: 0.47722533090275016 Test-f1: 0.44745098039215686 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9089/20000] Train-Loss: 0.483094265668117 Test-Loss: 0.14816837488954376 Train-f1: 0.47647177384019485 Test-f1: 0.45984654731457797 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9090/20000] Train-Loss: 0.483777686517674 Test-Loss: 0.14807670869123743 Train-f1: 0.48143184576824255 Test-f1: 0.45000000000000007 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9091/20000] Train-Loss: 0.48353786825114753 Test-Loss: 0.1510659645354761 Train-f1: 0.4678680625381991 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9092/20000] Train-Loss: 0.48103353735085835 Test-Loss: 0.14915513550711976 Train-f1: 0.4639817685909315 Test-f1: 0.45393483709273186 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9093/20000] Train-Loss: 0.4780965140540882 Test-Loss: 0.14793317860369717 Train-f1: 0.47116282410400057 Test-f1: 0.460952380952381 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9094/20000] Train-Loss: 0.4829753760684517 Test-Loss: 0.15153884322488892 Train-f1: 0.4778201171105946 Test-f1: 0.43639684222929187 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9095/20000] Train-Loss: 0.4812831830945067 Test-Loss: 0.14936153503883612 Train-f1: 0.4802648500379375 Test-f1: 0.449645003282331 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9096/20000] Train-Loss: 0.4800296141674475 Test-Loss: 0.15410266686031046 Train-f1: 0.4678680625381991 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9097/20000] Train-Loss: 0.47508246192177284 Test-Loss: 0.14963413592059327 Train-f1: 0.4790424251631774 Test-f1: 0.4509610604805303 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9098/20000] Train-Loss: 0.4803138221664736 Test-Loss: 0.1519888356097176 Train-f1: 0.46602546473134704 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9099/20000] Train-Loss: 0.47797989908094207 Test-Loss: 0.15166575184465939 Train-f1: 0.47466923089048974 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9100/20000] Train-Loss: 0.4788132568280036 Test-Loss: 0.14932428096581687 Train-f1: 0.4721433288257405 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9101/20000] Train-Loss: 0.48437879838544334 Test-Loss: 0.15039502201683294 Train-f1: 0.48105112506057585 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9102/20000] Train-Loss: 0.4790956737791841 Test-Loss: 0.14878692186500456 Train-f1: 0.4735631575631576 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9103/20000] Train-Loss: 0.4788365988786909 Test-Loss: 0.14860682813787993 Train-f1: 0.47627979763809664 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9104/20000] Train-Loss: 0.4784915879151474 Test-Loss: 0.1571706460282798 Train-f1: 0.48308175144638826 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9105/20000] Train-Loss: 0.48073774245556483 Test-Loss: 0.14824724881403947 Train-f1: 0.47348003891770957 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9106/20000] Train-Loss: 0.48065795677704626 Test-Loss: 0.1492358169378339 Train-f1: 0.47009328420520635 Test-f1: 0.4356787406570312 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [9107/20000] Train-Loss: 0.48018494072110296 Test-Loss: 0.15293692465908904 Train-f1: 0.4749939608994255 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9108/20000] Train-Loss: 0.4777711848633021 Test-Loss: 0.1477089014410849 Train-f1: 0.4788842320069069 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9109/20000] Train-Loss: 0.4753512161871092 Test-Loss: 0.15133248891604428 Train-f1: 0.4830950972070194 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9110/20000] Train-Loss: 0.4814908308484769 Test-Loss: 0.14999618588351737 Train-f1: 0.48167126567126567 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9111/20000] Train-Loss: 0.47867617648668076 Test-Loss: 0.15108943096575672 Train-f1: 0.4767655502392344 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9112/20000] Train-Loss: 0.4782577933391435 Test-Loss: 0.1624758496624804 Train-f1: 0.4837301175542823 Test-f1: 0.4397742127153892 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [9113/20000] Train-Loss: 0.4859007840105322 Test-Loss: 0.14788455094003003 Train-f1: 0.47604057635207386 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9114/20000] Train-Loss: 0.4823837869693361 Test-Loss: 0.15115559460594435 Train-f1: 0.47355305254193214 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9115/20000] Train-Loss: 0.47777036943107953 Test-Loss: 0.15982569142647793 Train-f1: 0.476817367030133 Test-f1: 0.44092054752026055 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9116/20000] Train-Loss: 0.48323211337850286 Test-Loss: 0.15090898556491666 Train-f1: 0.47363692017894987 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9117/20000] Train-Loss: 0.4805641450181546 Test-Loss: 0.14897767521915004 Train-f1: 0.47782875860734747 Test-f1: 0.4533424822739276 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9118/20000] Train-Loss: 0.47307778998935207 Test-Loss: 0.15029980140728194 Train-f1: 0.47951793906510537 Test-f1: 0.5001028112247151 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9119/20000] Train-Loss: 0.48437382176818783 Test-Loss: 0.154149487031103 Train-f1: 0.47027615862398464 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9120/20000] Train-Loss: 0.48240935358400405 Test-Loss: 0.14951143254100369 Train-f1: 0.48155089867357354 Test-f1: 0.44854612862701837 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9121/20000] Train-Loss: 0.47758401888657404 Test-Loss: 0.1484846154607915 Train-f1: 0.4835061147973253 Test-f1: 0.43932367149758456 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [9122/20000] Train-Loss: 0.4741854037597899 Test-Loss: 0.14776687166940375 Train-f1: 0.4635212261367602 Test-f1: 0.46016451233842537 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [9123/20000] Train-Loss: 0.4785149721501847 Test-Loss: 0.15290854228268935 Train-f1: 0.4774341392245451 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9124/20000] Train-Loss: 0.48019149287765106 Test-Loss: 0.14971428595091685 Train-f1: 0.4738701298701299 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9125/20000] Train-Loss: 0.4812613118839699 Test-Loss: 0.1510133309544554 Train-f1: 0.48140105547197737 Test-f1: 0.4466252587991718 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9126/20000] Train-Loss: 0.4818309686118029 Test-Loss: 0.15385713378439764 Train-f1: 0.47835503626807974 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9127/20000] Train-Loss: 0.48128523230362874 Test-Loss: 0.14860913860527827 Train-f1: 0.4843846592292955 Test-f1: 0.449645003282331 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9128/20000] Train-Loss: 0.4803983848267535 Test-Loss: 0.1484735262843294 Train-f1: 0.4796883116883118 Test-f1: 0.45984654731457797 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9129/20000] Train-Loss: 0.47999349048040746 Test-Loss: 0.14874441275239733 Train-f1: 0.4757983910555815 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9130/20000] Train-Loss: 0.4807266382237281 Test-Loss: 0.14875469609203087 Train-f1: 0.46808740024603157 Test-f1: 0.4533424822739276 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9131/20000] Train-Loss: 0.47715532155380264 Test-Loss: 0.1479602543102681 Train-f1: 0.47880171886442396 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9132/20000] Train-Loss: 0.481400223999115 Test-Loss: 0.15348590691219582 Train-f1: 0.4692760767213322 Test-f1: 0.4594670406732118 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9133/20000] Train-Loss: 0.4823167942098617 Test-Loss: 0.14785706690194558 Train-f1: 0.4785194794909556 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9134/20000] Train-Loss: 0.4787392960961544 Test-Loss: 0.1524093236542952 Train-f1: 0.4757983910555815 Test-f1: 0.45675057208237985 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9135/20000] Train-Loss: 0.4823115434427182 Test-Loss: 0.1480500961072707 Train-f1: 0.47538624674454566 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9136/20000] Train-Loss: 0.47583833098622286 Test-Loss: 0.1562664261409927 Train-f1: 0.48342627434979074 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9137/20000] Train-Loss: 0.476724110933377 Test-Loss: 0.14827288334829403 Train-f1: 0.47652432211199247 Test-f1: 0.4623826291079812 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9138/20000] Train-Loss: 0.4756935694881362 Test-Loss: 0.15704002180913432 Train-f1: 0.4814745406286473 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9139/20000] Train-Loss: 0.4793443215831284 Test-Loss: 0.14824144641824208 Train-f1: 0.4714566334714254 Test-f1: 0.45984654731457797 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9140/20000] Train-Loss: 0.47747490414149446 Test-Loss: 0.15062910783344968 Train-f1: 0.47168082115679777 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9141/20000] Train-Loss: 0.47858729697279156 Test-Loss: 0.15126190953872318 Train-f1: 0.47363692017894987 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9142/20000] Train-Loss: 0.4773769943644758 Test-Loss: 0.1492311455669397 Train-f1: 0.4737960159432582 Test-f1: 0.4423271037905184 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [9143/20000] Train-Loss: 0.4780237118783657 Test-Loss: 0.14911366468229584 Train-f1: 0.47538624674454566 Test-f1: 0.4510087552341073 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9144/20000] Train-Loss: 0.4811647541900373 Test-Loss: 0.14902354070970622 Train-f1: 0.46721547066072616 Test-f1: 0.46016451233842537 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [9145/20000] Train-Loss: 0.481698301399295 Test-Loss: 0.1495998704932052 Train-f1: 0.47841675045839 Test-f1: 0.4466252587991718 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9146/20000] Train-Loss: 0.4779314034992283 Test-Loss: 0.15098403897637994 Train-f1: 0.4783300366252018 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9147/20000] Train-Loss: 0.4808712289086528 Test-Loss: 0.14793013718477524 Train-f1: 0.4765085983599307 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9148/20000] Train-Loss: 0.4761144429288004 Test-Loss: 0.14767690476585285 Train-f1: 0.47604057635207386 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9149/20000] Train-Loss: 0.47925580342607954 Test-Loss: 0.14792207932006374 Train-f1: 0.4812966687543575 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9150/20000] Train-Loss: 0.47638530127145395 Test-Loss: 0.15736619866289664 Train-f1: 0.46704326122469964 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [9151/20000] Train-Loss: 0.4784244212104109 Test-Loss: 0.15400332105593792 Train-f1: 0.47222435651171735 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9152/20000] Train-Loss: 0.4785123931231438 Test-Loss: 0.15119388675765377 Train-f1: 0.47043669770953944 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9153/20000] Train-Loss: 0.48124372967841084 Test-Loss: 0.14780694178177473 Train-f1: 0.46874998430669496 Test-f1: 0.45119841858166543 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9154/20000] Train-Loss: 0.47912879602771236 Test-Loss: 0.14769090627646642 Train-f1: 0.4707901029487342 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9155/20000] Train-Loss: 0.47956690789157874 Test-Loss: 0.1490813902169288 Train-f1: 0.47604057635207386 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9156/20000] Train-Loss: 0.4804048519593424 Test-Loss: 0.14986543787618042 Train-f1: 0.4635040262743592 Test-f1: 0.45313466162830646 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9157/20000] Train-Loss: 0.48100441807977584 Test-Loss: 0.14849026758864742 Train-f1: 0.47321266768605075 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9158/20000] Train-Loss: 0.47674375946307795 Test-Loss: 0.1477279457058535 Train-f1: 0.4785194794909556 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9159/20000] Train-Loss: 0.4768781236311543 Test-Loss: 0.15748409577125613 Train-f1: 0.47647919577331344 Test-f1: 0.44092054752026055 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [9160/20000] Train-Loss: 0.4801256262531082 Test-Loss: 0.1480140135643793 Train-f1: 0.47830731712133334 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9161/20000] Train-Loss: 0.4741373501773247 Test-Loss: 0.15165026950664742 Train-f1: 0.482915862712756 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9162/20000] Train-Loss: 0.4757234799400152 Test-Loss: 0.1504432174129056 Train-f1: 0.4740247350796629 Test-f1: 0.45313466162830646 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9163/20000] Train-Loss: 0.4791834374197982 Test-Loss: 0.14977389115371592 Train-f1: 0.4761237102967503 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9164/20000] Train-Loss: 0.47826032038052085 Test-Loss: 0.14921095506591392 Train-f1: 0.475952393713448 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9165/20000] Train-Loss: 0.47331953903244717 Test-Loss: 0.15282627337525617 Train-f1: 0.483636711114972 Test-f1: 0.4652641421224871 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [9166/20000] Train-Loss: 0.4804315079868009 Test-Loss: 0.1551451758740541 Train-f1: 0.4739279531986404 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9167/20000] Train-Loss: 0.4816103256654597 Test-Loss: 0.14840328549445744 Train-f1: 0.466463050821058 Test-f1: 0.44990810208201515 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9168/20000] Train-Loss: 0.47317681393281696 Test-Loss: 0.15571221899452656 Train-f1: 0.4786553558663706 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9169/20000] Train-Loss: 0.48193855654809375 Test-Loss: 0.15238846713687376 Train-f1: 0.48165443789605 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9170/20000] Train-Loss: 0.4785434547171764 Test-Loss: 0.14983338043601013 Train-f1: 0.47858172406541577 Test-f1: 0.45461235883771095 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9171/20000] Train-Loss: 0.4782506169720722 Test-Loss: 0.14865653664868608 Train-f1: 0.4787488980537207 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9172/20000] Train-Loss: 0.4766855705435909 Test-Loss: 0.155141121694501 Train-f1: 0.46172062084257204 Test-f1: 0.4594670406732118 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [9173/20000] Train-Loss: 0.4789332691622833 Test-Loss: 0.15156872771343596 Train-f1: 0.4757983910555815 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9174/20000] Train-Loss: 0.4763945663297109 Test-Loss: 0.15127674906162866 Train-f1: 0.47893530673699536 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9175/20000] Train-Loss: 0.4804562228854364 Test-Loss: 0.14820237015231977 Train-f1: 0.46755507050027595 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9176/20000] Train-Loss: 0.47343176170950846 Test-Loss: 0.15920063109533303 Train-f1: 0.480648496572013 Test-f1: 0.4532716620413197 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9177/20000] Train-Loss: 0.48274418881781667 Test-Loss: 0.15229564315097915 Train-f1: 0.47105627501765956 Test-f1: 0.4594670406732118 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9178/20000] Train-Loss: 0.48307833124400207 Test-Loss: 0.14911065424495026 Train-f1: 0.4618997433378751 Test-f1: 0.45233400741489715 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [9179/20000] Train-Loss: 0.4803154139641739 Test-Loss: 0.15349122928570091 Train-f1: 0.4727310738042446 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9180/20000] Train-Loss: 0.47994954230184733 Test-Loss: 0.14910067668820148 Train-f1: 0.4711470093207416 Test-f1: 0.4423271037905184 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [9181/20000] Train-Loss: 0.47871328135221825 Test-Loss: 0.15243014059284038 Train-f1: 0.4832987544153188 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9182/20000] Train-Loss: 0.4720412321062328 Test-Loss: 0.14790967528096277 Train-f1: 0.47953435272604195 Test-f1: 0.45984654731457797 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9183/20000] Train-Loss: 0.4808142827343759 Test-Loss: 0.14754876004882378 Train-f1: 0.4730746589525932 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9184/20000] Train-Loss: 0.47554125211651976 Test-Loss: 0.1511231953205807 Train-f1: 0.47697458146227384 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9185/20000] Train-Loss: 0.4769540627356252 Test-Loss: 0.15224127123297385 Train-f1: 0.47612726012726014 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9186/20000] Train-Loss: 0.4785591135619474 Test-Loss: 0.15050506026917349 Train-f1: 0.4841315438348639 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9187/20000] Train-Loss: 0.4781273692164794 Test-Loss: 0.14831524777804753 Train-f1: 0.47945439045183286 Test-f1: 0.4356787406570312 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9188/20000] Train-Loss: 0.4764844722396064 Test-Loss: 0.1500639614270069 Train-f1: 0.4776091225847323 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9189/20000] Train-Loss: 0.47867309712799905 Test-Loss: 0.14860701829329986 Train-f1: 0.4762658602658602 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9190/20000] Train-Loss: 0.4748267526656542 Test-Loss: 0.15730113106175758 Train-f1: 0.47068102211643054 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9191/20000] Train-Loss: 0.47448522555687006 Test-Loss: 0.14739111283184975 Train-f1: 0.4777415000475299 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9192/20000] Train-Loss: 0.47649183435851794 Test-Loss: 0.14778350620866376 Train-f1: 0.47865964787760495 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9193/20000] Train-Loss: 0.4795154297987201 Test-Loss: 0.15149695840595917 Train-f1: 0.4769660799370353 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9194/20000] Train-Loss: 0.477597728539741 Test-Loss: 0.14779517264426523 Train-f1: 0.4690909090909091 Test-f1: 0.4623826291079812 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9195/20000] Train-Loss: 0.47818710468448594 Test-Loss: 0.14920268624205513 Train-f1: 0.4659419026319447 Test-f1: 0.46016451233842537 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [9196/20000] Train-Loss: 0.47979452706548226 Test-Loss: 0.1484343633228972 Train-f1: 0.47823170909515056 Test-f1: 0.44281573498964805 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9197/20000] Train-Loss: 0.477993338295415 Test-Loss: 0.14758912461336707 Train-f1: 0.4688713407140299 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9198/20000] Train-Loss: 0.4777282629599755 Test-Loss: 0.14783438039207683 Train-f1: 0.4780959595959596 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9199/20000] Train-Loss: 0.4721352922759308 Test-Loss: 0.15686392288095788 Train-f1: 0.4620273321449792 Test-f1: 0.43986583427042547 Train-acc: 0.7814569536423841 Test-acc: 0.75\n",
      "Epoch [9200/20000] Train-Loss: 0.4779048657242391 Test-Loss: 0.14932513688183255 Train-f1: 0.470735099916341 Test-f1: 0.44854612862701837 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9201/20000] Train-Loss: 0.4812333748000025 Test-Loss: 0.14802680265307175 Train-f1: 0.46316814716814714 Test-f1: 0.4497330282227307 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9202/20000] Train-Loss: 0.4782571665893161 Test-Loss: 0.14891393516649737 Train-f1: 0.4672960814080035 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9203/20000] Train-Loss: 0.47026126580802835 Test-Loss: 0.14965436920070527 Train-f1: 0.48368381667964755 Test-f1: 0.4368741765480896 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [9204/20000] Train-Loss: 0.4776326580558034 Test-Loss: 0.1484184350376361 Train-f1: 0.4626780770198843 Test-f1: 0.45984654731457797 Train-acc: 0.7847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [9205/20000] Train-Loss: 0.48187166930039566 Test-Loss: 0.14759201634201685 Train-f1: 0.4745574620027175 Test-f1: 0.4620254862508384 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9206/20000] Train-Loss: 0.4778175304927605 Test-Loss: 0.1487166608921712 Train-f1: 0.4815598480194754 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9207/20000] Train-Loss: 0.4797181590390835 Test-Loss: 0.1481418587517355 Train-f1: 0.46814937654681027 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9208/20000] Train-Loss: 0.47832302935467114 Test-Loss: 0.14923694703132961 Train-f1: 0.46522404363337255 Test-f1: 0.45233400741489715 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9209/20000] Train-Loss: 0.4771579703280419 Test-Loss: 0.1514866731755191 Train-f1: 0.47940036103916495 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9210/20000] Train-Loss: 0.4784509866392463 Test-Loss: 0.15059687903117014 Train-f1: 0.4723341283873852 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9211/20000] Train-Loss: 0.48036300017080635 Test-Loss: 0.1481544919035445 Train-f1: 0.4757349422290476 Test-f1: 0.449645003282331 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9212/20000] Train-Loss: 0.47756180019040306 Test-Loss: 0.15184845013601345 Train-f1: 0.47342074814382773 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9213/20000] Train-Loss: 0.4754594045615846 Test-Loss: 0.15192986719792023 Train-f1: 0.47363692017894987 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9214/20000] Train-Loss: 0.4750361191932323 Test-Loss: 0.1489519058734307 Train-f1: 0.4711470093207416 Test-f1: 0.4510087552341073 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9215/20000] Train-Loss: 0.4779758667845487 Test-Loss: 0.14911646716495083 Train-f1: 0.47635466157570505 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9216/20000] Train-Loss: 0.47576862731725234 Test-Loss: 0.1673313850959228 Train-f1: 0.4792881207871469 Test-f1: 0.4495524296675192 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9217/20000] Train-Loss: 0.4762335094159291 Test-Loss: 0.1481007069811785 Train-f1: 0.4868850794150642 Test-f1: 0.44990810208201515 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9218/20000] Train-Loss: 0.4760648725085532 Test-Loss: 0.1480327356264576 Train-f1: 0.47949588601244225 Test-f1: 0.45119841858166543 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9219/20000] Train-Loss: 0.4795533349767212 Test-Loss: 0.14989414090110964 Train-f1: 0.47835503626807974 Test-f1: 0.43639684222929187 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9220/20000] Train-Loss: 0.4771030268860022 Test-Loss: 0.14966921051359275 Train-f1: 0.4781953183991946 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9221/20000] Train-Loss: 0.4777998303642225 Test-Loss: 0.14861970856850387 Train-f1: 0.47355305254193214 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9222/20000] Train-Loss: 0.47356926614926353 Test-Loss: 0.1475463718602862 Train-f1: 0.47047949293121005 Test-f1: 0.45984654731457797 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9223/20000] Train-Loss: 0.47909098953227663 Test-Loss: 0.14737435995361164 Train-f1: 0.47253997316936414 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9224/20000] Train-Loss: 0.47808645937206234 Test-Loss: 0.14758562976392592 Train-f1: 0.4790392253340944 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9225/20000] Train-Loss: 0.4805123783361967 Test-Loss: 0.1489298895759719 Train-f1: 0.4729366977095394 Test-f1: 0.4510087552341073 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9226/20000] Train-Loss: 0.47639029483495876 Test-Loss: 0.14810390281337357 Train-f1: 0.4631292203102704 Test-f1: 0.4620254862508384 Train-acc: 0.7814569536423841 Test-acc: 0.7763157894736842\n",
      "Epoch [9227/20000] Train-Loss: 0.4768623206267313 Test-Loss: 0.15372532199004918 Train-f1: 0.47333787364937113 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9228/20000] Train-Loss: 0.4735648684766326 Test-Loss: 0.14900211044592415 Train-f1: 0.47583257203510365 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9229/20000] Train-Loss: 0.4778188549175094 Test-Loss: 0.1482377212295748 Train-f1: 0.47736107833184993 Test-f1: 0.4510087552341073 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9230/20000] Train-Loss: 0.4784735366971147 Test-Loss: 0.14948011052231125 Train-f1: 0.47833108675195196 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9231/20000] Train-Loss: 0.477666752362868 Test-Loss: 0.15068171741103314 Train-f1: 0.47196925596925593 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9232/20000] Train-Loss: 0.4722642043099509 Test-Loss: 0.16090668603897115 Train-f1: 0.4798903498724821 Test-f1: 0.4593201178892971 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9233/20000] Train-Loss: 0.4755630061238807 Test-Loss: 0.14718768751824077 Train-f1: 0.4741339712918661 Test-f1: 0.4493236714975846 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9234/20000] Train-Loss: 0.4779359425485242 Test-Loss: 0.14880949104867816 Train-f1: 0.4697089771542327 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9235/20000] Train-Loss: 0.4746193903860842 Test-Loss: 0.14696684408995025 Train-f1: 0.4845515064946307 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9236/20000] Train-Loss: 0.47632588562483164 Test-Loss: 0.1479897087160838 Train-f1: 0.47935783892657213 Test-f1: 0.4535117056856187 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9237/20000] Train-Loss: 0.47672254530948194 Test-Loss: 0.15099621980456837 Train-f1: 0.46814937654681027 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9238/20000] Train-Loss: 0.4780242184818342 Test-Loss: 0.14989538768067823 Train-f1: 0.47105627501765956 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9239/20000] Train-Loss: 0.4750494521298863 Test-Loss: 0.1494707567294384 Train-f1: 0.4786812576701373 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9240/20000] Train-Loss: 0.4717664765719832 Test-Loss: 0.1475764569082593 Train-f1: 0.4794863825445413 Test-f1: 0.4509610604805303 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9241/20000] Train-Loss: 0.4796543363472874 Test-Loss: 0.14844169973276305 Train-f1: 0.47627979763809664 Test-f1: 0.43932367149758456 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9242/20000] Train-Loss: 0.47800769913382396 Test-Loss: 0.14786779935577987 Train-f1: 0.4712727272727273 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9243/20000] Train-Loss: 0.47468978256233235 Test-Loss: 0.1476875240747525 Train-f1: 0.47320030632001286 Test-f1: 0.4623826291079812 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9244/20000] Train-Loss: 0.47794369822315014 Test-Loss: 0.15014064697938337 Train-f1: 0.47426383066647065 Test-f1: 0.45313466162830646 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9245/20000] Train-Loss: 0.47832704892046723 Test-Loss: 0.15166882733146633 Train-f1: 0.47374441191814426 Test-f1: 0.45461235883771095 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9246/20000] Train-Loss: 0.4787520185965703 Test-Loss: 0.15312655546223047 Train-f1: 0.46718032890775996 Test-f1: 0.44683117900509206 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9247/20000] Train-Loss: 0.4803918469401604 Test-Loss: 0.15100712085996168 Train-f1: 0.4730959595959595 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9248/20000] Train-Loss: 0.472931456834644 Test-Loss: 0.1480080852367109 Train-f1: 0.47237912774063007 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9249/20000] Train-Loss: 0.47475586563768635 Test-Loss: 0.14744107360591632 Train-f1: 0.4838140585244419 Test-f1: 0.44990810208201515 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9250/20000] Train-Loss: 0.4787155189547557 Test-Loss: 0.14739676136395458 Train-f1: 0.47880171886442396 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9251/20000] Train-Loss: 0.4773524947443974 Test-Loss: 0.14700032795515558 Train-f1: 0.48124056792632974 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9252/20000] Train-Loss: 0.4775423527023214 Test-Loss: 0.14977689238123973 Train-f1: 0.48099039152688505 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9253/20000] Train-Loss: 0.4729860772862537 Test-Loss: 0.15278467910035676 Train-f1: 0.4688234728234727 Test-f1: 0.4624973437035148 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9254/20000] Train-Loss: 0.47595367264438565 Test-Loss: 0.14906352254366478 Train-f1: 0.46981287325812876 Test-f1: 0.449645003282331 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9255/20000] Train-Loss: 0.4784511916975275 Test-Loss: 0.14822535685087826 Train-f1: 0.4701780770198843 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9256/20000] Train-Loss: 0.47470699792994514 Test-Loss: 0.1471668338426687 Train-f1: 0.47676782963407166 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9257/20000] Train-Loss: 0.47616311187041827 Test-Loss: 0.14790781090273133 Train-f1: 0.48105112506057585 Test-f1: 0.4510087552341073 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9258/20000] Train-Loss: 0.47948895269059943 Test-Loss: 0.14854526234995338 Train-f1: 0.46241945633022913 Test-f1: 0.4427614169867692 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [9259/20000] Train-Loss: 0.478024666458319 Test-Loss: 0.1477020377052895 Train-f1: 0.4792881207871469 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9260/20000] Train-Loss: 0.47939144038240655 Test-Loss: 0.14834109606374496 Train-f1: 0.46808740024603157 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9261/20000] Train-Loss: 0.4761861390564729 Test-Loss: 0.15046203995572477 Train-f1: 0.4790392253340944 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9262/20000] Train-Loss: 0.47579464388572484 Test-Loss: 0.14814530284131802 Train-f1: 0.4730328313661647 Test-f1: 0.44854612862701837 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9263/20000] Train-Loss: 0.4775748988769976 Test-Loss: 0.14701756932659404 Train-f1: 0.4817702801443282 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9264/20000] Train-Loss: 0.47298009567758903 Test-Loss: 0.1480064169494322 Train-f1: 0.4890267824771032 Test-f1: 0.4385580319084156 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [9265/20000] Train-Loss: 0.4755732756009548 Test-Loss: 0.15220111885739313 Train-f1: 0.46659866962305985 Test-f1: 0.4652641421224871 Train-acc: 0.7913907284768212 Test-acc: 0.7894736842105263\n",
      "Epoch [9266/20000] Train-Loss: 0.4764188226886519 Test-Loss: 0.15015699419550546 Train-f1: 0.46942440392485896 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9267/20000] Train-Loss: 0.47757273802364253 Test-Loss: 0.14738941393105015 Train-f1: 0.4738927431646241 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9268/20000] Train-Loss: 0.47752947631377046 Test-Loss: 0.14811322589652423 Train-f1: 0.4806953183991946 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9269/20000] Train-Loss: 0.4744765854624227 Test-Loss: 0.14691510794947715 Train-f1: 0.47333787364937113 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9270/20000] Train-Loss: 0.47686059362597905 Test-Loss: 0.14952016124269055 Train-f1: 0.46747727581060916 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9271/20000] Train-Loss: 0.4743522876502116 Test-Loss: 0.14783783682001114 Train-f1: 0.4892348629348803 Test-f1: 0.44683117900509206 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9272/20000] Train-Loss: 0.47130930057623543 Test-Loss: 0.14674462954936687 Train-f1: 0.4798001638001638 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9273/20000] Train-Loss: 0.476926067671257 Test-Loss: 0.14687080025030705 Train-f1: 0.4806953183991946 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9274/20000] Train-Loss: 0.47777939667443126 Test-Loss: 0.15390962828610624 Train-f1: 0.475772777946691 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9275/20000] Train-Loss: 0.47594232237652506 Test-Loss: 0.15224448389627104 Train-f1: 0.4790424251631774 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9276/20000] Train-Loss: 0.47658837134268867 Test-Loss: 0.15428160536812818 Train-f1: 0.4754366977095394 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9277/20000] Train-Loss: 0.4766816683596633 Test-Loss: 0.14694859841158653 Train-f1: 0.4896306172616313 Test-f1: 0.44990810208201515 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9278/20000] Train-Loss: 0.4769343324110764 Test-Loss: 0.14760240502024577 Train-f1: 0.470899547172579 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9279/20000] Train-Loss: 0.4757597253114815 Test-Loss: 0.14663579510381028 Train-f1: 0.4737960159432582 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9280/20000] Train-Loss: 0.4787157654908851 Test-Loss: 0.1483426406733707 Train-f1: 0.47782875860734747 Test-f1: 0.44357937096261785 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9281/20000] Train-Loss: 0.47683747347010813 Test-Loss: 0.1472250345745313 Train-f1: 0.4800111566286063 Test-f1: 0.45119841858166543 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9282/20000] Train-Loss: 0.47704270398557763 Test-Loss: 0.1507988180540489 Train-f1: 0.4785699807438938 Test-f1: 0.4356787406570312 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9283/20000] Train-Loss: 0.4782893478483936 Test-Loss: 0.14941076685513266 Train-f1: 0.4781640245223235 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9284/20000] Train-Loss: 0.4784856365853325 Test-Loss: 0.14779974212854846 Train-f1: 0.4813940653940653 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9285/20000] Train-Loss: 0.47333356859245007 Test-Loss: 0.15014348236801164 Train-f1: 0.4725624200076755 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9286/20000] Train-Loss: 0.4739960830730568 Test-Loss: 0.15001606123829306 Train-f1: 0.4841315438348639 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9287/20000] Train-Loss: 0.47150019770299056 Test-Loss: 0.14829578970823554 Train-f1: 0.4762376163003214 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9288/20000] Train-Loss: 0.47575611345685775 Test-Loss: 0.14946571907327819 Train-f1: 0.47355305254193214 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9289/20000] Train-Loss: 0.47897425473483135 Test-Loss: 0.151609470843785 Train-f1: 0.46767807701988423 Test-f1: 0.44683117900509206 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9290/20000] Train-Loss: 0.47494998852688264 Test-Loss: 0.14832733696861558 Train-f1: 0.4706648258480003 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9291/20000] Train-Loss: 0.4740735824750345 Test-Loss: 0.1482877843942409 Train-f1: 0.48077424858246776 Test-f1: 0.44854612862701837 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9292/20000] Train-Loss: 0.47549065037584776 Test-Loss: 0.15543034724484367 Train-f1: 0.4790424251631774 Test-f1: 0.4466252587991718 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9293/20000] Train-Loss: 0.4737349786232676 Test-Loss: 0.1561667752332898 Train-f1: 0.4715114004736199 Test-f1: 0.4545727136431784 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9294/20000] Train-Loss: 0.48064799219981713 Test-Loss: 0.15067911859373648 Train-f1: 0.4738927431646241 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9295/20000] Train-Loss: 0.4722955194563142 Test-Loss: 0.14695552014718538 Train-f1: 0.47459209403734953 Test-f1: 0.45984654731457797 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9296/20000] Train-Loss: 0.4744131300078607 Test-Loss: 0.14760806178245978 Train-f1: 0.47457937188143545 Test-f1: 0.449645003282331 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9297/20000] Train-Loss: 0.47430133554861603 Test-Loss: 0.14764955559859438 Train-f1: 0.47538624674454566 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9298/20000] Train-Loss: 0.4710974269179601 Test-Loss: 0.15868375782666752 Train-f1: 0.4848794704509739 Test-f1: 0.4545727136431784 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9299/20000] Train-Loss: 0.4821061055731763 Test-Loss: 0.14849700898320245 Train-f1: 0.47684687938992576 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9300/20000] Train-Loss: 0.47638772832690623 Test-Loss: 0.14673982768674615 Train-f1: 0.4755959595959595 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9301/20000] Train-Loss: 0.47198812579970645 Test-Loss: 0.1504758071432489 Train-f1: 0.4802648500379375 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9302/20000] Train-Loss: 0.47716562306452 Test-Loss: 0.14992460995466422 Train-f1: 0.4657322497322497 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9303/20000] Train-Loss: 0.47681059223227246 Test-Loss: 0.14690723580604687 Train-f1: 0.47681971587787453 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9304/20000] Train-Loss: 0.4770907687132929 Test-Loss: 0.1484607791072264 Train-f1: 0.4709990549990549 Test-f1: 0.46016451233842537 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9305/20000] Train-Loss: 0.4754432050546878 Test-Loss: 0.15114146212160492 Train-f1: 0.47012399691707846 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9306/20000] Train-Loss: 0.4733541182295859 Test-Loss: 0.146911614101796 Train-f1: 0.4767655502392344 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9307/20000] Train-Loss: 0.4743078561280646 Test-Loss: 0.14832156323446247 Train-f1: 0.4707901029487342 Test-f1: 0.4510087552341073 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9308/20000] Train-Loss: 0.47324752486942034 Test-Loss: 0.15345214892079517 Train-f1: 0.4749939608994255 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9309/20000] Train-Loss: 0.47728957945741873 Test-Loss: 0.1479562310593503 Train-f1: 0.487165885579598 Test-f1: 0.4385580319084156 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [9310/20000] Train-Loss: 0.4773637814212058 Test-Loss: 0.1515522169706056 Train-f1: 0.47012399691707846 Test-f1: 0.4624973437035148 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9311/20000] Train-Loss: 0.4771693603682614 Test-Loss: 0.15114030877632087 Train-f1: 0.4677397669038127 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9312/20000] Train-Loss: 0.47561502234236364 Test-Loss: 0.1500538257363137 Train-f1: 0.468767184169096 Test-f1: 0.45461235883771095 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9313/20000] Train-Loss: 0.47606310957581205 Test-Loss: 0.16207477228828523 Train-f1: 0.4676395248673891 Test-f1: 0.4495524296675192 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9314/20000] Train-Loss: 0.47453791871660495 Test-Loss: 0.14788624239227605 Train-f1: 0.48362205605859776 Test-f1: 0.44683117900509206 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9315/20000] Train-Loss: 0.47384992243977014 Test-Loss: 0.1500274359326746 Train-f1: 0.48105112506057585 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9316/20000] Train-Loss: 0.4699335752836156 Test-Loss: 0.15309548321919278 Train-f1: 0.475952393713448 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9317/20000] Train-Loss: 0.476906935816537 Test-Loss: 0.1477030317418684 Train-f1: 0.468905737383251 Test-f1: 0.44854612862701837 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9318/20000] Train-Loss: 0.474071038281499 Test-Loss: 0.14879248131740574 Train-f1: 0.4762658602658602 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9319/20000] Train-Loss: 0.47740463029843194 Test-Loss: 0.14843134249815795 Train-f1: 0.4859107463994802 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9320/20000] Train-Loss: 0.474275375555944 Test-Loss: 0.14907228910465492 Train-f1: 0.4815326655326656 Test-f1: 0.44683117900509206 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9321/20000] Train-Loss: 0.4748243647737197 Test-Loss: 0.15006450269821167 Train-f1: 0.47498020825836457 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9322/20000] Train-Loss: 0.4787954707817136 Test-Loss: 0.14644815377860243 Train-f1: 0.47233892075160816 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9323/20000] Train-Loss: 0.4710446524813936 Test-Loss: 0.15556808806270245 Train-f1: 0.45531433953378864 Test-f1: 0.4356787406570312 Train-acc: 0.7748344370860927 Test-acc: 0.75\n",
      "Epoch [9324/20000] Train-Loss: 0.4777627568702323 Test-Loss: 0.1511173519665092 Train-f1: 0.4738643706655143 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9325/20000] Train-Loss: 0.476312615545222 Test-Loss: 0.14885017716806742 Train-f1: 0.4734928056514369 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9326/20000] Train-Loss: 0.47208858616547567 Test-Loss: 0.14935917466119358 Train-f1: 0.47782875860734747 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9327/20000] Train-Loss: 0.4724575957877084 Test-Loss: 0.1468583578571943 Train-f1: 0.4790424251631774 Test-f1: 0.45984654731457797 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9328/20000] Train-Loss: 0.47346550811526733 Test-Loss: 0.14909888696529805 Train-f1: 0.4718622193154567 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9329/20000] Train-Loss: 0.4778008073317351 Test-Loss: 0.1476074344903674 Train-f1: 0.47644502256577476 Test-f1: 0.45675057208237985 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9330/20000] Train-Loss: 0.47702300107481493 Test-Loss: 0.15029341523506584 Train-f1: 0.47569531839919466 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9331/20000] Train-Loss: 0.4744164056294732 Test-Loss: 0.1536889768621654 Train-f1: 0.477021645021645 Test-f1: 0.4466252587991718 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9332/20000] Train-Loss: 0.4693400695657973 Test-Loss: 0.14813623362427505 Train-f1: 0.47520380273321444 Test-f1: 0.4485461286270184 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9333/20000] Train-Loss: 0.47746633293045226 Test-Loss: 0.14808405043837466 Train-f1: 0.4722453945081682 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9334/20000] Train-Loss: 0.4675999477264509 Test-Loss: 0.15413485538670046 Train-f1: 0.4743087972699497 Test-f1: 0.4624973437035148 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9335/20000] Train-Loss: 0.47696596265343283 Test-Loss: 0.14684366307392557 Train-f1: 0.46635473551250717 Test-f1: 0.4497330282227307 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9336/20000] Train-Loss: 0.47359272613928655 Test-Loss: 0.14915704181444453 Train-f1: 0.4843734793187348 Test-f1: 0.4466252587991718 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9337/20000] Train-Loss: 0.472341845125288 Test-Loss: 0.1470591381363851 Train-f1: 0.4763035868456165 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9338/20000] Train-Loss: 0.47084801636277757 Test-Loss: 0.14932530773141606 Train-f1: 0.4753666499086796 Test-f1: 0.45984654731457797 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9339/20000] Train-Loss: 0.47094239198855603 Test-Loss: 0.15821021406466704 Train-f1: 0.4832987544153188 Test-f1: 0.4397742127153892 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [9340/20000] Train-Loss: 0.4696937087919887 Test-Loss: 0.1470417513774525 Train-f1: 0.476839922271755 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9341/20000] Train-Loss: 0.477736990546312 Test-Loss: 0.14858854172915312 Train-f1: 0.47355305254193214 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9342/20000] Train-Loss: 0.47213637304387357 Test-Loss: 0.14906014686379926 Train-f1: 0.4785194794909556 Test-f1: 0.44854612862701837 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9343/20000] Train-Loss: 0.47367735736592037 Test-Loss: 0.14846763312198283 Train-f1: 0.4868850794150642 Test-f1: 0.44990810208201515 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9344/20000] Train-Loss: 0.47069137749438356 Test-Loss: 0.1525388981885018 Train-f1: 0.4678181818181818 Test-f1: 0.4594670406732118 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9345/20000] Train-Loss: 0.47711678882740716 Test-Loss: 0.14859218866069393 Train-f1: 0.47363692017894987 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9346/20000] Train-Loss: 0.4743304551350001 Test-Loss: 0.14996430245089623 Train-f1: 0.47131792376317927 Test-f1: 0.45313466162830646 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9347/20000] Train-Loss: 0.4739831471840386 Test-Loss: 0.14816801963699283 Train-f1: 0.47370175770175765 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9348/20000] Train-Loss: 0.4746942705408575 Test-Loss: 0.14928748898003122 Train-f1: 0.4868850794150642 Test-f1: 0.44854612862701837 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9349/20000] Train-Loss: 0.47511642351542643 Test-Loss: 0.15053568454390445 Train-f1: 0.4868850794150642 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9350/20000] Train-Loss: 0.472169390294751 Test-Loss: 0.14742663812891815 Train-f1: 0.4757349422290476 Test-f1: 0.4535117056856187 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9351/20000] Train-Loss: 0.477216141750947 Test-Loss: 0.1483885582288629 Train-f1: 0.47012399691707846 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9352/20000] Train-Loss: 0.471277571582148 Test-Loss: 0.1530978599720518 Train-f1: 0.4745574620027175 Test-f1: 0.4624973437035148 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9353/20000] Train-Loss: 0.4735091664625441 Test-Loss: 0.14718426156738929 Train-f1: 0.48336308155662994 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9354/20000] Train-Loss: 0.4729003316437443 Test-Loss: 0.1464927290697454 Train-f1: 0.4787545787545787 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9355/20000] Train-Loss: 0.4721163005866066 Test-Loss: 0.14771058895648778 Train-f1: 0.46600865800865793 Test-f1: 0.4510087552341073 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9356/20000] Train-Loss: 0.47208245330306864 Test-Loss: 0.15374576693248532 Train-f1: 0.47348003891770957 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9357/20000] Train-Loss: 0.47514497528264965 Test-Loss: 0.1476555088871738 Train-f1: 0.4867121889961542 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9358/20000] Train-Loss: 0.4744947902653088 Test-Loss: 0.14919799970874725 Train-f1: 0.4785194794909556 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9359/20000] Train-Loss: 0.4757468581639461 Test-Loss: 0.15161928070275785 Train-f1: 0.4802978944098165 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9360/20000] Train-Loss: 0.4768957964429288 Test-Loss: 0.15076033125162433 Train-f1: 0.47355305254193214 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9361/20000] Train-Loss: 0.4748440546458167 Test-Loss: 0.15008796674038888 Train-f1: 0.4711470093207416 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9362/20000] Train-Loss: 0.47655486198121255 Test-Loss: 0.14746655005889842 Train-f1: 0.47138156325406333 Test-f1: 0.4493236714975846 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9363/20000] Train-Loss: 0.47399232552917 Test-Loss: 0.14736726790912102 Train-f1: 0.4607445887445888 Test-f1: 0.45119841858166543 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "Epoch [9364/20000] Train-Loss: 0.47536966242814516 Test-Loss: 0.15030061517507204 Train-f1: 0.47569531839919466 Test-f1: 0.45313466162830646 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9365/20000] Train-Loss: 0.475437151034691 Test-Loss: 0.1478052839217639 Train-f1: 0.4769027737786736 Test-f1: 0.4356787406570312 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9366/20000] Train-Loss: 0.47733967299699087 Test-Loss: 0.1469385482906436 Train-f1: 0.47538624674454566 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9367/20000] Train-Loss: 0.47343200655899015 Test-Loss: 0.1464194665101411 Train-f1: 0.4895352436030157 Test-f1: 0.4493236714975846 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9368/20000] Train-Loss: 0.4732740250886751 Test-Loss: 0.14668627073070098 Train-f1: 0.48269599014124565 Test-f1: 0.44990810208201515 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9369/20000] Train-Loss: 0.47371066130749623 Test-Loss: 0.14675813439024735 Train-f1: 0.48129104975541354 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9370/20000] Train-Loss: 0.47287647106233693 Test-Loss: 0.14684118455001224 Train-f1: 0.4712427922954238 Test-f1: 0.46146799496163976 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9371/20000] Train-Loss: 0.4741252138475514 Test-Loss: 0.14792282700176704 Train-f1: 0.4660292050272381 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9372/20000] Train-Loss: 0.47370361058004185 Test-Loss: 0.147356589739768 Train-f1: 0.4788842320069069 Test-f1: 0.4535117056856187 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9373/20000] Train-Loss: 0.47143326232091304 Test-Loss: 0.1495258337448441 Train-f1: 0.4648617470896113 Test-f1: 0.45313466162830646 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9374/20000] Train-Loss: 0.4745109684556905 Test-Loss: 0.14803418402496607 Train-f1: 0.4813658214285265 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9375/20000] Train-Loss: 0.4753425312408414 Test-Loss: 0.14671943905587972 Train-f1: 0.4864933019313241 Test-f1: 0.4503312334891282 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9376/20000] Train-Loss: 0.4709011156411591 Test-Loss: 0.1517026263586539 Train-f1: 0.4694472239170226 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9377/20000] Train-Loss: 0.47468625985019414 Test-Loss: 0.1477957087492993 Train-f1: 0.46561982246105116 Test-f1: 0.45233400741489715 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9378/20000] Train-Loss: 0.4747449081585479 Test-Loss: 0.16442059145795765 Train-f1: 0.47342074814382773 Test-f1: 0.4495524296675192 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9379/20000] Train-Loss: 0.471125348959162 Test-Loss: 0.15356995687756478 Train-f1: 0.47793669770953945 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9380/20000] Train-Loss: 0.47084289119007844 Test-Loss: 0.15571367396854194 Train-f1: 0.4763035868456165 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9381/20000] Train-Loss: 0.4736844105215899 Test-Loss: 0.14728793384014197 Train-f1: 0.4606541908809033 Test-f1: 0.45119841858166543 Train-acc: 0.7814569536423841 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9382/20000] Train-Loss: 0.4749879248532895 Test-Loss: 0.14620345494120635 Train-f1: 0.48624584082472255 Test-f1: 0.4493236714975846 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9383/20000] Train-Loss: 0.4788037494620872 Test-Loss: 0.14867106225214022 Train-f1: 0.4859107463994802 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9384/20000] Train-Loss: 0.4741758885496519 Test-Loss: 0.14796054226063665 Train-f1: 0.4798561633014188 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9385/20000] Train-Loss: 0.4743975763289521 Test-Loss: 0.14613613133220868 Train-f1: 0.4688592308740228 Test-f1: 0.46016451233842537 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9386/20000] Train-Loss: 0.4748541957627138 Test-Loss: 0.1471052891243303 Train-f1: 0.4684349524349525 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9387/20000] Train-Loss: 0.47161646873505453 Test-Loss: 0.1471083873877309 Train-f1: 0.4710894526560255 Test-f1: 0.46016451233842537 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9388/20000] Train-Loss: 0.4755155438313119 Test-Loss: 0.14863344081825078 Train-f1: 0.47801293055753147 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9389/20000] Train-Loss: 0.47738908004810887 Test-Loss: 0.14641353971131024 Train-f1: 0.4762175653402402 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9390/20000] Train-Loss: 0.4722952907945111 Test-Loss: 0.14816885256969767 Train-f1: 0.48527349228611494 Test-f1: 0.45675057208237985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9391/20000] Train-Loss: 0.4783229920809505 Test-Loss: 0.15456361584940875 Train-f1: 0.4715023923444976 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9392/20000] Train-Loss: 0.47600925783685866 Test-Loss: 0.14843082474986363 Train-f1: 0.4781953183991946 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9393/20000] Train-Loss: 0.4711714940347762 Test-Loss: 0.15054100745233684 Train-f1: 0.47892676992801037 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9394/20000] Train-Loss: 0.47011589624773226 Test-Loss: 0.1487346318414426 Train-f1: 0.4757349422290476 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9395/20000] Train-Loss: 0.4750524091044035 Test-Loss: 0.14962530787279837 Train-f1: 0.46983829138062544 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9396/20000] Train-Loss: 0.475857857993108 Test-Loss: 0.1482468720408943 Train-f1: 0.47161812121999114 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9397/20000] Train-Loss: 0.4709910664467693 Test-Loss: 0.14663246413858852 Train-f1: 0.46767807701988423 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9398/20000] Train-Loss: 0.4715988826423603 Test-Loss: 0.1510849722495791 Train-f1: 0.47535962280487826 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9399/20000] Train-Loss: 0.4725118393880809 Test-Loss: 0.14746321151118844 Train-f1: 0.4790424251631774 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9400/20000] Train-Loss: 0.46900488400956325 Test-Loss: 0.1557797985299489 Train-f1: 0.4754739438948091 Test-f1: 0.44753623188405794 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9401/20000] Train-Loss: 0.47201465231088363 Test-Loss: 0.14646158510865612 Train-f1: 0.4699582018416474 Test-f1: 0.45000000000000007 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9402/20000] Train-Loss: 0.47220321534709503 Test-Loss: 0.14899286460464356 Train-f1: 0.47519602914908077 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9403/20000] Train-Loss: 0.47254800395455315 Test-Loss: 0.15266129336426207 Train-f1: 0.47635466157570505 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9404/20000] Train-Loss: 0.4700927998083401 Test-Loss: 0.15458484684541787 Train-f1: 0.47675464671645845 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9405/20000] Train-Loss: 0.4780865022795986 Test-Loss: 0.1462727253797712 Train-f1: 0.4629423206456874 Test-f1: 0.44990810208201515 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9406/20000] Train-Loss: 0.4739659631238544 Test-Loss: 0.14893030266701932 Train-f1: 0.48377221349595 Test-f1: 0.43639684222929187 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [9407/20000] Train-Loss: 0.46641570370661045 Test-Loss: 0.16324351951814386 Train-f1: 0.4855531329983885 Test-f1: 0.44886782299126543 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9408/20000] Train-Loss: 0.4788347316558981 Test-Loss: 0.15074352191021018 Train-f1: 0.4801188354129531 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9409/20000] Train-Loss: 0.47025332023582217 Test-Loss: 0.14605066232709607 Train-f1: 0.48058942364824714 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9410/20000] Train-Loss: 0.47369263525464717 Test-Loss: 0.14661326818902365 Train-f1: 0.4802978944098165 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9411/20000] Train-Loss: 0.475044650424487 Test-Loss: 0.1520294199420275 Train-f1: 0.46899175888862726 Test-f1: 0.45675057208237985 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9412/20000] Train-Loss: 0.47199434300637483 Test-Loss: 0.14610739797875738 Train-f1: 0.4727478019232142 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9413/20000] Train-Loss: 0.47586854106409227 Test-Loss: 0.14740939436356157 Train-f1: 0.4677397669038127 Test-f1: 0.43639684222929187 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [9414/20000] Train-Loss: 0.4739665895308254 Test-Loss: 0.14899284179685401 Train-f1: 0.47569531839919466 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9415/20000] Train-Loss: 0.4717091262998817 Test-Loss: 0.1461194008823637 Train-f1: 0.468767184169096 Test-f1: 0.46016451233842537 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9416/20000] Train-Loss: 0.47367562749332776 Test-Loss: 0.148149186728828 Train-f1: 0.4721433288257405 Test-f1: 0.45461235883771095 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9417/20000] Train-Loss: 0.4731711535464575 Test-Loss: 0.14755048218053524 Train-f1: 0.46653657763335177 Test-f1: 0.4497330282227307 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9418/20000] Train-Loss: 0.4732140493381307 Test-Loss: 0.14740261377846894 Train-f1: 0.4757983910555815 Test-f1: 0.4620254862508384 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9419/20000] Train-Loss: 0.4744930804176083 Test-Loss: 0.1467628051384898 Train-f1: 0.47590102718794985 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9420/20000] Train-Loss: 0.4692775822905112 Test-Loss: 0.145951894997905 Train-f1: 0.4785883470527108 Test-f1: 0.4493236714975846 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9421/20000] Train-Loss: 0.4722789916927894 Test-Loss: 0.15017297309772848 Train-f1: 0.4870748143372257 Test-f1: 0.43639684222929187 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [9422/20000] Train-Loss: 0.4723757669670722 Test-Loss: 0.1460933945070197 Train-f1: 0.47399594233903064 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9423/20000] Train-Loss: 0.47183263473045933 Test-Loss: 0.1480928553487151 Train-f1: 0.4679412960890332 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9424/20000] Train-Loss: 0.46911075310361 Test-Loss: 0.14708128801115597 Train-f1: 0.4791387011969598 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9425/20000] Train-Loss: 0.4733429414982683 Test-Loss: 0.1509275457560859 Train-f1: 0.4745564413828185 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9426/20000] Train-Loss: 0.471043196114244 Test-Loss: 0.1469189054617607 Train-f1: 0.4819024998721142 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9427/20000] Train-Loss: 0.4739073453816222 Test-Loss: 0.1463201651485864 Train-f1: 0.47583257203510365 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9428/20000] Train-Loss: 0.47548642770672633 Test-Loss: 0.14589999468048503 Train-f1: 0.4727712134446932 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9429/20000] Train-Loss: 0.4751611242284321 Test-Loss: 0.14838993327386907 Train-f1: 0.4729017746948562 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9430/20000] Train-Loss: 0.4709013365083937 Test-Loss: 0.15384912309663318 Train-f1: 0.48046187498269466 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [9431/20000] Train-Loss: 0.4698784774475222 Test-Loss: 0.1459410509728648 Train-f1: 0.4822090497885335 Test-f1: 0.4493236714975846 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9432/20000] Train-Loss: 0.4703671278312986 Test-Loss: 0.14795819675109953 Train-f1: 0.47538624674454566 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9433/20000] Train-Loss: 0.46973276719926416 Test-Loss: 0.1476533257471764 Train-f1: 0.47489007724301846 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9434/20000] Train-Loss: 0.4748228870262046 Test-Loss: 0.14615495015709595 Train-f1: 0.4673462191393007 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9435/20000] Train-Loss: 0.472702400789586 Test-Loss: 0.14774760843935497 Train-f1: 0.47627979763809664 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9436/20000] Train-Loss: 0.4735230612832146 Test-Loss: 0.1473977359764457 Train-f1: 0.47233892075160816 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9437/20000] Train-Loss: 0.47096894869467304 Test-Loss: 0.14671833426031353 Train-f1: 0.4781953183991946 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9438/20000] Train-Loss: 0.4700759212721583 Test-Loss: 0.14833822343188507 Train-f1: 0.47799089466255884 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9439/20000] Train-Loss: 0.47232606753496337 Test-Loss: 0.14727085752796248 Train-f1: 0.4813658214285265 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9440/20000] Train-Loss: 0.47703751260534033 Test-Loss: 0.14741914968893166 Train-f1: 0.4781953183991946 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9441/20000] Train-Loss: 0.4747856551402961 Test-Loss: 0.1473485337899139 Train-f1: 0.47880171886442396 Test-f1: 0.43932367149758456 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9442/20000] Train-Loss: 0.47057790421784046 Test-Loss: 0.1517276627221018 Train-f1: 0.48883550463519043 Test-f1: 0.4624973437035148 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9443/20000] Train-Loss: 0.47426115171793737 Test-Loss: 0.14874699051446622 Train-f1: 0.48729186602870805 Test-f1: 0.44854612862701837 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9444/20000] Train-Loss: 0.4725394999371703 Test-Loss: 0.14602364628523976 Train-f1: 0.4817702801443282 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9445/20000] Train-Loss: 0.4714050342440668 Test-Loss: 0.14886910291954428 Train-f1: 0.4808395083177691 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9446/20000] Train-Loss: 0.4704530853887178 Test-Loss: 0.14954560940840456 Train-f1: 0.4781953183991946 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9447/20000] Train-Loss: 0.4726703729077995 Test-Loss: 0.14788766931477515 Train-f1: 0.4790392253340944 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9448/20000] Train-Loss: 0.4722414976459009 Test-Loss: 0.14690362335827475 Train-f1: 0.466032406047321 Test-f1: 0.44649416182596957 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9449/20000] Train-Loss: 0.47260964101756886 Test-Loss: 0.15119261730230402 Train-f1: 0.48288316999973435 Test-f1: 0.43639684222929187 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [9450/20000] Train-Loss: 0.4676355048873212 Test-Loss: 0.1458320460805468 Train-f1: 0.4795244860301443 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9451/20000] Train-Loss: 0.47787124927387686 Test-Loss: 0.1455474232746637 Train-f1: 0.47707662574292475 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9452/20000] Train-Loss: 0.4738015092372185 Test-Loss: 0.14797961916652896 Train-f1: 0.4785883470527108 Test-f1: 0.43639684222929187 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9453/20000] Train-Loss: 0.47036174825869065 Test-Loss: 0.14570600359677033 Train-f1: 0.48377221349595 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9454/20000] Train-Loss: 0.4744829454629962 Test-Loss: 0.14861407931387616 Train-f1: 0.47253997316936414 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9455/20000] Train-Loss: 0.47430914222760223 Test-Loss: 0.1491300066166029 Train-f1: 0.47611715510603475 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9456/20000] Train-Loss: 0.47536702743910364 Test-Loss: 0.1457810069540401 Train-f1: 0.4792591002337675 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9457/20000] Train-Loss: 0.47137254653336824 Test-Loss: 0.14606761451851447 Train-f1: 0.46553962954539463 Test-f1: 0.46016451233842537 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [9458/20000] Train-Loss: 0.47191518614364414 Test-Loss: 0.15058104306229764 Train-f1: 0.48929905150940767 Test-f1: 0.44558629776021075 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9459/20000] Train-Loss: 0.4698539915989651 Test-Loss: 0.14739090857954207 Train-f1: 0.4677397669038127 Test-f1: 0.45393483709273186 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9460/20000] Train-Loss: 0.4752206664379977 Test-Loss: 0.14696741486247852 Train-f1: 0.4712611642172556 Test-f1: 0.46016451233842537 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9461/20000] Train-Loss: 0.47139183511100774 Test-Loss: 0.14968268393618187 Train-f1: 0.4719915490441083 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9462/20000] Train-Loss: 0.4703843110772629 Test-Loss: 0.15350112092929938 Train-f1: 0.4727606306736741 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9463/20000] Train-Loss: 0.4704337598325439 Test-Loss: 0.1479255338959898 Train-f1: 0.4782472765329908 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9464/20000] Train-Loss: 0.4746826061712898 Test-Loss: 0.14618049671578087 Train-f1: 0.48595393908884976 Test-f1: 0.44990810208201515 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9465/20000] Train-Loss: 0.47083412765279115 Test-Loss: 0.14988905804125982 Train-f1: 0.47604057635207386 Test-f1: 0.44683117900509206 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9466/20000] Train-Loss: 0.4696579676484669 Test-Loss: 0.14785363677321153 Train-f1: 0.4842981579823685 Test-f1: 0.4620254862508384 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9467/20000] Train-Loss: 0.4631905440158447 Test-Loss: 0.1534450327260412 Train-f1: 0.4804268774703557 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9468/20000] Train-Loss: 0.46687964693825834 Test-Loss: 0.1462862565995718 Train-f1: 0.4671568335221138 Test-f1: 0.4602828448452552 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [9469/20000] Train-Loss: 0.47805638045151455 Test-Loss: 0.14851702106622997 Train-f1: 0.47535687457781484 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9470/20000] Train-Loss: 0.47357952880149545 Test-Loss: 0.14776381979216327 Train-f1: 0.46624175824175823 Test-f1: 0.45233400741489715 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9471/20000] Train-Loss: 0.4679837237968011 Test-Loss: 0.15351272280206296 Train-f1: 0.4833239803674586 Test-f1: 0.44683117900509206 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9472/20000] Train-Loss: 0.4718671912786719 Test-Loss: 0.145456043571824 Train-f1: 0.48439826839826844 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9473/20000] Train-Loss: 0.4724621784274176 Test-Loss: 0.14724839360942069 Train-f1: 0.4677397669038127 Test-f1: 0.45393483709273186 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9474/20000] Train-Loss: 0.47216930704063287 Test-Loss: 0.1508530638899264 Train-f1: 0.4731953183991946 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9475/20000] Train-Loss: 0.4716639093969351 Test-Loss: 0.14740423721456072 Train-f1: 0.4813658214285265 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9476/20000] Train-Loss: 0.47035604563184347 Test-Loss: 0.14621615184575998 Train-f1: 0.4738927431646241 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9477/20000] Train-Loss: 0.4739603504001658 Test-Loss: 0.1462428152352248 Train-f1: 0.48556423300948853 Test-f1: 0.4497330282227307 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9478/20000] Train-Loss: 0.4729017543375542 Test-Loss: 0.14572141065507135 Train-f1: 0.4734432217155643 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9479/20000] Train-Loss: 0.47443635718871807 Test-Loss: 0.14978516429863473 Train-f1: 0.4749939608994255 Test-f1: 0.4356787406570312 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9480/20000] Train-Loss: 0.4711038503308491 Test-Loss: 0.14782804447150463 Train-f1: 0.48129104975541354 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9481/20000] Train-Loss: 0.4684914772525972 Test-Loss: 0.1489669279128473 Train-f1: 0.4876134902624969 Test-f1: 0.4620254862508384 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9482/20000] Train-Loss: 0.47553237185538966 Test-Loss: 0.14607208292513638 Train-f1: 0.48218809309600486 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9483/20000] Train-Loss: 0.46774201540568294 Test-Loss: 0.14698473317696348 Train-f1: 0.4782742485824677 Test-f1: 0.4423271037905184 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9484/20000] Train-Loss: 0.4702460629770151 Test-Loss: 0.148659601920452 Train-f1: 0.4725624200076755 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9485/20000] Train-Loss: 0.47243960416138686 Test-Loss: 0.14537536950428426 Train-f1: 0.4742390707165843 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9486/20000] Train-Loss: 0.4706160825417799 Test-Loss: 0.15440602901932915 Train-f1: 0.47536612938053324 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [9487/20000] Train-Loss: 0.47447487470476357 Test-Loss: 0.1490718886659549 Train-f1: 0.4818008658008658 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9488/20000] Train-Loss: 0.4741576569645657 Test-Loss: 0.14675762544991444 Train-f1: 0.4661123427496626 Test-f1: 0.44854612862701837 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9489/20000] Train-Loss: 0.47073449118375377 Test-Loss: 0.14792849569616592 Train-f1: 0.48595393908884976 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9490/20000] Train-Loss: 0.4706934441574543 Test-Loss: 0.1457548842498119 Train-f1: 0.48099039152688505 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9491/20000] Train-Loss: 0.46784411295657563 Test-Loss: 0.14743073093028086 Train-f1: 0.4676395248673891 Test-f1: 0.4423271037905184 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [9492/20000] Train-Loss: 0.46735446294549826 Test-Loss: 0.14576756053495202 Train-f1: 0.4723400853872108 Test-f1: 0.4594011008433066 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9493/20000] Train-Loss: 0.47506153159047476 Test-Loss: 0.14695056159221595 Train-f1: 0.4807701111421959 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9494/20000] Train-Loss: 0.4694733512681414 Test-Loss: 0.14871242981995775 Train-f1: 0.4710894526560255 Test-f1: 0.45675057208237985 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9495/20000] Train-Loss: 0.47334198598260246 Test-Loss: 0.15333969164771594 Train-f1: 0.4687822704321601 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9496/20000] Train-Loss: 0.4698200777463435 Test-Loss: 0.14681516366811773 Train-f1: 0.4795244860301443 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9497/20000] Train-Loss: 0.47108775887373217 Test-Loss: 0.15061235012407834 Train-f1: 0.46943797689121425 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9498/20000] Train-Loss: 0.4697452535679611 Test-Loss: 0.14892763901776324 Train-f1: 0.4819024998721142 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9499/20000] Train-Loss: 0.4691206373891773 Test-Loss: 0.1459950071217163 Train-f1: 0.49014718614718616 Test-f1: 0.46016451233842537 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9500/20000] Train-Loss: 0.47440008611598233 Test-Loss: 0.1456226955371968 Train-f1: 0.47068102211643054 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9501/20000] Train-Loss: 0.4703460555978032 Test-Loss: 0.15299897045599728 Train-f1: 0.47882996282996276 Test-f1: 0.4356787406570312 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9502/20000] Train-Loss: 0.47060037574617686 Test-Loss: 0.14874503894840463 Train-f1: 0.47899082113806346 Test-f1: 0.4356787406570312 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9503/20000] Train-Loss: 0.4703610739274808 Test-Loss: 0.14717135052283148 Train-f1: 0.4781953183991946 Test-f1: 0.4385580319084156 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9504/20000] Train-Loss: 0.47073264541482185 Test-Loss: 0.14894876762184167 Train-f1: 0.4834539390888498 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9505/20000] Train-Loss: 0.4653959824744403 Test-Loss: 0.1501529866903622 Train-f1: 0.47547396823090526 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9506/20000] Train-Loss: 0.47177079018239754 Test-Loss: 0.15567281862268414 Train-f1: 0.4819024998721142 Test-f1: 0.44027149321266973 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [9507/20000] Train-Loss: 0.47265425424757784 Test-Loss: 0.1581230163409183 Train-f1: 0.4729017746948562 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [9508/20000] Train-Loss: 0.469185717587131 Test-Loss: 0.1463836857844989 Train-f1: 0.48477631981675107 Test-f1: 0.45984654731457797 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9509/20000] Train-Loss: 0.4717629876774845 Test-Loss: 0.14892433146331252 Train-f1: 0.48226834176774985 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9510/20000] Train-Loss: 0.4658480961304777 Test-Loss: 0.1454345824437402 Train-f1: 0.4919469285641586 Test-f1: 0.4497330282227307 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [9511/20000] Train-Loss: 0.4709599360181639 Test-Loss: 0.14754845950503673 Train-f1: 0.4757349422290476 Test-f1: 0.44854612862701837 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9512/20000] Train-Loss: 0.4706368226903712 Test-Loss: 0.14773981664615193 Train-f1: 0.47635466157570505 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9513/20000] Train-Loss: 0.46955270904861585 Test-Loss: 0.14964143051943016 Train-f1: 0.4830950972070194 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9514/20000] Train-Loss: 0.46940104625133755 Test-Loss: 0.14795623787948858 Train-f1: 0.4788842320069069 Test-f1: 0.4427614169867692 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9515/20000] Train-Loss: 0.4717898702908311 Test-Loss: 0.1477358879375709 Train-f1: 0.47604057635207386 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9516/20000] Train-Loss: 0.46833742782848525 Test-Loss: 0.14581635360148312 Train-f1: 0.47939712918660293 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9517/20000] Train-Loss: 0.4693785548727587 Test-Loss: 0.14669530477759538 Train-f1: 0.4863865911566875 Test-f1: 0.44990810208201515 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9518/20000] Train-Loss: 0.471620276729103 Test-Loss: 0.14631923953314252 Train-f1: 0.48754978354978357 Test-f1: 0.46016451233842537 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9519/20000] Train-Loss: 0.4708629849758558 Test-Loss: 0.14571437520081126 Train-f1: 0.4802648500379375 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9520/20000] Train-Loss: 0.46939270067118744 Test-Loss: 0.15016860111029062 Train-f1: 0.4785883470527108 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9521/20000] Train-Loss: 0.4729715065413014 Test-Loss: 0.14575724513778068 Train-f1: 0.46838960835099286 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9522/20000] Train-Loss: 0.4651596559351172 Test-Loss: 0.14929438869854628 Train-f1: 0.47841675045839 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9523/20000] Train-Loss: 0.4693821204827107 Test-Loss: 0.15244742425200578 Train-f1: 0.4776034710946946 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9524/20000] Train-Loss: 0.4628137360801266 Test-Loss: 0.14702677710937165 Train-f1: 0.4835324675324676 Test-f1: 0.4485461286270184 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9525/20000] Train-Loss: 0.47418048915256095 Test-Loss: 0.1464520669732775 Train-f1: 0.48418119237084756 Test-f1: 0.4318216788305991 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [9526/20000] Train-Loss: 0.4706363310757669 Test-Loss: 0.14780086457552027 Train-f1: 0.4831329686217024 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9527/20000] Train-Loss: 0.4703135345600458 Test-Loss: 0.14585923576890583 Train-f1: 0.48129104975541354 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9528/20000] Train-Loss: 0.47207295817432865 Test-Loss: 0.1455861739491366 Train-f1: 0.47880171886442396 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9529/20000] Train-Loss: 0.46888441911395173 Test-Loss: 0.14857147917391245 Train-f1: 0.48046187498269466 Test-f1: 0.44683117900509206 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9530/20000] Train-Loss: 0.4721381485694224 Test-Loss: 0.14789683041628954 Train-f1: 0.48405038518691584 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9531/20000] Train-Loss: 0.47039963924771505 Test-Loss: 0.14699265091564906 Train-f1: 0.4948348520931331 Test-f1: 0.45233400741489715 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [9532/20000] Train-Loss: 0.4695550675609055 Test-Loss: 0.15518975968959342 Train-f1: 0.47377401641441474 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9533/20000] Train-Loss: 0.46983296714206124 Test-Loss: 0.1474121571193155 Train-f1: 0.471517356230732 Test-f1: 0.45675057208237985 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9534/20000] Train-Loss: 0.46972042166917904 Test-Loss: 0.14833915784021084 Train-f1: 0.4721433288257405 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9535/20000] Train-Loss: 0.473701648853381 Test-Loss: 0.14736097782723007 Train-f1: 0.4708684887835429 Test-f1: 0.44558629776021075 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9536/20000] Train-Loss: 0.4701470220097694 Test-Loss: 0.14637588053282563 Train-f1: 0.47009328420520635 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9537/20000] Train-Loss: 0.47011013686294867 Test-Loss: 0.1480246205546528 Train-f1: 0.47161812121999114 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9538/20000] Train-Loss: 0.4690658565125685 Test-Loss: 0.14761422203961796 Train-f1: 0.4712727272727273 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9539/20000] Train-Loss: 0.46924493549664825 Test-Loss: 0.1463784958022582 Train-f1: 0.46397884676030376 Test-f1: 0.4497330282227307 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9540/20000] Train-Loss: 0.4706224917402291 Test-Loss: 0.14770758514891186 Train-f1: 0.46830438621944037 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9541/20000] Train-Loss: 0.46771582386347255 Test-Loss: 0.14657804845704148 Train-f1: 0.47880171886442396 Test-f1: 0.45393483709273186 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9542/20000] Train-Loss: 0.47133714586069075 Test-Loss: 0.14715534642256034 Train-f1: 0.47355305254193214 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9543/20000] Train-Loss: 0.46953501531098923 Test-Loss: 0.14744787889566588 Train-f1: 0.4788145045867755 Test-f1: 0.45119841858166543 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9544/20000] Train-Loss: 0.47139147532693754 Test-Loss: 0.14553401720505182 Train-f1: 0.4729017746948562 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9545/20000] Train-Loss: 0.46891517563893637 Test-Loss: 0.1497756719280276 Train-f1: 0.4703181818181818 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9546/20000] Train-Loss: 0.46478894052580566 Test-Loss: 0.14780297565554415 Train-f1: 0.4819494519112637 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9547/20000] Train-Loss: 0.46824730211442495 Test-Loss: 0.15277121239575464 Train-f1: 0.4870839160839161 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9548/20000] Train-Loss: 0.47281214453729853 Test-Loss: 0.151241827573006 Train-f1: 0.4687613493721791 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9549/20000] Train-Loss: 0.46393613088167723 Test-Loss: 0.14722121870560279 Train-f1: 0.4649194563302291 Test-f1: 0.4356787406570312 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "Epoch [9550/20000] Train-Loss: 0.46698559337833223 Test-Loss: 0.15462126512811578 Train-f1: 0.47330672118297235 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9551/20000] Train-Loss: 0.47154682802240055 Test-Loss: 0.1477524996646257 Train-f1: 0.4763934185406608 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9552/20000] Train-Loss: 0.4654458586101501 Test-Loss: 0.14782698451272322 Train-f1: 0.47880171886442396 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9553/20000] Train-Loss: 0.46807747073708067 Test-Loss: 0.14622390042607677 Train-f1: 0.487165885579598 Test-f1: 0.4497330282227307 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9554/20000] Train-Loss: 0.4667746647020059 Test-Loss: 0.14751742026233516 Train-f1: 0.475952393713448 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9555/20000] Train-Loss: 0.46716521117712984 Test-Loss: 0.14957912238818186 Train-f1: 0.46864678513228836 Test-f1: 0.4322732776751003 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [9556/20000] Train-Loss: 0.47293415798065225 Test-Loss: 0.14605811764625617 Train-f1: 0.4658242437363459 Test-f1: 0.44990810208201515 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9557/20000] Train-Loss: 0.4686373000494518 Test-Loss: 0.14789643890546483 Train-f1: 0.47825066549948775 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9558/20000] Train-Loss: 0.46499061528076757 Test-Loss: 0.14585603217371004 Train-f1: 0.47242816273056115 Test-f1: 0.42846033429278385 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [9559/20000] Train-Loss: 0.46701280487503777 Test-Loss: 0.15260677930335145 Train-f1: 0.47882996282996276 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9560/20000] Train-Loss: 0.46993195463714454 Test-Loss: 0.1500908456917707 Train-f1: 0.4785194794909556 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9561/20000] Train-Loss: 0.46790634145171167 Test-Loss: 0.146766263087677 Train-f1: 0.4737229416843262 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9562/20000] Train-Loss: 0.46921551156536945 Test-Loss: 0.14829659350120045 Train-f1: 0.4755578334708769 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9563/20000] Train-Loss: 0.46918844662991366 Test-Loss: 0.14604134787879314 Train-f1: 0.4791387011969598 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9564/20000] Train-Loss: 0.46949411883102354 Test-Loss: 0.15161537888408577 Train-f1: 0.48137817883511075 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9565/20000] Train-Loss: 0.4717315420827804 Test-Loss: 0.14652672148108575 Train-f1: 0.47355305254193214 Test-f1: 0.4421230942970073 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [9566/20000] Train-Loss: 0.47112196111640553 Test-Loss: 0.1458130027498596 Train-f1: 0.4729366977095394 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9567/20000] Train-Loss: 0.46987648629805145 Test-Loss: 0.14713085634873493 Train-f1: 0.48369109441848546 Test-f1: 0.4385580319084156 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [9568/20000] Train-Loss: 0.4653736080134852 Test-Loss: 0.14885842807662 Train-f1: 0.4778108623078762 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9569/20000] Train-Loss: 0.4729574716753486 Test-Loss: 0.14591378213122239 Train-f1: 0.47681971587787453 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9570/20000] Train-Loss: 0.47014506120990907 Test-Loss: 0.14871182230567323 Train-f1: 0.47377401641441474 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9571/20000] Train-Loss: 0.4707719248197144 Test-Loss: 0.14986059467929727 Train-f1: 0.47486996559304523 Test-f1: 0.42567287784679086 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [9572/20000] Train-Loss: 0.4667698538441461 Test-Loss: 0.1465590067512663 Train-f1: 0.46811705514736335 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9573/20000] Train-Loss: 0.4703313422065983 Test-Loss: 0.14808898158527192 Train-f1: 0.48377221349595 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9574/20000] Train-Loss: 0.46875234563295615 Test-Loss: 0.14524401914632804 Train-f1: 0.48405038518691584 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9575/20000] Train-Loss: 0.4688709567617821 Test-Loss: 0.14901364404984055 Train-f1: 0.48099039152688505 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9576/20000] Train-Loss: 0.473850817278656 Test-Loss: 0.15096701263374274 Train-f1: 0.46838960835099286 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9577/20000] Train-Loss: 0.47047322824884036 Test-Loss: 0.14505683350848061 Train-f1: 0.4712034632034632 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9578/20000] Train-Loss: 0.47001963307345 Test-Loss: 0.15549918173138244 Train-f1: 0.47848403928306826 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9579/20000] Train-Loss: 0.46443494447919287 Test-Loss: 0.14549667112015943 Train-f1: 0.49701098901098906 Test-f1: 0.460952380952381 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [9580/20000] Train-Loss: 0.46991826315403523 Test-Loss: 0.1458472058996301 Train-f1: 0.4772300040385146 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9581/20000] Train-Loss: 0.46814465431400143 Test-Loss: 0.14715317419734728 Train-f1: 0.4731424064982946 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9582/20000] Train-Loss: 0.47120582201643096 Test-Loss: 0.14820921178976929 Train-f1: 0.4763934185406608 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9583/20000] Train-Loss: 0.46851328059131087 Test-Loss: 0.14882444026512834 Train-f1: 0.4787488980537207 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9584/20000] Train-Loss: 0.46903623270687195 Test-Loss: 0.14675510004194384 Train-f1: 0.47652432211199247 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9585/20000] Train-Loss: 0.4694186448170848 Test-Loss: 0.1495756142717631 Train-f1: 0.4727606306736741 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9586/20000] Train-Loss: 0.47177712998899124 Test-Loss: 0.1491987039538385 Train-f1: 0.4835061147973253 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9587/20000] Train-Loss: 0.4707154705408131 Test-Loss: 0.14565357852654895 Train-f1: 0.4711470093207416 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9588/20000] Train-Loss: 0.46942704215230774 Test-Loss: 0.14905272640297848 Train-f1: 0.480648496572013 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9589/20000] Train-Loss: 0.4672040057241667 Test-Loss: 0.1476803648769264 Train-f1: 0.4781953183991946 Test-f1: 0.45119841858166543 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9590/20000] Train-Loss: 0.4700659563444739 Test-Loss: 0.14934124827979678 Train-f1: 0.4801188354129531 Test-f1: 0.44683117900509206 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9591/20000] Train-Loss: 0.46662605156161685 Test-Loss: 0.14580696650694047 Train-f1: 0.4709990549990549 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9592/20000] Train-Loss: 0.46964018230665194 Test-Loss: 0.1475409977122775 Train-f1: 0.47098894997782964 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9593/20000] Train-Loss: 0.4691497104021959 Test-Loss: 0.14665342211646384 Train-f1: 0.47034941698014476 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9594/20000] Train-Loss: 0.4677910000861335 Test-Loss: 0.1473828476208034 Train-f1: 0.48099039152688505 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9595/20000] Train-Loss: 0.46598551403886895 Test-Loss: 0.15586944556053664 Train-f1: 0.48288316999973435 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9596/20000] Train-Loss: 0.4712887440517061 Test-Loss: 0.14512889268375637 Train-f1: 0.46666135810661363 Test-f1: 0.44990810208201515 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9597/20000] Train-Loss: 0.466134723107652 Test-Loss: 0.14927581454501587 Train-f1: 0.486495136907483 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9598/20000] Train-Loss: 0.46704969998074575 Test-Loss: 0.1526724270096088 Train-f1: 0.48129104975541354 Test-f1: 0.42567287784679086 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [9599/20000] Train-Loss: 0.46545464533414055 Test-Loss: 0.14776208459331253 Train-f1: 0.487064935064935 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9600/20000] Train-Loss: 0.469387481267886 Test-Loss: 0.14700768140053458 Train-f1: 0.46632878298736424 Test-f1: 0.45233400741489715 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9601/20000] Train-Loss: 0.47133540575898897 Test-Loss: 0.1454369524109958 Train-f1: 0.47105627501765956 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9602/20000] Train-Loss: 0.4679398373590784 Test-Loss: 0.14920496621913917 Train-f1: 0.47804230552056637 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9603/20000] Train-Loss: 0.4709970641682639 Test-Loss: 0.1451090572840973 Train-f1: 0.46601520618492 Test-f1: 0.4497330282227307 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9604/20000] Train-Loss: 0.4646630789626271 Test-Loss: 0.1474641853372384 Train-f1: 0.48423723035798255 Test-f1: 0.4594011008433066 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9605/20000] Train-Loss: 0.4700475203426848 Test-Loss: 0.147643917916326 Train-f1: 0.4772364182002736 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9606/20000] Train-Loss: 0.4686674983746803 Test-Loss: 0.14905114041443407 Train-f1: 0.47165423238429227 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9607/20000] Train-Loss: 0.4694168221415792 Test-Loss: 0.14942969085718968 Train-f1: 0.4785883470527108 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9608/20000] Train-Loss: 0.4633987569789685 Test-Loss: 0.1497919095390851 Train-f1: 0.4847397213569514 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9609/20000] Train-Loss: 0.47348512610561944 Test-Loss: 0.14619128472020135 Train-f1: 0.47604057635207386 Test-f1: 0.4318216788305991 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9610/20000] Train-Loss: 0.4683178229922922 Test-Loss: 0.1475707157879009 Train-f1: 0.4757349422290476 Test-f1: 0.4385580319084156 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9611/20000] Train-Loss: 0.46255955037844276 Test-Loss: 0.15261014486833194 Train-f1: 0.4867994707994708 Test-f1: 0.45675057208237985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9612/20000] Train-Loss: 0.46677954596598 Test-Loss: 0.14915758338473448 Train-f1: 0.4834231642017531 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9613/20000] Train-Loss: 0.46924365290320674 Test-Loss: 0.14636169057761153 Train-f1: 0.47771674602259306 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9614/20000] Train-Loss: 0.46643893374250356 Test-Loss: 0.14959863947106683 Train-f1: 0.48386022190370015 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9615/20000] Train-Loss: 0.4691946202439804 Test-Loss: 0.146133056994551 Train-f1: 0.47939712918660293 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9616/20000] Train-Loss: 0.4689680857748592 Test-Loss: 0.1448755720803644 Train-f1: 0.47896856296856305 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9617/20000] Train-Loss: 0.4675099646248753 Test-Loss: 0.1473892933718238 Train-f1: 0.47213321957847504 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9618/20000] Train-Loss: 0.46645504770673124 Test-Loss: 0.1452643065615249 Train-f1: 0.47715373685706164 Test-f1: 0.460952380952381 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9619/20000] Train-Loss: 0.468235932202026 Test-Loss: 0.14907530057473486 Train-f1: 0.48269599014124565 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9620/20000] Train-Loss: 0.4676136881956247 Test-Loss: 0.14525836736050804 Train-f1: 0.48433233433233436 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9621/20000] Train-Loss: 0.46756256076354463 Test-Loss: 0.14521276977985437 Train-f1: 0.47585326953748003 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9622/20000] Train-Loss: 0.4718632004643624 Test-Loss: 0.14631885171656553 Train-f1: 0.48105112506057585 Test-f1: 0.44683117900509206 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9623/20000] Train-Loss: 0.4701131588303146 Test-Loss: 0.14703574018359944 Train-f1: 0.47782875860734747 Test-f1: 0.4356787406570312 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9624/20000] Train-Loss: 0.4672126667176769 Test-Loss: 0.14558079941614557 Train-f1: 0.4785194794909556 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9625/20000] Train-Loss: 0.4677426533552794 Test-Loss: 0.14554134185225254 Train-f1: 0.4783300366252018 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9626/20000] Train-Loss: 0.4641258543179256 Test-Loss: 0.15798850275713258 Train-f1: 0.4867994707994708 Test-f1: 0.4397742127153892 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [9627/20000] Train-Loss: 0.46583883735967435 Test-Loss: 0.14594628153518743 Train-f1: 0.4707901029487342 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9628/20000] Train-Loss: 0.4687407528294403 Test-Loss: 0.14696787186755508 Train-f1: 0.47604057635207386 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9629/20000] Train-Loss: 0.4682025090105476 Test-Loss: 0.14511696538658134 Train-f1: 0.48430443425377395 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9630/20000] Train-Loss: 0.4695234071739187 Test-Loss: 0.14856593110419186 Train-f1: 0.4815326655326656 Test-f1: 0.44683117900509206 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9631/20000] Train-Loss: 0.4658327724597339 Test-Loss: 0.1447861286069257 Train-f1: 0.4790392253340944 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9632/20000] Train-Loss: 0.4653827318793215 Test-Loss: 0.1500872917995749 Train-f1: 0.4755959595959595 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9633/20000] Train-Loss: 0.4703124577091785 Test-Loss: 0.147874808473011 Train-f1: 0.47604057635207386 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9634/20000] Train-Loss: 0.46494303633085243 Test-Loss: 0.15317311328369496 Train-f1: 0.4800111566286063 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9635/20000] Train-Loss: 0.46300542995185084 Test-Loss: 0.14862806373197335 Train-f1: 0.4875214349666905 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9636/20000] Train-Loss: 0.46846795802732116 Test-Loss: 0.14856290177736398 Train-f1: 0.4765367965367965 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9637/20000] Train-Loss: 0.46860409519189394 Test-Loss: 0.14462080819626208 Train-f1: 0.47342124542124536 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9638/20000] Train-Loss: 0.4679492239126795 Test-Loss: 0.14468797184615148 Train-f1: 0.4877868992321547 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9639/20000] Train-Loss: 0.46030175579102217 Test-Loss: 0.1503035806454139 Train-f1: 0.48417016116649025 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9640/20000] Train-Loss: 0.46483395694619933 Test-Loss: 0.15643930112364643 Train-f1: 0.4790770004352994 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9641/20000] Train-Loss: 0.4698053217588769 Test-Loss: 0.14540691729791044 Train-f1: 0.4793520493138611 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9642/20000] Train-Loss: 0.4654369015147258 Test-Loss: 0.1474180750687735 Train-f1: 0.47611715510603475 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9643/20000] Train-Loss: 0.4686194843489974 Test-Loss: 0.14588599542148942 Train-f1: 0.4781953183991946 Test-f1: 0.4385580319084156 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9644/20000] Train-Loss: 0.47033986436160247 Test-Loss: 0.14471644940006959 Train-f1: 0.4778040559226174 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9645/20000] Train-Loss: 0.4729134100326973 Test-Loss: 0.1449343017268109 Train-f1: 0.47538624674454566 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9646/20000] Train-Loss: 0.4684989581395862 Test-Loss: 0.14881923828918986 Train-f1: 0.48129104975541354 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9647/20000] Train-Loss: 0.46530845907780277 Test-Loss: 0.1458572782207511 Train-f1: 0.47604057635207386 Test-f1: 0.4421230942970073 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9648/20000] Train-Loss: 0.47099954265660204 Test-Loss: 0.14655162541407915 Train-f1: 0.4627083203629322 Test-f1: 0.4423271037905184 Train-acc: 0.7847682119205298 Test-acc: 0.75\n",
      "Epoch [9649/20000] Train-Loss: 0.4675899618405954 Test-Loss: 0.14660421094538656 Train-f1: 0.4626611727877551 Test-f1: 0.45393483709273186 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9650/20000] Train-Loss: 0.46705472815348203 Test-Loss: 0.15180213568269585 Train-f1: 0.4930621096195843 Test-f1: 0.45675057208237985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [9651/20000] Train-Loss: 0.46936903945276354 Test-Loss: 0.14943593861459578 Train-f1: 0.47519332519332513 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9652/20000] Train-Loss: 0.4670421566337169 Test-Loss: 0.1494991009920549 Train-f1: 0.48122376531046546 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9653/20000] Train-Loss: 0.4653518468479526 Test-Loss: 0.1501642961108341 Train-f1: 0.46852254779323504 Test-f1: 0.4554347826086957 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9654/20000] Train-Loss: 0.4679953718108696 Test-Loss: 0.1456872963718391 Train-f1: 0.47047949293121005 Test-f1: 0.4385580319084156 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [9655/20000] Train-Loss: 0.46426429770096556 Test-Loss: 0.1551579160971649 Train-f1: 0.4658242437363459 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9656/20000] Train-Loss: 0.4657282930045373 Test-Loss: 0.1621187587883068 Train-f1: 0.47141505513259235 Test-f1: 0.4495524296675192 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9657/20000] Train-Loss: 0.46623478419833136 Test-Loss: 0.145221212530644 Train-f1: 0.4633031975034199 Test-f1: 0.44990810208201515 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [9658/20000] Train-Loss: 0.46601061817989187 Test-Loss: 0.14684608224074278 Train-f1: 0.47538624674454566 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9659/20000] Train-Loss: 0.46563880922261563 Test-Loss: 0.14599719473457642 Train-f1: 0.4684403315982263 Test-f1: 0.45393483709273186 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9660/20000] Train-Loss: 0.46717366827819723 Test-Loss: 0.1447713361129667 Train-f1: 0.48409676809676816 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9661/20000] Train-Loss: 0.4668330231252581 Test-Loss: 0.14594269419463135 Train-f1: 0.47253997316936414 Test-f1: 0.43193277310924366 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [9662/20000] Train-Loss: 0.46312909754698023 Test-Loss: 0.14605654400181087 Train-f1: 0.4890249475009444 Test-f1: 0.46016451233842537 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9663/20000] Train-Loss: 0.4685315770749974 Test-Loss: 0.1488686129923547 Train-f1: 0.4853785103785103 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9664/20000] Train-Loss: 0.46291943218653964 Test-Loss: 0.14720685225044533 Train-f1: 0.490234945266997 Test-f1: 0.45984654731457797 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9665/20000] Train-Loss: 0.4670635873472379 Test-Loss: 0.15365740613645198 Train-f1: 0.4729952162016503 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9666/20000] Train-Loss: 0.4660989952443952 Test-Loss: 0.15610333791256512 Train-f1: 0.4709981227912044 Test-f1: 0.4554347826086957 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9667/20000] Train-Loss: 0.46543355239916395 Test-Loss: 0.14917519192656972 Train-f1: 0.4763934185406608 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9668/20000] Train-Loss: 0.4672698191212415 Test-Loss: 0.1465060818594599 Train-f1: 0.4800100849543666 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9669/20000] Train-Loss: 0.4645115591536077 Test-Loss: 0.14570412901803526 Train-f1: 0.46601520618492 Test-f1: 0.45393483709273186 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9670/20000] Train-Loss: 0.46747962759736167 Test-Loss: 0.1467944313688315 Train-f1: 0.47698782443307997 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9671/20000] Train-Loss: 0.4645977328886746 Test-Loss: 0.15261748761079288 Train-f1: 0.4892143903666984 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9672/20000] Train-Loss: 0.4684466244090907 Test-Loss: 0.15022697032922366 Train-f1: 0.4670583351817231 Test-f1: 0.44558629776021075 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9673/20000] Train-Loss: 0.46945562833124693 Test-Loss: 0.14709506732647598 Train-f1: 0.4785883470527108 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9674/20000] Train-Loss: 0.46734031895476497 Test-Loss: 0.14479926818113836 Train-f1: 0.4767030426913445 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9675/20000] Train-Loss: 0.4689737122820094 Test-Loss: 0.1473787085124573 Train-f1: 0.46685183429708976 Test-f1: 0.44937417654808953 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9676/20000] Train-Loss: 0.4656500236598144 Test-Loss: 0.14521198420214354 Train-f1: 0.48206702770583154 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9677/20000] Train-Loss: 0.46912082399429905 Test-Loss: 0.1556868140836053 Train-f1: 0.4783737373737374 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9678/20000] Train-Loss: 0.46156699956136654 Test-Loss: 0.15445362330675932 Train-f1: 0.47459209403734953 Test-f1: 0.44027149321266973 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [9679/20000] Train-Loss: 0.46425372425589134 Test-Loss: 0.14533215721748508 Train-f1: 0.4786812576701373 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9680/20000] Train-Loss: 0.4653454993493702 Test-Loss: 0.14488602201891054 Train-f1: 0.47321266768605075 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9681/20000] Train-Loss: 0.4658794357651971 Test-Loss: 0.14474032525503502 Train-f1: 0.4762376163003214 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9682/20000] Train-Loss: 0.46841293351257995 Test-Loss: 0.14741387761439656 Train-f1: 0.4875444749897305 Test-f1: 0.44854612862701837 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9683/20000] Train-Loss: 0.4705054744436857 Test-Loss: 0.14481569227564697 Train-f1: 0.4842347195288371 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9684/20000] Train-Loss: 0.4652919833025845 Test-Loss: 0.14583383544478334 Train-f1: 0.47830731712133334 Test-f1: 0.4421230942970073 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9685/20000] Train-Loss: 0.4643755829848732 Test-Loss: 0.14481039483624422 Train-f1: 0.4791215013345587 Test-f1: 0.45984654731457797 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9686/20000] Train-Loss: 0.46284788599474075 Test-Loss: 0.1492765823188089 Train-f1: 0.47346387996953815 Test-f1: 0.45984654731457797 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9687/20000] Train-Loss: 0.4698598199413733 Test-Loss: 0.14432913642145026 Train-f1: 0.4676395248673891 Test-f1: 0.46016451233842537 Train-acc: 0.7913907284768212 Test-acc: 0.7763157894736842\n",
      "Epoch [9688/20000] Train-Loss: 0.4636552893307054 Test-Loss: 0.15006354120880636 Train-f1: 0.4757349422290476 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9689/20000] Train-Loss: 0.4676219974438003 Test-Loss: 0.1490585014094334 Train-f1: 0.497378705799571 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9690/20000] Train-Loss: 0.4691112827585803 Test-Loss: 0.14739165916102073 Train-f1: 0.48155089867357354 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9691/20000] Train-Loss: 0.464857645165165 Test-Loss: 0.1449462055543992 Train-f1: 0.4763934185406608 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9692/20000] Train-Loss: 0.46270277349154987 Test-Loss: 0.15344581081362366 Train-f1: 0.48317118497940414 Test-f1: 0.4504334365325078 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9693/20000] Train-Loss: 0.46784687487645 Test-Loss: 0.1487725710206669 Train-f1: 0.48369109441848546 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9694/20000] Train-Loss: 0.46353839898401783 Test-Loss: 0.14472453209304922 Train-f1: 0.48423723035798255 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9695/20000] Train-Loss: 0.466714811250923 Test-Loss: 0.14886452247459017 Train-f1: 0.4770070509200943 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9696/20000] Train-Loss: 0.46650690050075133 Test-Loss: 0.14565151509917504 Train-f1: 0.4785883470527108 Test-f1: 0.43193277310924366 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [9697/20000] Train-Loss: 0.4644809210679641 Test-Loss: 0.1479615724129237 Train-f1: 0.4712727272727273 Test-f1: 0.45675057208237985 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9698/20000] Train-Loss: 0.46320576973923416 Test-Loss: 0.1451465700980575 Train-f1: 0.480648496572013 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9699/20000] Train-Loss: 0.4681347159506587 Test-Loss: 0.14457603319771173 Train-f1: 0.4783300366252018 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9700/20000] Train-Loss: 0.46770809095894705 Test-Loss: 0.14442866398845383 Train-f1: 0.4790392253340944 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9701/20000] Train-Loss: 0.46427889819804674 Test-Loss: 0.14832623200328166 Train-f1: 0.4786553558663706 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9702/20000] Train-Loss: 0.4653595946583013 Test-Loss: 0.14507378005568067 Train-f1: 0.47370175770175765 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9703/20000] Train-Loss: 0.466213628388528 Test-Loss: 0.1512749900629678 Train-f1: 0.47841675045839 Test-f1: 0.42567287784679086 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [9704/20000] Train-Loss: 0.4634122772519105 Test-Loss: 0.14815550672047054 Train-f1: 0.4811753711480452 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9705/20000] Train-Loss: 0.4635580428501967 Test-Loss: 0.14758432781543848 Train-f1: 0.4704271666296983 Test-f1: 0.4526190476190476 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9706/20000] Train-Loss: 0.46326142272169013 Test-Loss: 0.14531119065561565 Train-f1: 0.4663241749637555 Test-f1: 0.45984654731457797 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [9707/20000] Train-Loss: 0.46625672658815903 Test-Loss: 0.14552338615854263 Train-f1: 0.4889711804681601 Test-f1: 0.42857142857142855 Train-acc: 0.8178807947019867 Test-acc: 0.7368421052631579\n",
      "Epoch [9708/20000] Train-Loss: 0.46426799796897905 Test-Loss: 0.14453057256864574 Train-f1: 0.4836318475982135 Test-f1: 0.45984654731457797 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9709/20000] Train-Loss: 0.4660345897594903 Test-Loss: 0.1460144385184325 Train-f1: 0.48612118316466135 Test-f1: 0.44854612862701837 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9710/20000] Train-Loss: 0.4672038144446433 Test-Loss: 0.14542466805960244 Train-f1: 0.4818008658008658 Test-f1: 0.44683117900509206 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9711/20000] Train-Loss: 0.4633456655679717 Test-Loss: 0.15012586314755502 Train-f1: 0.4806953183991946 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9712/20000] Train-Loss: 0.46943886581085764 Test-Loss: 0.1465328586030311 Train-f1: 0.47732659287021695 Test-f1: 0.47043478260869565 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [9713/20000] Train-Loss: 0.4663050200415145 Test-Loss: 0.14457371131530872 Train-f1: 0.4909615521909375 Test-f1: 0.4497330282227307 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [9714/20000] Train-Loss: 0.4684489587271831 Test-Loss: 0.14585618291961586 Train-f1: 0.47047949293121005 Test-f1: 0.4385580319084156 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [9715/20000] Train-Loss: 0.4652702698595094 Test-Loss: 0.1498845429102066 Train-f1: 0.4819678212083275 Test-f1: 0.43639684222929187 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [9716/20000] Train-Loss: 0.46623320549150604 Test-Loss: 0.14917719939609433 Train-f1: 0.47406512693136893 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9717/20000] Train-Loss: 0.4605695019502724 Test-Loss: 0.15430885447482387 Train-f1: 0.4881562881562882 Test-f1: 0.4624973437035148 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9718/20000] Train-Loss: 0.4730406247227228 Test-Loss: 0.14828439117567874 Train-f1: 0.47187175711086426 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9719/20000] Train-Loss: 0.46475071019703845 Test-Loss: 0.14775777392063513 Train-f1: 0.47763197947247027 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9720/20000] Train-Loss: 0.4612584711515539 Test-Loss: 0.15707318579281143 Train-f1: 0.4887615640951628 Test-f1: 0.4545727136431784 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9721/20000] Train-Loss: 0.4690546290888443 Test-Loss: 0.14816223022816394 Train-f1: 0.4782472765329908 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9722/20000] Train-Loss: 0.46255898957199193 Test-Loss: 0.14964907905149394 Train-f1: 0.4830950972070194 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9723/20000] Train-Loss: 0.4675826350280511 Test-Loss: 0.14693102089447627 Train-f1: 0.4712611642172556 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9724/20000] Train-Loss: 0.4644939900425795 Test-Loss: 0.14884752143714172 Train-f1: 0.4785194794909556 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9725/20000] Train-Loss: 0.4686221273449412 Test-Loss: 0.1456786490340575 Train-f1: 0.4785883470527108 Test-f1: 0.4421230942970073 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9726/20000] Train-Loss: 0.4641067178032184 Test-Loss: 0.14586677608931606 Train-f1: 0.48099039152688505 Test-f1: 0.4421230942970073 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [9727/20000] Train-Loss: 0.4626509285822282 Test-Loss: 0.14995466369461782 Train-f1: 0.48173493173493176 Test-f1: 0.45675057208237985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9728/20000] Train-Loss: 0.4627580703682148 Test-Loss: 0.1472529758780948 Train-f1: 0.47551093947209055 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9729/20000] Train-Loss: 0.462720489108354 Test-Loss: 0.1449784936728443 Train-f1: 0.47604057635207386 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9730/20000] Train-Loss: 0.46878897007837905 Test-Loss: 0.14679031628910866 Train-f1: 0.4707901029487342 Test-f1: 0.44683117900509206 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9731/20000] Train-Loss: 0.46185140603747193 Test-Loss: 0.14611118650454666 Train-f1: 0.47825066549948775 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9732/20000] Train-Loss: 0.4651591203471393 Test-Loss: 0.14636734833497708 Train-f1: 0.48260855922600887 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9733/20000] Train-Loss: 0.4644769382968257 Test-Loss: 0.14673588162070753 Train-f1: 0.47440076701821665 Test-f1: 0.43193277310924366 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [9734/20000] Train-Loss: 0.46243389751022457 Test-Loss: 0.1522643557747817 Train-f1: 0.478272777946691 Test-f1: 0.4356787406570312 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9735/20000] Train-Loss: 0.4659847770518282 Test-Loss: 0.1465730544475974 Train-f1: 0.4781953183991946 Test-f1: 0.4356787406570312 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9736/20000] Train-Loss: 0.46510800076825554 Test-Loss: 0.14797607019944384 Train-f1: 0.48595393908884976 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9737/20000] Train-Loss: 0.4629535630077543 Test-Loss: 0.14749505096912913 Train-f1: 0.48089308716187207 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9738/20000] Train-Loss: 0.4605640901207659 Test-Loss: 0.1466948863981571 Train-f1: 0.4862536350059662 Test-f1: 0.45984654731457797 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9739/20000] Train-Loss: 0.46804281332468417 Test-Loss: 0.1446353946727089 Train-f1: 0.47538624674454566 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9740/20000] Train-Loss: 0.4583133539428962 Test-Loss: 0.1729599439011491 Train-f1: 0.4833239803674586 Test-f1: 0.4464795587280987 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [9741/20000] Train-Loss: 0.46065423002711847 Test-Loss: 0.15665071220863439 Train-f1: 0.4794708622209403 Test-f1: 0.4628528560711606 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9742/20000] Train-Loss: 0.4670483894343214 Test-Loss: 0.1469392710964278 Train-f1: 0.4711933712531245 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9743/20000] Train-Loss: 0.46857898170603673 Test-Loss: 0.14917547533865447 Train-f1: 0.4729366977095394 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9744/20000] Train-Loss: 0.46477345185736285 Test-Loss: 0.14452241952347042 Train-f1: 0.47466923089048974 Test-f1: 0.460952380952381 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9745/20000] Train-Loss: 0.4660974641880429 Test-Loss: 0.1450915558521975 Train-f1: 0.4679412960890332 Test-f1: 0.44990810208201515 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9746/20000] Train-Loss: 0.46012234311505373 Test-Loss: 0.15000584868061537 Train-f1: 0.4806953183991946 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9747/20000] Train-Loss: 0.46248651230148546 Test-Loss: 0.1534675121354425 Train-f1: 0.4814648771681972 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9748/20000] Train-Loss: 0.4709859539878273 Test-Loss: 0.14451939352296905 Train-f1: 0.4742758197076523 Test-f1: 0.44990810208201515 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9749/20000] Train-Loss: 0.46068614314483103 Test-Loss: 0.15645332006277166 Train-f1: 0.47519602914908077 Test-f1: 0.4545727136431784 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9750/20000] Train-Loss: 0.4698519834735146 Test-Loss: 0.1459665094631726 Train-f1: 0.4871194373510706 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9751/20000] Train-Loss: 0.46681990511948035 Test-Loss: 0.14745699300590231 Train-f1: 0.4852931229843982 Test-f1: 0.44683117900509206 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9752/20000] Train-Loss: 0.46103871976375876 Test-Loss: 0.1479942003613873 Train-f1: 0.4719915490441083 Test-f1: 0.4427614169867692 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [9753/20000] Train-Loss: 0.46439287374522087 Test-Loss: 0.14462741154984918 Train-f1: 0.4845050582661032 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9754/20000] Train-Loss: 0.4650252171936197 Test-Loss: 0.145452058553073 Train-f1: 0.480749515352214 Test-f1: 0.4421230942970073 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [9755/20000] Train-Loss: 0.4668713527594501 Test-Loss: 0.145435972422766 Train-f1: 0.47611715510603475 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9756/20000] Train-Loss: 0.46376851939439395 Test-Loss: 0.1472338597217397 Train-f1: 0.4804820663600006 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9757/20000] Train-Loss: 0.46591008938028666 Test-Loss: 0.14522083523772084 Train-f1: 0.46601520618492 Test-f1: 0.43932367149758456 Train-acc: 0.7880794701986755 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [9758/20000] Train-Loss: 0.4644158569607715 Test-Loss: 0.14419306426234654 Train-f1: 0.4865655278903686 Test-f1: 0.4497330282227307 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9759/20000] Train-Loss: 0.46360364116201036 Test-Loss: 0.15677311332283733 Train-f1: 0.47763197947247027 Test-f1: 0.4495524296675192 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9760/20000] Train-Loss: 0.4645026159620743 Test-Loss: 0.14574708125127128 Train-f1: 0.4895352436030157 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9761/20000] Train-Loss: 0.46570050197181706 Test-Loss: 0.15071842100707375 Train-f1: 0.47377401641441474 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9762/20000] Train-Loss: 0.46186141757355437 Test-Loss: 0.14421297197282534 Train-f1: 0.4893788556628208 Test-f1: 0.460952380952381 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9763/20000] Train-Loss: 0.46529135842792807 Test-Loss: 0.14425692029227505 Train-f1: 0.47880171886442396 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9764/20000] Train-Loss: 0.4634587082140378 Test-Loss: 0.14389794317748197 Train-f1: 0.4863038590655703 Test-f1: 0.46016451233842537 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9765/20000] Train-Loss: 0.46512064941441444 Test-Loss: 0.14599315402974436 Train-f1: 0.47882996282996276 Test-f1: 0.4385580319084156 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9766/20000] Train-Loss: 0.4633983051305546 Test-Loss: 0.1462027118832341 Train-f1: 0.48908929385939015 Test-f1: 0.4503312334891282 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9767/20000] Train-Loss: 0.4649460254101945 Test-Loss: 0.14741285052068798 Train-f1: 0.4757983910555815 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9768/20000] Train-Loss: 0.46543196385507046 Test-Loss: 0.14450333918923958 Train-f1: 0.4918631540735102 Test-f1: 0.4497330282227307 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [9769/20000] Train-Loss: 0.465724456776462 Test-Loss: 0.14484519054772219 Train-f1: 0.4817530802819271 Test-f1: 0.43932367149758456 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [9770/20000] Train-Loss: 0.4632489058766246 Test-Loss: 0.14925582918818994 Train-f1: 0.47801293055753147 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9771/20000] Train-Loss: 0.45930104408987843 Test-Loss: 0.15300270109736733 Train-f1: 0.4856845856845856 Test-f1: 0.4652641421224871 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [9772/20000] Train-Loss: 0.46506404657898637 Test-Loss: 0.14435774588227707 Train-f1: 0.4788842320069069 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9773/20000] Train-Loss: 0.4635092067290695 Test-Loss: 0.14382866952284779 Train-f1: 0.4866144877510184 Test-f1: 0.46016451233842537 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9774/20000] Train-Loss: 0.4609340289979231 Test-Loss: 0.14458432047570477 Train-f1: 0.4763934185406608 Test-f1: 0.4602828448452552 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9775/20000] Train-Loss: 0.46197294638576797 Test-Loss: 0.14746377681858122 Train-f1: 0.48129104975541354 Test-f1: 0.46146799496163976 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9776/20000] Train-Loss: 0.46382006629007 Test-Loss: 0.14476167419747524 Train-f1: 0.4757349422290476 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9777/20000] Train-Loss: 0.4630902319020927 Test-Loss: 0.14456983820811892 Train-f1: 0.4840177571897318 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9778/20000] Train-Loss: 0.46327291679514176 Test-Loss: 0.14975315015871293 Train-f1: 0.4785883470527108 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9779/20000] Train-Loss: 0.46021173900772383 Test-Loss: 0.14721395587235342 Train-f1: 0.4800779322518453 Test-f1: 0.4973934516188037 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9780/20000] Train-Loss: 0.466929613517205 Test-Loss: 0.14553382854305483 Train-f1: 0.47237912774063007 Test-f1: 0.4318216788305991 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [9781/20000] Train-Loss: 0.46500674900417927 Test-Loss: 0.14653369799348023 Train-f1: 0.4764068057546318 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9782/20000] Train-Loss: 0.4641037394290448 Test-Loss: 0.14625256883938742 Train-f1: 0.4767655502392344 Test-f1: 0.45984654731457797 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9783/20000] Train-Loss: 0.46544154924258163 Test-Loss: 0.14732923349330718 Train-f1: 0.47503155581014467 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9784/20000] Train-Loss: 0.4600881916717787 Test-Loss: 0.1548656533575007 Train-f1: 0.48202870813397125 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9785/20000] Train-Loss: 0.467064025949358 Test-Loss: 0.15169827221981233 Train-f1: 0.480648496572013 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9786/20000] Train-Loss: 0.46489262195488823 Test-Loss: 0.1464878072388605 Train-f1: 0.4763934185406608 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9787/20000] Train-Loss: 0.46585631786660675 Test-Loss: 0.14931528747950865 Train-f1: 0.47848403928306826 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9788/20000] Train-Loss: 0.46466739598974294 Test-Loss: 0.14967846596851136 Train-f1: 0.4763800939359545 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9789/20000] Train-Loss: 0.4648142455966024 Test-Loss: 0.1452650093475505 Train-f1: 0.4765071222495914 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9790/20000] Train-Loss: 0.45893326282245744 Test-Loss: 0.15184952471117752 Train-f1: 0.4900085804929625 Test-f1: 0.4624973437035148 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [9791/20000] Train-Loss: 0.46576828952829924 Test-Loss: 0.1471289450053105 Train-f1: 0.4868850794150642 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9792/20000] Train-Loss: 0.46884655965351074 Test-Loss: 0.14552441168563066 Train-f1: 0.47880171886442396 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9793/20000] Train-Loss: 0.4635855259550018 Test-Loss: 0.14451743885580728 Train-f1: 0.4781953183991946 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9794/20000] Train-Loss: 0.4625097816537744 Test-Loss: 0.15097471798927617 Train-f1: 0.4865415231587532 Test-f1: 0.44683117900509206 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9795/20000] Train-Loss: 0.4591360764617632 Test-Loss: 0.1439714401268526 Train-f1: 0.4791341991341991 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9796/20000] Train-Loss: 0.46383768025560446 Test-Loss: 0.1492336027146628 Train-f1: 0.4723129553373456 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9797/20000] Train-Loss: 0.4638969798715875 Test-Loss: 0.14420522044915654 Train-f1: 0.4748559354590857 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9798/20000] Train-Loss: 0.4642270329994221 Test-Loss: 0.1457750781968949 Train-f1: 0.4785194794909556 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9799/20000] Train-Loss: 0.4642387088780059 Test-Loss: 0.1438709020267068 Train-f1: 0.4763035868456165 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9800/20000] Train-Loss: 0.4630965850526448 Test-Loss: 0.1452083791355505 Train-f1: 0.49204552232948745 Test-f1: 0.42846033429278385 Train-acc: 0.8211920529801324 Test-acc: 0.7368421052631579\n",
      "Epoch [9801/20000] Train-Loss: 0.46408791527528703 Test-Loss: 0.14610950356417385 Train-f1: 0.4747881161174464 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9802/20000] Train-Loss: 0.4603581474431658 Test-Loss: 0.14445126099112018 Train-f1: 0.4813658214285265 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9803/20000] Train-Loss: 0.46214330369273837 Test-Loss: 0.1443723765200322 Train-f1: 0.47667435788066764 Test-f1: 0.43932367149758456 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9804/20000] Train-Loss: 0.4643297300227373 Test-Loss: 0.14676694385192182 Train-f1: 0.47611715510603475 Test-f1: 0.43193277310924366 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9805/20000] Train-Loss: 0.4624615527030442 Test-Loss: 0.1439947106981628 Train-f1: 0.47674977378556616 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9806/20000] Train-Loss: 0.46654554967034745 Test-Loss: 0.14991663619147091 Train-f1: 0.480648496572013 Test-f1: 0.4624973437035148 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9807/20000] Train-Loss: 0.4582058682216237 Test-Loss: 0.14741430228776825 Train-f1: 0.47652432211199247 Test-f1: 0.5315492957746479 Train-acc: 0.8013245033112583 Test-acc: 0.8026315789473685\n",
      "Epoch [9808/20000] Train-Loss: 0.4600689159096389 Test-Loss: 0.14639150455649574 Train-f1: 0.4874082783424427 Test-f1: 0.4594011008433066 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9809/20000] Train-Loss: 0.4603958920371242 Test-Loss: 0.1466255565839426 Train-f1: 0.48129104975541354 Test-f1: 0.46683117900509197 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [9810/20000] Train-Loss: 0.46377685274487435 Test-Loss: 0.14637023493482199 Train-f1: 0.4834539390888498 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [9811/20000] Train-Loss: 0.4617278846860545 Test-Loss: 0.14775319416476065 Train-f1: 0.47399594233903064 Test-f1: 0.44683117900509206 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9812/20000] Train-Loss: 0.46771379205958935 Test-Loss: 0.15036408199211404 Train-f1: 0.48105112506057585 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9813/20000] Train-Loss: 0.4617846945965685 Test-Loss: 0.14471981975311954 Train-f1: 0.4712727272727273 Test-f1: 0.43932367149758456 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [9814/20000] Train-Loss: 0.4628090419636797 Test-Loss: 0.14729229374086283 Train-f1: 0.4834539390888498 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9815/20000] Train-Loss: 0.4667191190355758 Test-Loss: 0.1449952816735938 Train-f1: 0.47315192232948106 Test-f1: 0.44990810208201515 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9816/20000] Train-Loss: 0.4622346875833308 Test-Loss: 0.14399254356601723 Train-f1: 0.48388278388278383 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9817/20000] Train-Loss: 0.4686706404066306 Test-Loss: 0.1515614577093306 Train-f1: 0.47253997316936414 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [9818/20000] Train-Loss: 0.46335303318121085 Test-Loss: 0.15976658472768088 Train-f1: 0.4802978944098165 Test-f1: 0.4545727136431784 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9819/20000] Train-Loss: 0.46071475062548056 Test-Loss: 0.14637696985120718 Train-f1: 0.4821919506563145 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9820/20000] Train-Loss: 0.4625069459667344 Test-Loss: 0.15903704177865033 Train-f1: 0.481411540476835 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [9821/20000] Train-Loss: 0.45845325825197186 Test-Loss: 0.15366106552418018 Train-f1: 0.47138156325406333 Test-f1: 0.42567287784679086 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [9822/20000] Train-Loss: 0.46469717411666694 Test-Loss: 0.1453640836113994 Train-f1: 0.4763035868456165 Test-f1: 0.460952380952381 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9823/20000] Train-Loss: 0.46120836723287595 Test-Loss: 0.14385825802205446 Train-f1: 0.4814648771681972 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [9824/20000] Train-Loss: 0.46266169965397796 Test-Loss: 0.14572853669189736 Train-f1: 0.48099039152688505 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9825/20000] Train-Loss: 0.4647479800868241 Test-Loss: 0.1482552238478046 Train-f1: 0.47598783392133537 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9826/20000] Train-Loss: 0.4651225298631266 Test-Loss: 0.1446990412195278 Train-f1: 0.4763035868456165 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9827/20000] Train-Loss: 0.4630107522506949 Test-Loss: 0.1464273716495849 Train-f1: 0.48354412882714765 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9828/20000] Train-Loss: 0.46535898301760803 Test-Loss: 0.14841291744000407 Train-f1: 0.4868850794150642 Test-f1: 0.42567287784679086 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [9829/20000] Train-Loss: 0.4615637099385631 Test-Loss: 0.14735772243242587 Train-f1: 0.4806953183991946 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9830/20000] Train-Loss: 0.45571347498103304 Test-Loss: 0.14657349074022757 Train-f1: 0.47696038346604164 Test-f1: 0.4376874738165061 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9831/20000] Train-Loss: 0.467147891662828 Test-Loss: 0.1497586725154571 Train-f1: 0.47983884728410275 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [9832/20000] Train-Loss: 0.463771206857324 Test-Loss: 0.14364524801945022 Train-f1: 0.4765071222495914 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9833/20000] Train-Loss: 0.46202348557179296 Test-Loss: 0.15194485419374654 Train-f1: 0.47782875860734747 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9834/20000] Train-Loss: 0.46422813158595133 Test-Loss: 0.14605569490083992 Train-f1: 0.47880171886442396 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9835/20000] Train-Loss: 0.4630976816244048 Test-Loss: 0.14573509224537365 Train-f1: 0.47029479652767325 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9836/20000] Train-Loss: 0.4626627646264349 Test-Loss: 0.14651299360302297 Train-f1: 0.4738927431646241 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9837/20000] Train-Loss: 0.458170013696503 Test-Loss: 0.15264374479676351 Train-f1: 0.48342627434979074 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9838/20000] Train-Loss: 0.4628057589359222 Test-Loss: 0.144418848200705 Train-f1: 0.4822090497885335 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9839/20000] Train-Loss: 0.4624089244778428 Test-Loss: 0.14704894504692959 Train-f1: 0.4708684887835429 Test-f1: 0.4356787406570312 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [9840/20000] Train-Loss: 0.46041690876339836 Test-Loss: 0.1434721388830969 Train-f1: 0.4781953183991946 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9841/20000] Train-Loss: 0.46402833752804545 Test-Loss: 0.1477883240012081 Train-f1: 0.4781953183991946 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9842/20000] Train-Loss: 0.4606596276156646 Test-Loss: 0.1448159355969147 Train-f1: 0.4801170323928945 Test-f1: 0.5213472706155633 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9843/20000] Train-Loss: 0.46655981907823074 Test-Loss: 0.14450849902592341 Train-f1: 0.48099039152688505 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9844/20000] Train-Loss: 0.46201646652926637 Test-Loss: 0.1451923967903473 Train-f1: 0.4883614358066913 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9845/20000] Train-Loss: 0.46097366058400935 Test-Loss: 0.15078660991144163 Train-f1: 0.47604057635207386 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9846/20000] Train-Loss: 0.4615663732754323 Test-Loss: 0.14659241841984108 Train-f1: 0.4814862826228133 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9847/20000] Train-Loss: 0.4632117760071923 Test-Loss: 0.15192901960888103 Train-f1: 0.4787488980537207 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9848/20000] Train-Loss: 0.46190150829666227 Test-Loss: 0.15209586478143242 Train-f1: 0.4846623546684656 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9849/20000] Train-Loss: 0.4618976691311552 Test-Loss: 0.1491494795613787 Train-f1: 0.4742200589334347 Test-f1: 0.5058710801393729 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9850/20000] Train-Loss: 0.4610544473037333 Test-Loss: 0.14531085641811645 Train-f1: 0.4890249475009444 Test-f1: 0.42846033429278385 Train-acc: 0.8178807947019867 Test-acc: 0.7368421052631579\n",
      "Epoch [9851/20000] Train-Loss: 0.46169976532591284 Test-Loss: 0.1452399988854577 Train-f1: 0.48077424858246776 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9852/20000] Train-Loss: 0.46180076167366185 Test-Loss: 0.14500299082789025 Train-f1: 0.47355305254193214 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9853/20000] Train-Loss: 0.4640403046291734 Test-Loss: 0.14708471563320993 Train-f1: 0.48062776371632693 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9854/20000] Train-Loss: 0.463003919737805 Test-Loss: 0.14842757332198073 Train-f1: 0.4765367965367965 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9855/20000] Train-Loss: 0.4612169875217133 Test-Loss: 0.16552125510054147 Train-f1: 0.4707901029487342 Test-f1: 0.4495524296675192 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9856/20000] Train-Loss: 0.46119623363377926 Test-Loss: 0.14434201703339483 Train-f1: 0.4763934185406608 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9857/20000] Train-Loss: 0.4655527658334873 Test-Loss: 0.14486374261002086 Train-f1: 0.4736700978173157 Test-f1: 0.460952380952381 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9858/20000] Train-Loss: 0.46210366289344784 Test-Loss: 0.1474030563186055 Train-f1: 0.48342627434979074 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9859/20000] Train-Loss: 0.46470423548980555 Test-Loss: 0.14445899635330767 Train-f1: 0.48383882045605053 Test-f1: 0.44990810208201515 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9860/20000] Train-Loss: 0.4611534418373784 Test-Loss: 0.15412535883956271 Train-f1: 0.4820572153187498 Test-f1: 0.4652641421224871 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [9861/20000] Train-Loss: 0.4648047159083107 Test-Loss: 0.14914977668344614 Train-f1: 0.4765071222495914 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9862/20000] Train-Loss: 0.46426144314782414 Test-Loss: 0.14694852812376932 Train-f1: 0.4757349422290476 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9863/20000] Train-Loss: 0.4601327318840796 Test-Loss: 0.143553961227576 Train-f1: 0.4843846592292955 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9864/20000] Train-Loss: 0.464600408020967 Test-Loss: 0.14401223746192146 Train-f1: 0.4707901029487342 Test-f1: 0.460952380952381 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9865/20000] Train-Loss: 0.45977701669502524 Test-Loss: 0.14355929214080246 Train-f1: 0.491212559778505 Test-f1: 0.46016451233842537 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [9866/20000] Train-Loss: 0.46294106549083996 Test-Loss: 0.14468118828050805 Train-f1: 0.4678680625381991 Test-f1: 0.42846033429278385 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [9867/20000] Train-Loss: 0.45835790818545996 Test-Loss: 0.14926360033455605 Train-f1: 0.4647460884942144 Test-f1: 0.45313466162830646 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [9868/20000] Train-Loss: 0.464137963166027 Test-Loss: 0.14827051103736083 Train-f1: 0.48405038518691584 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9869/20000] Train-Loss: 0.465667313157659 Test-Loss: 0.14557908512733608 Train-f1: 0.48992344497607654 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9870/20000] Train-Loss: 0.4588208727948581 Test-Loss: 0.14699571331554215 Train-f1: 0.4807663887699281 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9871/20000] Train-Loss: 0.46255862720749513 Test-Loss: 0.1465059224188284 Train-f1: 0.481495697285171 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9872/20000] Train-Loss: 0.4608004825129262 Test-Loss: 0.14542874137330802 Train-f1: 0.4834539390888498 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9873/20000] Train-Loss: 0.4627546953314029 Test-Loss: 0.1514434515683107 Train-f1: 0.4865655278903686 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9874/20000] Train-Loss: 0.4615826565360828 Test-Loss: 0.15954524895102976 Train-f1: 0.4729366977095394 Test-f1: 0.4495524296675192 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9875/20000] Train-Loss: 0.46538551532298306 Test-Loss: 0.1480147328304763 Train-f1: 0.47450144659410737 Test-f1: 0.43639684222929187 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [9876/20000] Train-Loss: 0.46222649602053717 Test-Loss: 0.14452703455185242 Train-f1: 0.4780345225550705 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9877/20000] Train-Loss: 0.462963220729812 Test-Loss: 0.14670749895700863 Train-f1: 0.4813658214285265 Test-f1: 0.44854612862701837 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9878/20000] Train-Loss: 0.4606750086901448 Test-Loss: 0.1443086445743688 Train-f1: 0.47611715510603475 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9879/20000] Train-Loss: 0.4601254264203473 Test-Loss: 0.1450056201258773 Train-f1: 0.4733493896063895 Test-f1: 0.47043478260869565 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [9880/20000] Train-Loss: 0.4635372790736651 Test-Loss: 0.14987043397659092 Train-f1: 0.4861515151515151 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9881/20000] Train-Loss: 0.46115952079813183 Test-Loss: 0.14555619445778242 Train-f1: 0.4834539390888498 Test-f1: 0.44990810208201515 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9882/20000] Train-Loss: 0.46137959837343545 Test-Loss: 0.14551062758003316 Train-f1: 0.4859032511113048 Test-f1: 0.45984654731457797 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9883/20000] Train-Loss: 0.46368097141537934 Test-Loss: 0.14641722864047266 Train-f1: 0.4774192326486085 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9884/20000] Train-Loss: 0.46220096390775417 Test-Loss: 0.1443291619991766 Train-f1: 0.4767655502392344 Test-f1: 0.44990810208201515 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9885/20000] Train-Loss: 0.46439793278600194 Test-Loss: 0.14757470572744383 Train-f1: 0.4831329686217024 Test-f1: 0.4356787406570312 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [9886/20000] Train-Loss: 0.4662600306172686 Test-Loss: 0.14608448593282655 Train-f1: 0.4783642176047239 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9887/20000] Train-Loss: 0.45831930617873207 Test-Loss: 0.1444719253440482 Train-f1: 0.4757349422290476 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9888/20000] Train-Loss: 0.4612092825936988 Test-Loss: 0.14580952139633252 Train-f1: 0.47075895048233535 Test-f1: 0.44937417654808953 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9889/20000] Train-Loss: 0.4608821998194797 Test-Loss: 0.15200800642921192 Train-f1: 0.4845515064946307 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9890/20000] Train-Loss: 0.46390318713353734 Test-Loss: 0.14681818598914304 Train-f1: 0.48099039152688505 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9891/20000] Train-Loss: 0.4662428353494742 Test-Loss: 0.14566007852523546 Train-f1: 0.4864468864468864 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9892/20000] Train-Loss: 0.4587032927785164 Test-Loss: 0.14956359488692822 Train-f1: 0.4829000918913781 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9893/20000] Train-Loss: 0.4615030657168209 Test-Loss: 0.15153684525459687 Train-f1: 0.4725624200076755 Test-f1: 0.4594670406732118 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9894/20000] Train-Loss: 0.4624846619739104 Test-Loss: 0.14490272053373895 Train-f1: 0.4763934185406608 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9895/20000] Train-Loss: 0.4614157306148081 Test-Loss: 0.1468037953784186 Train-f1: 0.47782875860734747 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9896/20000] Train-Loss: 0.4638397584948463 Test-Loss: 0.14573685399178915 Train-f1: 0.47589716030702645 Test-f1: 0.43193277310924366 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9897/20000] Train-Loss: 0.46078137816898507 Test-Loss: 0.14805266720587004 Train-f1: 0.48105112506057585 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9898/20000] Train-Loss: 0.4618734170558086 Test-Loss: 0.15437396050353502 Train-f1: 0.4857681035709204 Test-f1: 0.4652641421224871 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [9899/20000] Train-Loss: 0.4621474279105475 Test-Loss: 0.14384405234809233 Train-f1: 0.4949969743307747 Test-f1: 0.46016451233842537 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [9900/20000] Train-Loss: 0.45808660275520413 Test-Loss: 0.15668658992645557 Train-f1: 0.4831329686217024 Test-f1: 0.420031268321282 Train-acc: 0.8112582781456954 Test-acc: 0.7236842105263158\n",
      "Epoch [9901/20000] Train-Loss: 0.46417866679516406 Test-Loss: 0.14786191516865882 Train-f1: 0.48377221349595 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9902/20000] Train-Loss: 0.4602446069320301 Test-Loss: 0.1440831151294354 Train-f1: 0.475076942522198 Test-f1: 0.43932367149758456 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [9903/20000] Train-Loss: 0.4605432448369929 Test-Loss: 0.1455367552473326 Train-f1: 0.4781953183991946 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9904/20000] Train-Loss: 0.4591211140308933 Test-Loss: 0.14790519876909353 Train-f1: 0.4742758197076523 Test-f1: 0.44649416182596957 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9905/20000] Train-Loss: 0.4605791579881679 Test-Loss: 0.14822898443698088 Train-f1: 0.47068102211643054 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9906/20000] Train-Loss: 0.46256506890571814 Test-Loss: 0.1479292401900169 Train-f1: 0.48099039152688505 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9907/20000] Train-Loss: 0.4595225901033064 Test-Loss: 0.14963662531839986 Train-f1: 0.4863038590655703 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9908/20000] Train-Loss: 0.4620119736363942 Test-Loss: 0.14534148218670923 Train-f1: 0.47707662574292475 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9909/20000] Train-Loss: 0.4638058840980421 Test-Loss: 0.14586397437546325 Train-f1: 0.4788387595841012 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9910/20000] Train-Loss: 0.46771580670734003 Test-Loss: 0.14387404072455495 Train-f1: 0.4816198704953846 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9911/20000] Train-Loss: 0.4594059489026368 Test-Loss: 0.14407504320776088 Train-f1: 0.4786520146520147 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9912/20000] Train-Loss: 0.45777599527500407 Test-Loss: 0.14612991732442582 Train-f1: 0.4812966687543575 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9913/20000] Train-Loss: 0.46201284681285276 Test-Loss: 0.1442742389466224 Train-f1: 0.469989181233559 Test-f1: 0.42846033429278385 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [9914/20000] Train-Loss: 0.4615550535885733 Test-Loss: 0.1440180682780822 Train-f1: 0.47611715510603475 Test-f1: 0.43932367149758456 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [9915/20000] Train-Loss: 0.4615744434158836 Test-Loss: 0.14424973441525207 Train-f1: 0.4743549783549783 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9916/20000] Train-Loss: 0.4609096968594116 Test-Loss: 0.1442952967326849 Train-f1: 0.48099039152688505 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9917/20000] Train-Loss: 0.4605839592312358 Test-Loss: 0.14642937521140467 Train-f1: 0.4806953183991946 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9918/20000] Train-Loss: 0.4609636099559193 Test-Loss: 0.1483809706290698 Train-f1: 0.48377221349595 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9919/20000] Train-Loss: 0.4605903658058206 Test-Loss: 0.1462648314079535 Train-f1: 0.4866144877510184 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9920/20000] Train-Loss: 0.4626084774126651 Test-Loss: 0.14423348131642555 Train-f1: 0.47652432211199247 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9921/20000] Train-Loss: 0.4603960517001207 Test-Loss: 0.1447747651985801 Train-f1: 0.48624584082472255 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [9922/20000] Train-Loss: 0.4625179536555104 Test-Loss: 0.1445850743127734 Train-f1: 0.48163982776058 Test-f1: 0.4318216788305991 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [9923/20000] Train-Loss: 0.4613454153327734 Test-Loss: 0.14409552977617868 Train-f1: 0.475952393713448 Test-f1: 0.4503312334891282 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9924/20000] Train-Loss: 0.46185609480510553 Test-Loss: 0.1466120824635069 Train-f1: 0.48105112506057585 Test-f1: 0.44854612862701837 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9925/20000] Train-Loss: 0.46169692363308007 Test-Loss: 0.14398182651333502 Train-f1: 0.48423723035798255 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9926/20000] Train-Loss: 0.4616077571556069 Test-Loss: 0.1441126554708562 Train-f1: 0.4835061147973253 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9927/20000] Train-Loss: 0.45880127010518407 Test-Loss: 0.14813394667439708 Train-f1: 0.480648496572013 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9928/20000] Train-Loss: 0.4605042001013511 Test-Loss: 0.1451014361448509 Train-f1: 0.48155089867357354 Test-f1: 0.4356787406570312 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [9929/20000] Train-Loss: 0.45755204738339644 Test-Loss: 0.14763916810521005 Train-f1: 0.4833737373737373 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9930/20000] Train-Loss: 0.46277755446731444 Test-Loss: 0.1435486641990297 Train-f1: 0.47105627501765956 Test-f1: 0.4602828448452552 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9931/20000] Train-Loss: 0.45456329459793077 Test-Loss: 0.14827495668564214 Train-f1: 0.4920662760662761 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [9932/20000] Train-Loss: 0.4617400683384983 Test-Loss: 0.14470387421759615 Train-f1: 0.4788842320069069 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9933/20000] Train-Loss: 0.4584450822842261 Test-Loss: 0.1481547021043065 Train-f1: 0.48845393908884976 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9934/20000] Train-Loss: 0.45984928936620445 Test-Loss: 0.1455850493372699 Train-f1: 0.48099039152688505 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9935/20000] Train-Loss: 0.46000831775775497 Test-Loss: 0.14963416923144174 Train-f1: 0.4765071222495914 Test-f1: 0.4624973437035148 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [9936/20000] Train-Loss: 0.4572554659476704 Test-Loss: 0.14325107460406813 Train-f1: 0.4747120717093335 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9937/20000] Train-Loss: 0.4627689516670128 Test-Loss: 0.14624020503869595 Train-f1: 0.4755959595959595 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9938/20000] Train-Loss: 0.4585374856047975 Test-Loss: 0.14856048529879148 Train-f1: 0.47782875860734747 Test-f1: 0.4594670406732118 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9939/20000] Train-Loss: 0.46053439690571923 Test-Loss: 0.1501364372481554 Train-f1: 0.4875214349666905 Test-f1: 0.4594670406732118 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9940/20000] Train-Loss: 0.4653325511706187 Test-Loss: 0.14395725572620507 Train-f1: 0.48306926484229684 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9941/20000] Train-Loss: 0.46062260332937066 Test-Loss: 0.14498296593187243 Train-f1: 0.47627979763809664 Test-f1: 0.44854612862701837 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9942/20000] Train-Loss: 0.46345105668062764 Test-Loss: 0.14325634946118698 Train-f1: 0.4708684887835429 Test-f1: 0.4497330282227307 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9943/20000] Train-Loss: 0.46112297896978305 Test-Loss: 0.14337101771850325 Train-f1: 0.4778707187942352 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9944/20000] Train-Loss: 0.46082178673384194 Test-Loss: 0.14807421282881691 Train-f1: 0.48620681768892576 Test-f1: 0.42567287784679086 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [9945/20000] Train-Loss: 0.4611665613725382 Test-Loss: 0.1445374205216525 Train-f1: 0.48377221349595 Test-f1: 0.43932367149758456 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [9946/20000] Train-Loss: 0.45910718860654154 Test-Loss: 0.14839528103701846 Train-f1: 0.48129104975541354 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9947/20000] Train-Loss: 0.45801544561684454 Test-Loss: 0.14348647306443219 Train-f1: 0.4956483710936266 Test-f1: 0.46016451233842537 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [9948/20000] Train-Loss: 0.45495803322945355 Test-Loss: 0.1506624330716406 Train-f1: 0.4760912465610452 Test-f1: 0.45675057208237985 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9949/20000] Train-Loss: 0.4617722022322474 Test-Loss: 0.14463380334364914 Train-f1: 0.48077424858246776 Test-f1: 0.4423271037905184 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [9950/20000] Train-Loss: 0.46173649914588444 Test-Loss: 0.14608341111919548 Train-f1: 0.4817702801443282 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9951/20000] Train-Loss: 0.4611785892658722 Test-Loss: 0.14581436469287215 Train-f1: 0.4802648500379375 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9952/20000] Train-Loss: 0.4586509431860132 Test-Loss: 0.1439365526148598 Train-f1: 0.4791341991341991 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9953/20000] Train-Loss: 0.4608279849977772 Test-Loss: 0.1442982361972329 Train-f1: 0.47355305254193214 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [9954/20000] Train-Loss: 0.45968133555795637 Test-Loss: 0.14567547797539715 Train-f1: 0.4840177571897318 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9955/20000] Train-Loss: 0.4615514788449888 Test-Loss: 0.1434059021782216 Train-f1: 0.47172101916627457 Test-f1: 0.460952380952381 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [9956/20000] Train-Loss: 0.46044872867243364 Test-Loss: 0.14460223822845375 Train-f1: 0.47882996282996276 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9957/20000] Train-Loss: 0.45682751307424196 Test-Loss: 0.1462240862565275 Train-f1: 0.4759179933163004 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9958/20000] Train-Loss: 0.4635695580796077 Test-Loss: 0.1441918855046217 Train-f1: 0.4757983910555815 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [9959/20000] Train-Loss: 0.4602537696674159 Test-Loss: 0.14311157681606043 Train-f1: 0.476087912087912 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [9960/20000] Train-Loss: 0.4603297687379517 Test-Loss: 0.1481737951977418 Train-f1: 0.4751779917424089 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9961/20000] Train-Loss: 0.45844763586101733 Test-Loss: 0.1435955836309109 Train-f1: 0.4718726911054277 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [9962/20000] Train-Loss: 0.4600814892956356 Test-Loss: 0.14798890087538058 Train-f1: 0.4781953183991946 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9963/20000] Train-Loss: 0.45638041408594354 Test-Loss: 0.14326128047664324 Train-f1: 0.47961904761904767 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9964/20000] Train-Loss: 0.4598965736022006 Test-Loss: 0.14656971429634122 Train-f1: 0.4804402387735721 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9965/20000] Train-Loss: 0.4602382793603732 Test-Loss: 0.14336319885705098 Train-f1: 0.48202870813397125 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9966/20000] Train-Loss: 0.46032883777107747 Test-Loss: 0.14404346485885908 Train-f1: 0.48112335101962034 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9967/20000] Train-Loss: 0.45889668528223404 Test-Loss: 0.14350993859755862 Train-f1: 0.4788842320069069 Test-f1: 0.44990810208201515 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9968/20000] Train-Loss: 0.4602498363832146 Test-Loss: 0.14518875839379308 Train-f1: 0.4760401459854015 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9969/20000] Train-Loss: 0.4521226262678521 Test-Loss: 0.1601593087532096 Train-f1: 0.48025611208123264 Test-f1: 0.46444838805087246 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [9970/20000] Train-Loss: 0.46282885163556425 Test-Loss: 0.15365731418853695 Train-f1: 0.4844878584037022 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9971/20000] Train-Loss: 0.4599398976269595 Test-Loss: 0.1446167173195531 Train-f1: 0.4684403315982263 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [9972/20000] Train-Loss: 0.45948347144883195 Test-Loss: 0.143184171733063 Train-f1: 0.4785883470527108 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [9973/20000] Train-Loss: 0.46088261875447156 Test-Loss: 0.14594712127489734 Train-f1: 0.47652432211199247 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9974/20000] Train-Loss: 0.46279403169312433 Test-Loss: 0.14930842077101525 Train-f1: 0.4774146048598603 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9975/20000] Train-Loss: 0.4566071850119483 Test-Loss: 0.15154499550184375 Train-f1: 0.48025611208123264 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9976/20000] Train-Loss: 0.46253674698523123 Test-Loss: 0.1455336230299827 Train-f1: 0.4795244860301443 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9977/20000] Train-Loss: 0.4580526544898312 Test-Loss: 0.1467195039620091 Train-f1: 0.4758856443500081 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9978/20000] Train-Loss: 0.4590305289330446 Test-Loss: 0.14735949962566397 Train-f1: 0.48099039152688505 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9979/20000] Train-Loss: 0.4583815986297225 Test-Loss: 0.14311956006463827 Train-f1: 0.4742758197076523 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [9980/20000] Train-Loss: 0.4581306510257472 Test-Loss: 0.1553188940711719 Train-f1: 0.4800418574231523 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [9981/20000] Train-Loss: 0.46220394228392486 Test-Loss: 0.1451072753121997 Train-f1: 0.4736404075372759 Test-f1: 0.43932367149758456 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [9982/20000] Train-Loss: 0.45956963085087815 Test-Loss: 0.1433185647916475 Train-f1: 0.481495697285171 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [9983/20000] Train-Loss: 0.45967967616642674 Test-Loss: 0.14321148104083856 Train-f1: 0.48105112506057585 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9984/20000] Train-Loss: 0.4558436599008989 Test-Loss: 0.14727580354145853 Train-f1: 0.4890249475009444 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [9985/20000] Train-Loss: 0.4593416549630627 Test-Loss: 0.14612151388019512 Train-f1: 0.47715373685706164 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9986/20000] Train-Loss: 0.4573818676553125 Test-Loss: 0.14909429701379065 Train-f1: 0.4831329686217024 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [9987/20000] Train-Loss: 0.4609561814734536 Test-Loss: 0.1451350919530725 Train-f1: 0.47616549796425894 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [9988/20000] Train-Loss: 0.45583742353360446 Test-Loss: 0.1432396782542307 Train-f1: 0.4784449310076634 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [9989/20000] Train-Loss: 0.46018603563691984 Test-Loss: 0.14652212789190605 Train-f1: 0.4821538099354622 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9990/20000] Train-Loss: 0.462539660862206 Test-Loss: 0.14571019262249577 Train-f1: 0.4731424064982946 Test-f1: 0.42567287784679086 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [9991/20000] Train-Loss: 0.4576145263244694 Test-Loss: 0.14702946831771227 Train-f1: 0.47611715510603475 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [9992/20000] Train-Loss: 0.45898956280683306 Test-Loss: 0.1444087340505939 Train-f1: 0.4800100849543666 Test-f1: 0.43932367149758456 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [9993/20000] Train-Loss: 0.4608213123609032 Test-Loss: 0.14463151787249237 Train-f1: 0.4865415231587532 Test-f1: 0.4318216788305991 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [9994/20000] Train-Loss: 0.4615523612016057 Test-Loss: 0.1469436416037887 Train-f1: 0.479622863727372 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [9995/20000] Train-Loss: 0.45947229525020894 Test-Loss: 0.14547015843721822 Train-f1: 0.5009244378809596 Test-f1: 0.44854612862701837 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [9996/20000] Train-Loss: 0.46088559156448256 Test-Loss: 0.14473301122932258 Train-f1: 0.4821230196391971 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [9997/20000] Train-Loss: 0.4587989415805411 Test-Loss: 0.14360832396955286 Train-f1: 0.4813658214285265 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [9998/20000] Train-Loss: 0.45585321749946145 Test-Loss: 0.14282527835694434 Train-f1: 0.48595393908884976 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [9999/20000] Train-Loss: 0.4596450202288735 Test-Loss: 0.1468867194782156 Train-f1: 0.4834539390888498 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10000/20000] Train-Loss: 0.46046156813783773 Test-Loss: 0.14664305061610222 Train-f1: 0.47343259134764554 Test-f1: 0.44558629776021075 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10001/20000] Train-Loss: 0.4510533795714148 Test-Loss: 0.14353232458732224 Train-f1: 0.498881270002218 Test-f1: 0.45984654731457797 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [10002/20000] Train-Loss: 0.46130544539184354 Test-Loss: 0.14653858089353175 Train-f1: 0.48249112589538123 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10003/20000] Train-Loss: 0.45682078359736966 Test-Loss: 0.1451265794089712 Train-f1: 0.4774143514962441 Test-f1: 0.45984654731457797 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10004/20000] Train-Loss: 0.4635686659236312 Test-Loss: 0.14741436016802703 Train-f1: 0.4727478019232142 Test-f1: 0.4526190476190476 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10005/20000] Train-Loss: 0.4606015725718429 Test-Loss: 0.147638906054898 Train-f1: 0.4887125597785049 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10006/20000] Train-Loss: 0.4607740034079834 Test-Loss: 0.14431540154571443 Train-f1: 0.47880171886442396 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10007/20000] Train-Loss: 0.4576089464062804 Test-Loss: 0.14353331586015242 Train-f1: 0.4847938992574754 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10008/20000] Train-Loss: 0.45916740108868165 Test-Loss: 0.14341619860054608 Train-f1: 0.4834539390888498 Test-f1: 0.43932367149758456 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10009/20000] Train-Loss: 0.4553217075772114 Test-Loss: 0.1434130047293344 Train-f1: 0.48801383019592776 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10010/20000] Train-Loss: 0.46188651512764195 Test-Loss: 0.14289738870013496 Train-f1: 0.48129104975541354 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10011/20000] Train-Loss: 0.45906226812452655 Test-Loss: 0.14329335157002165 Train-f1: 0.48412727623253937 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10012/20000] Train-Loss: 0.4593955478359784 Test-Loss: 0.14661581889864891 Train-f1: 0.47333787364937113 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10013/20000] Train-Loss: 0.46167878268101376 Test-Loss: 0.14512635143301228 Train-f1: 0.4785194794909556 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10014/20000] Train-Loss: 0.4565464188791566 Test-Loss: 0.1518875906446438 Train-f1: 0.4893788556628208 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10015/20000] Train-Loss: 0.4593946366279164 Test-Loss: 0.14576926846835866 Train-f1: 0.4871194373510706 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10016/20000] Train-Loss: 0.45915882742089364 Test-Loss: 0.1449709187768817 Train-f1: 0.4762011273530836 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10017/20000] Train-Loss: 0.4556152020981788 Test-Loss: 0.1467372443553789 Train-f1: 0.4754366977095394 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10018/20000] Train-Loss: 0.46344966843505825 Test-Loss: 0.14513322806462658 Train-f1: 0.4896623376623376 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10019/20000] Train-Loss: 0.4553095908397555 Test-Loss: 0.14632349946348153 Train-f1: 0.4781640245223235 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10020/20000] Train-Loss: 0.45715806088611155 Test-Loss: 0.1467070964653763 Train-f1: 0.48624584082472255 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10021/20000] Train-Loss: 0.4607230840226465 Test-Loss: 0.14415251957881267 Train-f1: 0.4791387011969598 Test-f1: 0.4421230942970073 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10022/20000] Train-Loss: 0.4551775950001321 Test-Loss: 0.1476296094110828 Train-f1: 0.4754366977095394 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10023/20000] Train-Loss: 0.4568632512347581 Test-Loss: 0.14453088082155635 Train-f1: 0.4750081371553794 Test-f1: 0.46683117900509197 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [10024/20000] Train-Loss: 0.4568210614072973 Test-Loss: 0.15481892407843864 Train-f1: 0.48534379768912145 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10025/20000] Train-Loss: 0.45562951755768843 Test-Loss: 0.14299023476632555 Train-f1: 0.4843846592292955 Test-f1: 0.4714285714285714 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10026/20000] Train-Loss: 0.4578310763104669 Test-Loss: 0.1446884090830742 Train-f1: 0.486115869176902 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10027/20000] Train-Loss: 0.46009542830622097 Test-Loss: 0.14395645277983365 Train-f1: 0.4793520493138611 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10028/20000] Train-Loss: 0.4613403252385976 Test-Loss: 0.14418845792263543 Train-f1: 0.48405038518691584 Test-f1: 0.43639684222929187 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10029/20000] Train-Loss: 0.45920109631043066 Test-Loss: 0.14366907049254074 Train-f1: 0.48105112506057585 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10030/20000] Train-Loss: 0.45887463777313336 Test-Loss: 0.1439230826377175 Train-f1: 0.4763035868456165 Test-f1: 0.4714285714285714 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10031/20000] Train-Loss: 0.4596622254335683 Test-Loss: 0.14375315502149974 Train-f1: 0.47316986409338047 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10032/20000] Train-Loss: 0.4592348398728532 Test-Loss: 0.14412591321277074 Train-f1: 0.473875543302223 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [10033/20000] Train-Loss: 0.4544637482101724 Test-Loss: 0.14334364134214878 Train-f1: 0.4843846592292955 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10034/20000] Train-Loss: 0.4605868604971839 Test-Loss: 0.14426002618115813 Train-f1: 0.47825066549948775 Test-f1: 0.4421230942970073 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10035/20000] Train-Loss: 0.45933544401314635 Test-Loss: 0.14500519902562423 Train-f1: 0.4785194794909556 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10036/20000] Train-Loss: 0.4606613761693123 Test-Loss: 0.14292235463938655 Train-f1: 0.48099039152688505 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10037/20000] Train-Loss: 0.4578647817039794 Test-Loss: 0.14350831536047953 Train-f1: 0.4812966687543575 Test-f1: 0.44990810208201515 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10038/20000] Train-Loss: 0.4579542603644911 Test-Loss: 0.14290170700583898 Train-f1: 0.4753770309899341 Test-f1: 0.4714285714285714 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [10039/20000] Train-Loss: 0.4590178673706496 Test-Loss: 0.1451216349619982 Train-f1: 0.48377221349595 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10040/20000] Train-Loss: 0.4556697923666595 Test-Loss: 0.1441656201826463 Train-f1: 0.4729366977095394 Test-f1: 0.43193277310924366 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [10041/20000] Train-Loss: 0.45428411314136696 Test-Loss: 0.14510454211527182 Train-f1: 0.4868862369804991 Test-f1: 0.43193277310924366 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10042/20000] Train-Loss: 0.4583068801896915 Test-Loss: 0.1463365933873199 Train-f1: 0.4835827706301961 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10043/20000] Train-Loss: 0.4556784695582852 Test-Loss: 0.14287584797154967 Train-f1: 0.48538225124855944 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [10044/20000] Train-Loss: 0.45508433504874335 Test-Loss: 0.14275399943599443 Train-f1: 0.4814648771681972 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10045/20000] Train-Loss: 0.4612254328723816 Test-Loss: 0.14317204988654098 Train-f1: 0.4757983910555815 Test-f1: 0.460952380952381 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10046/20000] Train-Loss: 0.459121185708322 Test-Loss: 0.1459039219411546 Train-f1: 0.4798561633014188 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10047/20000] Train-Loss: 0.45640838263858935 Test-Loss: 0.14878571136082366 Train-f1: 0.48553201590410067 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10048/20000] Train-Loss: 0.4510045133794639 Test-Loss: 0.15764745031609312 Train-f1: 0.47612726012726014 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10049/20000] Train-Loss: 0.4655220407264948 Test-Loss: 0.14382322206299242 Train-f1: 0.475223410761327 Test-f1: 0.42857142857142855 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [10050/20000] Train-Loss: 0.4616090121910686 Test-Loss: 0.14282868877540036 Train-f1: 0.4792591002337675 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10051/20000] Train-Loss: 0.45554250200722374 Test-Loss: 0.15093616940063473 Train-f1: 0.4774146048598603 Test-f1: 0.4594670406732118 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10052/20000] Train-Loss: 0.4603178851793155 Test-Loss: 0.144647719260656 Train-f1: 0.47662752128639907 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10053/20000] Train-Loss: 0.45972243881552916 Test-Loss: 0.1464574453902781 Train-f1: 0.4864933019313241 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10054/20000] Train-Loss: 0.4558223692074 Test-Loss: 0.15324000483743688 Train-f1: 0.47342074814382773 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [10055/20000] Train-Loss: 0.4535445219689738 Test-Loss: 0.16090776293606993 Train-f1: 0.4791341991341991 Test-f1: 0.4495524296675192 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10056/20000] Train-Loss: 0.4566012386510104 Test-Loss: 0.142636039198337 Train-f1: 0.4840441563705823 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10057/20000] Train-Loss: 0.46040661823466605 Test-Loss: 0.14752201458615566 Train-f1: 0.48099039152688505 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10058/20000] Train-Loss: 0.45853719414922656 Test-Loss: 0.14705346457463836 Train-f1: 0.47611715510603475 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10059/20000] Train-Loss: 0.45907505191512993 Test-Loss: 0.14521580683911595 Train-f1: 0.4880807402546533 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10060/20000] Train-Loss: 0.4570249804969392 Test-Loss: 0.14322868994709265 Train-f1: 0.480648496572013 Test-f1: 0.43932367149758456 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10061/20000] Train-Loss: 0.4525485007360265 Test-Loss: 0.14276440475279972 Train-f1: 0.4817702801443282 Test-f1: 0.4714285714285714 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10062/20000] Train-Loss: 0.45802731662844726 Test-Loss: 0.1454521431122503 Train-f1: 0.48163982776058 Test-f1: 0.44854612862701837 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10063/20000] Train-Loss: 0.45710956950111287 Test-Loss: 0.14542360701364374 Train-f1: 0.4790392253340944 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10064/20000] Train-Loss: 0.46047301408050556 Test-Loss: 0.14368995664365689 Train-f1: 0.4729366977095394 Test-f1: 0.4385580319084156 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [10065/20000] Train-Loss: 0.45563391288067406 Test-Loss: 0.14589570636285898 Train-f1: 0.4769913895628181 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10066/20000] Train-Loss: 0.45894675294875514 Test-Loss: 0.14773057502793577 Train-f1: 0.48409676809676816 Test-f1: 0.43639684222929187 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10067/20000] Train-Loss: 0.4589006234183656 Test-Loss: 0.15606988453567047 Train-f1: 0.47953051354999143 Test-f1: 0.42994652406417116 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10068/20000] Train-Loss: 0.4556014747318914 Test-Loss: 0.15200245944762794 Train-f1: 0.48367316632968943 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10069/20000] Train-Loss: 0.4603607789148544 Test-Loss: 0.1446108356609853 Train-f1: 0.47796299792872654 Test-f1: 0.47043478260869565 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10070/20000] Train-Loss: 0.45531018320570765 Test-Loss: 0.14372857554043883 Train-f1: 0.47566151490202124 Test-f1: 0.4318216788305991 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10071/20000] Train-Loss: 0.45760245950784983 Test-Loss: 0.14302972987288345 Train-f1: 0.4793156003347086 Test-f1: 0.43932367149758456 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10072/20000] Train-Loss: 0.4533456423125967 Test-Loss: 0.14443144527091373 Train-f1: 0.4868850794150642 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10073/20000] Train-Loss: 0.4583326497584999 Test-Loss: 0.14440318617250292 Train-f1: 0.47880171886442396 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10074/20000] Train-Loss: 0.4625260152152521 Test-Loss: 0.14721328741390213 Train-f1: 0.4817702801443282 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10075/20000] Train-Loss: 0.45645804423923725 Test-Loss: 0.1439601540546611 Train-f1: 0.4813951497480658 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10076/20000] Train-Loss: 0.45981480818097653 Test-Loss: 0.14656733388475954 Train-f1: 0.47882996282996276 Test-f1: 0.46683117900509197 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10077/20000] Train-Loss: 0.45793579761602166 Test-Loss: 0.14497533624049322 Train-f1: 0.46840533030188203 Test-f1: 0.44558629776021075 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [10078/20000] Train-Loss: 0.45646560432717914 Test-Loss: 0.1489459116251335 Train-f1: 0.48621255977850497 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [10079/20000] Train-Loss: 0.4576083686678532 Test-Loss: 0.1425285228575808 Train-f1: 0.47652432211199247 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10080/20000] Train-Loss: 0.4570031033237163 Test-Loss: 0.14367235972293235 Train-f1: 0.4762175653402402 Test-f1: 0.4714285714285714 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10081/20000] Train-Loss: 0.4563961169370998 Test-Loss: 0.1432850081438947 Train-f1: 0.46767807701988423 Test-f1: 0.43932367149758456 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [10082/20000] Train-Loss: 0.4540229482672808 Test-Loss: 0.14768386452354468 Train-f1: 0.4822630897083452 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10083/20000] Train-Loss: 0.46017093053748853 Test-Loss: 0.14509704686789945 Train-f1: 0.47222435651171735 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [10084/20000] Train-Loss: 0.4568300609542673 Test-Loss: 0.1452830677927873 Train-f1: 0.46899175888862726 Test-f1: 0.4526190476190476 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [10085/20000] Train-Loss: 0.45394742934750887 Test-Loss: 0.143352121654341 Train-f1: 0.4845168789570815 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10086/20000] Train-Loss: 0.4622036136406522 Test-Loss: 0.14341078541749705 Train-f1: 0.47355305254193214 Test-f1: 0.43932367149758456 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [10087/20000] Train-Loss: 0.45466601658153905 Test-Loss: 0.14351487209286864 Train-f1: 0.48869965091000706 Test-f1: 0.4594011008433066 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10088/20000] Train-Loss: 0.45844622070334523 Test-Loss: 0.14352373571123692 Train-f1: 0.47983884728410275 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10089/20000] Train-Loss: 0.4591010017860672 Test-Loss: 0.1433953924635702 Train-f1: 0.48099039152688505 Test-f1: 0.42567287784679086 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10090/20000] Train-Loss: 0.45507146987812264 Test-Loss: 0.14335067882555205 Train-f1: 0.4847336943724983 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10091/20000] Train-Loss: 0.4571682374153373 Test-Loss: 0.14331272488190955 Train-f1: 0.475952393713448 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10092/20000] Train-Loss: 0.45691282644491993 Test-Loss: 0.14341751967395666 Train-f1: 0.46718032890775996 Test-f1: 0.4594011008433066 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [10093/20000] Train-Loss: 0.45502218564312946 Test-Loss: 0.14482393030432528 Train-f1: 0.49020782058304135 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10094/20000] Train-Loss: 0.4576556521278193 Test-Loss: 0.14293354087911866 Train-f1: 0.476087912087912 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10095/20000] Train-Loss: 0.45843290725917196 Test-Loss: 0.14334654282596646 Train-f1: 0.48377221349595 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10096/20000] Train-Loss: 0.45506632403943653 Test-Loss: 0.1435048235840336 Train-f1: 0.4787488980537207 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10097/20000] Train-Loss: 0.456037190238698 Test-Loss: 0.14593303702861604 Train-f1: 0.4817702801443282 Test-f1: 0.5341466983720504 Train-acc: 0.8079470198675497 Test-acc: 0.8026315789473685\n",
      "Epoch [10098/20000] Train-Loss: 0.45894153905194424 Test-Loss: 0.1457240906766455 Train-f1: 0.4855357391764496 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10099/20000] Train-Loss: 0.45991956856957095 Test-Loss: 0.1445177127888095 Train-f1: 0.47646753246753254 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10100/20000] Train-Loss: 0.45174697908744293 Test-Loss: 0.143889973575951 Train-f1: 0.4843100239121939 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10101/20000] Train-Loss: 0.45397886046149216 Test-Loss: 0.14291784007277838 Train-f1: 0.4723400853872108 Test-f1: 0.460952380952381 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [10102/20000] Train-Loss: 0.45588705865334955 Test-Loss: 0.1436235057987899 Train-f1: 0.47882996282996276 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10103/20000] Train-Loss: 0.4563747110095144 Test-Loss: 0.14291222401543346 Train-f1: 0.4863038590655703 Test-f1: 0.43932367149758456 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [10104/20000] Train-Loss: 0.4538491750722099 Test-Loss: 0.14391579993074133 Train-f1: 0.47830731712133334 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10105/20000] Train-Loss: 0.4570682676694491 Test-Loss: 0.14563347612000307 Train-f1: 0.48386022190370015 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10106/20000] Train-Loss: 0.4593935305616349 Test-Loss: 0.14673107806646632 Train-f1: 0.4817702801443282 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10107/20000] Train-Loss: 0.45525581098432005 Test-Loss: 0.1443787702105811 Train-f1: 0.47707662574292475 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10108/20000] Train-Loss: 0.4584742990263058 Test-Loss: 0.1449820046610921 Train-f1: 0.47538624674454566 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10109/20000] Train-Loss: 0.4565664354092718 Test-Loss: 0.14576941309229424 Train-f1: 0.48129104975541354 Test-f1: 0.43193277310924366 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10110/20000] Train-Loss: 0.45067631366903405 Test-Loss: 0.15015056023610798 Train-f1: 0.48791501554671546 Test-f1: 0.4594670406732118 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [10111/20000] Train-Loss: 0.4581059482011931 Test-Loss: 0.14249560256511734 Train-f1: 0.4737960159432582 Test-f1: 0.47043478260869565 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [10112/20000] Train-Loss: 0.4581556722815775 Test-Loss: 0.14686833088910706 Train-f1: 0.48605985209758795 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10113/20000] Train-Loss: 0.4541918238448444 Test-Loss: 0.14456591724290366 Train-f1: 0.48106692030742665 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10114/20000] Train-Loss: 0.45876874303974513 Test-Loss: 0.14365351305446686 Train-f1: 0.47925349324959765 Test-f1: 0.4421230942970073 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [10115/20000] Train-Loss: 0.4506605898398983 Test-Loss: 0.14237486054790216 Train-f1: 0.48699903831426283 Test-f1: 0.46016451233842537 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10116/20000] Train-Loss: 0.4547520400032244 Test-Loss: 0.1447095200247339 Train-f1: 0.47553452255507045 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10117/20000] Train-Loss: 0.4587968784268553 Test-Loss: 0.14317382171330723 Train-f1: 0.4858766190361602 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10118/20000] Train-Loss: 0.45387835087358824 Test-Loss: 0.14558960912602945 Train-f1: 0.47348003891770957 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10119/20000] Train-Loss: 0.45737849376372414 Test-Loss: 0.14241884335869182 Train-f1: 0.48445432513793973 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10120/20000] Train-Loss: 0.4571821075684418 Test-Loss: 0.14375759469872393 Train-f1: 0.47034941698014476 Test-f1: 0.4423271037905184 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "best score_dx!!\n",
      "Epoch [10121/20000] Train-Loss: 0.4528898705771411 Test-Loss: 0.1422955230636832 Train-f1: 0.4893562577773104 Test-f1: 0.46016451233842537 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10122/20000] Train-Loss: 0.4558383741977558 Test-Loss: 0.14459958758948965 Train-f1: 0.4786812576701373 Test-f1: 0.42567287784679086 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10123/20000] Train-Loss: 0.4509261793921215 Test-Loss: 0.1498145393776679 Train-f1: 0.4835061147973253 Test-f1: 0.4628528560711606 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10124/20000] Train-Loss: 0.4592860834735413 Test-Loss: 0.1449092705914049 Train-f1: 0.4741572441190559 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10125/20000] Train-Loss: 0.4546187063314783 Test-Loss: 0.14913990667448376 Train-f1: 0.487064935064935 Test-f1: 0.42567287784679086 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10126/20000] Train-Loss: 0.4552403430394249 Test-Loss: 0.1492353388212068 Train-f1: 0.475952393713448 Test-f1: 0.43639684222929187 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [10127/20000] Train-Loss: 0.459514991352567 Test-Loss: 0.1440645682675459 Train-f1: 0.4677397669038127 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [10128/20000] Train-Loss: 0.4585858716685905 Test-Loss: 0.14501104609825827 Train-f1: 0.4909315633242013 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10129/20000] Train-Loss: 0.4554073071718073 Test-Loss: 0.14346067788251024 Train-f1: 0.4788145045867755 Test-f1: 0.4318216788305991 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10130/20000] Train-Loss: 0.45464596166149157 Test-Loss: 0.1442425960925511 Train-f1: 0.4859032511113048 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10131/20000] Train-Loss: 0.4545932635934027 Test-Loss: 0.14322984807587938 Train-f1: 0.47363692017894987 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10132/20000] Train-Loss: 0.45832422535672224 Test-Loss: 0.1429228420761036 Train-f1: 0.4919595008241112 Test-f1: 0.4503312334891282 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10133/20000] Train-Loss: 0.4568617743496974 Test-Loss: 0.14617421255349017 Train-f1: 0.49299409128822413 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10134/20000] Train-Loss: 0.45510292103179223 Test-Loss: 0.1434756412376613 Train-f1: 0.4845660737380483 Test-f1: 0.4356787406570312 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10135/20000] Train-Loss: 0.4546990003934275 Test-Loss: 0.15699029903320405 Train-f1: 0.48077424858246776 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10136/20000] Train-Loss: 0.45845600960662586 Test-Loss: 0.14272450321260421 Train-f1: 0.4716245665612754 Test-f1: 0.43932367149758456 Train-acc: 0.7947019867549668 Test-acc: 0.75\n",
      "Epoch [10137/20000] Train-Loss: 0.458204915214854 Test-Loss: 0.14268470382019663 Train-f1: 0.4785883470527108 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10138/20000] Train-Loss: 0.4559464642712041 Test-Loss: 0.15221372599631966 Train-f1: 0.48640962537599125 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10139/20000] Train-Loss: 0.454866951408047 Test-Loss: 0.1429980218314609 Train-f1: 0.48699903831426283 Test-f1: 0.42846033429278385 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10140/20000] Train-Loss: 0.45533488672811506 Test-Loss: 0.14435141991814654 Train-f1: 0.4767655502392344 Test-f1: 0.47043478260869565 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10141/20000] Train-Loss: 0.4615081527135984 Test-Loss: 0.14287272270934745 Train-f1: 0.4781640245223235 Test-f1: 0.43932367149758456 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10142/20000] Train-Loss: 0.4583498191282962 Test-Loss: 0.14384455516208725 Train-f1: 0.4808230403917736 Test-f1: 0.4385580319084156 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10143/20000] Train-Loss: 0.45542867413432087 Test-Loss: 0.14572039111285404 Train-f1: 0.48679821050153055 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10144/20000] Train-Loss: 0.4529530572538854 Test-Loss: 0.14413141293880283 Train-f1: 0.47171171714354976 Test-f1: 0.4503312334891282 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [10145/20000] Train-Loss: 0.4568843310086229 Test-Loss: 0.14551084083432736 Train-f1: 0.4791341991341991 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10146/20000] Train-Loss: 0.45173389420286963 Test-Loss: 0.14680923482193448 Train-f1: 0.48577424858246776 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10147/20000] Train-Loss: 0.4516343117223003 Test-Loss: 0.1485762897203607 Train-f1: 0.47782523752855754 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10148/20000] Train-Loss: 0.4566334155560538 Test-Loss: 0.14303453274013492 Train-f1: 0.48388278388278383 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10149/20000] Train-Loss: 0.45601172786620986 Test-Loss: 0.14503081142743832 Train-f1: 0.4832339874019035 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10150/20000] Train-Loss: 0.45771885759767444 Test-Loss: 0.14681669834746175 Train-f1: 0.48388278388278383 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10151/20000] Train-Loss: 0.45246632271045634 Test-Loss: 0.14334824851430378 Train-f1: 0.47635466157570505 Test-f1: 0.42857142857142855 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10152/20000] Train-Loss: 0.4574996537479239 Test-Loss: 0.1424409669915451 Train-f1: 0.4947816528229348 Test-f1: 0.4497330282227307 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [10153/20000] Train-Loss: 0.45511859255515474 Test-Loss: 0.14294362593351692 Train-f1: 0.48423723035798255 Test-f1: 0.42857142857142855 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10154/20000] Train-Loss: 0.4560837773790936 Test-Loss: 0.14372606250503153 Train-f1: 0.47342124542124536 Test-f1: 0.43193277310924366 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [10155/20000] Train-Loss: 0.45511936753463855 Test-Loss: 0.1446823281082111 Train-f1: 0.48369109441848546 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10156/20000] Train-Loss: 0.45325815008299697 Test-Loss: 0.14384237410795797 Train-f1: 0.4808230403917736 Test-f1: 0.43193277310924366 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10157/20000] Train-Loss: 0.4576337268045712 Test-Loss: 0.14826871482368723 Train-f1: 0.49585409449637813 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10158/20000] Train-Loss: 0.45453216849718214 Test-Loss: 0.1468535234946484 Train-f1: 0.47880171886442396 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10159/20000] Train-Loss: 0.44743550867442794 Test-Loss: 0.1565647090823522 Train-f1: 0.4781953183991946 Test-f1: 0.4495524296675192 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10160/20000] Train-Loss: 0.45629819961945617 Test-Loss: 0.14303001426259315 Train-f1: 0.4791341991341991 Test-f1: 0.4395848595848596 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10161/20000] Train-Loss: 0.452888186679851 Test-Loss: 0.14873285278349915 Train-f1: 0.48269513647147655 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10162/20000] Train-Loss: 0.456478301290916 Test-Loss: 0.14474233573953282 Train-f1: 0.47939712918660293 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10163/20000] Train-Loss: 0.45652400973031343 Test-Loss: 0.1457492035052262 Train-f1: 0.4829598387619377 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10164/20000] Train-Loss: 0.45403845981873453 Test-Loss: 0.14413168675810534 Train-f1: 0.48099039152688505 Test-f1: 0.4395848595848596 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10165/20000] Train-Loss: 0.4553711372305081 Test-Loss: 0.1440595877735415 Train-f1: 0.4791341991341991 Test-f1: 0.43932367149758456 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10166/20000] Train-Loss: 0.45500150394873057 Test-Loss: 0.14519379926213877 Train-f1: 0.47782875860734747 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10167/20000] Train-Loss: 0.45640121119536853 Test-Loss: 0.1421051579062608 Train-f1: 0.4723400853872108 Test-f1: 0.46016451233842537 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [10168/20000] Train-Loss: 0.4537516556339096 Test-Loss: 0.14816708159028114 Train-f1: 0.491212559778505 Test-f1: 0.4624973437035148 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [10169/20000] Train-Loss: 0.4536143106618884 Test-Loss: 0.1501599257069857 Train-f1: 0.48446753246753244 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10170/20000] Train-Loss: 0.45985139676984604 Test-Loss: 0.14394012220808872 Train-f1: 0.47961904761904767 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10171/20000] Train-Loss: 0.45196425105657545 Test-Loss: 0.145564661548329 Train-f1: 0.4785883470527108 Test-f1: 0.4423271037905184 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10172/20000] Train-Loss: 0.45425875216950123 Test-Loss: 0.14466088400968774 Train-f1: 0.4787488980537207 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10173/20000] Train-Loss: 0.4555697113028327 Test-Loss: 0.1443831873220039 Train-f1: 0.4813658214285265 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10174/20000] Train-Loss: 0.4579544686304507 Test-Loss: 0.1421869259834343 Train-f1: 0.4839299239926292 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10175/20000] Train-Loss: 0.4553550126638834 Test-Loss: 0.14240118776146984 Train-f1: 0.47583257203510365 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10176/20000] Train-Loss: 0.4552070593071773 Test-Loss: 0.1439409357185344 Train-f1: 0.4946261674907778 Test-f1: 0.45233400741489715 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [10177/20000] Train-Loss: 0.4561207200486312 Test-Loss: 0.1428284804227834 Train-f1: 0.4867994707994708 Test-f1: 0.42846033429278385 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10178/20000] Train-Loss: 0.45588239015991605 Test-Loss: 0.14302574332462945 Train-f1: 0.48917859031512095 Test-f1: 0.42846033429278385 Train-acc: 0.8178807947019867 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [10179/20000] Train-Loss: 0.453427933022234 Test-Loss: 0.1419129572782411 Train-f1: 0.48163982776058 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10180/20000] Train-Loss: 0.4518587637374187 Test-Loss: 0.15315293093439894 Train-f1: 0.47426309189326477 Test-f1: 0.4603698150924537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10181/20000] Train-Loss: 0.4526574356527096 Test-Loss: 0.14663968114205725 Train-f1: 0.4765071222495914 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10182/20000] Train-Loss: 0.45302780847795804 Test-Loss: 0.14834335570374102 Train-f1: 0.4819024998721142 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10183/20000] Train-Loss: 0.45029892897534324 Test-Loss: 0.14227165558673452 Train-f1: 0.4931512354543017 Test-f1: 0.47043478260869565 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [10184/20000] Train-Loss: 0.45109924978347565 Test-Loss: 0.15021478689014334 Train-f1: 0.49080662831496163 Test-f1: 0.4594670406732118 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [10185/20000] Train-Loss: 0.4597693061262128 Test-Loss: 0.14289843760068266 Train-f1: 0.47675464671645845 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10186/20000] Train-Loss: 0.45439367974406 Test-Loss: 0.14464604538889578 Train-f1: 0.4830950972070194 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10187/20000] Train-Loss: 0.4538529967897294 Test-Loss: 0.1422570170702397 Train-f1: 0.4767030426913445 Test-f1: 0.4594011008433066 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10188/20000] Train-Loss: 0.4503817749370302 Test-Loss: 0.150754570981608 Train-f1: 0.47985825703885243 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10189/20000] Train-Loss: 0.45330675811664217 Test-Loss: 0.14818794521385129 Train-f1: 0.47971807442006115 Test-f1: 0.4594670406732118 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10190/20000] Train-Loss: 0.4536814067519836 Test-Loss: 0.14882722109917867 Train-f1: 0.47899082113806346 Test-f1: 0.4624973437035148 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10191/20000] Train-Loss: 0.4551603957917955 Test-Loss: 0.14683925987454086 Train-f1: 0.4777589277226781 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10192/20000] Train-Loss: 0.45244736641587185 Test-Loss: 0.14703743737438987 Train-f1: 0.4786812576701373 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10193/20000] Train-Loss: 0.45814519693428335 Test-Loss: 0.14536762945484694 Train-f1: 0.4791387011969598 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10194/20000] Train-Loss: 0.4576791790972542 Test-Loss: 0.1453774623510608 Train-f1: 0.47882996282996276 Test-f1: 0.42567287784679086 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10195/20000] Train-Loss: 0.4470053462847675 Test-Loss: 0.1514161825106563 Train-f1: 0.48818350459501714 Test-f1: 0.4652641421224871 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10196/20000] Train-Loss: 0.4562281162397182 Test-Loss: 0.14397191639280496 Train-f1: 0.48121611721611723 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10197/20000] Train-Loss: 0.4522274418047741 Test-Loss: 0.14374340409051514 Train-f1: 0.47953435272604195 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10198/20000] Train-Loss: 0.4489252219969007 Test-Loss: 0.156473167751579 Train-f1: 0.4830950972070194 Test-f1: 0.46198757763975157 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10199/20000] Train-Loss: 0.45645044108548044 Test-Loss: 0.15113045038252457 Train-f1: 0.4878878497468177 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10200/20000] Train-Loss: 0.4565573865230681 Test-Loss: 0.14563723757918853 Train-f1: 0.4784497607655503 Test-f1: 0.45675057208237985 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10201/20000] Train-Loss: 0.45732677502747365 Test-Loss: 0.14197887082072452 Train-f1: 0.4757349422290476 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10202/20000] Train-Loss: 0.4541952334723101 Test-Loss: 0.1420255524371468 Train-f1: 0.4845050582661032 Test-f1: 0.4714285714285714 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10203/20000] Train-Loss: 0.4531325902012381 Test-Loss: 0.14768809792956022 Train-f1: 0.48409676809676816 Test-f1: 0.42567287784679086 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [10204/20000] Train-Loss: 0.45552100367894155 Test-Loss: 0.14181677527170578 Train-f1: 0.4791387011969598 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10205/20000] Train-Loss: 0.4519899068933095 Test-Loss: 0.14189147560532 Train-f1: 0.48405038518691584 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10206/20000] Train-Loss: 0.45074276702619814 Test-Loss: 0.14723246686345182 Train-f1: 0.47553452255507045 Test-f1: 0.5315492957746479 Train-acc: 0.8013245033112583 Test-acc: 0.8026315789473685\n",
      "Epoch [10207/20000] Train-Loss: 0.45075850146547103 Test-Loss: 0.15460183933252347 Train-f1: 0.4747519797275895 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10208/20000] Train-Loss: 0.45259781301771945 Test-Loss: 0.14634677996102333 Train-f1: 0.48245121927217616 Test-f1: 0.45675057208237985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10209/20000] Train-Loss: 0.4546077661388331 Test-Loss: 0.14237307507846222 Train-f1: 0.48155089867357354 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10210/20000] Train-Loss: 0.4517641324629103 Test-Loss: 0.14361521778272737 Train-f1: 0.4914028580115536 Test-f1: 0.4423271037905184 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [10211/20000] Train-Loss: 0.45798654991184945 Test-Loss: 0.1438066632998351 Train-f1: 0.48388278388278383 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10212/20000] Train-Loss: 0.45649865455298766 Test-Loss: 0.14200985764993654 Train-f1: 0.4787461431255492 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10213/20000] Train-Loss: 0.4536966529700841 Test-Loss: 0.14341265733437059 Train-f1: 0.48423723035798255 Test-f1: 0.5297402597402597 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [10214/20000] Train-Loss: 0.45194289735491017 Test-Loss: 0.14219478241053102 Train-f1: 0.48437259138521405 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10215/20000] Train-Loss: 0.4573852702583185 Test-Loss: 0.142646031413738 Train-f1: 0.4783300366252018 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10216/20000] Train-Loss: 0.45392064982328806 Test-Loss: 0.14487232845132741 Train-f1: 0.4896623376623376 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10217/20000] Train-Loss: 0.4543145260296855 Test-Loss: 0.14538151964570872 Train-f1: 0.47553452255507045 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10218/20000] Train-Loss: 0.44525066923047746 Test-Loss: 0.15219451512264837 Train-f1: 0.48568715235381904 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10219/20000] Train-Loss: 0.45449155960779297 Test-Loss: 0.14194041240139407 Train-f1: 0.4935996826816561 Test-f1: 0.46016451233842537 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [10220/20000] Train-Loss: 0.4499588600068268 Test-Loss: 0.15109731666863718 Train-f1: 0.47983884728410275 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10221/20000] Train-Loss: 0.45521538393541683 Test-Loss: 0.14538094548574176 Train-f1: 0.4766007326007326 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10222/20000] Train-Loss: 0.454016298758283 Test-Loss: 0.14189246296088295 Train-f1: 0.4818734793187348 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10223/20000] Train-Loss: 0.4504216678261461 Test-Loss: 0.14915639276604004 Train-f1: 0.5009276509653868 Test-f1: 0.4594670406732118 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10224/20000] Train-Loss: 0.4546484230914067 Test-Loss: 0.1567405637443249 Train-f1: 0.47619550835413965 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10225/20000] Train-Loss: 0.45310281297712046 Test-Loss: 0.14675463057321347 Train-f1: 0.4835061147973253 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10226/20000] Train-Loss: 0.44680559830992544 Test-Loss: 0.1430974083454149 Train-f1: 0.4746616090633863 Test-f1: 0.460952380952381 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [10227/20000] Train-Loss: 0.4554561738002503 Test-Loss: 0.14187350283777342 Train-f1: 0.4709990549990549 Test-f1: 0.4714285714285714 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [10228/20000] Train-Loss: 0.4539497398007998 Test-Loss: 0.14219408294501007 Train-f1: 0.4890267824771032 Test-f1: 0.46016451233842537 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10229/20000] Train-Loss: 0.4515278637101349 Test-Loss: 0.14284333702545973 Train-f1: 0.49418483526066403 Test-f1: 0.42846033429278385 Train-acc: 0.8245033112582781 Test-acc: 0.7368421052631579\n",
      "Epoch [10230/20000] Train-Loss: 0.454208601900655 Test-Loss: 0.14632210493511444 Train-f1: 0.48124056792632974 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10231/20000] Train-Loss: 0.45056267826796514 Test-Loss: 0.14401122367163835 Train-f1: 0.48601397460986495 Test-f1: 0.4423271037905184 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [10232/20000] Train-Loss: 0.45428982572395893 Test-Loss: 0.14458843470933458 Train-f1: 0.4861515151515151 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10233/20000] Train-Loss: 0.45118146897127726 Test-Loss: 0.14687448330156588 Train-f1: 0.48122376531046546 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10234/20000] Train-Loss: 0.45370904952164 Test-Loss: 0.14969891705528282 Train-f1: 0.4813658214285265 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10235/20000] Train-Loss: 0.4501071163532648 Test-Loss: 0.14237684911377776 Train-f1: 0.48526428391643595 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10236/20000] Train-Loss: 0.4545283166697605 Test-Loss: 0.1455591555764248 Train-f1: 0.48269513647147655 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10237/20000] Train-Loss: 0.44926572028838496 Test-Loss: 0.15048347459109931 Train-f1: 0.4902828599971457 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10238/20000] Train-Loss: 0.45307164560601254 Test-Loss: 0.14420371166224943 Train-f1: 0.48412727623253937 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10239/20000] Train-Loss: 0.45695439622285033 Test-Loss: 0.14565151465372503 Train-f1: 0.47589716030702645 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10240/20000] Train-Loss: 0.450130380418705 Test-Loss: 0.14666394232042673 Train-f1: 0.47674977378556616 Test-f1: 0.5056027456027457 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10241/20000] Train-Loss: 0.4553217362705888 Test-Loss: 0.1489415359517249 Train-f1: 0.5047353315051847 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10242/20000] Train-Loss: 0.4558022265945919 Test-Loss: 0.14727644554231614 Train-f1: 0.48908929385939015 Test-f1: 0.4624973437035148 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10243/20000] Train-Loss: 0.4564022726887772 Test-Loss: 0.1425599205097242 Train-f1: 0.4917919965620928 Test-f1: 0.4395848595848596 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [10244/20000] Train-Loss: 0.45751374027702413 Test-Loss: 0.14183739958584804 Train-f1: 0.4712611642172556 Test-f1: 0.460952380952381 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [10245/20000] Train-Loss: 0.45385137046353224 Test-Loss: 0.14589112632815102 Train-f1: 0.47333787364937113 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10246/20000] Train-Loss: 0.45365035896252565 Test-Loss: 0.14275292885494167 Train-f1: 0.4767655502392344 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10247/20000] Train-Loss: 0.4530056023469107 Test-Loss: 0.148811564418907 Train-f1: 0.48865151515151517 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10248/20000] Train-Loss: 0.4550625245316205 Test-Loss: 0.14426366100830054 Train-f1: 0.4835061147973253 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10249/20000] Train-Loss: 0.4559907171023835 Test-Loss: 0.14198529515239314 Train-f1: 0.48105112506057585 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10250/20000] Train-Loss: 0.4537714092516267 Test-Loss: 0.14296276388333734 Train-f1: 0.4792591002337675 Test-f1: 0.42567287784679086 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10251/20000] Train-Loss: 0.4445620550612992 Test-Loss: 0.14229748089966648 Train-f1: 0.47616549796425894 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10252/20000] Train-Loss: 0.45083377803947433 Test-Loss: 0.14199542453943848 Train-f1: 0.4661733447001989 Test-f1: 0.46016451233842537 Train-acc: 0.7880794701986755 Test-acc: 0.7763157894736842\n",
      "Epoch [10253/20000] Train-Loss: 0.4508205823206478 Test-Loss: 0.1494531351163859 Train-f1: 0.4805345225550705 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10254/20000] Train-Loss: 0.4545469271575674 Test-Loss: 0.1424040674985273 Train-f1: 0.4814648771681972 Test-f1: 0.43932367149758456 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10255/20000] Train-Loss: 0.4529113096985947 Test-Loss: 0.14204734765065788 Train-f1: 0.4842347195288371 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10256/20000] Train-Loss: 0.4490081596489554 Test-Loss: 0.1476301940702294 Train-f1: 0.491212559778505 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10257/20000] Train-Loss: 0.45038587566476745 Test-Loss: 0.14221554447382165 Train-f1: 0.4863930564071512 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [10258/20000] Train-Loss: 0.45216031288747177 Test-Loss: 0.1416231732873244 Train-f1: 0.48418119237084756 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [10259/20000] Train-Loss: 0.45343885995551714 Test-Loss: 0.14158457426525337 Train-f1: 0.4808737373737374 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10260/20000] Train-Loss: 0.45349761289783685 Test-Loss: 0.1458556323698424 Train-f1: 0.48201863292157726 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10261/20000] Train-Loss: 0.4516412252616373 Test-Loss: 0.14337554748013268 Train-f1: 0.48383882045605053 Test-f1: 0.4395848595848596 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10262/20000] Train-Loss: 0.45847670766202225 Test-Loss: 0.14370834866216461 Train-f1: 0.4788842320069069 Test-f1: 0.44854612862701837 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10263/20000] Train-Loss: 0.4542200739530343 Test-Loss: 0.14229335748593783 Train-f1: 0.4813951497480658 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10264/20000] Train-Loss: 0.4551584639211214 Test-Loss: 0.14199770533357345 Train-f1: 0.48929905150940767 Test-f1: 0.4395848595848596 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [10265/20000] Train-Loss: 0.4545938524406208 Test-Loss: 0.1417407722747766 Train-f1: 0.4758856443500081 Test-f1: 0.43932367149758456 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [10266/20000] Train-Loss: 0.45131757772194836 Test-Loss: 0.14333567251046245 Train-f1: 0.48245121927217616 Test-f1: 0.4714285714285714 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10267/20000] Train-Loss: 0.4530820045000013 Test-Loss: 0.1416991870120869 Train-f1: 0.4847938992574754 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10268/20000] Train-Loss: 0.450336803169636 Test-Loss: 0.1444731262935928 Train-f1: 0.4812379774405091 Test-f1: 0.5183710801393728 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10269/20000] Train-Loss: 0.44849574276291465 Test-Loss: 0.14792056476500018 Train-f1: 0.47538624674454566 Test-f1: 0.4624973437035148 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10270/20000] Train-Loss: 0.4516007879048735 Test-Loss: 0.14443283486412464 Train-f1: 0.4868850794150642 Test-f1: 0.4385580319084156 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [10271/20000] Train-Loss: 0.4497150456510486 Test-Loss: 0.14582393974261743 Train-f1: 0.48568715235381904 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10272/20000] Train-Loss: 0.45348328065194854 Test-Loss: 0.14276626973644013 Train-f1: 0.48369109441848546 Test-f1: 0.4318216788305991 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10273/20000] Train-Loss: 0.4501216924540032 Test-Loss: 0.14241364485595778 Train-f1: 0.4824457532597032 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10274/20000] Train-Loss: 0.45020160860429753 Test-Loss: 0.14553723647367536 Train-f1: 0.48269599014124565 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10275/20000] Train-Loss: 0.454420875993327 Test-Loss: 0.14260426721356173 Train-f1: 0.47899082113806346 Test-f1: 0.4318216788305991 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [10276/20000] Train-Loss: 0.4509767087650415 Test-Loss: 0.14134630141620705 Train-f1: 0.4810615604374112 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10277/20000] Train-Loss: 0.4511479928942893 Test-Loss: 0.14550883214187155 Train-f1: 0.4857184594515929 Test-f1: 0.43193277310924366 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10278/20000] Train-Loss: 0.45616257056194925 Test-Loss: 0.1419972755873519 Train-f1: 0.4890249475009444 Test-f1: 0.46016451233842537 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10279/20000] Train-Loss: 0.4543152971648552 Test-Loss: 0.14604440409834485 Train-f1: 0.48624584082472255 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10280/20000] Train-Loss: 0.4489208620286432 Test-Loss: 0.14905720034629868 Train-f1: 0.48577424858246776 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10281/20000] Train-Loss: 0.4518948331348612 Test-Loss: 0.14236782680373358 Train-f1: 0.47399010754211374 Test-f1: 0.460952380952381 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [10282/20000] Train-Loss: 0.4531526470203626 Test-Loss: 0.14538312531333136 Train-f1: 0.4834539390888498 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10283/20000] Train-Loss: 0.4542574305834332 Test-Loss: 0.14186031391569104 Train-f1: 0.4709702535122832 Test-f1: 0.460952380952381 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [10284/20000] Train-Loss: 0.45565935128599266 Test-Loss: 0.14497238044007843 Train-f1: 0.4837554108800857 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10285/20000] Train-Loss: 0.4455131010404863 Test-Loss: 0.14682779432000037 Train-f1: 0.47740895691788704 Test-f1: 0.4561490683229813 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10286/20000] Train-Loss: 0.45327077161703316 Test-Loss: 0.1526603421670332 Train-f1: 0.4887125597785049 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10287/20000] Train-Loss: 0.45310754188964564 Test-Loss: 0.1462291690774011 Train-f1: 0.4813951497480658 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10288/20000] Train-Loss: 0.45379146467206416 Test-Loss: 0.14209825545752436 Train-f1: 0.4792591002337675 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10289/20000] Train-Loss: 0.4502618272301366 Test-Loss: 0.15413985840551547 Train-f1: 0.4892358883849214 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10290/20000] Train-Loss: 0.45283625053425636 Test-Loss: 0.14254460729698876 Train-f1: 0.47647177384019485 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10291/20000] Train-Loss: 0.4482064825518734 Test-Loss: 0.15816418933383633 Train-f1: 0.48534431422236307 Test-f1: 0.4495524296675192 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10292/20000] Train-Loss: 0.4525434041185954 Test-Loss: 0.14353412406186963 Train-f1: 0.4811294947660527 Test-f1: 0.43193277310924366 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10293/20000] Train-Loss: 0.45419465625965727 Test-Loss: 0.14765777964358892 Train-f1: 0.48101568485268853 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [10294/20000] Train-Loss: 0.452454861883256 Test-Loss: 0.14134235633452627 Train-f1: 0.47976118950131397 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10295/20000] Train-Loss: 0.4539713835756549 Test-Loss: 0.14248859712603665 Train-f1: 0.4842888343830965 Test-f1: 0.46683117900509197 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10296/20000] Train-Loss: 0.45323921320812094 Test-Loss: 0.14259202001387247 Train-f1: 0.48099039152688505 Test-f1: 0.4421230942970073 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10297/20000] Train-Loss: 0.45314941295749844 Test-Loss: 0.14333086282571864 Train-f1: 0.4707901029487342 Test-f1: 0.45233400741489715 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [10298/20000] Train-Loss: 0.45152651200694016 Test-Loss: 0.1414254841700561 Train-f1: 0.4792591002337675 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10299/20000] Train-Loss: 0.4508157820848576 Test-Loss: 0.145725047719846 Train-f1: 0.4785194794909556 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10300/20000] Train-Loss: 0.45365148135623334 Test-Loss: 0.143013658647057 Train-f1: 0.47355305254193214 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10301/20000] Train-Loss: 0.44562325305655415 Test-Loss: 0.14386097498767536 Train-f1: 0.48245121927217616 Test-f1: 0.4594011008433066 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10302/20000] Train-Loss: 0.4523573629893314 Test-Loss: 0.14284575618522558 Train-f1: 0.4725624200076755 Test-f1: 0.4385580319084156 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [10303/20000] Train-Loss: 0.4523445846307932 Test-Loss: 0.14518547793566794 Train-f1: 0.4830950972070194 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10304/20000] Train-Loss: 0.45458501118613603 Test-Loss: 0.1428390257198689 Train-f1: 0.48077424858246776 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10305/20000] Train-Loss: 0.44905826483095945 Test-Loss: 0.142388234630326 Train-f1: 0.4829598387619377 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10306/20000] Train-Loss: 0.44951985986486354 Test-Loss: 0.1424747476022126 Train-f1: 0.49159416368410513 Test-f1: 0.4395204226783174 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [10307/20000] Train-Loss: 0.4525447758283135 Test-Loss: 0.14254484956396252 Train-f1: 0.4785883470527108 Test-f1: 0.42857142857142855 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10308/20000] Train-Loss: 0.4561569112661722 Test-Loss: 0.14399384579404864 Train-f1: 0.4790392253340944 Test-f1: 0.4385580319084156 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10309/20000] Train-Loss: 0.44920486226189965 Test-Loss: 0.15496056597997074 Train-f1: 0.4761135000265434 Test-f1: 0.4590062111801242 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10310/20000] Train-Loss: 0.4530435108990206 Test-Loss: 0.14604774687282268 Train-f1: 0.46911686163299066 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [10311/20000] Train-Loss: 0.45612639335648897 Test-Loss: 0.143590422006555 Train-f1: 0.4818734793187348 Test-f1: 0.43639684222929187 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10312/20000] Train-Loss: 0.4522288686903892 Test-Loss: 0.14654589793554704 Train-f1: 0.4848406831263974 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10313/20000] Train-Loss: 0.4503537428994722 Test-Loss: 0.14239025412515052 Train-f1: 0.4783300366252018 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [10314/20000] Train-Loss: 0.4476451280791901 Test-Loss: 0.1413073412334163 Train-f1: 0.49492640692640694 Test-f1: 0.47043478260869565 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [10315/20000] Train-Loss: 0.4490853440558665 Test-Loss: 0.14127793516483278 Train-f1: 0.48367726546668965 Test-f1: 0.4714285714285714 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10316/20000] Train-Loss: 0.4547452388875196 Test-Loss: 0.14190438107736664 Train-f1: 0.4763035868456165 Test-f1: 0.4594011008433066 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10317/20000] Train-Loss: 0.45127162953329086 Test-Loss: 0.14184797541481664 Train-f1: 0.48624584082472255 Test-f1: 0.42846033429278385 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10318/20000] Train-Loss: 0.45279149176906924 Test-Loss: 0.14250504002197603 Train-f1: 0.47535962280487826 Test-f1: 0.4421230942970073 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [10319/20000] Train-Loss: 0.45100321974637964 Test-Loss: 0.1415825537782746 Train-f1: 0.4817702801443282 Test-f1: 0.43932367149758456 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10320/20000] Train-Loss: 0.4525991724445462 Test-Loss: 0.14473102102351765 Train-f1: 0.48595393908884976 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10321/20000] Train-Loss: 0.4515252919183141 Test-Loss: 0.1419080388313956 Train-f1: 0.4863038590655703 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10322/20000] Train-Loss: 0.45044825658458926 Test-Loss: 0.1440060766483439 Train-f1: 0.4866144877510184 Test-f1: 0.43639684222929187 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [10323/20000] Train-Loss: 0.45115751759038925 Test-Loss: 0.1413984695063513 Train-f1: 0.4790059613589025 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10324/20000] Train-Loss: 0.4507678735889074 Test-Loss: 0.15448527757075892 Train-f1: 0.482469831896757 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10325/20000] Train-Loss: 0.4480200708855373 Test-Loss: 0.14963392608300782 Train-f1: 0.4808737373737374 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10326/20000] Train-Loss: 0.4535962174028 Test-Loss: 0.14504037544644705 Train-f1: 0.4835827706301961 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10327/20000] Train-Loss: 0.44801012013313407 Test-Loss: 0.14445114468717757 Train-f1: 0.4727419158023185 Test-f1: 0.4526190476190476 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [10328/20000] Train-Loss: 0.45313618454509 Test-Loss: 0.14474774664699244 Train-f1: 0.4757983910555815 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10329/20000] Train-Loss: 0.45310058906058776 Test-Loss: 0.14427072585044243 Train-f1: 0.48746220222730285 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10330/20000] Train-Loss: 0.4450386067881226 Test-Loss: 0.1534421742506618 Train-f1: 0.47366395099153713 Test-f1: 0.4603698150924537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [10331/20000] Train-Loss: 0.45406366858457614 Test-Loss: 0.1462956973764102 Train-f1: 0.4819494519112637 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10332/20000] Train-Loss: 0.45285072340785665 Test-Loss: 0.1411552615480285 Train-f1: 0.4771894966347522 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10333/20000] Train-Loss: 0.4526866692393685 Test-Loss: 0.14604865303348752 Train-f1: 0.48099039152688505 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10334/20000] Train-Loss: 0.44991750168627576 Test-Loss: 0.142816088057074 Train-f1: 0.4783642176047239 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10335/20000] Train-Loss: 0.45302371512966916 Test-Loss: 0.14382640624083404 Train-f1: 0.4786520146520147 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10336/20000] Train-Loss: 0.450777673884551 Test-Loss: 0.1468487406366662 Train-f1: 0.491212559778505 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10337/20000] Train-Loss: 0.44795096779214244 Test-Loss: 0.14406263151763246 Train-f1: 0.48397579490935616 Test-f1: 0.518795518207283 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10338/20000] Train-Loss: 0.4529512727162222 Test-Loss: 0.1430151894244177 Train-f1: 0.475952393713448 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10339/20000] Train-Loss: 0.44700029010538 Test-Loss: 0.14279002239668395 Train-f1: 0.48200105593637765 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10340/20000] Train-Loss: 0.45222745625027694 Test-Loss: 0.15261072339785475 Train-f1: 0.48093247372883036 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10341/20000] Train-Loss: 0.4502431428583604 Test-Loss: 0.143433136965319 Train-f1: 0.505667172333839 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10342/20000] Train-Loss: 0.4512097555401999 Test-Loss: 0.14201406435803623 Train-f1: 0.4801188354129531 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10343/20000] Train-Loss: 0.45245498485045077 Test-Loss: 0.1424493702141925 Train-f1: 0.4832339874019035 Test-f1: 0.4395204226783174 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10344/20000] Train-Loss: 0.4505933314928691 Test-Loss: 0.1453499979572153 Train-f1: 0.480648496572013 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10345/20000] Train-Loss: 0.4558172308019439 Test-Loss: 0.14199042370964424 Train-f1: 0.4830076784529339 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10346/20000] Train-Loss: 0.44668641547346144 Test-Loss: 0.1529987844436415 Train-f1: 0.4783642176047239 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10347/20000] Train-Loss: 0.4470866201216164 Test-Loss: 0.1506130632759478 Train-f1: 0.4790392253340944 Test-f1: 0.4594670406732118 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10348/20000] Train-Loss: 0.45153336610420897 Test-Loss: 0.1423930836986082 Train-f1: 0.47363692017894987 Test-f1: 0.42846033429278385 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [10349/20000] Train-Loss: 0.4526903205951252 Test-Loss: 0.14508844985340583 Train-f1: 0.4971118203960021 Test-f1: 0.45675057208237985 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [10350/20000] Train-Loss: 0.4498620697000475 Test-Loss: 0.14421825904826469 Train-f1: 0.4861515151515151 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10351/20000] Train-Loss: 0.44991660669495087 Test-Loss: 0.14164511614303943 Train-f1: 0.47663911416463034 Test-f1: 0.460952380952381 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10352/20000] Train-Loss: 0.4474767978098846 Test-Loss: 0.1459971806099714 Train-f1: 0.4853203020499482 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10353/20000] Train-Loss: 0.45364423818417016 Test-Loss: 0.14345160824729664 Train-f1: 0.48159570154095704 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10354/20000] Train-Loss: 0.45140012550280295 Test-Loss: 0.14193490726082916 Train-f1: 0.47150823749653936 Test-f1: 0.42846033429278385 Train-acc: 0.7947019867549668 Test-acc: 0.7368421052631579\n",
      "Epoch [10355/20000] Train-Loss: 0.44884111322128534 Test-Loss: 0.14621047696455386 Train-f1: 0.4890267824771032 Test-f1: 0.44649416182596957 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10356/20000] Train-Loss: 0.44319147916449575 Test-Loss: 0.1440937218012687 Train-f1: 0.4822546296998852 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10357/20000] Train-Loss: 0.45006504843574374 Test-Loss: 0.1424152333737162 Train-f1: 0.4835061147973253 Test-f1: 0.4423271037905184 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10358/20000] Train-Loss: 0.44977073285564884 Test-Loss: 0.1418513960486441 Train-f1: 0.47351649134174234 Test-f1: 0.42846033429278385 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [10359/20000] Train-Loss: 0.4491008324763009 Test-Loss: 0.14346835344208597 Train-f1: 0.4855357391764496 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10360/20000] Train-Loss: 0.452321570341966 Test-Loss: 0.14480425949218323 Train-f1: 0.48539988116458704 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10361/20000] Train-Loss: 0.4470486718133046 Test-Loss: 0.1417546332920076 Train-f1: 0.49159416368410513 Test-f1: 0.4395848595848596 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [10362/20000] Train-Loss: 0.4512548229949228 Test-Loss: 0.14468135789252173 Train-f1: 0.48219127317146687 Test-f1: 0.43639684222929187 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10363/20000] Train-Loss: 0.4514593349572102 Test-Loss: 0.14718309895594403 Train-f1: 0.48077424858246776 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10364/20000] Train-Loss: 0.4510694102684565 Test-Loss: 0.1452187728171443 Train-f1: 0.47667435788066764 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10365/20000] Train-Loss: 0.4479474800416804 Test-Loss: 0.1421283982749601 Train-f1: 0.48106692030742665 Test-f1: 0.44854612862701837 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10366/20000] Train-Loss: 0.4458277761729086 Test-Loss: 0.14347518954398578 Train-f1: 0.49232900432900434 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10367/20000] Train-Loss: 0.45007684481369975 Test-Loss: 0.14092751853456137 Train-f1: 0.4791387011969598 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10368/20000] Train-Loss: 0.4463882409301916 Test-Loss: 0.14509808266969917 Train-f1: 0.49117299622694743 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10369/20000] Train-Loss: 0.4506706784061916 Test-Loss: 0.14659326633336245 Train-f1: 0.48377221349595 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10370/20000] Train-Loss: 0.4524577322969477 Test-Loss: 0.1413508529079221 Train-f1: 0.4788842320069069 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10371/20000] Train-Loss: 0.450639682590838 Test-Loss: 0.14285126199213766 Train-f1: 0.47825066549948775 Test-f1: 0.43193277310924366 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10372/20000] Train-Loss: 0.44941298052385314 Test-Loss: 0.14318334050362586 Train-f1: 0.4805959595959596 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10373/20000] Train-Loss: 0.44957393962558323 Test-Loss: 0.1419137658949004 Train-f1: 0.4817919462239238 Test-f1: 0.43932367149758456 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10374/20000] Train-Loss: 0.4502523651479803 Test-Loss: 0.14465026753754037 Train-f1: 0.474808450452559 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10375/20000] Train-Loss: 0.4529529470613691 Test-Loss: 0.14162560298768298 Train-f1: 0.47965613668547896 Test-f1: 0.43932367149758456 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10376/20000] Train-Loss: 0.4480594954558064 Test-Loss: 0.14396491345781984 Train-f1: 0.4781336945263325 Test-f1: 0.44558629776021075 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10377/20000] Train-Loss: 0.4455845570983432 Test-Loss: 0.1417287252774693 Train-f1: 0.48239440719200843 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10378/20000] Train-Loss: 0.4490222795654749 Test-Loss: 0.14523029174200958 Train-f1: 0.47280070936568314 Test-f1: 0.46683117900509197 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [10379/20000] Train-Loss: 0.45197560129216974 Test-Loss: 0.14133409229341995 Train-f1: 0.4799691536533642 Test-f1: 0.43932367149758456 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10380/20000] Train-Loss: 0.44580733600185113 Test-Loss: 0.14164239784817456 Train-f1: 0.48033959627019146 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10381/20000] Train-Loss: 0.45634170194773915 Test-Loss: 0.14115663696784028 Train-f1: 0.48099039152688505 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10382/20000] Train-Loss: 0.45201888753996106 Test-Loss: 0.14798622754389257 Train-f1: 0.4835827706301961 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10383/20000] Train-Loss: 0.4513480356805203 Test-Loss: 0.14470977131399762 Train-f1: 0.48679821050153055 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10384/20000] Train-Loss: 0.4502660371294967 Test-Loss: 0.1414697423088623 Train-f1: 0.5004008337341671 Test-f1: 0.43932367149758456 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10385/20000] Train-Loss: 0.4481123953541469 Test-Loss: 0.14807691933358305 Train-f1: 0.4857527421453801 Test-f1: 0.4652641421224871 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10386/20000] Train-Loss: 0.4525241194517421 Test-Loss: 0.1426453430021694 Train-f1: 0.48331708978378246 Test-f1: 0.44854612862701837 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10387/20000] Train-Loss: 0.4512861940322648 Test-Loss: 0.1409534244576982 Train-f1: 0.4763934185406608 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10388/20000] Train-Loss: 0.45154844963438845 Test-Loss: 0.14978516457374533 Train-f1: 0.4863865911566875 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10389/20000] Train-Loss: 0.4524830249405381 Test-Loss: 0.14681024673211168 Train-f1: 0.48163982776058 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10390/20000] Train-Loss: 0.45061218436708406 Test-Loss: 0.14296536935584822 Train-f1: 0.4767655502392344 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10391/20000] Train-Loss: 0.4477797099160639 Test-Loss: 0.14774713801365924 Train-f1: 0.4804402387735721 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10392/20000] Train-Loss: 0.45594408581204976 Test-Loss: 0.1417809213875131 Train-f1: 0.47662752128639907 Test-f1: 0.47043478260869565 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10393/20000] Train-Loss: 0.4465831684405595 Test-Loss: 0.1423672637805021 Train-f1: 0.4814936420519259 Test-f1: 0.4421230942970073 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10394/20000] Train-Loss: 0.4473640107860115 Test-Loss: 0.144796368684709 Train-f1: 0.4840177571897318 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10395/20000] Train-Loss: 0.4482482328281909 Test-Loss: 0.14991801395155469 Train-f1: 0.4800859672025316 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10396/20000] Train-Loss: 0.4541181405207261 Test-Loss: 0.14357666790330678 Train-f1: 0.46601520618492 Test-f1: 0.45233400741489715 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [10397/20000] Train-Loss: 0.44711445019574453 Test-Loss: 0.1413385167841292 Train-f1: 0.48728186783970606 Test-f1: 0.4390972912712042 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10398/20000] Train-Loss: 0.4505734313480634 Test-Loss: 0.14436307776261817 Train-f1: 0.4750929410164574 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10399/20000] Train-Loss: 0.44745828864814685 Test-Loss: 0.14941869819232126 Train-f1: 0.4813658214285265 Test-f1: 0.4628528560711606 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10400/20000] Train-Loss: 0.4515793305144076 Test-Loss: 0.14115753651227422 Train-f1: 0.48206702770583154 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10401/20000] Train-Loss: 0.45113933075702 Test-Loss: 0.14872963464631475 Train-f1: 0.48839521844916967 Test-f1: 0.44558629776021075 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10402/20000] Train-Loss: 0.4474436100728335 Test-Loss: 0.14085261032643054 Train-f1: 0.4826244640393339 Test-f1: 0.4714285714285714 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10403/20000] Train-Loss: 0.4525490906296442 Test-Loss: 0.14653578159207806 Train-f1: 0.4853203020499482 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10404/20000] Train-Loss: 0.4501308263382916 Test-Loss: 0.14296117750866025 Train-f1: 0.4876134902624969 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10405/20000] Train-Loss: 0.4467445804932136 Test-Loss: 0.1467377672203869 Train-f1: 0.48404594453467825 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10406/20000] Train-Loss: 0.45313725382703923 Test-Loss: 0.14626259668331398 Train-f1: 0.47722533090275016 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10407/20000] Train-Loss: 0.44263701585751636 Test-Loss: 0.14959229188700515 Train-f1: 0.5058907663795001 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10408/20000] Train-Loss: 0.4519906378252729 Test-Loss: 0.14260075694715438 Train-f1: 0.4791341991341991 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10409/20000] Train-Loss: 0.4465746156165389 Test-Loss: 0.1412300312518539 Train-f1: 0.4835074662468394 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10410/20000] Train-Loss: 0.4491184853072834 Test-Loss: 0.14379702403048034 Train-f1: 0.4836272539993388 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10411/20000] Train-Loss: 0.44253654500723943 Test-Loss: 0.1412262701365618 Train-f1: 0.48611334052020955 Test-f1: 0.4594011008433066 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10412/20000] Train-Loss: 0.44994601945767854 Test-Loss: 0.14691050718541035 Train-f1: 0.4845004144274217 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10413/20000] Train-Loss: 0.4566957866742127 Test-Loss: 0.14156443410217764 Train-f1: 0.476087912087912 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10414/20000] Train-Loss: 0.4490411363080453 Test-Loss: 0.14129401224638216 Train-f1: 0.4866144877510184 Test-f1: 0.4395848595848596 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [10415/20000] Train-Loss: 0.4491793537924003 Test-Loss: 0.1493183488332445 Train-f1: 0.4905883482570599 Test-f1: 0.4652641421224871 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [10416/20000] Train-Loss: 0.4536488250072663 Test-Loss: 0.14157984824889702 Train-f1: 0.48731433953378855 Test-f1: 0.42567287784679086 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10417/20000] Train-Loss: 0.448230718150897 Test-Loss: 0.1411004194221081 Train-f1: 0.48047536671224833 Test-f1: 0.4390972912712042 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10418/20000] Train-Loss: 0.44791787125628196 Test-Loss: 0.14295237315980605 Train-f1: 0.4893098080573185 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10419/20000] Train-Loss: 0.4488061426583681 Test-Loss: 0.14483809677287868 Train-f1: 0.49117299622694743 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10420/20000] Train-Loss: 0.44698793045260327 Test-Loss: 0.1481732279024474 Train-f1: 0.4888329141493698 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10421/20000] Train-Loss: 0.4501969324427971 Test-Loss: 0.14437735658559311 Train-f1: 0.4788842320069069 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10422/20000] Train-Loss: 0.4479205395173297 Test-Loss: 0.14841643000985452 Train-f1: 0.4845168789570815 Test-f1: 0.43639684222929187 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10423/20000] Train-Loss: 0.4465157323005228 Test-Loss: 0.14211351512346457 Train-f1: 0.48724746515629674 Test-f1: 0.5297402597402597 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [10424/20000] Train-Loss: 0.44972491928401537 Test-Loss: 0.1420550605615506 Train-f1: 0.47744794862948686 Test-f1: 0.43193277310924366 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10425/20000] Train-Loss: 0.4404664695975568 Test-Loss: 0.15451565036928527 Train-f1: 0.4881562881562882 Test-f1: 0.45740959307580253 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10426/20000] Train-Loss: 0.45295243681532205 Test-Loss: 0.14620075436880617 Train-f1: 0.48492334447051083 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10427/20000] Train-Loss: 0.4470907484732612 Test-Loss: 0.14291162553100117 Train-f1: 0.4892054466931629 Test-f1: 0.5077443609022556 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10428/20000] Train-Loss: 0.4517684045315319 Test-Loss: 0.1516731367783587 Train-f1: 0.47882996282996276 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10429/20000] Train-Loss: 0.4520374492987721 Test-Loss: 0.14794566068419004 Train-f1: 0.4752322739729036 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10430/20000] Train-Loss: 0.44995947910682793 Test-Loss: 0.1424337047463815 Train-f1: 0.48105112506057585 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10431/20000] Train-Loss: 0.45174141516864486 Test-Loss: 0.14181704425137992 Train-f1: 0.47279479652767326 Test-f1: 0.4390972912712042 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [10432/20000] Train-Loss: 0.4505745779095525 Test-Loss: 0.14191710706004826 Train-f1: 0.4786520146520147 Test-f1: 0.4395848595848596 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10433/20000] Train-Loss: 0.4486477066020695 Test-Loss: 0.1447295435581937 Train-f1: 0.477095731770193 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10434/20000] Train-Loss: 0.45051886392385704 Test-Loss: 0.14623263912448356 Train-f1: 0.4865415231587532 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10435/20000] Train-Loss: 0.4493015454194591 Test-Loss: 0.14137084880252665 Train-f1: 0.4738743712427923 Test-f1: 0.4602828448452552 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [10436/20000] Train-Loss: 0.4486925472442948 Test-Loss: 0.14863648683778613 Train-f1: 0.48839521844916967 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10437/20000] Train-Loss: 0.4517180977048264 Test-Loss: 0.14422836412895518 Train-f1: 0.487165885579598 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10438/20000] Train-Loss: 0.4449481472954941 Test-Loss: 0.14555708740750414 Train-f1: 0.4918343870263797 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10439/20000] Train-Loss: 0.44537111779822836 Test-Loss: 0.14643196960987412 Train-f1: 0.4783642176047239 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10440/20000] Train-Loss: 0.4506663919833221 Test-Loss: 0.14961296513839026 Train-f1: 0.48383882045605053 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10441/20000] Train-Loss: 0.45595025447378745 Test-Loss: 0.14097536280573827 Train-f1: 0.4765071222495914 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10442/20000] Train-Loss: 0.4475093934478298 Test-Loss: 0.1464456540960079 Train-f1: 0.48269513647147655 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10443/20000] Train-Loss: 0.44739192320345983 Test-Loss: 0.14290097528779067 Train-f1: 0.48173493173493176 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10444/20000] Train-Loss: 0.4481483273425818 Test-Loss: 0.14358162494544782 Train-f1: 0.4888329141493698 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10445/20000] Train-Loss: 0.44962690208127404 Test-Loss: 0.14143958675943913 Train-f1: 0.48624584082472255 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10446/20000] Train-Loss: 0.45180681189957544 Test-Loss: 0.14175560036762333 Train-f1: 0.48377221349595 Test-f1: 0.42857142857142855 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10447/20000] Train-Loss: 0.44912664606072944 Test-Loss: 0.14511034897577532 Train-f1: 0.487064935064935 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10448/20000] Train-Loss: 0.44960111427874166 Test-Loss: 0.14395719529195708 Train-f1: 0.4787488980537207 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10449/20000] Train-Loss: 0.4486214173130651 Test-Loss: 0.1433030325881548 Train-f1: 0.4868862369804991 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10450/20000] Train-Loss: 0.4445162220475944 Test-Loss: 0.15184442011810914 Train-f1: 0.4904759999263207 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10451/20000] Train-Loss: 0.4529591848913671 Test-Loss: 0.1435438332891175 Train-f1: 0.4776034710946946 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10452/20000] Train-Loss: 0.44560231182440013 Test-Loss: 0.1422197676475734 Train-f1: 0.4876125095947568 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [10453/20000] Train-Loss: 0.44634227296691176 Test-Loss: 0.14084201775084654 Train-f1: 0.48173493173493176 Test-f1: 0.4714285714285714 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10454/20000] Train-Loss: 0.447269895077325 Test-Loss: 0.14756791645868364 Train-f1: 0.4860662783895827 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10455/20000] Train-Loss: 0.44983684495886905 Test-Loss: 0.1420693282689767 Train-f1: 0.4816203404438698 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10456/20000] Train-Loss: 0.4470954284020935 Test-Loss: 0.14107189960007144 Train-f1: 0.4919595008241112 Test-f1: 0.4714285714285714 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [10457/20000] Train-Loss: 0.44714825292777666 Test-Loss: 0.1436342820022235 Train-f1: 0.4802648500379375 Test-f1: 0.4385580319084156 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10458/20000] Train-Loss: 0.4443477457636878 Test-Loss: 0.14205684108756492 Train-f1: 0.4818734793187348 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10459/20000] Train-Loss: 0.45217465808439544 Test-Loss: 0.14284771939315088 Train-f1: 0.46838960835099286 Test-f1: 0.45233400741489715 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [10460/20000] Train-Loss: 0.44654992685283196 Test-Loss: 0.1417045266291641 Train-f1: 0.48025108526886146 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10461/20000] Train-Loss: 0.4481526149742035 Test-Loss: 0.14417544648265304 Train-f1: 0.4909615521909375 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10462/20000] Train-Loss: 0.4454959547299451 Test-Loss: 0.1408920290092979 Train-f1: 0.4893788556628208 Test-f1: 0.47043478260869565 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10463/20000] Train-Loss: 0.44931653098442653 Test-Loss: 0.14114962420204535 Train-f1: 0.4816198704953846 Test-f1: 0.4390972912712042 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10464/20000] Train-Loss: 0.4447887830411596 Test-Loss: 0.1490648202347258 Train-f1: 0.49117299622694743 Test-f1: 0.4624973437035148 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [10465/20000] Train-Loss: 0.44994480303814444 Test-Loss: 0.14565489205642146 Train-f1: 0.48163982776058 Test-f1: 0.4826930070273724 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10466/20000] Train-Loss: 0.4543335223688211 Test-Loss: 0.14536038184372208 Train-f1: 0.48369109441848546 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10467/20000] Train-Loss: 0.4471999901964968 Test-Loss: 0.14342349121513617 Train-f1: 0.4863865911566875 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10468/20000] Train-Loss: 0.44909248260022866 Test-Loss: 0.14383161135395356 Train-f1: 0.48383882045605053 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10469/20000] Train-Loss: 0.44993524704248644 Test-Loss: 0.140559984449794 Train-f1: 0.48662480153187265 Test-f1: 0.4714285714285714 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10470/20000] Train-Loss: 0.44746431139453274 Test-Loss: 0.1463762038443587 Train-f1: 0.4890249475009444 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10471/20000] Train-Loss: 0.4474998164981084 Test-Loss: 0.14267409884017496 Train-f1: 0.49215028833474384 Test-f1: 0.42567287784679086 Train-acc: 0.8211920529801324 Test-acc: 0.7368421052631579\n",
      "Epoch [10472/20000] Train-Loss: 0.4471606137852957 Test-Loss: 0.1410638383145101 Train-f1: 0.48158822373546595 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10473/20000] Train-Loss: 0.44499275109226955 Test-Loss: 0.1423054577015665 Train-f1: 0.4840177571897318 Test-f1: 0.4714285714285714 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10474/20000] Train-Loss: 0.4498337076665579 Test-Loss: 0.14399453723585676 Train-f1: 0.48860565521435084 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10475/20000] Train-Loss: 0.44507161083503355 Test-Loss: 0.14636335380981558 Train-f1: 0.4813658214285265 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10476/20000] Train-Loss: 0.44749650126592505 Test-Loss: 0.14067479085059167 Train-f1: 0.49195366037471305 Test-f1: 0.47043478260869565 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [10477/20000] Train-Loss: 0.44937056956778243 Test-Loss: 0.143048734312913 Train-f1: 0.4821307090988453 Test-f1: 0.42567287784679086 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10478/20000] Train-Loss: 0.44922554330633113 Test-Loss: 0.1427098612672644 Train-f1: 0.4839634913378628 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10479/20000] Train-Loss: 0.44648769779063574 Test-Loss: 0.15087175232560895 Train-f1: 0.4738701298701299 Test-f1: 0.4360203681942812 Train-acc: 0.7980132450331126 Test-acc: 0.75\n",
      "Epoch [10480/20000] Train-Loss: 0.4479256695228337 Test-Loss: 0.1462140879439879 Train-f1: 0.48106692030742665 Test-f1: 0.49428571428571433 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10481/20000] Train-Loss: 0.4462890629536802 Test-Loss: 0.15195109509061044 Train-f1: 0.4842142666560446 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10482/20000] Train-Loss: 0.44818670988495746 Test-Loss: 0.14493765826323035 Train-f1: 0.4814648771681972 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10483/20000] Train-Loss: 0.44821463234243986 Test-Loss: 0.1465589806285213 Train-f1: 0.481867872334565 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10484/20000] Train-Loss: 0.4427876449929529 Test-Loss: 0.1409355138787388 Train-f1: 0.48630126857974953 Test-f1: 0.4714285714285714 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10485/20000] Train-Loss: 0.4411131117395623 Test-Loss: 0.150284856809365 Train-f1: 0.4856174406713919 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10486/20000] Train-Loss: 0.449718811360653 Test-Loss: 0.14088471032450958 Train-f1: 0.4839299239926292 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10487/20000] Train-Loss: 0.45148527440136943 Test-Loss: 0.14499297139828973 Train-f1: 0.484095701540957 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10488/20000] Train-Loss: 0.4432549670732867 Test-Loss: 0.14478850386334297 Train-f1: 0.4840177571897318 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10489/20000] Train-Loss: 0.4461473930705614 Test-Loss: 0.14570438427196455 Train-f1: 0.48839521844916967 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10490/20000] Train-Loss: 0.44842045600675545 Test-Loss: 0.14231553678233932 Train-f1: 0.46838960835099286 Test-f1: 0.4526190476190476 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [10491/20000] Train-Loss: 0.44654729068745913 Test-Loss: 0.1430653041590361 Train-f1: 0.48492334447051083 Test-f1: 0.5077443609022556 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10492/20000] Train-Loss: 0.44957261541537813 Test-Loss: 0.14322553602051447 Train-f1: 0.4748057616142722 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10493/20000] Train-Loss: 0.4480180011736374 Test-Loss: 0.14668073147825986 Train-f1: 0.4800643762512536 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10494/20000] Train-Loss: 0.449140450428806 Test-Loss: 0.14516333519387808 Train-f1: 0.47342124542124536 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10495/20000] Train-Loss: 0.449219004622924 Test-Loss: 0.14342721521682023 Train-f1: 0.4792419003713664 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10496/20000] Train-Loss: 0.4462263532097385 Test-Loss: 0.14247023405301426 Train-f1: 0.4963718021109325 Test-f1: 0.44937417654808953 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [10497/20000] Train-Loss: 0.44712738320268264 Test-Loss: 0.1420370924827487 Train-f1: 0.48180531851542313 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10498/20000] Train-Loss: 0.44590810253088425 Test-Loss: 0.14451748480885399 Train-f1: 0.480648496572013 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10499/20000] Train-Loss: 0.4476617543309559 Test-Loss: 0.1429109038904247 Train-f1: 0.4852059618234115 Test-f1: 0.42567287784679086 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10500/20000] Train-Loss: 0.44664475706410456 Test-Loss: 0.14424746448154757 Train-f1: 0.4787488980537207 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10501/20000] Train-Loss: 0.44756356805262304 Test-Loss: 0.14365304260336934 Train-f1: 0.4767655502392344 Test-f1: 0.4826930070273724 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [10502/20000] Train-Loss: 0.45127820844757455 Test-Loss: 0.14208868682556705 Train-f1: 0.4859107463994802 Test-f1: 0.43193277310924366 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10503/20000] Train-Loss: 0.44831113885737317 Test-Loss: 0.14340004606323645 Train-f1: 0.48202870813397125 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10504/20000] Train-Loss: 0.44454765605920993 Test-Loss: 0.1463626532666609 Train-f1: 0.47627465701999866 Test-f1: 0.4624973437035148 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10505/20000] Train-Loss: 0.451036589216727 Test-Loss: 0.14178961752349223 Train-f1: 0.4786812576701373 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10506/20000] Train-Loss: 0.44509508886104776 Test-Loss: 0.1421984270576926 Train-f1: 0.4888329141493698 Test-f1: 0.4356787406570312 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [10507/20000] Train-Loss: 0.446931638971997 Test-Loss: 0.1424734487374817 Train-f1: 0.4808230403917736 Test-f1: 0.43193277310924366 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10508/20000] Train-Loss: 0.447000426220427 Test-Loss: 0.1447187422004265 Train-f1: 0.481495697285171 Test-f1: 0.42567287784679086 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10509/20000] Train-Loss: 0.44665971869759646 Test-Loss: 0.14517753615581375 Train-f1: 0.4811294947660527 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10510/20000] Train-Loss: 0.44938728446731174 Test-Loss: 0.1420418648726436 Train-f1: 0.4808230403917736 Test-f1: 0.43193277310924366 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10511/20000] Train-Loss: 0.44605854037759773 Test-Loss: 0.1450416404141607 Train-f1: 0.4819024998721142 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10512/20000] Train-Loss: 0.449376874046806 Test-Loss: 0.14057625523475972 Train-f1: 0.4786520146520147 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10513/20000] Train-Loss: 0.4492982999453346 Test-Loss: 0.1439119842456691 Train-f1: 0.480648496572013 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10514/20000] Train-Loss: 0.4465844206451707 Test-Loss: 0.14675216314184877 Train-f1: 0.4757349422290476 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10515/20000] Train-Loss: 0.44361623506369974 Test-Loss: 0.14272615388807047 Train-f1: 0.4850741340960244 Test-f1: 0.4376874738165061 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10516/20000] Train-Loss: 0.4475337027405247 Test-Loss: 0.14231495952720136 Train-f1: 0.4905762278486428 Test-f1: 0.44558629776021075 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10517/20000] Train-Loss: 0.4446891541322159 Test-Loss: 0.14158817886090888 Train-f1: 0.4871194373510706 Test-f1: 0.42846033429278385 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10518/20000] Train-Loss: 0.4441000828747416 Test-Loss: 0.14105985135138385 Train-f1: 0.48171773039322713 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10519/20000] Train-Loss: 0.4433882347634495 Test-Loss: 0.14717195307262332 Train-f1: 0.4783300366252018 Test-f1: 0.4594670406732118 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10520/20000] Train-Loss: 0.4497694045800491 Test-Loss: 0.15018312330107753 Train-f1: 0.4781953183991946 Test-f1: 0.4628528560711606 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10521/20000] Train-Loss: 0.4457815646501742 Test-Loss: 0.14080354061926476 Train-f1: 0.4862746864861863 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10522/20000] Train-Loss: 0.44630312473713707 Test-Loss: 0.14214750876429474 Train-f1: 0.4924842664973612 Test-f1: 0.42846033429278385 Train-acc: 0.8211920529801324 Test-acc: 0.7368421052631579\n",
      "Epoch [10523/20000] Train-Loss: 0.4463347962505705 Test-Loss: 0.14351552799552134 Train-f1: 0.48377221349595 Test-f1: 0.42567287784679086 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10524/20000] Train-Loss: 0.44710279938976943 Test-Loss: 0.14332978801226554 Train-f1: 0.48605985209758795 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10525/20000] Train-Loss: 0.4436699423244601 Test-Loss: 0.14055772684218343 Train-f1: 0.48551586342469505 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10526/20000] Train-Loss: 0.44955522058213726 Test-Loss: 0.1433422526437617 Train-f1: 0.4783642176047239 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10527/20000] Train-Loss: 0.44903723278154534 Test-Loss: 0.14149184125549671 Train-f1: 0.48624584082472255 Test-f1: 0.4318216788305991 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10528/20000] Train-Loss: 0.44755278761733314 Test-Loss: 0.1413611741250502 Train-f1: 0.47604057635207386 Test-f1: 0.43193277310924366 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10529/20000] Train-Loss: 0.44013159490749343 Test-Loss: 0.15143173426928755 Train-f1: 0.48797104029434457 Test-f1: 0.4726570048309179 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10530/20000] Train-Loss: 0.451876977745513 Test-Loss: 0.14342971832421728 Train-f1: 0.4826244640393339 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10531/20000] Train-Loss: 0.44828208143715426 Test-Loss: 0.14762608474333547 Train-f1: 0.48948363957790164 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10532/20000] Train-Loss: 0.4447661876978872 Test-Loss: 0.14858058595930054 Train-f1: 0.4835473304223087 Test-f1: 0.4652641421224871 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10533/20000] Train-Loss: 0.44781542376588235 Test-Loss: 0.14506501708446737 Train-f1: 0.4816203404438698 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10534/20000] Train-Loss: 0.4461982335630649 Test-Loss: 0.1416075129662327 Train-f1: 0.4788842320069069 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10535/20000] Train-Loss: 0.4456494842823073 Test-Loss: 0.14355163249498085 Train-f1: 0.4829093745760412 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10536/20000] Train-Loss: 0.44517027754312816 Test-Loss: 0.14064824301808868 Train-f1: 0.4835985658770469 Test-f1: 0.4714285714285714 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10537/20000] Train-Loss: 0.4456117221842009 Test-Loss: 0.14329752869392254 Train-f1: 0.4833737373737373 Test-f1: 0.42567287784679086 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10538/20000] Train-Loss: 0.4460657915536137 Test-Loss: 0.15035754864786463 Train-f1: 0.48383882045605053 Test-f1: 0.42567287784679086 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10539/20000] Train-Loss: 0.4482573873845023 Test-Loss: 0.15136017105025704 Train-f1: 0.482915862712756 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10540/20000] Train-Loss: 0.4455382027055257 Test-Loss: 0.1419651399091965 Train-f1: 0.48106692030742665 Test-f1: 0.4395204226783174 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10541/20000] Train-Loss: 0.4477428991583978 Test-Loss: 0.14215885878793755 Train-f1: 0.48468545550508246 Test-f1: 0.43193277310924366 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10542/20000] Train-Loss: 0.44655650976599326 Test-Loss: 0.14666946030541325 Train-f1: 0.5200031200973821 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10543/20000] Train-Loss: 0.44826221061726873 Test-Loss: 0.14034467179340832 Train-f1: 0.4783642176047239 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10544/20000] Train-Loss: 0.4478220595946678 Test-Loss: 0.14046083719497476 Train-f1: 0.4834539390888498 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10545/20000] Train-Loss: 0.4475832605372191 Test-Loss: 0.14376377027277779 Train-f1: 0.4800859672025316 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10546/20000] Train-Loss: 0.45263563604115753 Test-Loss: 0.14799242914359367 Train-f1: 0.4821919506563145 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10547/20000] Train-Loss: 0.4506214413405196 Test-Loss: 0.14126687224729553 Train-f1: 0.47913871462560975 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "best score_dx!!\n",
      "Epoch [10548/20000] Train-Loss: 0.44013637825247587 Test-Loss: 0.14020450437073023 Train-f1: 0.47949588601244225 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10549/20000] Train-Loss: 0.4474390924470042 Test-Loss: 0.14474868590786155 Train-f1: 0.49326061604502297 Test-f1: 0.44937417654808953 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [10550/20000] Train-Loss: 0.4469376127610292 Test-Loss: 0.14333786236799617 Train-f1: 0.4831329686217024 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10551/20000] Train-Loss: 0.4466710960751016 Test-Loss: 0.14380461086849147 Train-f1: 0.4818734793187348 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10552/20000] Train-Loss: 0.4447478245225656 Test-Loss: 0.14710390278717866 Train-f1: 0.4904879281792033 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10553/20000] Train-Loss: 0.44589845057382543 Test-Loss: 0.14465087846376587 Train-f1: 0.4868862369804991 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10554/20000] Train-Loss: 0.4459432381508347 Test-Loss: 0.14346033770411878 Train-f1: 0.4808230403917736 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10555/20000] Train-Loss: 0.44522940574953523 Test-Loss: 0.14396025378443797 Train-f1: 0.48405038518691584 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10556/20000] Train-Loss: 0.4452769526877204 Test-Loss: 0.14375303231251912 Train-f1: 0.5035049898838991 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10557/20000] Train-Loss: 0.44398372695348376 Test-Loss: 0.14127323526965432 Train-f1: 0.48601397460986495 Test-f1: 0.42857142857142855 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10558/20000] Train-Loss: 0.4435156218272812 Test-Loss: 0.14991981992169376 Train-f1: 0.4817919462239238 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10559/20000] Train-Loss: 0.44692966473765067 Test-Loss: 0.14407824679667683 Train-f1: 0.481495697285171 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10560/20000] Train-Loss: 0.4515830183916267 Test-Loss: 0.1583256824564069 Train-f1: 0.47848403928306826 Test-f1: 0.4397742127153892 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10561/20000] Train-Loss: 0.45058407146207674 Test-Loss: 0.14223916934014902 Train-f1: 0.4699969523498935 Test-f1: 0.45233400741489715 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [10562/20000] Train-Loss: 0.443308163654465 Test-Loss: 0.14663279336953605 Train-f1: 0.4859107463994802 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10563/20000] Train-Loss: 0.4469878013414628 Test-Loss: 0.14050950272143742 Train-f1: 0.4921326462004183 Test-f1: 0.4497330282227307 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10564/20000] Train-Loss: 0.44483625914079006 Test-Loss: 0.1400929298811751 Train-f1: 0.48223791606550537 Test-f1: 0.4714285714285714 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10565/20000] Train-Loss: 0.445717229049566 Test-Loss: 0.14303480259769616 Train-f1: 0.48595393908884976 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10566/20000] Train-Loss: 0.44288580945925515 Test-Loss: 0.14179089707429052 Train-f1: 0.47830731712133334 Test-f1: 0.43193277310924366 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10567/20000] Train-Loss: 0.4442525862144113 Test-Loss: 0.14155467217000045 Train-f1: 0.5062405770330299 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10568/20000] Train-Loss: 0.44449525998252964 Test-Loss: 0.1421360123405209 Train-f1: 0.4813940653940653 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10569/20000] Train-Loss: 0.4427301711565476 Test-Loss: 0.14053136635600824 Train-f1: 0.48701623817666395 Test-f1: 0.4503312334891282 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10570/20000] Train-Loss: 0.445855828019302 Test-Loss: 0.1440831852522372 Train-f1: 0.4934032017365351 Test-f1: 0.4526190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [10571/20000] Train-Loss: 0.44522433406369893 Test-Loss: 0.14491202703525918 Train-f1: 0.48093247372883036 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10572/20000] Train-Loss: 0.4482662976167997 Test-Loss: 0.14169813897792619 Train-f1: 0.4716527487575403 Test-f1: 0.45984654731457797 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [10573/20000] Train-Loss: 0.4470828360810328 Test-Loss: 0.14028006709433039 Train-f1: 0.48839521844916967 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10574/20000] Train-Loss: 0.4460106547310942 Test-Loss: 0.14003941271612977 Train-f1: 0.4900085804929625 Test-f1: 0.47043478260869565 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10575/20000] Train-Loss: 0.4466514254431955 Test-Loss: 0.14193053052705293 Train-f1: 0.47553452255507045 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10576/20000] Train-Loss: 0.4429145051515966 Test-Loss: 0.14050542348398012 Train-f1: 0.4737772031889679 Test-f1: 0.4602828448452552 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [10577/20000] Train-Loss: 0.446034846076965 Test-Loss: 0.1417109025901626 Train-f1: 0.4855357391764496 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10578/20000] Train-Loss: 0.4451220625760011 Test-Loss: 0.14404738996853447 Train-f1: 0.4866144877510184 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10579/20000] Train-Loss: 0.443606682529363 Test-Loss: 0.14484584067790446 Train-f1: 0.4914663019550357 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10580/20000] Train-Loss: 0.44412884355188076 Test-Loss: 0.14154447074364865 Train-f1: 0.47566151490202124 Test-f1: 0.4318216788305991 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10581/20000] Train-Loss: 0.4452252093942684 Test-Loss: 0.1401640022777029 Train-f1: 0.48318081918081923 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10582/20000] Train-Loss: 0.4436621367909135 Test-Loss: 0.14366580621524253 Train-f1: 0.48492247982389464 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10583/20000] Train-Loss: 0.44682905243785576 Test-Loss: 0.14119806749228656 Train-f1: 0.4781953183991946 Test-f1: 0.4385580319084156 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10584/20000] Train-Loss: 0.4438334599050095 Test-Loss: 0.1439190174146163 Train-f1: 0.48595393908884976 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10585/20000] Train-Loss: 0.4443451376002066 Test-Loss: 0.14107193177108815 Train-f1: 0.4896306172616313 Test-f1: 0.4503312334891282 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10586/20000] Train-Loss: 0.44377196169190564 Test-Loss: 0.1402268782447862 Train-f1: 0.48405038518691584 Test-f1: 0.4602828448452552 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10587/20000] Train-Loss: 0.4493455403061737 Test-Loss: 0.1435706965626171 Train-f1: 0.47894844163797 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10588/20000] Train-Loss: 0.4454813978715714 Test-Loss: 0.14070685169561042 Train-f1: 0.4817530802819271 Test-f1: 0.42857142857142855 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10589/20000] Train-Loss: 0.4443228493589877 Test-Loss: 0.14355756494456828 Train-f1: 0.4965334755092168 Test-f1: 0.4526190476190476 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [10590/20000] Train-Loss: 0.4464288950952341 Test-Loss: 0.1472413866202231 Train-f1: 0.48567109223993155 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10591/20000] Train-Loss: 0.4480324049584678 Test-Loss: 0.14610886149381908 Train-f1: 0.4808737373737374 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10592/20000] Train-Loss: 0.45106933306690494 Test-Loss: 0.14186006973440163 Train-f1: 0.4843846592292955 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10593/20000] Train-Loss: 0.44662749468427687 Test-Loss: 0.14366244139859152 Train-f1: 0.4831329686217024 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10594/20000] Train-Loss: 0.4431003558701239 Test-Loss: 0.1401962288675703 Train-f1: 0.4765071222495914 Test-f1: 0.4714285714285714 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10595/20000] Train-Loss: 0.44905593464904564 Test-Loss: 0.14275841578514212 Train-f1: 0.4840177571897318 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10596/20000] Train-Loss: 0.4457553699691402 Test-Loss: 0.14138619866138322 Train-f1: 0.4900955238658119 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10597/20000] Train-Loss: 0.44433125193410167 Test-Loss: 0.14237874586810662 Train-f1: 0.4887125597785049 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10598/20000] Train-Loss: 0.4412389948566786 Test-Loss: 0.1402853400028908 Train-f1: 0.4842888343830965 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10599/20000] Train-Loss: 0.4425771288101013 Test-Loss: 0.14248613900946644 Train-f1: 0.4837802197802198 Test-f1: 0.5195545314900153 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10600/20000] Train-Loss: 0.4404124177393091 Test-Loss: 0.14572181703124643 Train-f1: 0.5012735520469824 Test-f1: 0.45233400741489715 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [10601/20000] Train-Loss: 0.4465996074643361 Test-Loss: 0.1450114668059028 Train-f1: 0.5112761624521769 Test-f1: 0.46683117900509197 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [10602/20000] Train-Loss: 0.44290170798213824 Test-Loss: 0.14686515154303612 Train-f1: 0.4835061147973253 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10603/20000] Train-Loss: 0.4459274595091545 Test-Loss: 0.14218630781820218 Train-f1: 0.4790059613589025 Test-f1: 0.5297402597402597 Train-acc: 0.804635761589404 Test-acc: 0.8026315789473685\n",
      "Epoch [10604/20000] Train-Loss: 0.4448503504319315 Test-Loss: 0.14719475583440678 Train-f1: 0.47841675045839 Test-f1: 0.4594670406732118 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10605/20000] Train-Loss: 0.4446295387340054 Test-Loss: 0.1467635279463261 Train-f1: 0.4813951497480658 Test-f1: 0.4624973437035148 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10606/20000] Train-Loss: 0.445382752357998 Test-Loss: 0.1433272500852836 Train-f1: 0.48430443425377395 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10607/20000] Train-Loss: 0.44538239892150444 Test-Loss: 0.1425927918683619 Train-f1: 0.48105112506057585 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10608/20000] Train-Loss: 0.4458021691169403 Test-Loss: 0.1420512344721852 Train-f1: 0.48101568485268853 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10609/20000] Train-Loss: 0.44096499350474894 Test-Loss: 0.14095126370768196 Train-f1: 0.4817316017316017 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10610/20000] Train-Loss: 0.4442401691114717 Test-Loss: 0.14377236623427805 Train-f1: 0.48797104029434457 Test-f1: 0.4826930070273724 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [10611/20000] Train-Loss: 0.4418038369037912 Test-Loss: 0.14401612562165875 Train-f1: 0.48481873226398775 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10612/20000] Train-Loss: 0.4432866553441477 Test-Loss: 0.14746180338947126 Train-f1: 0.4783642176047239 Test-f1: 0.4624973437035148 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10613/20000] Train-Loss: 0.444892119091595 Test-Loss: 0.14530860249800834 Train-f1: 0.48948363957790164 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10614/20000] Train-Loss: 0.44373237568397356 Test-Loss: 0.14285763408028349 Train-f1: 0.48636182922507887 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10615/20000] Train-Loss: 0.4408351201251776 Test-Loss: 0.15844946513198757 Train-f1: 0.48245445243658464 Test-f1: 0.4495524296675192 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10616/20000] Train-Loss: 0.4454178157946238 Test-Loss: 0.1410950574756417 Train-f1: 0.4758856443500081 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10617/20000] Train-Loss: 0.44529314360630406 Test-Loss: 0.1420567179904674 Train-f1: 0.4793156003347086 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10618/20000] Train-Loss: 0.4466191362452843 Test-Loss: 0.14529146990984707 Train-f1: 0.48077424858246776 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10619/20000] Train-Loss: 0.4456618539554459 Test-Loss: 0.14087619852125144 Train-f1: 0.4971118203960021 Test-f1: 0.42846033429278385 Train-acc: 0.8278145695364238 Test-acc: 0.7368421052631579\n",
      "Epoch [10620/20000] Train-Loss: 0.4434310130116236 Test-Loss: 0.1403578170062903 Train-f1: 0.4868862369804991 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10621/20000] Train-Loss: 0.44460023567390444 Test-Loss: 0.14467539424448264 Train-f1: 0.48820672987544145 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10622/20000] Train-Loss: 0.4456528565842451 Test-Loss: 0.1414309033341286 Train-f1: 0.48306926484229684 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10623/20000] Train-Loss: 0.44553831034073693 Test-Loss: 0.14169346116313905 Train-f1: 0.4757983910555815 Test-f1: 0.43193277310924366 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10624/20000] Train-Loss: 0.4465740485847984 Test-Loss: 0.14513561422307 Train-f1: 0.47698782443307997 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10625/20000] Train-Loss: 0.44285219467810516 Test-Loss: 0.14513693921163096 Train-f1: 0.48101568485268853 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10626/20000] Train-Loss: 0.4456028397506683 Test-Loss: 0.14020838590483897 Train-f1: 0.4817530802819271 Test-f1: 0.4714285714285714 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [10627/20000] Train-Loss: 0.443164767400391 Test-Loss: 0.1397716976031815 Train-f1: 0.4813267882868396 Test-f1: 0.4714285714285714 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10628/20000] Train-Loss: 0.44410810631026654 Test-Loss: 0.14521137264619077 Train-f1: 0.47825066549948775 Test-f1: 0.42567287784679086 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10629/20000] Train-Loss: 0.44862364085307627 Test-Loss: 0.14428179928635004 Train-f1: 0.4835061147973253 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10630/20000] Train-Loss: 0.44578497154184005 Test-Loss: 0.14193693833579654 Train-f1: 0.4813658214285265 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10631/20000] Train-Loss: 0.44483061602242924 Test-Loss: 0.14111975451195438 Train-f1: 0.4817530802819271 Test-f1: 0.42567287784679086 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10632/20000] Train-Loss: 0.4430640265869966 Test-Loss: 0.14077483975238825 Train-f1: 0.4868862369804991 Test-f1: 0.42846033429278385 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10633/20000] Train-Loss: 0.4444136826805462 Test-Loss: 0.14076967755260844 Train-f1: 0.4835061147973253 Test-f1: 0.4395204226783174 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10634/20000] Train-Loss: 0.44820989146788076 Test-Loss: 0.14400345882626972 Train-f1: 0.4792060034502879 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10635/20000] Train-Loss: 0.4438627465002784 Test-Loss: 0.14136672271708367 Train-f1: 0.4850318644334665 Test-f1: 0.5297402597402597 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [10636/20000] Train-Loss: 0.4455156145339695 Test-Loss: 0.1479600064712584 Train-f1: 0.4932469441172126 Test-f1: 0.4624973437035148 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [10637/20000] Train-Loss: 0.44490931952622387 Test-Loss: 0.14310686423746055 Train-f1: 0.4767655502392344 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10638/20000] Train-Loss: 0.44250302063526914 Test-Loss: 0.14322078203244706 Train-f1: 0.49274453048832856 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10639/20000] Train-Loss: 0.44403539753193294 Test-Loss: 0.14004928184194287 Train-f1: 0.4896839769217466 Test-f1: 0.4714285714285714 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10640/20000] Train-Loss: 0.4397351673938017 Test-Loss: 0.1484287972483028 Train-f1: 0.4858084524171481 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10641/20000] Train-Loss: 0.4463691404094924 Test-Loss: 0.14214310828985577 Train-f1: 0.47696038346604164 Test-f1: 0.4423271037905184 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [10642/20000] Train-Loss: 0.4484443886121813 Test-Loss: 0.14079451873181464 Train-f1: 0.48747876989950106 Test-f1: 0.42857142857142855 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10643/20000] Train-Loss: 0.4406593001930098 Test-Loss: 0.14167848748900802 Train-f1: 0.4841315438348639 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10644/20000] Train-Loss: 0.44599040288220293 Test-Loss: 0.14093737367105472 Train-f1: 0.4784449310076634 Test-f1: 0.46683117900509197 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10645/20000] Train-Loss: 0.44392751488697046 Test-Loss: 0.14190999037468663 Train-f1: 0.4867246788299419 Test-f1: 0.4714285714285714 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10646/20000] Train-Loss: 0.4471828605768431 Test-Loss: 0.14515429820291942 Train-f1: 0.4932469441172126 Test-f1: 0.4554347826086957 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [10647/20000] Train-Loss: 0.4439608045915976 Test-Loss: 0.1457754796689729 Train-f1: 0.4867246788299419 Test-f1: 0.45675057208237985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10648/20000] Train-Loss: 0.4475869595864111 Test-Loss: 0.14332829342722964 Train-f1: 0.48839521844916967 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10649/20000] Train-Loss: 0.44068983107200005 Test-Loss: 0.14896283938522317 Train-f1: 0.4817376490199082 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10650/20000] Train-Loss: 0.44542505314101744 Test-Loss: 0.14366589410739689 Train-f1: 0.4790649350649351 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10651/20000] Train-Loss: 0.4407626411403507 Test-Loss: 0.14011304665139146 Train-f1: 0.4841315438348639 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10652/20000] Train-Loss: 0.44354587809287027 Test-Loss: 0.14106986342023495 Train-f1: 0.47825066549948775 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10653/20000] Train-Loss: 0.4452308872214376 Test-Loss: 0.1404155982138308 Train-f1: 0.49125370063726226 Test-f1: 0.42846033429278385 Train-acc: 0.8211920529801324 Test-acc: 0.7368421052631579\n",
      "Epoch [10654/20000] Train-Loss: 0.44633321465993603 Test-Loss: 0.1401374387849208 Train-f1: 0.48833447376197653 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10655/20000] Train-Loss: 0.44964262400260857 Test-Loss: 0.1425350052779948 Train-f1: 0.4853203020499482 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10656/20000] Train-Loss: 0.4461925629154161 Test-Loss: 0.14311990609357486 Train-f1: 0.48383882045605053 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10657/20000] Train-Loss: 0.440579249413288 Test-Loss: 0.14989233769326438 Train-f1: 0.4888329141493698 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10658/20000] Train-Loss: 0.44196760784897166 Test-Loss: 0.14974063908327861 Train-f1: 0.4831329686217024 Test-f1: 0.4726570048309179 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10659/20000] Train-Loss: 0.44237152193176005 Test-Loss: 0.1405422661741521 Train-f1: 0.48638007532373734 Test-f1: 0.42857142857142855 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10660/20000] Train-Loss: 0.44564460136134043 Test-Loss: 0.1425139496155963 Train-f1: 0.4887615640951628 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10661/20000] Train-Loss: 0.44155875034878633 Test-Loss: 0.14214375204842933 Train-f1: 0.48333876366221384 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10662/20000] Train-Loss: 0.44353746941720745 Test-Loss: 0.14448536666552542 Train-f1: 0.4812966687543575 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10663/20000] Train-Loss: 0.4418710430085203 Test-Loss: 0.1395955422190297 Train-f1: 0.4893562577773104 Test-f1: 0.47043478260869565 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10664/20000] Train-Loss: 0.43963625724136157 Test-Loss: 0.1528891460992317 Train-f1: 0.4719688821816481 Test-f1: 0.4652641421224871 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [10665/20000] Train-Loss: 0.4467247311702184 Test-Loss: 0.1418898762833788 Train-f1: 0.4848587971168616 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10666/20000] Train-Loss: 0.4406654074846323 Test-Loss: 0.1415158265434388 Train-f1: 0.48624584082472255 Test-f1: 0.44854612862701837 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10667/20000] Train-Loss: 0.4397265664503424 Test-Loss: 0.14081339996232248 Train-f1: 0.47913752913752916 Test-f1: 0.43193277310924366 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10668/20000] Train-Loss: 0.4418342537149307 Test-Loss: 0.15386892417172435 Train-f1: 0.4889292929292929 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10669/20000] Train-Loss: 0.44636078944737 Test-Loss: 0.14194147981491556 Train-f1: 0.47662752128639907 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10670/20000] Train-Loss: 0.44414464335332704 Test-Loss: 0.14254606907662007 Train-f1: 0.47551093947209055 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10671/20000] Train-Loss: 0.44337364351528796 Test-Loss: 0.14024258995770772 Train-f1: 0.49742187636924473 Test-f1: 0.5183710801393728 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [10672/20000] Train-Loss: 0.44442226469384355 Test-Loss: 0.14543054128541935 Train-f1: 0.4778707187942352 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10673/20000] Train-Loss: 0.44228420635784016 Test-Loss: 0.1416828422561685 Train-f1: 0.48245356589882143 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10674/20000] Train-Loss: 0.4417916276626004 Test-Loss: 0.1419311113771909 Train-f1: 0.47825066549948775 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10675/20000] Train-Loss: 0.44809512131546286 Test-Loss: 0.13992536572196032 Train-f1: 0.4787461431255492 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10676/20000] Train-Loss: 0.4437645002306504 Test-Loss: 0.14101192411871796 Train-f1: 0.4847397213569514 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10677/20000] Train-Loss: 0.4427097959905198 Test-Loss: 0.15083231971776606 Train-f1: 0.4792944240857867 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10678/20000] Train-Loss: 0.4480208306779463 Test-Loss: 0.14211027946586302 Train-f1: 0.4868862369804991 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10679/20000] Train-Loss: 0.44374468384497545 Test-Loss: 0.14014558080345743 Train-f1: 0.4765071222495914 Test-f1: 0.46016451233842537 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10680/20000] Train-Loss: 0.44123969148200376 Test-Loss: 0.1408967048234727 Train-f1: 0.48194572947768516 Test-f1: 0.43193277310924366 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10681/20000] Train-Loss: 0.4428584780149806 Test-Loss: 0.14051705248627863 Train-f1: 0.48333876366221384 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10682/20000] Train-Loss: 0.4448222517390015 Test-Loss: 0.1396112004790611 Train-f1: 0.481495697285171 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10683/20000] Train-Loss: 0.44298095974253954 Test-Loss: 0.14063820953732034 Train-f1: 0.5062192983229793 Test-f1: 0.42567287784679086 Train-acc: 0.8178807947019867 Test-acc: 0.7368421052631579\n",
      "Epoch [10684/20000] Train-Loss: 0.4384367616449931 Test-Loss: 0.1519566111523105 Train-f1: 0.4833239803674586 Test-f1: 0.4652641421224871 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10685/20000] Train-Loss: 0.44301709287208024 Test-Loss: 0.14271587120739734 Train-f1: 0.48445432513793973 Test-f1: 0.4826930070273724 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10686/20000] Train-Loss: 0.44245785473476307 Test-Loss: 0.139640575263077 Train-f1: 0.47925349324959765 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10687/20000] Train-Loss: 0.4438600894470733 Test-Loss: 0.1405631667161037 Train-f1: 0.4893788556628208 Test-f1: 0.5183710801393728 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10688/20000] Train-Loss: 0.44517377911794376 Test-Loss: 0.14861513216021766 Train-f1: 0.48013159679826345 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10689/20000] Train-Loss: 0.4422969298698215 Test-Loss: 0.1481531026289047 Train-f1: 0.49228039221394315 Test-f1: 0.4624973437035148 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [10690/20000] Train-Loss: 0.44119749069864467 Test-Loss: 0.1408289768127204 Train-f1: 0.4811298985124144 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10691/20000] Train-Loss: 0.44321006024624465 Test-Loss: 0.14940375129067837 Train-f1: 0.4890249475009444 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10692/20000] Train-Loss: 0.4428131853986188 Test-Loss: 0.14435937006203867 Train-f1: 0.4767032967032967 Test-f1: 0.46683117900509197 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10693/20000] Train-Loss: 0.43994113272689 Test-Loss: 0.14123471049085928 Train-f1: 0.4887615640951628 Test-f1: 0.45675057208237985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10694/20000] Train-Loss: 0.4365877547147925 Test-Loss: 0.14365615302378765 Train-f1: 0.48605985209758795 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10695/20000] Train-Loss: 0.43978679906076434 Test-Loss: 0.14198339628300202 Train-f1: 0.48839521844916967 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10696/20000] Train-Loss: 0.4464803724786986 Test-Loss: 0.14227437552189037 Train-f1: 0.47899082113806346 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10697/20000] Train-Loss: 0.4451205467762703 Test-Loss: 0.14228380685761108 Train-f1: 0.4863038590655703 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10698/20000] Train-Loss: 0.43846673688056376 Test-Loss: 0.144638774350822 Train-f1: 0.47782875860734747 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10699/20000] Train-Loss: 0.44356986607768134 Test-Loss: 0.143394846010949 Train-f1: 0.4845050582661032 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10700/20000] Train-Loss: 0.43981238031770176 Test-Loss: 0.1459567627398663 Train-f1: 0.48575382860068617 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10701/20000] Train-Loss: 0.442271474417373 Test-Loss: 0.14729340557300577 Train-f1: 0.4807663887699281 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10702/20000] Train-Loss: 0.43706932772794954 Test-Loss: 0.14287007890882122 Train-f1: 0.4792246733541329 Test-f1: 0.5195545314900153 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [10703/20000] Train-Loss: 0.44488569197761374 Test-Loss: 0.13938728897737976 Train-f1: 0.4783642176047239 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10704/20000] Train-Loss: 0.44864899744694636 Test-Loss: 0.14032407661072108 Train-f1: 0.48383882045605053 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10705/20000] Train-Loss: 0.44407772115130983 Test-Loss: 0.13957849123221042 Train-f1: 0.4814648771681972 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10706/20000] Train-Loss: 0.44343309846406537 Test-Loss: 0.14628788663667838 Train-f1: 0.4858474858474858 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10707/20000] Train-Loss: 0.4415725193498734 Test-Loss: 0.13947917444785188 Train-f1: 0.47650595019016073 Test-f1: 0.4714285714285714 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10708/20000] Train-Loss: 0.4430889929215424 Test-Loss: 0.13958648176760155 Train-f1: 0.48800877873996046 Test-f1: 0.4714285714285714 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10709/20000] Train-Loss: 0.44364851557079366 Test-Loss: 0.1449586021017724 Train-f1: 0.4835061147973253 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10710/20000] Train-Loss: 0.443562174364287 Test-Loss: 0.14052828538581308 Train-f1: 0.48551586342469505 Test-f1: 0.5297402597402597 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [10711/20000] Train-Loss: 0.4429029658942986 Test-Loss: 0.141110607107381 Train-f1: 0.5059244378809595 Test-f1: 0.44854612862701837 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10712/20000] Train-Loss: 0.4408436361279065 Test-Loss: 0.14208608057543728 Train-f1: 0.48369109441848546 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10713/20000] Train-Loss: 0.44281263301443297 Test-Loss: 0.14110262236453228 Train-f1: 0.4865415231587532 Test-f1: 0.42567287784679086 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10714/20000] Train-Loss: 0.4430804218474179 Test-Loss: 0.14032201263700486 Train-f1: 0.47858172406541577 Test-f1: 0.42846033429278385 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10715/20000] Train-Loss: 0.4415882036617321 Test-Loss: 0.14585064028662323 Train-f1: 0.48062596140455033 Test-f1: 0.4624973437035148 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10716/20000] Train-Loss: 0.44330614479875624 Test-Loss: 0.1424308364737994 Train-f1: 0.4803410117862673 Test-f1: 0.4485461286270184 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10717/20000] Train-Loss: 0.4448323740011 Test-Loss: 0.1480508942135799 Train-f1: 0.4875407527289224 Test-f1: 0.44558629776021075 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10718/20000] Train-Loss: 0.44274083682346105 Test-Loss: 0.1433325227277963 Train-f1: 0.47939712918660293 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10719/20000] Train-Loss: 0.44195761053880245 Test-Loss: 0.14001884470902387 Train-f1: 0.4706648258480003 Test-f1: 0.4714285714285714 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [10720/20000] Train-Loss: 0.44248823713387986 Test-Loss: 0.14031722216304077 Train-f1: 0.4730705054155189 Test-f1: 0.42846033429278385 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [10721/20000] Train-Loss: 0.4433846110892639 Test-Loss: 0.14047786436306117 Train-f1: 0.48423723035798255 Test-f1: 0.42846033429278385 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10722/20000] Train-Loss: 0.4424671922019351 Test-Loss: 0.1428702357157344 Train-f1: 0.4808737373737374 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10723/20000] Train-Loss: 0.44458136915769497 Test-Loss: 0.1405469326491206 Train-f1: 0.47603395336477883 Test-f1: 0.42857142857142855 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10724/20000] Train-Loss: 0.44288102625840464 Test-Loss: 0.14214422164296048 Train-f1: 0.4835985658770469 Test-f1: 0.4390972912712042 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10725/20000] Train-Loss: 0.4394323979353541 Test-Loss: 0.14343671169846325 Train-f1: 0.48113611775334775 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10726/20000] Train-Loss: 0.4456637802148189 Test-Loss: 0.1455538491518883 Train-f1: 0.4788842320069069 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10727/20000] Train-Loss: 0.44835393521545996 Test-Loss: 0.14217593673788467 Train-f1: 0.4947816528229348 Test-f1: 0.4526190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [10728/20000] Train-Loss: 0.4426055577721505 Test-Loss: 0.14227165509642473 Train-f1: 0.48409676809676816 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10729/20000] Train-Loss: 0.44504247330065977 Test-Loss: 0.1439760431860014 Train-f1: 0.48636182922507887 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10730/20000] Train-Loss: 0.4432134323048063 Test-Loss: 0.13933913266845882 Train-f1: 0.4916776556776557 Test-f1: 0.47043478260869565 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [10731/20000] Train-Loss: 0.43868390426908527 Test-Loss: 0.14275055010766094 Train-f1: 0.48845393908884976 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10732/20000] Train-Loss: 0.43759407937729866 Test-Loss: 0.14520660506744146 Train-f1: 0.4848587971168616 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10733/20000] Train-Loss: 0.44255762243404506 Test-Loss: 0.1430116047183239 Train-f1: 0.46626182827662016 Test-f1: 0.45233400741489715 Train-acc: 0.7880794701986755 Test-acc: 0.7631578947368421\n",
      "Epoch [10734/20000] Train-Loss: 0.4402352985546809 Test-Loss: 0.1394404387327957 Train-f1: 0.4871194373510706 Test-f1: 0.4714285714285714 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10735/20000] Train-Loss: 0.44378127176213517 Test-Loss: 0.1467826137212848 Train-f1: 0.48568715235381904 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10736/20000] Train-Loss: 0.4440936241399765 Test-Loss: 0.13925495897284465 Train-f1: 0.48163982776058 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10737/20000] Train-Loss: 0.4400411176914239 Test-Loss: 0.14158641046435422 Train-f1: 0.48177837834257364 Test-f1: 0.48274560274560274 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10738/20000] Train-Loss: 0.4418058690119521 Test-Loss: 0.1404899669419249 Train-f1: 0.48717167927536026 Test-f1: 0.42846033429278385 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10739/20000] Train-Loss: 0.4451066807958694 Test-Loss: 0.14184490745078498 Train-f1: 0.4894657245763545 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10740/20000] Train-Loss: 0.4417403177816239 Test-Loss: 0.14315712457924587 Train-f1: 0.4813951497480658 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10741/20000] Train-Loss: 0.4391638720442843 Test-Loss: 0.14221385949355875 Train-f1: 0.4788842320069069 Test-f1: 0.5195545314900153 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10742/20000] Train-Loss: 0.4443903302156446 Test-Loss: 0.14118088463020748 Train-f1: 0.4804402387735721 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10743/20000] Train-Loss: 0.4469066352559879 Test-Loss: 0.14507188007482752 Train-f1: 0.4785883470527108 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10744/20000] Train-Loss: 0.44289488248900016 Test-Loss: 0.1419866584220662 Train-f1: 0.47915631189290664 Test-f1: 0.4558710801393729 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10745/20000] Train-Loss: 0.4377874886379542 Test-Loss: 0.1446089455673755 Train-f1: 0.4861144161998164 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10746/20000] Train-Loss: 0.4427790575155354 Test-Loss: 0.13997082346479156 Train-f1: 0.4851202325654881 Test-f1: 0.4714285714285714 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10747/20000] Train-Loss: 0.442108280744424 Test-Loss: 0.14173883491894854 Train-f1: 0.48595393908884976 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10748/20000] Train-Loss: 0.4418605036166816 Test-Loss: 0.1409102987735532 Train-f1: 0.4785883470527108 Test-f1: 0.44854612862701837 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10749/20000] Train-Loss: 0.4407212120198352 Test-Loss: 0.14483564149199668 Train-f1: 0.4863865911566875 Test-f1: 0.42567287784679086 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10750/20000] Train-Loss: 0.43869781047605483 Test-Loss: 0.14112766287113682 Train-f1: 0.48105112506057585 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10751/20000] Train-Loss: 0.44193867723204505 Test-Loss: 0.1403053785259513 Train-f1: 0.4811294947660527 Test-f1: 0.42857142857142855 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10752/20000] Train-Loss: 0.44084709083644447 Test-Loss: 0.1451148263812841 Train-f1: 0.48845393908884976 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10753/20000] Train-Loss: 0.4419719951123883 Test-Loss: 0.14512934998446372 Train-f1: 0.4842347195288371 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10754/20000] Train-Loss: 0.44439890813869004 Test-Loss: 0.14153059818462885 Train-f1: 0.48202870813397125 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10755/20000] Train-Loss: 0.4424588138565628 Test-Loss: 0.14454792621350615 Train-f1: 0.48113611775334775 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10756/20000] Train-Loss: 0.4433341041868921 Test-Loss: 0.13969681642012988 Train-f1: 0.4890246353545196 Test-f1: 0.4714285714285714 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10757/20000] Train-Loss: 0.4446771167310888 Test-Loss: 0.14154894739522536 Train-f1: 0.47732659287021695 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10758/20000] Train-Loss: 0.43433798422781195 Test-Loss: 0.1443757637585479 Train-f1: 0.487031112347568 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10759/20000] Train-Loss: 0.44510015496247285 Test-Loss: 0.1444112335243813 Train-f1: 0.488513974609865 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10760/20000] Train-Loss: 0.44378888458086957 Test-Loss: 0.14108062737576327 Train-f1: 0.4681839250251536 Test-f1: 0.4395204226783174 Train-acc: 0.7913907284768212 Test-acc: 0.75\n",
      "Epoch [10761/20000] Train-Loss: 0.44089185581306845 Test-Loss: 0.144676363189223 Train-f1: 0.4841315438348639 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10762/20000] Train-Loss: 0.44240468707698916 Test-Loss: 0.1456707047202921 Train-f1: 0.48420051565667493 Test-f1: 0.4594670406732118 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [10763/20000] Train-Loss: 0.4450850343937437 Test-Loss: 0.13909615940450643 Train-f1: 0.47953051354999143 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10764/20000] Train-Loss: 0.4432746894451314 Test-Loss: 0.14025522955680064 Train-f1: 0.4813658214285265 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10765/20000] Train-Loss: 0.44196324177370055 Test-Loss: 0.14037375441111316 Train-f1: 0.4814936420519259 Test-f1: 0.42567287784679086 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10766/20000] Train-Loss: 0.44126000276422445 Test-Loss: 0.13933186121614977 Train-f1: 0.4794863825445413 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10767/20000] Train-Loss: 0.44080944842660263 Test-Loss: 0.14532672708736458 Train-f1: 0.4813658214285265 Test-f1: 0.45675057208237985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10768/20000] Train-Loss: 0.4432608533280537 Test-Loss: 0.1396764400876515 Train-f1: 0.47326674548596126 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [10769/20000] Train-Loss: 0.43955482682850117 Test-Loss: 0.1442106383134176 Train-f1: 0.4841315438348639 Test-f1: 0.4356787406570312 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10770/20000] Train-Loss: 0.4414014033198713 Test-Loss: 0.13959821586463794 Train-f1: 0.4813658214285265 Test-f1: 0.4714285714285714 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10771/20000] Train-Loss: 0.4404176932137914 Test-Loss: 0.14164899495382055 Train-f1: 0.4808958631743442 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10772/20000] Train-Loss: 0.4433646566686299 Test-Loss: 0.14136785903146157 Train-f1: 0.47406876953680027 Test-f1: 0.4526190476190476 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10773/20000] Train-Loss: 0.43865020423965395 Test-Loss: 0.1524542054340929 Train-f1: 0.4835061147973253 Test-f1: 0.4652641421224871 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10774/20000] Train-Loss: 0.443486873457306 Test-Loss: 0.1475781493720415 Train-f1: 0.48728749205758837 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10775/20000] Train-Loss: 0.4425428838595945 Test-Loss: 0.14777493427504015 Train-f1: 0.48158822373546595 Test-f1: 0.44558629776021075 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10776/20000] Train-Loss: 0.4412307330082963 Test-Loss: 0.1423968752758738 Train-f1: 0.4866144877510184 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10777/20000] Train-Loss: 0.438046190579201 Test-Loss: 0.14546451591816345 Train-f1: 0.4766229913551695 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10778/20000] Train-Loss: 0.44476151080357207 Test-Loss: 0.14021671133981967 Train-f1: 0.4770237848404964 Test-f1: 0.42846033429278385 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10779/20000] Train-Loss: 0.4412991721201772 Test-Loss: 0.13932467527859643 Train-f1: 0.48377221349595 Test-f1: 0.4714285714285714 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10780/20000] Train-Loss: 0.4439601569753097 Test-Loss: 0.14355513719625032 Train-f1: 0.4775809755805363 Test-f1: 0.42567287784679086 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10781/20000] Train-Loss: 0.4408102326450837 Test-Loss: 0.14823338276679 Train-f1: 0.4863038590655703 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10782/20000] Train-Loss: 0.4410625504347111 Test-Loss: 0.14167819657405828 Train-f1: 0.48630126857974953 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10783/20000] Train-Loss: 0.4444914681569965 Test-Loss: 0.14212094112256687 Train-f1: 0.4863865911566875 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10784/20000] Train-Loss: 0.4372148800753503 Test-Loss: 0.139768427740175 Train-f1: 0.4787461431255492 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10785/20000] Train-Loss: 0.441818696565475 Test-Loss: 0.14191589749346795 Train-f1: 0.48829939010760925 Test-f1: 0.46683117900509197 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10786/20000] Train-Loss: 0.43784321726694214 Test-Loss: 0.13974950389800458 Train-f1: 0.4742449626005142 Test-f1: 0.46683117900509197 Train-acc: 0.7947019867549668 Test-acc: 0.7894736842105263\n",
      "Epoch [10787/20000] Train-Loss: 0.44598000521763875 Test-Loss: 0.14092960205174548 Train-f1: 0.47279479652767326 Test-f1: 0.5077443609022556 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [10788/20000] Train-Loss: 0.4360373922640605 Test-Loss: 0.14218599613980323 Train-f1: 0.4821919506563145 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10789/20000] Train-Loss: 0.440321460895238 Test-Loss: 0.14478682423139672 Train-f1: 0.480648496572013 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10790/20000] Train-Loss: 0.4429725183703139 Test-Loss: 0.14073540618504043 Train-f1: 0.48978026466456537 Test-f1: 0.4390972912712042 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [10791/20000] Train-Loss: 0.44321393877887005 Test-Loss: 0.1451559549802879 Train-f1: 0.4812487417374755 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10792/20000] Train-Loss: 0.4422915169064648 Test-Loss: 0.14777528897053588 Train-f1: 0.4845168789570815 Test-f1: 0.46683117900509197 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10793/20000] Train-Loss: 0.44376863303728475 Test-Loss: 0.1425653771342167 Train-f1: 0.4853203020499482 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10794/20000] Train-Loss: 0.4380407375829806 Test-Loss: 0.1451661917633114 Train-f1: 0.5034606776333241 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10795/20000] Train-Loss: 0.44386762251787115 Test-Loss: 0.14081887217215178 Train-f1: 0.48724746515629674 Test-f1: 0.43193277310924366 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10796/20000] Train-Loss: 0.4386362713045861 Test-Loss: 0.14675891425597112 Train-f1: 0.49153388159695943 Test-f1: 0.4594670406732118 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [10797/20000] Train-Loss: 0.4413940707440634 Test-Loss: 0.1502163363718242 Train-f1: 0.49510412189423636 Test-f1: 0.4594670406732118 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [10798/20000] Train-Loss: 0.4392433293996584 Test-Loss: 0.1449705088955071 Train-f1: 0.48496448363998035 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10799/20000] Train-Loss: 0.43810756392424594 Test-Loss: 0.13915134020624914 Train-f1: 0.48992344497607654 Test-f1: 0.47043478260869565 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10800/20000] Train-Loss: 0.4394576307945495 Test-Loss: 0.16032608824190844 Train-f1: 0.4838701912744465 Test-f1: 0.4593201178892971 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10801/20000] Train-Loss: 0.43855883642130183 Test-Loss: 0.14235446841557067 Train-f1: 0.4766447211488002 Test-f1: 0.4826930070273724 Train-acc: 0.8013245033112583 Test-acc: 0.75\n",
      "Epoch [10802/20000] Train-Loss: 0.4413068346467443 Test-Loss: 0.1452415379413007 Train-f1: 0.4850705443082527 Test-f1: 0.44558629776021075 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10803/20000] Train-Loss: 0.4388855033401037 Test-Loss: 0.14351399707030793 Train-f1: 0.4832742485824678 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10804/20000] Train-Loss: 0.4428345952095348 Test-Loss: 0.14077318914107176 Train-f1: 0.48252282996808543 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10805/20000] Train-Loss: 0.4412690298133555 Test-Loss: 0.13935970017234 Train-f1: 0.4892738081543892 Test-f1: 0.4714285714285714 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10806/20000] Train-Loss: 0.4399428419243823 Test-Loss: 0.14434847380061386 Train-f1: 0.4812040305272203 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10807/20000] Train-Loss: 0.44256422312115234 Test-Loss: 0.14517688721519442 Train-f1: 0.48077424858246776 Test-f1: 0.4624973437035148 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10808/20000] Train-Loss: 0.4406637543252922 Test-Loss: 0.14238558655596184 Train-f1: 0.4786812576701373 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10809/20000] Train-Loss: 0.43886149125382184 Test-Loss: 0.1401620225655991 Train-f1: 0.5115362323304147 Test-f1: 0.42846033429278385 Train-acc: 0.8245033112582781 Test-acc: 0.7368421052631579\n",
      "Epoch [10810/20000] Train-Loss: 0.44085409039169254 Test-Loss: 0.14212130415696594 Train-f1: 0.47882996282996276 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10811/20000] Train-Loss: 0.4397806892763257 Test-Loss: 0.14012819227659884 Train-f1: 0.48489361354692273 Test-f1: 0.4284737484737485 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10812/20000] Train-Loss: 0.4402201564542799 Test-Loss: 0.14299282675012934 Train-f1: 0.49003422242858863 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10813/20000] Train-Loss: 0.4396412535988426 Test-Loss: 0.14373904474760868 Train-f1: 0.48729186602870805 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10814/20000] Train-Loss: 0.4381696720059086 Test-Loss: 0.1453667984864852 Train-f1: 0.4791387011969598 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10815/20000] Train-Loss: 0.4378685629437824 Test-Loss: 0.13977265347764622 Train-f1: 0.47401314220143176 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10816/20000] Train-Loss: 0.4363386119959332 Test-Loss: 0.1443312625509423 Train-f1: 0.4884770801412481 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10817/20000] Train-Loss: 0.4371811623011515 Test-Loss: 0.13968322884467516 Train-f1: 0.48678774842655226 Test-f1: 0.46683117900509197 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10818/20000] Train-Loss: 0.439203274009407 Test-Loss: 0.1420466382908164 Train-f1: 0.4806953183991946 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10819/20000] Train-Loss: 0.4433908051877457 Test-Loss: 0.1409866185000547 Train-f1: 0.4791215013345587 Test-f1: 0.43193277310924366 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10820/20000] Train-Loss: 0.4381545690612979 Test-Loss: 0.1428231203605418 Train-f1: 0.4841315438348639 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10821/20000] Train-Loss: 0.44284037723655345 Test-Loss: 0.13911234775865486 Train-f1: 0.47899082113806346 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10822/20000] Train-Loss: 0.44284781549324104 Test-Loss: 0.14112144317446004 Train-f1: 0.4808230403917736 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10823/20000] Train-Loss: 0.4361667522473765 Test-Loss: 0.1458828463164149 Train-f1: 0.49117299622694743 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10824/20000] Train-Loss: 0.4377742549012037 Test-Loss: 0.14829723532623995 Train-f1: 0.4835827706301961 Test-f1: 0.4594670406732118 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10825/20000] Train-Loss: 0.4432699057464306 Test-Loss: 0.1466607378998917 Train-f1: 0.4816203404438698 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10826/20000] Train-Loss: 0.43664416617204804 Test-Loss: 0.1419185279423195 Train-f1: 0.4725624200076755 Test-f1: 0.4526190476190476 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10827/20000] Train-Loss: 0.43730234562890835 Test-Loss: 0.14791113041773424 Train-f1: 0.48747876989950106 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10828/20000] Train-Loss: 0.44233027207998016 Test-Loss: 0.14254648673952586 Train-f1: 0.48319531839919455 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10829/20000] Train-Loss: 0.43799103904096776 Test-Loss: 0.14464253206519284 Train-f1: 0.518675757842276 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10830/20000] Train-Loss: 0.4396368514398199 Test-Loss: 0.1399932904522734 Train-f1: 0.4818736799766494 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10831/20000] Train-Loss: 0.43646205500439966 Test-Loss: 0.14485009024047368 Train-f1: 0.4832742485824678 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10832/20000] Train-Loss: 0.4421534545162695 Test-Loss: 0.1422353388199637 Train-f1: 0.49232900432900434 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10833/20000] Train-Loss: 0.4341748762129796 Test-Loss: 0.14194668537656546 Train-f1: 0.48595393908884976 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10834/20000] Train-Loss: 0.44028437503660545 Test-Loss: 0.13958013038829314 Train-f1: 0.4694944374510938 Test-f1: 0.42567287784679086 Train-acc: 0.7913907284768212 Test-acc: 0.7368421052631579\n",
      "Epoch [10835/20000] Train-Loss: 0.4419424452352757 Test-Loss: 0.1427699488543525 Train-f1: 0.4811294947660527 Test-f1: 0.516748734581552 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10836/20000] Train-Loss: 0.4420906840135413 Test-Loss: 0.1416850364379024 Train-f1: 0.4832742485824678 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10837/20000] Train-Loss: 0.44391544137142297 Test-Loss: 0.14082499657331166 Train-f1: 0.48449816849816846 Test-f1: 0.4423271037905184 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10838/20000] Train-Loss: 0.4393386473585331 Test-Loss: 0.14154283925111164 Train-f1: 0.48601397460986495 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10839/20000] Train-Loss: 0.4412034195661219 Test-Loss: 0.13989250199867567 Train-f1: 0.5011637011637011 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10840/20000] Train-Loss: 0.4390190277513686 Test-Loss: 0.140093563797118 Train-f1: 0.4916776556776557 Test-f1: 0.4485461286270184 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10841/20000] Train-Loss: 0.4430001454408491 Test-Loss: 0.14116822281130464 Train-f1: 0.49083057160916044 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10842/20000] Train-Loss: 0.43794003845746454 Test-Loss: 0.14735814269792324 Train-f1: 0.4808230403917736 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10843/20000] Train-Loss: 0.44218993567525006 Test-Loss: 0.14214908741871388 Train-f1: 0.4788842320069069 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10844/20000] Train-Loss: 0.43965706460594933 Test-Loss: 0.13903363517469702 Train-f1: 0.47390854759275813 Test-f1: 0.4714285714285714 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [10845/20000] Train-Loss: 0.4425178102063436 Test-Loss: 0.14044899838659786 Train-f1: 0.4804820663600006 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [10846/20000] Train-Loss: 0.4371732521452967 Test-Loss: 0.13893550493929782 Train-f1: 0.4792246733541329 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [10847/20000] Train-Loss: 0.4370586652618589 Test-Loss: 0.14417129812524052 Train-f1: 0.4885755753680282 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10848/20000] Train-Loss: 0.4401502746698629 Test-Loss: 0.1430621317727316 Train-f1: 0.4763934185406608 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10849/20000] Train-Loss: 0.43928297005216815 Test-Loss: 0.1395216351245768 Train-f1: 0.47681971587787453 Test-f1: 0.4714285714285714 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10850/20000] Train-Loss: 0.4419080903468528 Test-Loss: 0.14223177284929595 Train-f1: 0.4819037339788103 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10851/20000] Train-Loss: 0.43934623539646184 Test-Loss: 0.1408344160602014 Train-f1: 0.4897128201341176 Test-f1: 0.43193277310924366 Train-acc: 0.8178807947019867 Test-acc: 0.7368421052631579\n",
      "Epoch [10852/20000] Train-Loss: 0.43487502133661016 Test-Loss: 0.15942543902482342 Train-f1: 0.4867349489453051 Test-f1: 0.46444838805087246 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10853/20000] Train-Loss: 0.44162958614437203 Test-Loss: 0.1418719947551552 Train-f1: 0.4931098485278052 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10854/20000] Train-Loss: 0.4420101175759282 Test-Loss: 0.14248802759388654 Train-f1: 0.48624584082472255 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10855/20000] Train-Loss: 0.43866482478541263 Test-Loss: 0.140389593267051 Train-f1: 0.5021991341991342 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10856/20000] Train-Loss: 0.44194116876848044 Test-Loss: 0.14415193053248013 Train-f1: 0.48159570154095704 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10857/20000] Train-Loss: 0.4369862278745618 Test-Loss: 0.1402299364054356 Train-f1: 0.48699903831426283 Test-f1: 0.45675057208237985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [10858/20000] Train-Loss: 0.440964457003457 Test-Loss: 0.13890859997219904 Train-f1: 0.48649402655673174 Test-f1: 0.45675057208237985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10859/20000] Train-Loss: 0.4404928609531336 Test-Loss: 0.14097683749985604 Train-f1: 0.47611715510603475 Test-f1: 0.460952380952381 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [10860/20000] Train-Loss: 0.4385148381596347 Test-Loss: 0.13870831971586758 Train-f1: 0.4866144877510184 Test-f1: 0.4714285714285714 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10861/20000] Train-Loss: 0.4429272494968735 Test-Loss: 0.1400729814336583 Train-f1: 0.48333876366221384 Test-f1: 0.4385580319084156 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10862/20000] Train-Loss: 0.439736176772572 Test-Loss: 0.1411183261699609 Train-f1: 0.48946347769877185 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10863/20000] Train-Loss: 0.43578700989960617 Test-Loss: 0.14361284452591958 Train-f1: 0.48219127317146687 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10864/20000] Train-Loss: 0.43897984728566786 Test-Loss: 0.14203924512576238 Train-f1: 0.4835985658770469 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10865/20000] Train-Loss: 0.44152788684164096 Test-Loss: 0.13932837797060152 Train-f1: 0.4814648771681972 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10866/20000] Train-Loss: 0.4423573059095822 Test-Loss: 0.14254812704108072 Train-f1: 0.48409676809676816 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10867/20000] Train-Loss: 0.4382901126698002 Test-Loss: 0.1398359823543881 Train-f1: 0.48223791606550537 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [10868/20000] Train-Loss: 0.43949966068974544 Test-Loss: 0.13984751194214565 Train-f1: 0.48649006773814013 Test-f1: 0.5077443609022556 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10869/20000] Train-Loss: 0.4397508013045276 Test-Loss: 0.14933366960637165 Train-f1: 0.4957303471811553 Test-f1: 0.4356787406570312 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10870/20000] Train-Loss: 0.4401152882142924 Test-Loss: 0.1422193872370571 Train-f1: 0.49401775223877653 Test-f1: 0.5037301587301587 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [10871/20000] Train-Loss: 0.4373043540062321 Test-Loss: 0.1443953297366398 Train-f1: 0.4834539390888498 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10872/20000] Train-Loss: 0.43859424694666493 Test-Loss: 0.14122050857240837 Train-f1: 0.4737772031889679 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10873/20000] Train-Loss: 0.4386348474134127 Test-Loss: 0.14211300224980955 Train-f1: 0.48113611775334775 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10874/20000] Train-Loss: 0.44358347131949694 Test-Loss: 0.14437212819987522 Train-f1: 0.4862652172625218 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10875/20000] Train-Loss: 0.43812942219179724 Test-Loss: 0.13964324551757473 Train-f1: 0.4894657245763545 Test-f1: 0.460952380952381 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10876/20000] Train-Loss: 0.44044495787511634 Test-Loss: 0.14034501646095127 Train-f1: 0.4829132373386866 Test-f1: 0.43488721804511277 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [10877/20000] Train-Loss: 0.43708513100205765 Test-Loss: 0.14030395005678287 Train-f1: 0.48636182922507887 Test-f1: 0.4318216788305991 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10878/20000] Train-Loss: 0.4408979936383985 Test-Loss: 0.1395504616711372 Train-f1: 0.4762175653402402 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10879/20000] Train-Loss: 0.4383383917272738 Test-Loss: 0.14971169699057887 Train-f1: 0.49591186208533583 Test-f1: 0.42567287784679086 Train-acc: 0.804635761589404 Test-acc: 0.7368421052631579\n",
      "Epoch [10880/20000] Train-Loss: 0.4376281194603745 Test-Loss: 0.14536607065414633 Train-f1: 0.49129666380340237 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10881/20000] Train-Loss: 0.43852424948662244 Test-Loss: 0.1398121911320187 Train-f1: 0.4767655502392344 Test-f1: 0.4284737484737485 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10882/20000] Train-Loss: 0.4380849770354928 Test-Loss: 0.14405718416659338 Train-f1: 0.47814648593060627 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10883/20000] Train-Loss: 0.435589335180948 Test-Loss: 0.14176897697981972 Train-f1: 0.4897338164360379 Test-f1: 0.4826930070273724 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [10884/20000] Train-Loss: 0.4404860562683207 Test-Loss: 0.1434336956593291 Train-f1: 0.4958395387843241 Test-f1: 0.45233400741489715 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [10885/20000] Train-Loss: 0.43785375148097133 Test-Loss: 0.141339064162252 Train-f1: 0.4835985658770469 Test-f1: 0.518795518207283 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10886/20000] Train-Loss: 0.4385675424374835 Test-Loss: 0.14467628237204289 Train-f1: 0.4819024998721142 Test-f1: 0.45675057208237985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10887/20000] Train-Loss: 0.4397608026815201 Test-Loss: 0.1428051518327722 Train-f1: 0.48883550463519043 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10888/20000] Train-Loss: 0.4403098012730965 Test-Loss: 0.1409930044239145 Train-f1: 0.48200105593637765 Test-f1: 0.45675057208237985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10889/20000] Train-Loss: 0.4415425589616326 Test-Loss: 0.14059218690234518 Train-f1: 0.4829093745760412 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10890/20000] Train-Loss: 0.4395385243984629 Test-Loss: 0.14973880489201255 Train-f1: 0.4757983910555815 Test-f1: 0.4726570048309179 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [10891/20000] Train-Loss: 0.44027565800637064 Test-Loss: 0.1406463649015832 Train-f1: 0.4902970733531461 Test-f1: 0.4423271037905184 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [10892/20000] Train-Loss: 0.43663369460409035 Test-Loss: 0.14063622742814386 Train-f1: 0.484657177274627 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10893/20000] Train-Loss: 0.43917309080692996 Test-Loss: 0.14277355928372118 Train-f1: 0.47848403928306826 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10894/20000] Train-Loss: 0.4365646007960344 Test-Loss: 0.14254565377326833 Train-f1: 0.4859926080915586 Test-f1: 0.518795518207283 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10895/20000] Train-Loss: 0.4399593368159781 Test-Loss: 0.1389693498911957 Train-f1: 0.48124536023423986 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10896/20000] Train-Loss: 0.4361432203814981 Test-Loss: 0.14175204936878524 Train-f1: 0.47676782963407166 Test-f1: 0.4252857311181807 Train-acc: 0.8013245033112583 Test-acc: 0.7368421052631579\n",
      "Epoch [10897/20000] Train-Loss: 0.43880656364673315 Test-Loss: 0.1443029154926193 Train-f1: 0.4889095999257159 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10898/20000] Train-Loss: 0.4402620942957069 Test-Loss: 0.1418015228720724 Train-f1: 0.48624584082472255 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10899/20000] Train-Loss: 0.44038812943411293 Test-Loss: 0.1438304288903287 Train-f1: 0.48383882045605053 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10900/20000] Train-Loss: 0.43950198980703253 Test-Loss: 0.14370841600264553 Train-f1: 0.4835827706301961 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10901/20000] Train-Loss: 0.4414174616852873 Test-Loss: 0.1433842669927132 Train-f1: 0.48439826839826844 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10902/20000] Train-Loss: 0.440615363970822 Test-Loss: 0.14060760886676196 Train-f1: 0.4864468864468864 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10903/20000] Train-Loss: 0.44082899969756734 Test-Loss: 0.14048287012045466 Train-f1: 0.48418119237084756 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10904/20000] Train-Loss: 0.4397969068338394 Test-Loss: 0.14075853596673063 Train-f1: 0.48636182922507887 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10905/20000] Train-Loss: 0.43954690521326634 Test-Loss: 0.1427079113109533 Train-f1: 0.49248847267172013 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10906/20000] Train-Loss: 0.4384387152126678 Test-Loss: 0.14351503653545525 Train-f1: 0.4831329686217024 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10907/20000] Train-Loss: 0.436372576751204 Test-Loss: 0.14146488513371352 Train-f1: 0.48662480153187265 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10908/20000] Train-Loss: 0.4361699904186188 Test-Loss: 0.14596242526052705 Train-f1: 0.5066546626299029 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10909/20000] Train-Loss: 0.4324468436801514 Test-Loss: 0.14051002863556944 Train-f1: 0.4957242276127725 Test-f1: 0.5183710801393728 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [10910/20000] Train-Loss: 0.4383628853421581 Test-Loss: 0.13949921937995693 Train-f1: 0.47913871462560975 Test-f1: 0.4390972912712042 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10911/20000] Train-Loss: 0.4398085447552005 Test-Loss: 0.14036488369014044 Train-f1: 0.5036373694370553 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10912/20000] Train-Loss: 0.4361167921115621 Test-Loss: 0.13986105144656968 Train-f1: 0.4868862369804991 Test-f1: 0.4497330282227307 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10913/20000] Train-Loss: 0.4347485772509662 Test-Loss: 0.14933771525504444 Train-f1: 0.4890886890886891 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10914/20000] Train-Loss: 0.43798519745193026 Test-Loss: 0.14011022334787337 Train-f1: 0.4876125095947568 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10915/20000] Train-Loss: 0.4378195813638499 Test-Loss: 0.14114795765141538 Train-f1: 0.48556423300948853 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10916/20000] Train-Loss: 0.43714581154993776 Test-Loss: 0.14024010420440566 Train-f1: 0.48699903831426283 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10917/20000] Train-Loss: 0.4345868038560789 Test-Loss: 0.14616729631168318 Train-f1: 0.4940882386401849 Test-f1: 0.4628528560711606 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [10918/20000] Train-Loss: 0.43534195945719667 Test-Loss: 0.14328020704037336 Train-f1: 0.4884916359368914 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10919/20000] Train-Loss: 0.43684302858815155 Test-Loss: 0.1389521794203331 Train-f1: 0.4864468864468864 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10920/20000] Train-Loss: 0.4407261003448513 Test-Loss: 0.14548885135424844 Train-f1: 0.49024513318991847 Test-f1: 0.4624973437035148 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [10921/20000] Train-Loss: 0.43399890926860596 Test-Loss: 0.141912611382683 Train-f1: 0.47965727090421506 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10922/20000] Train-Loss: 0.43553880070334533 Test-Loss: 0.14297804796910543 Train-f1: 0.4808958631743442 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10923/20000] Train-Loss: 0.4378973404588054 Test-Loss: 0.14659976361931373 Train-f1: 0.48592792599063117 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10924/20000] Train-Loss: 0.4378724191218687 Test-Loss: 0.14244133329288464 Train-f1: 0.4835985658770469 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10925/20000] Train-Loss: 0.43728250222273285 Test-Loss: 0.14156274675632283 Train-f1: 0.48423723035798255 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10926/20000] Train-Loss: 0.4386444102331161 Test-Loss: 0.14020236066930764 Train-f1: 0.4813951497480658 Test-f1: 0.5077443609022556 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10927/20000] Train-Loss: 0.4404200949939896 Test-Loss: 0.1431343782916036 Train-f1: 0.491212559778505 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10928/20000] Train-Loss: 0.4427976440877785 Test-Loss: 0.14286694080264392 Train-f1: 0.4881562881562882 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10929/20000] Train-Loss: 0.4359663391505589 Test-Loss: 0.1397437107437932 Train-f1: 0.49208104217530424 Test-f1: 0.4395204226783174 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [10930/20000] Train-Loss: 0.4370512641458799 Test-Loss: 0.148371636354671 Train-f1: 0.4725624200076755 Test-f1: 0.5141038802093091 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [10931/20000] Train-Loss: 0.44048515259512067 Test-Loss: 0.1410554149948514 Train-f1: 0.49242927128708197 Test-f1: 0.42846033429278385 Train-acc: 0.8211920529801324 Test-acc: 0.7368421052631579\n",
      "Epoch [10932/20000] Train-Loss: 0.43729935786350216 Test-Loss: 0.14256799051914557 Train-f1: 0.48936357336357333 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10933/20000] Train-Loss: 0.43884805251518916 Test-Loss: 0.14004121631178315 Train-f1: 0.4882007340667208 Test-f1: 0.43193277310924366 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10934/20000] Train-Loss: 0.4390234674385465 Test-Loss: 0.1437901855061005 Train-f1: 0.4866144877510184 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10935/20000] Train-Loss: 0.440682995461565 Test-Loss: 0.14216364942510626 Train-f1: 0.48165443789605 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10936/20000] Train-Loss: 0.4320910105120683 Test-Loss: 0.141996429787478 Train-f1: 0.4985401459854015 Test-f1: 0.4594011008433066 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [10937/20000] Train-Loss: 0.43755917667244953 Test-Loss: 0.14250395391481116 Train-f1: 0.4812487417374755 Test-f1: 0.4356787406570312 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10938/20000] Train-Loss: 0.4379254766005731 Test-Loss: 0.14222498159195088 Train-f1: 0.4897522007829422 Test-f1: 0.4826930070273724 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [10939/20000] Train-Loss: 0.436924119542444 Test-Loss: 0.1421998537337203 Train-f1: 0.48595393908884976 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10940/20000] Train-Loss: 0.43496000272730795 Test-Loss: 0.14612810548089283 Train-f1: 0.4861302114466672 Test-f1: 0.4628528560711606 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10941/20000] Train-Loss: 0.4364897121818616 Test-Loss: 0.14574622838122944 Train-f1: 0.4917919965620928 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10942/20000] Train-Loss: 0.43587671530317923 Test-Loss: 0.14599090082046431 Train-f1: 0.4864468864468864 Test-f1: 0.42567287784679086 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [10943/20000] Train-Loss: 0.4414426940475873 Test-Loss: 0.1440232704275471 Train-f1: 0.4888329141493698 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [10944/20000] Train-Loss: 0.4391512401384309 Test-Loss: 0.13923259509362929 Train-f1: 0.48093247372883036 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10945/20000] Train-Loss: 0.4384030649375448 Test-Loss: 0.1401191110129724 Train-f1: 0.47374441191814426 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [10946/20000] Train-Loss: 0.4397256584344604 Test-Loss: 0.142608463705085 Train-f1: 0.47771335373003776 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [10947/20000] Train-Loss: 0.43532570341861854 Test-Loss: 0.1395441693725518 Train-f1: 0.46340082709995256 Test-f1: 0.4754621848739496 Train-acc: 0.7847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [10948/20000] Train-Loss: 0.4316203261971791 Test-Loss: 0.14792904257586298 Train-f1: 0.4782393589847006 Test-f1: 0.4628528560711606 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [10949/20000] Train-Loss: 0.4387488702922827 Test-Loss: 0.14574712538466403 Train-f1: 0.48687347931873487 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10950/20000] Train-Loss: 0.4423623286492677 Test-Loss: 0.14196397463281407 Train-f1: 0.48397579490935616 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10951/20000] Train-Loss: 0.4347684339536448 Test-Loss: 0.13930408155040336 Train-f1: 0.48948363957790164 Test-f1: 0.5297402597402597 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [10952/20000] Train-Loss: 0.4390224852997538 Test-Loss: 0.14028062500593774 Train-f1: 0.48423336807932305 Test-f1: 0.518795518207283 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [10953/20000] Train-Loss: 0.43959169497670186 Test-Loss: 0.14397245233077272 Train-f1: 0.48653041658436774 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10954/20000] Train-Loss: 0.43596763536478594 Test-Loss: 0.13949777029348584 Train-f1: 0.48445432513793973 Test-f1: 0.5297402597402597 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [10955/20000] Train-Loss: 0.43753847428197795 Test-Loss: 0.1387955388959983 Train-f1: 0.491212559778505 Test-f1: 0.4714285714285714 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [10956/20000] Train-Loss: 0.43809541612050723 Test-Loss: 0.14049700397094525 Train-f1: 0.47766246099579435 Test-f1: 0.45393483709273186 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10957/20000] Train-Loss: 0.4371640277720409 Test-Loss: 0.1395605064407925 Train-f1: 0.47889829468776834 Test-f1: 0.5297402597402597 Train-acc: 0.804635761589404 Test-acc: 0.8026315789473685\n",
      "Epoch [10958/20000] Train-Loss: 0.43831379593152753 Test-Loss: 0.14072367506520286 Train-f1: 0.4830950972070194 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10959/20000] Train-Loss: 0.43892248367606274 Test-Loss: 0.14306372636030257 Train-f1: 0.4852046283293032 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10960/20000] Train-Loss: 0.4393461227968424 Test-Loss: 0.14394708942157328 Train-f1: 0.48679821050153055 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10961/20000] Train-Loss: 0.43641361675176943 Test-Loss: 0.1398951230470198 Train-f1: 0.48388278388278383 Test-f1: 0.4390972912712042 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [10962/20000] Train-Loss: 0.43880499510492665 Test-Loss: 0.1388247650155734 Train-f1: 0.4837802197802198 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10963/20000] Train-Loss: 0.4357383142670837 Test-Loss: 0.14194548312310376 Train-f1: 0.48077424858246776 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10964/20000] Train-Loss: 0.4375253094047492 Test-Loss: 0.1425532747008113 Train-f1: 0.4865494505494505 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10965/20000] Train-Loss: 0.4355866095264852 Test-Loss: 0.14373729955674913 Train-f1: 0.5167655903778471 Test-f1: 0.45233400741489715 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [10966/20000] Train-Loss: 0.43868901121635917 Test-Loss: 0.1425430841141405 Train-f1: 0.48397579490935616 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10967/20000] Train-Loss: 0.435990606790145 Test-Loss: 0.14175675087365544 Train-f1: 0.4837554108800857 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10968/20000] Train-Loss: 0.4360824041022194 Test-Loss: 0.14182067651179728 Train-f1: 0.48724746515629674 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10969/20000] Train-Loss: 0.4360384059235392 Test-Loss: 0.14433323449445234 Train-f1: 0.4816198704953846 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10970/20000] Train-Loss: 0.4385717071462122 Test-Loss: 0.1491630293934223 Train-f1: 0.5171013824031885 Test-f1: 0.43639684222929187 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [10971/20000] Train-Loss: 0.43771357241351794 Test-Loss: 0.14392332287655293 Train-f1: 0.4845660737380483 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10972/20000] Train-Loss: 0.43492698256398254 Test-Loss: 0.14721760040083878 Train-f1: 0.4806953183991946 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10973/20000] Train-Loss: 0.44404942391432284 Test-Loss: 0.1399983010202257 Train-f1: 0.4749704996989765 Test-f1: 0.42846033429278385 Train-acc: 0.7980132450331126 Test-acc: 0.7368421052631579\n",
      "Epoch [10974/20000] Train-Loss: 0.43760612090796397 Test-Loss: 0.14073702876111502 Train-f1: 0.4896306172616313 Test-f1: 0.4360203681942812 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [10975/20000] Train-Loss: 0.4407495965161996 Test-Loss: 0.14382182114337155 Train-f1: 0.47810195195639943 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [10976/20000] Train-Loss: 0.4361500359319902 Test-Loss: 0.14860263189622616 Train-f1: 0.4882029315012823 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [10977/20000] Train-Loss: 0.4379443236710974 Test-Loss: 0.1462051904896471 Train-f1: 0.48158822373546595 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10978/20000] Train-Loss: 0.4368555276570435 Test-Loss: 0.14148047316772935 Train-f1: 0.49129666380340237 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10979/20000] Train-Loss: 0.43694384710774414 Test-Loss: 0.14327179776265697 Train-f1: 0.4909615521909375 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [10980/20000] Train-Loss: 0.44116174314364737 Test-Loss: 0.13981859545627284 Train-f1: 0.4698779219574879 Test-f1: 0.4537451737451737 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [10981/20000] Train-Loss: 0.43379485222782627 Test-Loss: 0.13943540631514192 Train-f1: 0.4872171010747026 Test-f1: 0.46683117900509197 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [10982/20000] Train-Loss: 0.43569759474785885 Test-Loss: 0.1424852834390231 Train-f1: 0.4808230403917736 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [10983/20000] Train-Loss: 0.43733553575536216 Test-Loss: 0.13877557658997858 Train-f1: 0.5145565251525517 Test-f1: 0.45675057208237985 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [10984/20000] Train-Loss: 0.4382882231689388 Test-Loss: 0.13912757120435354 Train-f1: 0.4858544869326541 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10985/20000] Train-Loss: 0.4367183281618023 Test-Loss: 0.14294659033281895 Train-f1: 0.48377221349595 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [10986/20000] Train-Loss: 0.43386030433631856 Test-Loss: 0.13989826172221606 Train-f1: 0.4879714879714879 Test-f1: 0.45675057208237985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [10987/20000] Train-Loss: 0.435731516959183 Test-Loss: 0.13959678599833897 Train-f1: 0.4845050582661032 Test-f1: 0.45675057208237985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [10988/20000] Train-Loss: 0.4342035352199615 Test-Loss: 0.14476515103403187 Train-f1: 0.514009365553515 Test-f1: 0.4356787406570312 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [10989/20000] Train-Loss: 0.43756978218913406 Test-Loss: 0.13908529339318199 Train-f1: 0.48250867876674325 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [10990/20000] Train-Loss: 0.43907648087548135 Test-Loss: 0.13971466752470676 Train-f1: 0.48354412882714765 Test-f1: 0.4318216788305991 Train-acc: 0.8112582781456954 Test-acc: 0.7368421052631579\n",
      "Epoch [10991/20000] Train-Loss: 0.43809652972117696 Test-Loss: 0.14108220397917928 Train-f1: 0.48883550463519043 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [10992/20000] Train-Loss: 0.43644703782799255 Test-Loss: 0.1416207645265221 Train-f1: 0.4860218380677656 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [10993/20000] Train-Loss: 0.4361316014069516 Test-Loss: 0.14530143564769987 Train-f1: 0.4796883116883118 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [10994/20000] Train-Loss: 0.4368784974068572 Test-Loss: 0.14572742401297378 Train-f1: 0.48202870813397125 Test-f1: 0.4360203681942812 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [10995/20000] Train-Loss: 0.43543135246245285 Test-Loss: 0.1397028415070825 Train-f1: 0.4783642176047239 Test-f1: 0.4421230942970073 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10996/20000] Train-Loss: 0.43539349266425087 Test-Loss: 0.14014769036417724 Train-f1: 0.4788842320069069 Test-f1: 0.4421230942970073 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [10997/20000] Train-Loss: 0.43544375830848037 Test-Loss: 0.1424960179762677 Train-f1: 0.49101397460986496 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [10998/20000] Train-Loss: 0.436077850617055 Test-Loss: 0.141588888984423 Train-f1: 0.4926496400948956 Test-f1: 0.42567287784679086 Train-acc: 0.8211920529801324 Test-acc: 0.7368421052631579\n",
      "Epoch [10999/20000] Train-Loss: 0.43610368910200326 Test-Loss: 0.13909524080887004 Train-f1: 0.4813951497480658 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11000/20000] Train-Loss: 0.43611390842056713 Test-Loss: 0.13892562104126802 Train-f1: 0.48636182922507887 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11001/20000] Train-Loss: 0.4362651622467646 Test-Loss: 0.13964762410137666 Train-f1: 0.48917859031512095 Test-f1: 0.5297402597402597 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [11002/20000] Train-Loss: 0.4366232155852967 Test-Loss: 0.1411415255299127 Train-f1: 0.4887125597785049 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11003/20000] Train-Loss: 0.4336393724357531 Test-Loss: 0.13958446826179546 Train-f1: 0.4813267882868396 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [11004/20000] Train-Loss: 0.43683796391581836 Test-Loss: 0.13857735066196983 Train-f1: 0.47684440838803255 Test-f1: 0.460952380952381 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11005/20000] Train-Loss: 0.44035099886523843 Test-Loss: 0.14052226959805042 Train-f1: 0.48106692030742665 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11006/20000] Train-Loss: 0.4371663006176414 Test-Loss: 0.14699396396342557 Train-f1: 0.48568715235381904 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11007/20000] Train-Loss: 0.43065493005232885 Test-Loss: 0.14798427416723128 Train-f1: 0.48165443789605 Test-f1: 0.4726570048309179 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11008/20000] Train-Loss: 0.4419319233310141 Test-Loss: 0.13957282529293463 Train-f1: 0.4843674593668944 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11009/20000] Train-Loss: 0.44203458397982065 Test-Loss: 0.14029121854267224 Train-f1: 0.4903726717920266 Test-f1: 0.42567287784679086 Train-acc: 0.8178807947019867 Test-acc: 0.7368421052631579\n",
      "Epoch [11010/20000] Train-Loss: 0.43298323886818585 Test-Loss: 0.14193035187642236 Train-f1: 0.4857184594515929 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11011/20000] Train-Loss: 0.43660902174513705 Test-Loss: 0.13960705666566842 Train-f1: 0.4844773750473054 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11012/20000] Train-Loss: 0.4357365732473417 Test-Loss: 0.14183988762681612 Train-f1: 0.4834539390888498 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11013/20000] Train-Loss: 0.43380829090197665 Test-Loss: 0.1393801921099184 Train-f1: 0.4813658214285265 Test-f1: 0.42846033429278385 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [11014/20000] Train-Loss: 0.43829071311896717 Test-Loss: 0.14275663119520973 Train-f1: 0.48439826839826844 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11015/20000] Train-Loss: 0.4376215420908913 Test-Loss: 0.14099138998870955 Train-f1: 0.47671508720333317 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11016/20000] Train-Loss: 0.4342917894735139 Test-Loss: 0.1391304250932191 Train-f1: 0.4918544533156795 Test-f1: 0.5066513056835638 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11017/20000] Train-Loss: 0.43433371673192117 Test-Loss: 0.1394704577705354 Train-f1: 0.4890249475009444 Test-f1: 0.518795518207283 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11018/20000] Train-Loss: 0.4394452354633521 Test-Loss: 0.1389535206628448 Train-f1: 0.49008553753079304 Test-f1: 0.4647619047619048 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11019/20000] Train-Loss: 0.43824319810307266 Test-Loss: 0.13992653381238157 Train-f1: 0.48948363957790164 Test-f1: 0.45675057208237985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11020/20000] Train-Loss: 0.43646926914271456 Test-Loss: 0.14118647759705505 Train-f1: 0.4811294947660527 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11021/20000] Train-Loss: 0.4361962194412933 Test-Loss: 0.1425140759894102 Train-f1: 0.48250319961163335 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11022/20000] Train-Loss: 0.4303924399909991 Test-Loss: 0.1384259178262364 Train-f1: 0.4932209289042774 Test-f1: 0.4714285714285714 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11023/20000] Train-Loss: 0.4338352729694358 Test-Loss: 0.1447093661175907 Train-f1: 0.4887125597785049 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11024/20000] Train-Loss: 0.4319592503526305 Test-Loss: 0.15024994758760674 Train-f1: 0.4817176448088828 Test-f1: 0.4751156336725254 Train-acc: 0.8079470198675497 Test-acc: 0.8026315789473685\n",
      "Epoch [11025/20000] Train-Loss: 0.4402892583445173 Test-Loss: 0.13922126901619267 Train-f1: 0.48193406593406596 Test-f1: 0.4390972912712042 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [11026/20000] Train-Loss: 0.436080046991456 Test-Loss: 0.14886119544064993 Train-f1: 0.4866811923708475 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11027/20000] Train-Loss: 0.43884356436755195 Test-Loss: 0.13889256873933575 Train-f1: 0.5021675435611513 Test-f1: 0.4714285714285714 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11028/20000] Train-Loss: 0.4353557361514906 Test-Loss: 0.14070124611926957 Train-f1: 0.4808958631743442 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11029/20000] Train-Loss: 0.4356856109818119 Test-Loss: 0.13886412387064412 Train-f1: 0.48027918027918026 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11030/20000] Train-Loss: 0.43639936209122704 Test-Loss: 0.14052796591510602 Train-f1: 0.4854610080393235 Test-f1: 0.4356787406570312 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [11031/20000] Train-Loss: 0.4358181714989548 Test-Loss: 0.1392932863439645 Train-f1: 0.4918544533156795 Test-f1: 0.4714285714285714 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11032/20000] Train-Loss: 0.4364080738356141 Test-Loss: 0.13900413768157932 Train-f1: 0.4837554108800857 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11033/20000] Train-Loss: 0.43166605613162634 Test-Loss: 0.14659793300898952 Train-f1: 0.488513974609865 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11034/20000] Train-Loss: 0.43575504439053603 Test-Loss: 0.14495809557427689 Train-f1: 0.4912294821137828 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11035/20000] Train-Loss: 0.43578528259567567 Test-Loss: 0.14674623412774118 Train-f1: 0.48845393908884976 Test-f1: 0.4624973437035148 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11036/20000] Train-Loss: 0.4374882239706412 Test-Loss: 0.13852043426356353 Train-f1: 0.48923677631840007 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11037/20000] Train-Loss: 0.43123596032108324 Test-Loss: 0.1391233734929869 Train-f1: 0.4867121889961542 Test-f1: 0.5183710801393728 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11038/20000] Train-Loss: 0.44186800073344684 Test-Loss: 0.1387389310281081 Train-f1: 0.473129869332401 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11039/20000] Train-Loss: 0.43455657690659133 Test-Loss: 0.14647508058772504 Train-f1: 0.48930696914659083 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11040/20000] Train-Loss: 0.43240650404884423 Test-Loss: 0.13935637756616395 Train-f1: 0.49101886062267097 Test-f1: 0.43932367149758456 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [11041/20000] Train-Loss: 0.43783123447410127 Test-Loss: 0.13990202269946816 Train-f1: 0.48397579490935616 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11042/20000] Train-Loss: 0.43402267602703914 Test-Loss: 0.1384863864848887 Train-f1: 0.49379019099721655 Test-f1: 0.460952380952381 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11043/20000] Train-Loss: 0.4399022610068303 Test-Loss: 0.15648029067554 Train-f1: 0.4890267824771032 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11044/20000] Train-Loss: 0.43722236014824467 Test-Loss: 0.14083806037503482 Train-f1: 0.48662480153187265 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11045/20000] Train-Loss: 0.43597512573129915 Test-Loss: 0.13939745673464707 Train-f1: 0.47899082113806346 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11046/20000] Train-Loss: 0.4336978175188306 Test-Loss: 0.1423468754331727 Train-f1: 0.488513974609865 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11047/20000] Train-Loss: 0.4351434644998212 Test-Loss: 0.1412688807201506 Train-f1: 0.5002314581975333 Test-f1: 0.49428571428571433 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11048/20000] Train-Loss: 0.4357714871720889 Test-Loss: 0.14558387615153676 Train-f1: 0.5065882820859497 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11049/20000] Train-Loss: 0.4287492711915006 Test-Loss: 0.14827905566064298 Train-f1: 0.48377221349595 Test-f1: 0.4726570048309179 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11050/20000] Train-Loss: 0.4353326765041682 Test-Loss: 0.13947023005940096 Train-f1: 0.4799897396599018 Test-f1: 0.4485461286270184 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11051/20000] Train-Loss: 0.43834559294960096 Test-Loss: 0.14004933870728437 Train-f1: 0.47892257168119234 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11052/20000] Train-Loss: 0.43713816073519696 Test-Loss: 0.14194933424061076 Train-f1: 0.48435450728133655 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11053/20000] Train-Loss: 0.4373971559446835 Test-Loss: 0.14011696081601313 Train-f1: 0.4969473684210527 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11054/20000] Train-Loss: 0.4359585731192141 Test-Loss: 0.1393881992366742 Train-f1: 0.4794085109521351 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11055/20000] Train-Loss: 0.4344547709451272 Test-Loss: 0.1391224326260574 Train-f1: 0.4811294947660527 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11056/20000] Train-Loss: 0.43754822665071735 Test-Loss: 0.13913090928625144 Train-f1: 0.4838140585244419 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11057/20000] Train-Loss: 0.43695647326114145 Test-Loss: 0.13974608735177468 Train-f1: 0.4812966687543575 Test-f1: 0.4318216788305991 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [11058/20000] Train-Loss: 0.433391035132478 Test-Loss: 0.1390148859713706 Train-f1: 0.4917860918544533 Test-f1: 0.518795518207283 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11059/20000] Train-Loss: 0.4396877040138435 Test-Loss: 0.1450218924041284 Train-f1: 0.48288316999973435 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11060/20000] Train-Loss: 0.4317974628227343 Test-Loss: 0.14024434060759783 Train-f1: 0.49989097210719 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11061/20000] Train-Loss: 0.4368615107652109 Test-Loss: 0.13901596049282722 Train-f1: 0.4837554108800857 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11062/20000] Train-Loss: 0.43783605556661576 Test-Loss: 0.14032365604801078 Train-f1: 0.4845660737380483 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11063/20000] Train-Loss: 0.43763156589222624 Test-Loss: 0.13890489911161963 Train-f1: 0.4863038590655703 Test-f1: 0.4503312334891282 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11064/20000] Train-Loss: 0.43334933800403114 Test-Loss: 0.13855762950910147 Train-f1: 0.4791033527875633 Test-f1: 0.47043478260869565 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11065/20000] Train-Loss: 0.43362948256576533 Test-Loss: 0.14162905248422422 Train-f1: 0.5105827920565855 Test-f1: 0.45233400741489715 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11066/20000] Train-Loss: 0.43552971225021997 Test-Loss: 0.13983483876456068 Train-f1: 0.5014147000901967 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11067/20000] Train-Loss: 0.4316991911069707 Test-Loss: 0.13847400377390034 Train-f1: 0.48911355311355315 Test-f1: 0.47043478260869565 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [11068/20000] Train-Loss: 0.4348497042091563 Test-Loss: 0.13835567039835836 Train-f1: 0.4805345225550705 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11069/20000] Train-Loss: 0.43087673678888294 Test-Loss: 0.14783492248030133 Train-f1: 0.4894888173771606 Test-f1: 0.47307359307359304 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11070/20000] Train-Loss: 0.4394894571102446 Test-Loss: 0.14411599826571125 Train-f1: 0.4784449310076634 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11071/20000] Train-Loss: 0.4332845635793756 Test-Loss: 0.13856330075718212 Train-f1: 0.48340517479947487 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11072/20000] Train-Loss: 0.43646619957640054 Test-Loss: 0.14474061404710115 Train-f1: 0.480434442165412 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11073/20000] Train-Loss: 0.43423789917702077 Test-Loss: 0.13918833253483875 Train-f1: 0.4896839769217466 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11074/20000] Train-Loss: 0.42899495607641064 Test-Loss: 0.14746135265022145 Train-f1: 0.48249112589538123 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11075/20000] Train-Loss: 0.4364787961123798 Test-Loss: 0.14151890715542204 Train-f1: 0.48260855922600887 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11076/20000] Train-Loss: 0.43099454367510825 Test-Loss: 0.1518450604882691 Train-f1: 0.4885755753680282 Test-f1: 0.4695652173913043 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11077/20000] Train-Loss: 0.43166807478556213 Test-Loss: 0.1399260390178326 Train-f1: 0.487669969089324 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11078/20000] Train-Loss: 0.4332898313941762 Test-Loss: 0.1406271666625894 Train-f1: 0.49002630479140546 Test-f1: 0.5297402597402597 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [11079/20000] Train-Loss: 0.4347664319606898 Test-Loss: 0.1394918725445802 Train-f1: 0.4832339874019035 Test-f1: 0.4421230942970073 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [11080/20000] Train-Loss: 0.4378079947386832 Test-Loss: 0.1400103302262612 Train-f1: 0.4813186813186813 Test-f1: 0.4385580319084156 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [11081/20000] Train-Loss: 0.43677049848697563 Test-Loss: 0.14119564842301507 Train-f1: 0.4816203404438698 Test-f1: 0.518795518207283 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11082/20000] Train-Loss: 0.4314303362327575 Test-Loss: 0.14282601523213637 Train-f1: 0.48789052558180074 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11083/20000] Train-Loss: 0.4352829503362642 Test-Loss: 0.14675925978394658 Train-f1: 0.4885755753680282 Test-f1: 0.4624973437035148 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11084/20000] Train-Loss: 0.4383979861691637 Test-Loss: 0.13992151060073635 Train-f1: 0.4767032967032967 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11085/20000] Train-Loss: 0.43336252524503366 Test-Loss: 0.13878009811767725 Train-f1: 0.4842347195288371 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11086/20000] Train-Loss: 0.43898965430779036 Test-Loss: 0.14236219383855125 Train-f1: 0.4811838391499144 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11087/20000] Train-Loss: 0.43581646517157413 Test-Loss: 0.14006826181758084 Train-f1: 0.48397579490935616 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11088/20000] Train-Loss: 0.4344647596718894 Test-Loss: 0.14538110003659008 Train-f1: 0.48409676809676816 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11089/20000] Train-Loss: 0.4318166746985231 Test-Loss: 0.14032439591973184 Train-f1: 0.4818906791811359 Test-f1: 0.47043478260869565 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11090/20000] Train-Loss: 0.4262628475676459 Test-Loss: 0.14841333704762125 Train-f1: 0.48550781853447533 Test-f1: 0.46683117900509197 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11091/20000] Train-Loss: 0.44002770965198307 Test-Loss: 0.14201341457677916 Train-f1: 0.48534431422236307 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11092/20000] Train-Loss: 0.4349497194164267 Test-Loss: 0.14060578242969468 Train-f1: 0.47538086352102527 Test-f1: 0.49417417417417414 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11093/20000] Train-Loss: 0.4379226146954594 Test-Loss: 0.1424855875255795 Train-f1: 0.4868862369804991 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11094/20000] Train-Loss: 0.43782999712891535 Test-Loss: 0.13999483633689436 Train-f1: 0.48636182922507887 Test-f1: 0.4421230942970073 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [11095/20000] Train-Loss: 0.4329725471022794 Test-Loss: 0.138534424300452 Train-f1: 0.4816203404438698 Test-f1: 0.4754621848739496 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11096/20000] Train-Loss: 0.4345164633123779 Test-Loss: 0.13892399718878887 Train-f1: 0.48990584359711875 Test-f1: 0.4647619047619048 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11097/20000] Train-Loss: 0.43752205953511725 Test-Loss: 0.14073242485730328 Train-f1: 0.4827188377679989 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11098/20000] Train-Loss: 0.43605470985280753 Test-Loss: 0.13855075674991865 Train-f1: 0.4840177571897318 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11099/20000] Train-Loss: 0.43767500449792074 Test-Loss: 0.13974357125091474 Train-f1: 0.48605985209758795 Test-f1: 0.42846033429278385 Train-acc: 0.8145695364238411 Test-acc: 0.7368421052631579\n",
      "Epoch [11100/20000] Train-Loss: 0.43611490813174175 Test-Loss: 0.1395446085076867 Train-f1: 0.4835061147973253 Test-f1: 0.43932367149758456 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [11101/20000] Train-Loss: 0.4315986150862304 Test-Loss: 0.14209347781474763 Train-f1: 0.4830950972070194 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11102/20000] Train-Loss: 0.43312561497912805 Test-Loss: 0.14137170347699068 Train-f1: 0.48756188623738295 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11103/20000] Train-Loss: 0.435636720196673 Test-Loss: 0.14250858190852642 Train-f1: 0.487165885579598 Test-f1: 0.49327561327561326 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11104/20000] Train-Loss: 0.43086559315809936 Test-Loss: 0.1428100208891994 Train-f1: 0.49008553753079304 Test-f1: 0.46683117900509197 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11105/20000] Train-Loss: 0.4313941830649345 Test-Loss: 0.14704183538192347 Train-f1: 0.4904100646845885 Test-f1: 0.4628528560711606 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11106/20000] Train-Loss: 0.4380651923562403 Test-Loss: 0.13898310294338745 Train-f1: 0.4790059613589025 Test-f1: 0.5077443609022556 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11107/20000] Train-Loss: 0.43609587359663676 Test-Loss: 0.1386473757662283 Train-f1: 0.48219213045019493 Test-f1: 0.518795518207283 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11108/20000] Train-Loss: 0.4375210456423036 Test-Loss: 0.13943845033262134 Train-f1: 0.4785883470527108 Test-f1: 0.44240059457450764 Train-acc: 0.804635761589404 Test-acc: 0.75\n",
      "Epoch [11109/20000] Train-Loss: 0.4343923197968809 Test-Loss: 0.1398406940393773 Train-f1: 0.4884770801412481 Test-f1: 0.49327561327561326 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11110/20000] Train-Loss: 0.4354601300776149 Test-Loss: 0.13967105431132495 Train-f1: 0.48201863292157726 Test-f1: 0.4318216788305991 Train-acc: 0.8079470198675497 Test-acc: 0.7368421052631579\n",
      "Epoch [11111/20000] Train-Loss: 0.4351214153637459 Test-Loss: 0.13873418500103626 Train-f1: 0.4863038590655703 Test-f1: 0.5297402597402597 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [11112/20000] Train-Loss: 0.43846653212954395 Test-Loss: 0.13991449481821616 Train-f1: 0.48412727623253937 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11113/20000] Train-Loss: 0.4356880321784519 Test-Loss: 0.13825117337356574 Train-f1: 0.4944272566376128 Test-f1: 0.460952380952381 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11114/20000] Train-Loss: 0.4315952120127087 Test-Loss: 0.1406865685190325 Train-f1: 0.49241985896702883 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11115/20000] Train-Loss: 0.43754736786022325 Test-Loss: 0.1437327874413454 Train-f1: 0.4857184594515929 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11116/20000] Train-Loss: 0.4346663687209349 Test-Loss: 0.14115744625340307 Train-f1: 0.484393085887477 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11117/20000] Train-Loss: 0.43908280172680714 Test-Loss: 0.13851004039431708 Train-f1: 0.4742781519555713 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11118/20000] Train-Loss: 0.4343785138977438 Test-Loss: 0.140375818576691 Train-f1: 0.48839521844916967 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11119/20000] Train-Loss: 0.43848177632950425 Test-Loss: 0.14069982600354675 Train-f1: 0.4798561633014188 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11120/20000] Train-Loss: 0.4315645837085286 Test-Loss: 0.14170828262393248 Train-f1: 0.48397579490935616 Test-f1: 0.4558710801393729 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11121/20000] Train-Loss: 0.4351702885825267 Test-Loss: 0.14415352791258418 Train-f1: 0.4820930747105243 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11122/20000] Train-Loss: 0.4304269185938521 Test-Loss: 0.15320757648366343 Train-f1: 0.4715221324859879 Test-f1: 0.4603698150924537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [11123/20000] Train-Loss: 0.43634641126269474 Test-Loss: 0.14463612214305327 Train-f1: 0.4946922358687065 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11124/20000] Train-Loss: 0.43120119006726315 Test-Loss: 0.13820193429979533 Train-f1: 0.49518660287081334 Test-f1: 0.4714285714285714 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11125/20000] Train-Loss: 0.43534473281476005 Test-Loss: 0.14425596151795747 Train-f1: 0.48513416086172045 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11126/20000] Train-Loss: 0.43356388691485964 Test-Loss: 0.1392134255501403 Train-f1: 0.48397579490935616 Test-f1: 0.518795518207283 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11127/20000] Train-Loss: 0.43593103811299916 Test-Loss: 0.1403955905050676 Train-f1: 0.4873828348280904 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11128/20000] Train-Loss: 0.4369438182342167 Test-Loss: 0.14291348330679793 Train-f1: 0.4800940525764098 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [11129/20000] Train-Loss: 0.4310908104452417 Test-Loss: 0.13814122217789734 Train-f1: 0.48591091100680134 Test-f1: 0.46016451233842537 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11130/20000] Train-Loss: 0.43189161136990784 Test-Loss: 0.14181021179193365 Train-f1: 0.48473904387210387 Test-f1: 0.49327561327561326 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11131/20000] Train-Loss: 0.43350555609540226 Test-Loss: 0.1386124146047256 Train-f1: 0.4699580249439526 Test-f1: 0.4497330282227307 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [11132/20000] Train-Loss: 0.4378031363404933 Test-Loss: 0.13901217008604505 Train-f1: 0.4888329141493698 Test-f1: 0.4537451737451737 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11133/20000] Train-Loss: 0.43464014765942877 Test-Loss: 0.1398835386981161 Train-f1: 0.5010659220850304 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11134/20000] Train-Loss: 0.4355089790467055 Test-Loss: 0.14023790187740742 Train-f1: 0.4865494505494505 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11135/20000] Train-Loss: 0.43074096317147026 Test-Loss: 0.1385653733202142 Train-f1: 0.47961148528890457 Test-f1: 0.5297402597402597 Train-acc: 0.804635761589404 Test-acc: 0.8026315789473685\n",
      "Epoch [11136/20000] Train-Loss: 0.426783731602854 Test-Loss: 0.13964936925280771 Train-f1: 0.49652749535881596 Test-f1: 0.44951827242524917 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11137/20000] Train-Loss: 0.43779502210017557 Test-Loss: 0.14223393144701066 Train-f1: 0.49093406593406597 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11138/20000] Train-Loss: 0.4294009181463692 Test-Loss: 0.14999813126829725 Train-f1: 0.4853203020499482 Test-f1: 0.4652641421224871 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11139/20000] Train-Loss: 0.43453646662431683 Test-Loss: 0.1406844514948645 Train-f1: 0.4813651903651904 Test-f1: 0.44649416182596957 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11140/20000] Train-Loss: 0.4333386195650772 Test-Loss: 0.13891907878630205 Train-f1: 0.4865415231587532 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11141/20000] Train-Loss: 0.43487660144640283 Test-Loss: 0.13868608906304883 Train-f1: 0.4869818384518618 Test-f1: 0.46016451233842537 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11142/20000] Train-Loss: 0.4323014247807533 Test-Loss: 0.14922838275812103 Train-f1: 0.48699903831426283 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11143/20000] Train-Loss: 0.4328716234594014 Test-Loss: 0.13896629216991788 Train-f1: 0.4869639132797027 Test-f1: 0.5297402597402597 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [11144/20000] Train-Loss: 0.4340406433529773 Test-Loss: 0.13946795473745532 Train-f1: 0.48857086999022475 Test-f1: 0.4423271037905184 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [11145/20000] Train-Loss: 0.43228156902848114 Test-Loss: 0.14210425100026514 Train-f1: 0.4933855510542626 Test-f1: 0.4526190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11146/20000] Train-Loss: 0.4339394141762711 Test-Loss: 0.144532528133534 Train-f1: 0.4786520146520147 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11147/20000] Train-Loss: 0.43110342211151254 Test-Loss: 0.14524352408350752 Train-f1: 0.4791341991341991 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11148/20000] Train-Loss: 0.43893258035475696 Test-Loss: 0.13830952038114908 Train-f1: 0.47821476768845195 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11149/20000] Train-Loss: 0.4350121693205355 Test-Loss: 0.138881899928764 Train-f1: 0.4900572849649592 Test-f1: 0.45675057208237985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11150/20000] Train-Loss: 0.42786096475073876 Test-Loss: 0.15156959458610175 Train-f1: 0.48860565521435084 Test-f1: 0.4695652173913043 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11151/20000] Train-Loss: 0.4336914367055881 Test-Loss: 0.13946805117129696 Train-f1: 0.4890617270439742 Test-f1: 0.45393483709273186 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11152/20000] Train-Loss: 0.43287527442259466 Test-Loss: 0.13952863523618747 Train-f1: 0.4866144877510184 Test-f1: 0.518795518207283 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11153/20000] Train-Loss: 0.43300888198672205 Test-Loss: 0.14219760577599355 Train-f1: 0.4804402387735721 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11154/20000] Train-Loss: 0.42968604761682844 Test-Loss: 0.14497607216746755 Train-f1: 0.49136455971899007 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11155/20000] Train-Loss: 0.4280282925202606 Test-Loss: 0.14864354672619787 Train-f1: 0.4946261674907778 Test-f1: 0.4726570048309179 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11156/20000] Train-Loss: 0.4347196761100309 Test-Loss: 0.13904101268910807 Train-f1: 0.4793520493138611 Test-f1: 0.5297402597402597 Train-acc: 0.804635761589404 Test-acc: 0.8026315789473685\n",
      "Epoch [11157/20000] Train-Loss: 0.43909368241148716 Test-Loss: 0.13881166591694244 Train-f1: 0.4890249475009444 Test-f1: 0.518795518207283 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11158/20000] Train-Loss: 0.43444246010787557 Test-Loss: 0.13837784959420776 Train-f1: 0.4737224694836481 Test-f1: 0.46016451233842537 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [11159/20000] Train-Loss: 0.4333969777534026 Test-Loss: 0.13978046072071698 Train-f1: 0.4919595008241112 Test-f1: 0.4421230942970073 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [11160/20000] Train-Loss: 0.43473187584557677 Test-Loss: 0.13974067590274966 Train-f1: 0.4763915822739352 Test-f1: 0.5056027456027457 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11161/20000] Train-Loss: 0.43277707232063317 Test-Loss: 0.1383247807332148 Train-f1: 0.47793669770953945 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11162/20000] Train-Loss: 0.43107330108722947 Test-Loss: 0.13850666899263156 Train-f1: 0.47566151490202124 Test-f1: 0.518795518207283 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [11163/20000] Train-Loss: 0.43700743039141626 Test-Loss: 0.1397690926861298 Train-f1: 0.4881562881562882 Test-f1: 0.4463157894736843 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [11164/20000] Train-Loss: 0.4352787664885265 Test-Loss: 0.13994446963282722 Train-f1: 0.48917859031512095 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11165/20000] Train-Loss: 0.42688413048163476 Test-Loss: 0.13893444305673336 Train-f1: 0.48699903831426283 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11166/20000] Train-Loss: 0.43586799483409666 Test-Loss: 0.13875740283260707 Train-f1: 0.4827188377679989 Test-f1: 0.4497330282227307 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11167/20000] Train-Loss: 0.4318022870991744 Test-Loss: 0.14098335179626986 Train-f1: 0.48333876366221384 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11168/20000] Train-Loss: 0.43658385892375295 Test-Loss: 0.13854288107364943 Train-f1: 0.48027918027918026 Test-f1: 0.4503312334891282 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11169/20000] Train-Loss: 0.4284689843869882 Test-Loss: 0.15003050302301263 Train-f1: 0.5341473302836048 Test-f1: 0.4652641421224871 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [11170/20000] Train-Loss: 0.4353844261178171 Test-Loss: 0.1429076060444973 Train-f1: 0.4861218453318067 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11171/20000] Train-Loss: 0.43217868986169383 Test-Loss: 0.13877894930948867 Train-f1: 0.4823418346626044 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11172/20000] Train-Loss: 0.43447466708890903 Test-Loss: 0.13829777665546866 Train-f1: 0.48397579490935616 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11173/20000] Train-Loss: 0.4330709192163314 Test-Loss: 0.14682238360222888 Train-f1: 0.4761315402295194 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11174/20000] Train-Loss: 0.43263412151101854 Test-Loss: 0.13980986432178066 Train-f1: 0.49775984426873776 Test-f1: 0.45233400741489715 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11175/20000] Train-Loss: 0.431587345762932 Test-Loss: 0.1453653399550749 Train-f1: 0.49155659307056465 Test-f1: 0.4624973437035148 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11176/20000] Train-Loss: 0.4282495836543748 Test-Loss: 0.14988799909538433 Train-f1: 0.5036140226698332 Test-f1: 0.4751156336725254 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [11177/20000] Train-Loss: 0.42973419803608337 Test-Loss: 0.14440141274866924 Train-f1: 0.4793520493138611 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11178/20000] Train-Loss: 0.43310339271535525 Test-Loss: 0.14124336214161443 Train-f1: 0.4768344036915465 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11179/20000] Train-Loss: 0.43623526756132713 Test-Loss: 0.13839343633260093 Train-f1: 0.48946347769877185 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11180/20000] Train-Loss: 0.4337260314431891 Test-Loss: 0.14141753190749226 Train-f1: 0.48106692030742665 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11181/20000] Train-Loss: 0.4363244144377624 Test-Loss: 0.14000703343449203 Train-f1: 0.48556423300948853 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11182/20000] Train-Loss: 0.4315068572241928 Test-Loss: 0.14481410245303666 Train-f1: 0.4853203020499482 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11183/20000] Train-Loss: 0.4330680963894077 Test-Loss: 0.138730281428053 Train-f1: 0.48340517479947487 Test-f1: 0.45675057208237985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11184/20000] Train-Loss: 0.4345966602128717 Test-Loss: 0.13869899673022937 Train-f1: 0.4916370645600271 Test-f1: 0.4390972912712042 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [11185/20000] Train-Loss: 0.4326580716230819 Test-Loss: 0.14180917185989983 Train-f1: 0.47763197947247027 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11186/20000] Train-Loss: 0.43363766652036373 Test-Loss: 0.14254911307828133 Train-f1: 0.4844773750473054 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11187/20000] Train-Loss: 0.4361909368184861 Test-Loss: 0.14084008200758696 Train-f1: 0.47886411833780257 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11188/20000] Train-Loss: 0.43388554248482397 Test-Loss: 0.14110841375856017 Train-f1: 0.4860218380677656 Test-f1: 0.49417417417417414 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11189/20000] Train-Loss: 0.4325633473371994 Test-Loss: 0.13955143709449927 Train-f1: 0.4857184594515929 Test-f1: 0.4533424822739276 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11190/20000] Train-Loss: 0.4301436806658893 Test-Loss: 0.14343308301800667 Train-f1: 0.4886618570162874 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11191/20000] Train-Loss: 0.42936716147327497 Test-Loss: 0.14580802329368406 Train-f1: 0.488513974609865 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11192/20000] Train-Loss: 0.4334148012349593 Test-Loss: 0.13963923668917536 Train-f1: 0.4765071222495914 Test-f1: 0.44558629776021075 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11193/20000] Train-Loss: 0.4320424517414382 Test-Loss: 0.142811501875216 Train-f1: 0.4840930998825736 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11194/20000] Train-Loss: 0.4337794235083544 Test-Loss: 0.14053887921850566 Train-f1: 0.4885755753680282 Test-f1: 0.4454978354978355 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [11195/20000] Train-Loss: 0.4372435274507781 Test-Loss: 0.1406727689736941 Train-f1: 0.47697270714582585 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11196/20000] Train-Loss: 0.4273067274519005 Test-Loss: 0.14453529536559537 Train-f1: 0.4876560351381432 Test-f1: 0.4628528560711606 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11197/20000] Train-Loss: 0.4388376594121427 Test-Loss: 0.1408792394023626 Train-f1: 0.47343259134764554 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [11198/20000] Train-Loss: 0.42992070659929976 Test-Loss: 0.1425348039572201 Train-f1: 0.48605985209758795 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11199/20000] Train-Loss: 0.43171785021734616 Test-Loss: 0.14239466700372985 Train-f1: 0.48911355311355315 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11200/20000] Train-Loss: 0.4350162013426039 Test-Loss: 0.14108316093799741 Train-f1: 0.4801188354129531 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11201/20000] Train-Loss: 0.43259131588800565 Test-Loss: 0.1466237457833378 Train-f1: 0.4814648771681972 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11202/20000] Train-Loss: 0.437531423483436 Test-Loss: 0.14300989842186818 Train-f1: 0.5139001810600096 Test-f1: 0.4466252587991718 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11203/20000] Train-Loss: 0.430021881188331 Test-Loss: 0.1418084168336871 Train-f1: 0.49433976726272977 Test-f1: 0.45233400741489715 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11204/20000] Train-Loss: 0.4322929450097814 Test-Loss: 0.1387350442211636 Train-f1: 0.4857184594515929 Test-f1: 0.4497330282227307 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11205/20000] Train-Loss: 0.42876600240516716 Test-Loss: 0.14082823750005663 Train-f1: 0.4889095999257159 Test-f1: 0.45675057208237985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11206/20000] Train-Loss: 0.43156145660064116 Test-Loss: 0.1400196771880294 Train-f1: 0.48571963316488864 Test-f1: 0.449645003282331 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11207/20000] Train-Loss: 0.43368317713454557 Test-Loss: 0.139810912065478 Train-f1: 0.4885755753680282 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11208/20000] Train-Loss: 0.42968616996593717 Test-Loss: 0.14928270566736493 Train-f1: 0.48056610056610055 Test-f1: 0.4751156336725254 Train-acc: 0.8079470198675497 Test-acc: 0.8026315789473685\n",
      "best score_dx!!\n",
      "Epoch [11209/20000] Train-Loss: 0.42953885379138484 Test-Loss: 0.13795392809291462 Train-f1: 0.49400424137266247 Test-f1: 0.47043478260869565 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [11210/20000] Train-Loss: 0.4320831656673854 Test-Loss: 0.13780845215072307 Train-f1: 0.4922838463525624 Test-f1: 0.460952380952381 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11211/20000] Train-Loss: 0.4299959309329752 Test-Loss: 0.14015555238699537 Train-f1: 0.4919595008241112 Test-f1: 0.4423271037905184 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [11212/20000] Train-Loss: 0.43012350323576787 Test-Loss: 0.13840036227782337 Train-f1: 0.4792246733541329 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11213/20000] Train-Loss: 0.42866787254743705 Test-Loss: 0.14313236142924485 Train-f1: 0.48159570154095704 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11214/20000] Train-Loss: 0.42905190828475603 Test-Loss: 0.14429117817302328 Train-f1: 0.49443509847696987 Test-f1: 0.4594670406732118 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11215/20000] Train-Loss: 0.43013470788575725 Test-Loss: 0.140362964315814 Train-f1: 0.4843674593668944 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11216/20000] Train-Loss: 0.43114692890700823 Test-Loss: 0.13965356301112405 Train-f1: 0.49208104217530424 Test-f1: 0.518795518207283 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11217/20000] Train-Loss: 0.42459825351842895 Test-Loss: 0.15085281601782521 Train-f1: 0.4970621421599798 Test-f1: 0.47307359307359304 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [11218/20000] Train-Loss: 0.4353484449579209 Test-Loss: 0.14388832397385756 Train-f1: 0.5021338495791051 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11219/20000] Train-Loss: 0.4306669327559019 Test-Loss: 0.1460776820901034 Train-f1: 0.4844353795484306 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11220/20000] Train-Loss: 0.4335080354468143 Test-Loss: 0.13950161434281882 Train-f1: 0.49267166404992657 Test-f1: 0.4497330282227307 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11221/20000] Train-Loss: 0.4316488025505651 Test-Loss: 0.14431861135442048 Train-f1: 0.48477319781667605 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11222/20000] Train-Loss: 0.4334443873482497 Test-Loss: 0.14033982591149954 Train-f1: 0.4801190933538225 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11223/20000] Train-Loss: 0.43022408244853033 Test-Loss: 0.14019247666185944 Train-f1: 0.4874394422781519 Test-f1: 0.44558629776021075 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11224/20000] Train-Loss: 0.43210525720894066 Test-Loss: 0.14268446182115313 Train-f1: 0.47763197947247027 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11225/20000] Train-Loss: 0.43708236180950505 Test-Loss: 0.14385669584874247 Train-f1: 0.48614201870952145 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11226/20000] Train-Loss: 0.432649693261781 Test-Loss: 0.14191549621457367 Train-f1: 0.48232295482295484 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11227/20000] Train-Loss: 0.4302878493015213 Test-Loss: 0.14024263740113785 Train-f1: 0.48797471843829454 Test-f1: 0.46683117900509197 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11228/20000] Train-Loss: 0.428352718179813 Test-Loss: 0.14643560623031426 Train-f1: 0.4919398130605027 Test-f1: 0.4594670406732118 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11229/20000] Train-Loss: 0.4272806421426876 Test-Loss: 0.13838964656436217 Train-f1: 0.48946347769877185 Test-f1: 0.4488311688311688 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11230/20000] Train-Loss: 0.43257699928505816 Test-Loss: 0.1426098474664941 Train-f1: 0.48556423300948853 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11231/20000] Train-Loss: 0.4282146502239273 Test-Loss: 0.14102429758984078 Train-f1: 0.488513990797956 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11232/20000] Train-Loss: 0.43198926769976265 Test-Loss: 0.14022286751088178 Train-f1: 0.48105112506057585 Test-f1: 0.4360203681942812 Train-acc: 0.8079470198675497 Test-acc: 0.75\n",
      "Epoch [11233/20000] Train-Loss: 0.43237980722584657 Test-Loss: 0.14036246961035334 Train-f1: 0.47674977378556616 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11234/20000] Train-Loss: 0.4302410770407838 Test-Loss: 0.13914360773520923 Train-f1: 0.48911355311355315 Test-f1: 0.4466252587991718 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11235/20000] Train-Loss: 0.433557357629979 Test-Loss: 0.13985223639069283 Train-f1: 0.48630126857974953 Test-f1: 0.504984028699199 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11236/20000] Train-Loss: 0.4314287335999545 Test-Loss: 0.14237862768824358 Train-f1: 0.4861302114466672 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11237/20000] Train-Loss: 0.426538364021227 Test-Loss: 0.14056751721988836 Train-f1: 0.48215304921120794 Test-f1: 0.518795518207283 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11238/20000] Train-Loss: 0.4310065473350826 Test-Loss: 0.13943559177658116 Train-f1: 0.5056108530561085 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11239/20000] Train-Loss: 0.42811091843838234 Test-Loss: 0.1391013143187572 Train-f1: 0.48873622441677406 Test-f1: 0.44573039363737044 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11240/20000] Train-Loss: 0.4350578415525544 Test-Loss: 0.14116193776617225 Train-f1: 0.47535830050303396 Test-f1: 0.4554347826086957 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11241/20000] Train-Loss: 0.43372740893075734 Test-Loss: 0.13908496966057493 Train-f1: 0.47899082113806346 Test-f1: 0.4573487773487773 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11242/20000] Train-Loss: 0.4302783700392191 Test-Loss: 0.13981132770569252 Train-f1: 0.4860218380677656 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11243/20000] Train-Loss: 0.43166974504436567 Test-Loss: 0.1398769876924413 Train-f1: 0.4868862369804991 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11244/20000] Train-Loss: 0.43300736577050364 Test-Loss: 0.14428784184488938 Train-f1: 0.486495136907483 Test-f1: 0.4624973437035148 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11245/20000] Train-Loss: 0.42914154152479667 Test-Loss: 0.14560325042423042 Train-f1: 0.4969084249084249 Test-f1: 0.4594670406732118 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11246/20000] Train-Loss: 0.43494480251880135 Test-Loss: 0.1381205072596945 Train-f1: 0.4748559354590857 Test-f1: 0.4503312334891282 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11247/20000] Train-Loss: 0.4351882841462575 Test-Loss: 0.1416124833149717 Train-f1: 0.474028685350192 Test-f1: 0.45233400741489715 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11248/20000] Train-Loss: 0.43298726746497673 Test-Loss: 0.14165618385505366 Train-f1: 0.4850263984209936 Test-f1: 0.4526190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11249/20000] Train-Loss: 0.43112941554243833 Test-Loss: 0.14153210921135262 Train-f1: 0.49159416368410513 Test-f1: 0.45675057208237985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11250/20000] Train-Loss: 0.42895755841623756 Test-Loss: 0.14319050305223086 Train-f1: 0.48163982776058 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11251/20000] Train-Loss: 0.4353197630767997 Test-Loss: 0.14253520806586673 Train-f1: 0.47583257203510365 Test-f1: 0.5037301587301587 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11252/20000] Train-Loss: 0.4315339884452639 Test-Loss: 0.13896721114003172 Train-f1: 0.49155659307056465 Test-f1: 0.4536974789915966 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11253/20000] Train-Loss: 0.4294419656652413 Test-Loss: 0.14268335071752478 Train-f1: 0.49049472933343896 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11254/20000] Train-Loss: 0.4289830481910014 Test-Loss: 0.14158969049535325 Train-f1: 0.48946347769877185 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11255/20000] Train-Loss: 0.43335905987906265 Test-Loss: 0.13971452310502214 Train-f1: 0.4845168789570815 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11256/20000] Train-Loss: 0.4330118304259026 Test-Loss: 0.13857770544191075 Train-f1: 0.4897510162984389 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11257/20000] Train-Loss: 0.4301538592894811 Test-Loss: 0.146355228777673 Train-f1: 0.47897791878148555 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11258/20000] Train-Loss: 0.4285118328708129 Test-Loss: 0.14148631641823636 Train-f1: 0.49372017288059267 Test-f1: 0.49327561327561326 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11259/20000] Train-Loss: 0.42717993517397346 Test-Loss: 0.14915450545292308 Train-f1: 0.4890249475009444 Test-f1: 0.4726570048309179 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11260/20000] Train-Loss: 0.43063093722365325 Test-Loss: 0.142126654965845 Train-f1: 0.4953206706724432 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11261/20000] Train-Loss: 0.4288571471055688 Test-Loss: 0.1438006076819951 Train-f1: 0.5220921363280338 Test-f1: 0.4461529397902675 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11262/20000] Train-Loss: 0.4356458229629499 Test-Loss: 0.14096361649215824 Train-f1: 0.481495697285171 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11263/20000] Train-Loss: 0.4272179105524981 Test-Loss: 0.14938648303132743 Train-f1: 0.4872019595945976 Test-f1: 0.4726570048309179 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11264/20000] Train-Loss: 0.43229341456488457 Test-Loss: 0.13915140844941407 Train-f1: 0.4753200988282362 Test-f1: 0.4466252587991718 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11265/20000] Train-Loss: 0.43399854854152314 Test-Loss: 0.1379803241131787 Train-f1: 0.4803410117862673 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11266/20000] Train-Loss: 0.4339848238184548 Test-Loss: 0.13786426585378203 Train-f1: 0.49129666380340237 Test-f1: 0.46016451233842537 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11267/20000] Train-Loss: 0.431709964033034 Test-Loss: 0.13950170031934342 Train-f1: 0.48377221349595 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11268/20000] Train-Loss: 0.4335683179432503 Test-Loss: 0.13755139714321726 Train-f1: 0.49129666380340237 Test-f1: 0.4573487773487773 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11269/20000] Train-Loss: 0.4331657285163099 Test-Loss: 0.1405051417432529 Train-f1: 0.47899792299251087 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11270/20000] Train-Loss: 0.4299974837981339 Test-Loss: 0.14313876644012352 Train-f1: 0.4893788556628208 Test-f1: 0.4461529397902675 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11271/20000] Train-Loss: 0.43368018403570724 Test-Loss: 0.13963400790156696 Train-f1: 0.4853203020499482 Test-f1: 0.4533424822739276 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11272/20000] Train-Loss: 0.42984820426370185 Test-Loss: 0.14556567282549984 Train-f1: 0.4876134902624969 Test-f1: 0.4360203681942812 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [11273/20000] Train-Loss: 0.434402741073702 Test-Loss: 0.13962586656747178 Train-f1: 0.4769448186222379 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11274/20000] Train-Loss: 0.42734574177961543 Test-Loss: 0.13962898805861912 Train-f1: 0.4896839769217466 Test-f1: 0.5078516057585826 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11275/20000] Train-Loss: 0.4314881500390423 Test-Loss: 0.1380897425553386 Train-f1: 0.4801735113112893 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11276/20000] Train-Loss: 0.4321236257769606 Test-Loss: 0.13892257367769617 Train-f1: 0.5001578132012915 Test-f1: 0.4360203681942812 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [11277/20000] Train-Loss: 0.43048789264773274 Test-Loss: 0.14178947934566616 Train-f1: 0.4835061147973253 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11278/20000] Train-Loss: 0.4331128010399085 Test-Loss: 0.13962056733055242 Train-f1: 0.4790059613589025 Test-f1: 0.4570308123249299 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11279/20000] Train-Loss: 0.43014797525096427 Test-Loss: 0.1455069570100527 Train-f1: 0.4834223897534743 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11280/20000] Train-Loss: 0.4264339907793825 Test-Loss: 0.13759143696487652 Train-f1: 0.48124938124938127 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11281/20000] Train-Loss: 0.431121035307507 Test-Loss: 0.14088325265121804 Train-f1: 0.47307730262020725 Test-f1: 0.518795518207283 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [11282/20000] Train-Loss: 0.4289644199769847 Test-Loss: 0.13907637818970445 Train-f1: 0.485293656904212 Test-f1: 0.5077443609022556 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11283/20000] Train-Loss: 0.431184510254115 Test-Loss: 0.14246791246734117 Train-f1: 0.4780345225550705 Test-f1: 0.49327561327561326 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11284/20000] Train-Loss: 0.43426126725825964 Test-Loss: 0.14075170865015033 Train-f1: 0.5037637130801688 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11285/20000] Train-Loss: 0.4257479382210967 Test-Loss: 0.14421679925779915 Train-f1: 0.48318081918081923 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11286/20000] Train-Loss: 0.4296612853205671 Test-Loss: 0.13769765260217232 Train-f1: 0.4862652172625218 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11287/20000] Train-Loss: 0.430470130992215 Test-Loss: 0.14136751385973992 Train-f1: 0.4812487417374755 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11288/20000] Train-Loss: 0.4311121447434854 Test-Loss: 0.14464481849470162 Train-f1: 0.48605985209758795 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11289/20000] Train-Loss: 0.43200868061113756 Test-Loss: 0.1408005564692452 Train-f1: 0.4936557707106986 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11290/20000] Train-Loss: 0.43180639220321587 Test-Loss: 0.1442301348360978 Train-f1: 0.48918119237084756 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11291/20000] Train-Loss: 0.43011654302129165 Test-Loss: 0.14021736222568443 Train-f1: 0.49136455971899007 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11292/20000] Train-Loss: 0.42730886497952486 Test-Loss: 0.14446228071941264 Train-f1: 0.48760704358228385 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11293/20000] Train-Loss: 0.43365868316412787 Test-Loss: 0.1406243844135165 Train-f1: 0.48908929385939015 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11294/20000] Train-Loss: 0.43306655306882363 Test-Loss: 0.14097679227227872 Train-f1: 0.47566151490202124 Test-f1: 0.504984984984985 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11295/20000] Train-Loss: 0.4302239551930243 Test-Loss: 0.13807245647915214 Train-f1: 0.4813951497480658 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11296/20000] Train-Loss: 0.42728479794070673 Test-Loss: 0.13802559820551671 Train-f1: 0.496689705085379 Test-f1: 0.4497330282227307 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11297/20000] Train-Loss: 0.43388352118744616 Test-Loss: 0.138098130332987 Train-f1: 0.4715824190276745 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [11298/20000] Train-Loss: 0.43064327838831684 Test-Loss: 0.13798359609603425 Train-f1: 0.4893788556628208 Test-f1: 0.46016451233842537 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11299/20000] Train-Loss: 0.43251693460889484 Test-Loss: 0.14108646121399013 Train-f1: 0.48544431081162714 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11300/20000] Train-Loss: 0.4294146805192415 Test-Loss: 0.13889579709858121 Train-f1: 0.4866144877510184 Test-f1: 0.452815734989648 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11301/20000] Train-Loss: 0.4278661329346752 Test-Loss: 0.14074781275272383 Train-f1: 0.47611715510603475 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11302/20000] Train-Loss: 0.4317451771403823 Test-Loss: 0.14013736433176002 Train-f1: 0.4941419343847421 Test-f1: 0.504984028699199 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11303/20000] Train-Loss: 0.4301566094515768 Test-Loss: 0.14621826455087197 Train-f1: 0.48760704358228385 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11304/20000] Train-Loss: 0.4308218796205795 Test-Loss: 0.13873649467376395 Train-f1: 0.4867121889961542 Test-f1: 0.4573487773487773 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11305/20000] Train-Loss: 0.42686501694471823 Test-Loss: 0.14500050709153758 Train-f1: 0.4844573708362801 Test-f1: 0.4624973437035148 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11306/20000] Train-Loss: 0.4325704680933273 Test-Loss: 0.1377733652924677 Train-f1: 0.4813197915601033 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11307/20000] Train-Loss: 0.430102778345296 Test-Loss: 0.14081741589923835 Train-f1: 0.4956889997878937 Test-f1: 0.4554347826086957 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11308/20000] Train-Loss: 0.4323398408752904 Test-Loss: 0.14812567743743993 Train-f1: 0.48911355311355315 Test-f1: 0.5141038802093091 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11309/20000] Train-Loss: 0.4279447693021566 Test-Loss: 0.14159147664208377 Train-f1: 0.4802932397919936 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11310/20000] Train-Loss: 0.4329185215099317 Test-Loss: 0.13963990226651113 Train-f1: 0.48219213045019493 Test-f1: 0.4558710801393729 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11311/20000] Train-Loss: 0.430089933680932 Test-Loss: 0.1445781059002042 Train-f1: 0.4867994707994708 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11312/20000] Train-Loss: 0.43050009560815755 Test-Loss: 0.14445041678955342 Train-f1: 0.48747876989950106 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11313/20000] Train-Loss: 0.4324425402844359 Test-Loss: 0.14356083086396085 Train-f1: 0.48577424858246776 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11314/20000] Train-Loss: 0.4189771502890082 Test-Loss: 0.1493340769088602 Train-f1: 0.49775984426873776 Test-f1: 0.4695652173913043 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [11315/20000] Train-Loss: 0.4296196043567891 Test-Loss: 0.13762749042400052 Train-f1: 0.48200105593637765 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11316/20000] Train-Loss: 0.43340417314678426 Test-Loss: 0.1376609941790988 Train-f1: 0.49159416368410513 Test-f1: 0.460952380952381 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11317/20000] Train-Loss: 0.4299864800446661 Test-Loss: 0.1388514364572994 Train-f1: 0.4909615521909375 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11318/20000] Train-Loss: 0.4326620618964501 Test-Loss: 0.1405256812590359 Train-f1: 0.48129104975541354 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11319/20000] Train-Loss: 0.4286966459144441 Test-Loss: 0.14775163464711855 Train-f1: 0.4839581679581679 Test-f1: 0.4726570048309179 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11320/20000] Train-Loss: 0.4324467052063515 Test-Loss: 0.13873833824489545 Train-f1: 0.4783414704887128 Test-f1: 0.4533333333333333 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11321/20000] Train-Loss: 0.43001209137772217 Test-Loss: 0.1426258385731734 Train-f1: 0.48605985209758795 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11322/20000] Train-Loss: 0.4268540384574374 Test-Loss: 0.13975917061646032 Train-f1: 0.4859333456366657 Test-f1: 0.45675057208237985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11323/20000] Train-Loss: 0.43313780221250575 Test-Loss: 0.1398592750703227 Train-f1: 0.48968740254580057 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11324/20000] Train-Loss: 0.42854323054616716 Test-Loss: 0.13816796018224936 Train-f1: 0.48662480153187265 Test-f1: 0.4497330282227307 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11325/20000] Train-Loss: 0.42758195190942994 Test-Loss: 0.13884190432641033 Train-f1: 0.4996647502607767 Test-f1: 0.5056027456027457 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [11326/20000] Train-Loss: 0.43629540215464035 Test-Loss: 0.1390479498537759 Train-f1: 0.48137574985401077 Test-f1: 0.4533424822739276 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11327/20000] Train-Loss: 0.43196045701277785 Test-Loss: 0.13812920279723476 Train-f1: 0.49473004879782084 Test-f1: 0.4503312334891282 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11328/20000] Train-Loss: 0.4295622366229108 Test-Loss: 0.1388719816378666 Train-f1: 0.4840177571897318 Test-f1: 0.4533424822739276 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11329/20000] Train-Loss: 0.4284110186693393 Test-Loss: 0.13860680814576692 Train-f1: 0.4894398130605027 Test-f1: 0.4573487773487773 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11330/20000] Train-Loss: 0.43241923153526113 Test-Loss: 0.1386836021156933 Train-f1: 0.4868850794150642 Test-f1: 0.4531469684787762 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11331/20000] Train-Loss: 0.42989922045071294 Test-Loss: 0.13891162193508283 Train-f1: 0.4946303786303787 Test-f1: 0.4533333333333333 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11332/20000] Train-Loss: 0.4324753895567319 Test-Loss: 0.14099155921052886 Train-f1: 0.4719254273706829 Test-f1: 0.5051919146036793 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [11333/20000] Train-Loss: 0.4288322551308217 Test-Loss: 0.138148789207417 Train-f1: 0.4868862369804991 Test-f1: 0.5077443609022556 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11334/20000] Train-Loss: 0.42998008341410215 Test-Loss: 0.1385610309213666 Train-f1: 0.48232295482295484 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11335/20000] Train-Loss: 0.4246305087171818 Test-Loss: 0.14116717035689721 Train-f1: 0.485232311548101 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11336/20000] Train-Loss: 0.43680481177741515 Test-Loss: 0.13775853981815298 Train-f1: 0.47892257168119234 Test-f1: 0.4497330282227307 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11337/20000] Train-Loss: 0.42858760654583944 Test-Loss: 0.1399430761665649 Train-f1: 0.486495136907483 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11338/20000] Train-Loss: 0.43108181474435775 Test-Loss: 0.1405468339903489 Train-f1: 0.4915578248911582 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11339/20000] Train-Loss: 0.43072619114159105 Test-Loss: 0.14108477464312882 Train-f1: 0.4922184211701066 Test-f1: 0.4526190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11340/20000] Train-Loss: 0.4328256074472157 Test-Loss: 0.1419315531031659 Train-f1: 0.48433233433233436 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11341/20000] Train-Loss: 0.42978438145466036 Test-Loss: 0.14195654918316322 Train-f1: 0.4861302114466672 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11342/20000] Train-Loss: 0.42801247564966505 Test-Loss: 0.14078875331054463 Train-f1: 0.4765256443375792 Test-f1: 0.504984984984985 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11343/20000] Train-Loss: 0.43152529509604115 Test-Loss: 0.13938458763439468 Train-f1: 0.49576279376235455 Test-f1: 0.45675057208237985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11344/20000] Train-Loss: 0.43009506248393503 Test-Loss: 0.13806851705683804 Train-f1: 0.48896543837858086 Test-f1: 0.4503312334891282 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11345/20000] Train-Loss: 0.4262379625334493 Test-Loss: 0.13957791876893383 Train-f1: 0.4852615560540089 Test-f1: 0.45675057208237985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11346/20000] Train-Loss: 0.43028367795145983 Test-Loss: 0.13867900885751036 Train-f1: 0.4854610080393235 Test-f1: 0.4533333333333333 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11347/20000] Train-Loss: 0.42818893928286467 Test-Loss: 0.14675382016488606 Train-f1: 0.4814648771681972 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11348/20000] Train-Loss: 0.4282197561823887 Test-Loss: 0.14402264830025865 Train-f1: 0.5041678516131072 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11349/20000] Train-Loss: 0.42693578729709636 Test-Loss: 0.13897685222092432 Train-f1: 0.49118521746709665 Test-f1: 0.46683117900509197 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11350/20000] Train-Loss: 0.4292038853895484 Test-Loss: 0.1474023078765554 Train-f1: 0.4812040305272203 Test-f1: 0.5141038802093091 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11351/20000] Train-Loss: 0.433587693432131 Test-Loss: 0.14111485446690644 Train-f1: 0.4794863825445413 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11352/20000] Train-Loss: 0.42852490693123446 Test-Loss: 0.14272799012573262 Train-f1: 0.48978026466456537 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11353/20000] Train-Loss: 0.4323610011896304 Test-Loss: 0.1386153216616804 Train-f1: 0.48449946677794775 Test-f1: 0.452815734989648 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11354/20000] Train-Loss: 0.42934800933881995 Test-Loss: 0.14148454243875622 Train-f1: 0.48219213045019493 Test-f1: 0.45675057208237985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11355/20000] Train-Loss: 0.4327695066774213 Test-Loss: 0.13763883651152065 Train-f1: 0.4820930747105243 Test-f1: 0.46016451233842537 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [11356/20000] Train-Loss: 0.4360217235873227 Test-Loss: 0.13754086378517114 Train-f1: 0.48077424858246776 Test-f1: 0.460952380952381 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11357/20000] Train-Loss: 0.43070396665604443 Test-Loss: 0.14076382511575494 Train-f1: 0.4857184594515929 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11358/20000] Train-Loss: 0.43007564858922687 Test-Loss: 0.13783688676371564 Train-f1: 0.490502228831785 Test-f1: 0.4714285714285714 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11359/20000] Train-Loss: 0.43150608521515676 Test-Loss: 0.13832423272177313 Train-f1: 0.4830950972070194 Test-f1: 0.4537451737451737 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11360/20000] Train-Loss: 0.42690699748901795 Test-Loss: 0.14055098751020462 Train-f1: 0.4835061147973253 Test-f1: 0.4558710801393729 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11361/20000] Train-Loss: 0.4262372824926553 Test-Loss: 0.13890593965085957 Train-f1: 0.48388278388278383 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11362/20000] Train-Loss: 0.43078215336852266 Test-Loss: 0.14996257645776745 Train-f1: 0.48473904387210387 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11363/20000] Train-Loss: 0.42902360519629223 Test-Loss: 0.14464314260436198 Train-f1: 0.4737224694836481 Test-f1: 0.4554347826086957 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [11364/20000] Train-Loss: 0.4304564935419581 Test-Loss: 0.14025575802957804 Train-f1: 0.4787461431255492 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11365/20000] Train-Loss: 0.42786387725259917 Test-Loss: 0.13875679884062375 Train-f1: 0.4945510629721156 Test-f1: 0.44649416182596957 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11366/20000] Train-Loss: 0.427970911011646 Test-Loss: 0.139212096159922 Train-f1: 0.47684440838803255 Test-f1: 0.4536974789915966 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11367/20000] Train-Loss: 0.429125284032113 Test-Loss: 0.1393738524552456 Train-f1: 0.48163982776058 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11368/20000] Train-Loss: 0.42679007224359894 Test-Loss: 0.13882906582731033 Train-f1: 0.4969084249084249 Test-f1: 0.4533424822739276 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11369/20000] Train-Loss: 0.42932316570993595 Test-Loss: 0.13833624398670688 Train-f1: 0.4922838463525624 Test-f1: 0.4503312334891282 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11370/20000] Train-Loss: 0.4239258444910015 Test-Loss: 0.14894997535986682 Train-f1: 0.4883614358066913 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11371/20000] Train-Loss: 0.4324151991335986 Test-Loss: 0.1395411502793672 Train-f1: 0.4811294947660527 Test-f1: 0.4568421052631579 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11372/20000] Train-Loss: 0.42904478384562433 Test-Loss: 0.14759536612835816 Train-f1: 0.48441305858570516 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11373/20000] Train-Loss: 0.42732674080190153 Test-Loss: 0.1428810643552629 Train-f1: 0.4813951497480658 Test-f1: 0.4558710801393729 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11374/20000] Train-Loss: 0.4286309315167996 Test-Loss: 0.1414326818839408 Train-f1: 0.48219213045019493 Test-f1: 0.4558710801393729 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11375/20000] Train-Loss: 0.4332923092083881 Test-Loss: 0.13790427289703072 Train-f1: 0.48423723035798255 Test-f1: 0.518795518207283 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11376/20000] Train-Loss: 0.4288056191657532 Test-Loss: 0.14213588494995394 Train-f1: 0.4862629924584474 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11377/20000] Train-Loss: 0.4272429634484701 Test-Loss: 0.13823116306932517 Train-f1: 0.4861095045305571 Test-f1: 0.518795518207283 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11378/20000] Train-Loss: 0.4302463251275955 Test-Loss: 0.13788830329881155 Train-f1: 0.4786812576701373 Test-f1: 0.4647619047619048 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11379/20000] Train-Loss: 0.43100149736256965 Test-Loss: 0.13783860746488424 Train-f1: 0.48077424858246776 Test-f1: 0.4503312334891282 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11380/20000] Train-Loss: 0.43041039610677445 Test-Loss: 0.13760985952079077 Train-f1: 0.4813951497480658 Test-f1: 0.4754621848739496 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11381/20000] Train-Loss: 0.4259791321188516 Test-Loss: 0.1388471985495751 Train-f1: 0.48194572947768516 Test-f1: 0.518795518207283 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11382/20000] Train-Loss: 0.42827119553742926 Test-Loss: 0.14056913240168453 Train-f1: 0.4863038590655703 Test-f1: 0.504984028699199 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11383/20000] Train-Loss: 0.42876588503231794 Test-Loss: 0.140513872374955 Train-f1: 0.4893734793187348 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11384/20000] Train-Loss: 0.4309629278878528 Test-Loss: 0.14112746069557142 Train-f1: 0.48262569814603007 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11385/20000] Train-Loss: 0.4294382732207187 Test-Loss: 0.146725248640718 Train-f1: 0.49181209143126 Test-f1: 0.4726570048309179 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11386/20000] Train-Loss: 0.43113567636999656 Test-Loss: 0.1393555500841472 Train-f1: 0.4818736799766494 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11387/20000] Train-Loss: 0.4269488705027404 Test-Loss: 0.14222206108369775 Train-f1: 0.49433976726272977 Test-f1: 0.4554347826086957 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11388/20000] Train-Loss: 0.4295142279287939 Test-Loss: 0.14167947840381725 Train-f1: 0.49129666380340237 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11389/20000] Train-Loss: 0.42845931530394304 Test-Loss: 0.14317238439494126 Train-f1: 0.49372017288059267 Test-f1: 0.4554347826086957 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11390/20000] Train-Loss: 0.4301659786337968 Test-Loss: 0.14334524182009484 Train-f1: 0.48797471843829454 Test-f1: 0.45675057208237985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [11391/20000] Train-Loss: 0.4302428928683324 Test-Loss: 0.13750593394617297 Train-f1: 0.4850705443082527 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11392/20000] Train-Loss: 0.42767167970454417 Test-Loss: 0.1441359839018972 Train-f1: 0.48595393908884976 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11393/20000] Train-Loss: 0.42887875186946744 Test-Loss: 0.14017205788364953 Train-f1: 0.4848587971168616 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11394/20000] Train-Loss: 0.4287774725355895 Test-Loss: 0.1396586334261957 Train-f1: 0.4909615521909375 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11395/20000] Train-Loss: 0.4262791453385308 Test-Loss: 0.1373779436848652 Train-f1: 0.48465006255889415 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11396/20000] Train-Loss: 0.4289118322839632 Test-Loss: 0.14041093957596223 Train-f1: 0.4938123870738426 Test-f1: 0.44937417654808953 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11397/20000] Train-Loss: 0.42800224818059796 Test-Loss: 0.13829749041578168 Train-f1: 0.513870817969712 Test-f1: 0.44649416182596957 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11398/20000] Train-Loss: 0.42819660155193856 Test-Loss: 0.1413883787485894 Train-f1: 0.4831329686217024 Test-f1: 0.504984028699199 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11399/20000] Train-Loss: 0.4282050871909862 Test-Loss: 0.1378073307450342 Train-f1: 0.48917859031512095 Test-f1: 0.518795518207283 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11400/20000] Train-Loss: 0.42811099555831034 Test-Loss: 0.13817551862346075 Train-f1: 0.4960969102088324 Test-f1: 0.4503312334891282 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11401/20000] Train-Loss: 0.42469919267289713 Test-Loss: 0.1527595062053767 Train-f1: 0.5010907293115883 Test-f1: 0.4606458675424193 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [11402/20000] Train-Loss: 0.43037529085844317 Test-Loss: 0.1390471774965763 Train-f1: 0.48173493173493176 Test-f1: 0.518795518207283 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11403/20000] Train-Loss: 0.42867750090742196 Test-Loss: 0.14085041803038129 Train-f1: 0.49248847267172013 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11404/20000] Train-Loss: 0.42896611827089276 Test-Loss: 0.1388447939414911 Train-f1: 0.4835985658770469 Test-f1: 0.4533424822739276 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11405/20000] Train-Loss: 0.42997663506364053 Test-Loss: 0.14030832521515696 Train-f1: 0.488513974609865 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11406/20000] Train-Loss: 0.4240047425359023 Test-Loss: 0.13865313167102147 Train-f1: 0.4949969743307747 Test-f1: 0.45675057208237985 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11407/20000] Train-Loss: 0.4262791012740893 Test-Loss: 0.13745282831752537 Train-f1: 0.47998635351576524 Test-f1: 0.4714285714285714 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11408/20000] Train-Loss: 0.422520737742684 Test-Loss: 0.14485590769579795 Train-f1: 0.4936574682766368 Test-f1: 0.4624973437035148 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11409/20000] Train-Loss: 0.4249767305184946 Test-Loss: 0.13790197577610833 Train-f1: 0.48881792376317923 Test-f1: 0.4488311688311688 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11410/20000] Train-Loss: 0.4283915716721333 Test-Loss: 0.14340417782634263 Train-f1: 0.49125370063726226 Test-f1: 0.504984028699199 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11411/20000] Train-Loss: 0.4282821943057933 Test-Loss: 0.13768098820478888 Train-f1: 0.4860695577501074 Test-f1: 0.4602828448452552 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11412/20000] Train-Loss: 0.4318613791387567 Test-Loss: 0.14550343233365792 Train-f1: 0.48896543837858086 Test-f1: 0.4624973437035148 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11413/20000] Train-Loss: 0.4276439304264083 Test-Loss: 0.14299442620069405 Train-f1: 0.5002930402930403 Test-f1: 0.45233400741489715 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [11414/20000] Train-Loss: 0.4229644488431841 Test-Loss: 0.13763719927666637 Train-f1: 0.49576678438851046 Test-f1: 0.45675057208237985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11415/20000] Train-Loss: 0.42854018059820614 Test-Loss: 0.14388895852880135 Train-f1: 0.48871698018850873 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11416/20000] Train-Loss: 0.42888970700878537 Test-Loss: 0.14067893540029316 Train-f1: 0.4867246788299419 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11417/20000] Train-Loss: 0.42586749359226617 Test-Loss: 0.1504507429963577 Train-f1: 0.4855357391764496 Test-f1: 0.4726570048309179 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11418/20000] Train-Loss: 0.4271412802989491 Test-Loss: 0.15348135081046627 Train-f1: 0.47774360918886466 Test-f1: 0.4652641421224871 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [11419/20000] Train-Loss: 0.4274984841109607 Test-Loss: 0.14288404718708478 Train-f1: 0.48683463295538515 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11420/20000] Train-Loss: 0.42573343998532115 Test-Loss: 0.13829599912984622 Train-f1: 0.48003078533545446 Test-f1: 0.45675057208237985 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11421/20000] Train-Loss: 0.4238741295298974 Test-Loss: 0.14458545012283214 Train-f1: 0.4882029315012823 Test-f1: 0.4628528560711606 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11422/20000] Train-Loss: 0.4205278279817696 Test-Loss: 0.13804558532611938 Train-f1: 0.49354828250297195 Test-f1: 0.518795518207283 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11423/20000] Train-Loss: 0.4286201244711446 Test-Loss: 0.14077889698548934 Train-f1: 0.4919595008241112 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11424/20000] Train-Loss: 0.42759182421479797 Test-Loss: 0.13789611547174718 Train-f1: 0.4764371448233421 Test-f1: 0.4497330282227307 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11425/20000] Train-Loss: 0.42588801633279905 Test-Loss: 0.13795789732550132 Train-f1: 0.49147575804935545 Test-f1: 0.44649416182596957 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11426/20000] Train-Loss: 0.42273903939879665 Test-Loss: 0.14817847109656382 Train-f1: 0.49342600661749597 Test-f1: 0.4751156336725254 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [11427/20000] Train-Loss: 0.4313750825347996 Test-Loss: 0.14033231887665557 Train-f1: 0.48684909861380443 Test-f1: 0.504984028699199 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11428/20000] Train-Loss: 0.4308637336229058 Test-Loss: 0.14129135265651677 Train-f1: 0.48595393908884976 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11429/20000] Train-Loss: 0.43156163606272374 Test-Loss: 0.13901566250740977 Train-f1: 0.4841315438348639 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11430/20000] Train-Loss: 0.4306890627457649 Test-Loss: 0.13959041586507398 Train-f1: 0.49682649814313135 Test-f1: 0.4558710801393729 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11431/20000] Train-Loss: 0.43143439522269905 Test-Loss: 0.1388994423902206 Train-f1: 0.47707662574292475 Test-f1: 0.4533333333333333 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11432/20000] Train-Loss: 0.4267898013943784 Test-Loss: 0.13996299617768274 Train-f1: 0.4885755753680282 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11433/20000] Train-Loss: 0.42786600222283017 Test-Loss: 0.1414374643521374 Train-f1: 0.4804402387735721 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11434/20000] Train-Loss: 0.4281234590632831 Test-Loss: 0.14168053912307774 Train-f1: 0.49150129012256016 Test-f1: 0.504984028699199 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11435/20000] Train-Loss: 0.4305020576664295 Test-Loss: 0.13949709968939686 Train-f1: 0.4830690879446021 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11436/20000] Train-Loss: 0.42751920186164555 Test-Loss: 0.14247669366146606 Train-f1: 0.4934032017365351 Test-f1: 0.44937417654808953 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11437/20000] Train-Loss: 0.42914665381699113 Test-Loss: 0.14368163896503441 Train-f1: 0.48047536671224833 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11438/20000] Train-Loss: 0.427596592101139 Test-Loss: 0.14315994191336626 Train-f1: 0.48084079958017273 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11439/20000] Train-Loss: 0.4248712232635538 Test-Loss: 0.13881021157460707 Train-f1: 0.48367726546668965 Test-f1: 0.4568421052631579 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11440/20000] Train-Loss: 0.42897838352497486 Test-Loss: 0.14023375483738984 Train-f1: 0.48605985209758795 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11441/20000] Train-Loss: 0.42654564515907756 Test-Loss: 0.14004562917725782 Train-f1: 0.4816203404438698 Test-f1: 0.504984984984985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11442/20000] Train-Loss: 0.42541595535492366 Test-Loss: 0.15002737210844103 Train-f1: 0.4814329136876595 Test-f1: 0.4652641421224871 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11443/20000] Train-Loss: 0.4285976543458503 Test-Loss: 0.14209863752807664 Train-f1: 0.47892257168119234 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11444/20000] Train-Loss: 0.4270899041100495 Test-Loss: 0.14981934746294528 Train-f1: 0.48177837834257364 Test-f1: 0.4751156336725254 Train-acc: 0.8079470198675497 Test-acc: 0.8026315789473685\n",
      "Epoch [11445/20000] Train-Loss: 0.42846062261712053 Test-Loss: 0.13957987319378604 Train-f1: 0.4929722496285964 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11446/20000] Train-Loss: 0.4275256646663077 Test-Loss: 0.13802393603459284 Train-f1: 0.5150058899524758 Test-f1: 0.4537451737451737 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11447/20000] Train-Loss: 0.42654501862628086 Test-Loss: 0.14088844275449117 Train-f1: 0.49424175824175826 Test-f1: 0.518795518207283 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11448/20000] Train-Loss: 0.4260215843184426 Test-Loss: 0.14682172837945284 Train-f1: 0.48649006773814013 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11449/20000] Train-Loss: 0.433079774678752 Test-Loss: 0.14123942642466775 Train-f1: 0.4893788556628208 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11450/20000] Train-Loss: 0.42588536510040553 Test-Loss: 0.14217953750658868 Train-f1: 0.4888329141493698 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11451/20000] Train-Loss: 0.42749827940738416 Test-Loss: 0.13961941825830507 Train-f1: 0.4795411795411796 Test-f1: 0.4570308123249299 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11452/20000] Train-Loss: 0.429868469392842 Test-Loss: 0.14008865583456467 Train-f1: 0.4887615640951628 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11453/20000] Train-Loss: 0.42669815475181483 Test-Loss: 0.13766079380109264 Train-f1: 0.48498168498168504 Test-f1: 0.4602828448452552 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11454/20000] Train-Loss: 0.4273264750626945 Test-Loss: 0.14293977505129046 Train-f1: 0.49083057160916044 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11455/20000] Train-Loss: 0.4277239112652633 Test-Loss: 0.13873354805724153 Train-f1: 0.4815044005625593 Test-f1: 0.5056027456027457 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11456/20000] Train-Loss: 0.4290506272247484 Test-Loss: 0.14286374631390913 Train-f1: 0.4857184594515929 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11457/20000] Train-Loss: 0.4293876942391122 Test-Loss: 0.1378640949974021 Train-f1: 0.49034067068122794 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11458/20000] Train-Loss: 0.4277178367740024 Test-Loss: 0.13984818501119792 Train-f1: 0.4885755753680282 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11459/20000] Train-Loss: 0.4247563845059998 Test-Loss: 0.13874941971291183 Train-f1: 0.4891224717311674 Test-f1: 0.4568421052631579 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11460/20000] Train-Loss: 0.4266067348112686 Test-Loss: 0.13847652330310162 Train-f1: 0.48911355311355315 Test-f1: 0.4536974789915966 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11461/20000] Train-Loss: 0.4262223181103003 Test-Loss: 0.15003549700928537 Train-f1: 0.48157080236325517 Test-f1: 0.4594670406732118 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11462/20000] Train-Loss: 0.4229152616836553 Test-Loss: 0.15495534940884417 Train-f1: 0.49109463595924635 Test-f1: 0.4751156336725254 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [11463/20000] Train-Loss: 0.4308700518680638 Test-Loss: 0.13930181520945314 Train-f1: 0.48843256090314907 Test-f1: 0.45675057208237985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11464/20000] Train-Loss: 0.4258292669006023 Test-Loss: 0.1442904194829877 Train-f1: 0.48194572947768516 Test-f1: 0.5134352946414658 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11465/20000] Train-Loss: 0.4289960376355348 Test-Loss: 0.13912318499777246 Train-f1: 0.49277277561148525 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11466/20000] Train-Loss: 0.4240473432143875 Test-Loss: 0.1569799754621334 Train-f1: 0.4853903081691935 Test-f1: 0.4751156336725254 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [11467/20000] Train-Loss: 0.4282843939632501 Test-Loss: 0.14006443551964426 Train-f1: 0.47436417346094767 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11468/20000] Train-Loss: 0.42692347561403915 Test-Loss: 0.1411499344896137 Train-f1: 0.4861302114466672 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11469/20000] Train-Loss: 0.4264435009273274 Test-Loss: 0.13940648793632254 Train-f1: 0.4790572899071995 Test-f1: 0.4558710801393729 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11470/20000] Train-Loss: 0.4278839744745166 Test-Loss: 0.13886443107355298 Train-f1: 0.48426929458952833 Test-f1: 0.4570308123249299 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11471/20000] Train-Loss: 0.4287155195289752 Test-Loss: 0.14016165078587353 Train-f1: 0.49155659307056465 Test-f1: 0.518795518207283 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11472/20000] Train-Loss: 0.42604716386965574 Test-Loss: 0.13831758712069572 Train-f1: 0.4896306172616313 Test-f1: 0.4573487773487773 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11473/20000] Train-Loss: 0.42484356419362107 Test-Loss: 0.14253503507630166 Train-f1: 0.48101568485268853 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11474/20000] Train-Loss: 0.4318607103777776 Test-Loss: 0.14418505582925945 Train-f1: 0.493461263031148 Test-f1: 0.44649416182596957 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11475/20000] Train-Loss: 0.42436073722999024 Test-Loss: 0.14546504298230153 Train-f1: 0.48505096849622403 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11476/20000] Train-Loss: 0.42809197541818134 Test-Loss: 0.13937452298141556 Train-f1: 0.49764396618856716 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11477/20000] Train-Loss: 0.422099248294436 Test-Loss: 0.14360942159520798 Train-f1: 0.4843100239121939 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11478/20000] Train-Loss: 0.4281928198809977 Test-Loss: 0.14062354559875184 Train-f1: 0.4786812576701373 Test-f1: 0.4558710801393729 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11479/20000] Train-Loss: 0.4252025858853338 Test-Loss: 0.14767921490438948 Train-f1: 0.4833920673920673 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11480/20000] Train-Loss: 0.4268765958387273 Test-Loss: 0.13901166008815277 Train-f1: 0.48605985209758795 Test-f1: 0.4570308123249299 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11481/20000] Train-Loss: 0.42714469440401975 Test-Loss: 0.13774647990283573 Train-f1: 0.4868862369804991 Test-f1: 0.45675057208237985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11482/20000] Train-Loss: 0.4264469763930994 Test-Loss: 0.1402440166265158 Train-f1: 0.48868852417725794 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11483/20000] Train-Loss: 0.4233261530144657 Test-Loss: 0.13848983024472133 Train-f1: 0.4938438611987501 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11484/20000] Train-Loss: 0.42867693398359086 Test-Loss: 0.13821524499913115 Train-f1: 0.4823670810425778 Test-f1: 0.4537451737451737 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11485/20000] Train-Loss: 0.4296020956944176 Test-Loss: 0.14095760075064726 Train-f1: 0.47598534798534803 Test-f1: 0.5056027456027457 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11486/20000] Train-Loss: 0.42624024995977844 Test-Loss: 0.14007763973335835 Train-f1: 0.47732659287021695 Test-f1: 0.45233400741489715 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11487/20000] Train-Loss: 0.42779705049371186 Test-Loss: 0.13787903151342032 Train-f1: 0.5569291635516801 Test-f1: 0.4537451737451737 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11488/20000] Train-Loss: 0.4270034673729421 Test-Loss: 0.13921485297626005 Train-f1: 0.4844773750473054 Test-f1: 0.4570308123249299 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11489/20000] Train-Loss: 0.4252344172573737 Test-Loss: 0.14263070764852562 Train-f1: 0.49195366037471305 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11490/20000] Train-Loss: 0.4302340587642802 Test-Loss: 0.13865434034850463 Train-f1: 0.49002008743944225 Test-f1: 0.4536974789915966 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11491/20000] Train-Loss: 0.42728937034194037 Test-Loss: 0.14132849044211152 Train-f1: 0.49287225250196354 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11492/20000] Train-Loss: 0.42628426703228534 Test-Loss: 0.14053409007458997 Train-f1: 0.4864468864468864 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11493/20000] Train-Loss: 0.42590777150628595 Test-Loss: 0.1441082406834078 Train-f1: 0.4885053027798266 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11494/20000] Train-Loss: 0.4214994876688198 Test-Loss: 0.15141682080330773 Train-f1: 0.47503155581014467 Test-f1: 0.4751156336725254 Train-acc: 0.8013245033112583 Test-acc: 0.8026315789473685\n",
      "Epoch [11495/20000] Train-Loss: 0.42970406692081403 Test-Loss: 0.1398645169951318 Train-f1: 0.473995021570326 Test-f1: 0.504984984984985 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [11496/20000] Train-Loss: 0.4266604368187163 Test-Loss: 0.14475291645954988 Train-f1: 0.4952605633802817 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11497/20000] Train-Loss: 0.42441303604091957 Test-Loss: 0.14347389182654302 Train-f1: 0.48397579490935616 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11498/20000] Train-Loss: 0.42867653234176 Test-Loss: 0.1390053448636116 Train-f1: 0.47899082113806346 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11499/20000] Train-Loss: 0.42760755846138876 Test-Loss: 0.14393511376514373 Train-f1: 0.5192837095136105 Test-f1: 0.5037301587301587 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [11500/20000] Train-Loss: 0.42637706509506457 Test-Loss: 0.13742730275501913 Train-f1: 0.47889829468776834 Test-f1: 0.4647619047619048 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11501/20000] Train-Loss: 0.42957217584194146 Test-Loss: 0.13781901142605077 Train-f1: 0.48231547701746375 Test-f1: 0.4573487773487773 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11502/20000] Train-Loss: 0.4269935448764703 Test-Loss: 0.13711963461271987 Train-f1: 0.4926867541061089 Test-f1: 0.460952380952381 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11503/20000] Train-Loss: 0.4178684654880637 Test-Loss: 0.1537467585635518 Train-f1: 0.48813436052699855 Test-f1: 0.49330195752730965 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11504/20000] Train-Loss: 0.43249039782614884 Test-Loss: 0.14354109599718395 Train-f1: 0.4917919965620928 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11505/20000] Train-Loss: 0.42530290041988883 Test-Loss: 0.13768725921112043 Train-f1: 0.4811294947660527 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11506/20000] Train-Loss: 0.42604834780239426 Test-Loss: 0.14112330974121257 Train-f1: 0.4928785103785104 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11507/20000] Train-Loss: 0.42220699264672795 Test-Loss: 0.1430516938388964 Train-f1: 0.4877176351628907 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11508/20000] Train-Loss: 0.427138623537132 Test-Loss: 0.140796436687566 Train-f1: 0.547513013302487 Test-f1: 0.4558710801393729 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11509/20000] Train-Loss: 0.41535189046332005 Test-Loss: 0.1397200868189676 Train-f1: 0.48740036103916495 Test-f1: 0.4415584415584416 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [11510/20000] Train-Loss: 0.4280530932221153 Test-Loss: 0.14326617891133167 Train-f1: 0.48813436052699855 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11511/20000] Train-Loss: 0.42838884698904933 Test-Loss: 0.14524408828092059 Train-f1: 0.47997545741681724 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11512/20000] Train-Loss: 0.42518990398444884 Test-Loss: 0.14012910723543373 Train-f1: 0.4894888173771606 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11513/20000] Train-Loss: 0.42570278611215984 Test-Loss: 0.140428450604664 Train-f1: 0.4839299239926292 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11514/20000] Train-Loss: 0.42830853566406824 Test-Loss: 0.13840958039486 Train-f1: 0.48883550463519043 Test-f1: 0.4568421052631579 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11515/20000] Train-Loss: 0.42402047663339076 Test-Loss: 0.13752777955134432 Train-f1: 0.49038430182955733 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11516/20000] Train-Loss: 0.4231717845167334 Test-Loss: 0.1443448049985501 Train-f1: 0.48556423300948853 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11517/20000] Train-Loss: 0.42260761475489245 Test-Loss: 0.1387267222499344 Train-f1: 0.517918205300115 Test-f1: 0.5041022846972504 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [11518/20000] Train-Loss: 0.42534441581936694 Test-Loss: 0.1391800841973074 Train-f1: 0.47747702706756395 Test-f1: 0.4570308123249299 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11519/20000] Train-Loss: 0.42659193944198753 Test-Loss: 0.13719223890742047 Train-f1: 0.48605985209758795 Test-f1: 0.4647619047619048 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11520/20000] Train-Loss: 0.4280613943225097 Test-Loss: 0.14397050791365912 Train-f1: 0.49742903487429035 Test-f1: 0.4558710801393729 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11521/20000] Train-Loss: 0.4273774774052332 Test-Loss: 0.13947020804739713 Train-f1: 0.48772629159626063 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11522/20000] Train-Loss: 0.4244096340268788 Test-Loss: 0.14579909690420775 Train-f1: 0.48106692030742665 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11523/20000] Train-Loss: 0.4266123259220712 Test-Loss: 0.13729195483033876 Train-f1: 0.485232311548101 Test-f1: 0.4503312334891282 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11524/20000] Train-Loss: 0.4241838719109882 Test-Loss: 0.13939758828316162 Train-f1: 0.4960046929109714 Test-f1: 0.4503227377907685 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11525/20000] Train-Loss: 0.4242638206438141 Test-Loss: 0.14242229327852862 Train-f1: 0.48605985209758795 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11526/20000] Train-Loss: 0.4265536313066211 Test-Loss: 0.14111463707899366 Train-f1: 0.5053991392019561 Test-f1: 0.4526190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11527/20000] Train-Loss: 0.42386413523787736 Test-Loss: 0.1440908224279158 Train-f1: 0.491042291042291 Test-f1: 0.4624973437035148 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11528/20000] Train-Loss: 0.4263818590231898 Test-Loss: 0.1391572538562475 Train-f1: 0.4861302114466672 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11529/20000] Train-Loss: 0.42701483435446647 Test-Loss: 0.13776717321213483 Train-f1: 0.48772629159626063 Test-f1: 0.4497330282227307 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11530/20000] Train-Loss: 0.4247053935065371 Test-Loss: 0.14061931648209508 Train-f1: 0.4857184594515929 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11531/20000] Train-Loss: 0.4240049039633771 Test-Loss: 0.14044719899451474 Train-f1: 0.5094459278331502 Test-f1: 0.463768115942029 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11532/20000] Train-Loss: 0.4282250227129227 Test-Loss: 0.13969866474358075 Train-f1: 0.48227658056229483 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11533/20000] Train-Loss: 0.4263297108000361 Test-Loss: 0.14371770236265644 Train-f1: 0.49008553753079304 Test-f1: 0.5028754578754578 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11534/20000] Train-Loss: 0.42567030771051173 Test-Loss: 0.14131179156052298 Train-f1: 0.48311761077413384 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11535/20000] Train-Loss: 0.4228663551080396 Test-Loss: 0.14186586291511438 Train-f1: 0.4866953748006379 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11536/20000] Train-Loss: 0.426862310311547 Test-Loss: 0.1381708313840469 Train-f1: 0.4783642176047239 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11537/20000] Train-Loss: 0.42581088521746113 Test-Loss: 0.13722749842378257 Train-f1: 0.48700902061692564 Test-f1: 0.4503312334891282 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11538/20000] Train-Loss: 0.4244990046590727 Test-Loss: 0.13756815567568317 Train-f1: 0.5009277043853932 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11539/20000] Train-Loss: 0.4263246614049454 Test-Loss: 0.1414832738304682 Train-f1: 0.4885755753680282 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11540/20000] Train-Loss: 0.4279642500747385 Test-Loss: 0.13828203610255674 Train-f1: 0.4997874578133585 Test-f1: 0.452815734989648 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [11541/20000] Train-Loss: 0.4227923003084813 Test-Loss: 0.14419212837937037 Train-f1: 0.49127728736560305 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11542/20000] Train-Loss: 0.4237176185193024 Test-Loss: 0.14068497749118591 Train-f1: 0.48465006255889415 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11543/20000] Train-Loss: 0.4295103601734369 Test-Loss: 0.13981992575364996 Train-f1: 0.48595393908884976 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11544/20000] Train-Loss: 0.4251416403344488 Test-Loss: 0.1428040576941957 Train-f1: 0.4894888173771606 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11545/20000] Train-Loss: 0.42131148157407516 Test-Loss: 0.14682398384728773 Train-f1: 0.49353581698107246 Test-f1: 0.5244444444444445 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [11546/20000] Train-Loss: 0.4242641513279019 Test-Loss: 0.1454790753663198 Train-f1: 0.4785883470527108 Test-f1: 0.5028754578754578 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11547/20000] Train-Loss: 0.42768270263015473 Test-Loss: 0.1440489923454853 Train-f1: 0.4831515151515152 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11548/20000] Train-Loss: 0.42649847472998054 Test-Loss: 0.13949585187429941 Train-f1: 0.4884770801412481 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11549/20000] Train-Loss: 0.42241968896905574 Test-Loss: 0.14386307228540332 Train-f1: 0.48653041658436774 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11550/20000] Train-Loss: 0.4257014338600802 Test-Loss: 0.1383505626914317 Train-f1: 0.48936204146730466 Test-f1: 0.518795518207283 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11551/20000] Train-Loss: 0.4246514292523837 Test-Loss: 0.14196949245932539 Train-f1: 0.4792060034502879 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11552/20000] Train-Loss: 0.42771278541615626 Test-Loss: 0.1377281055328299 Train-f1: 0.4911943386395941 Test-f1: 0.45675057208237985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11553/20000] Train-Loss: 0.42277492497345615 Test-Loss: 0.1382227371806242 Train-f1: 0.4832339874019035 Test-f1: 0.4573487773487773 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11554/20000] Train-Loss: 0.4267369197734807 Test-Loss: 0.1426488080031635 Train-f1: 0.4916776556776557 Test-f1: 0.5044444444444445 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11555/20000] Train-Loss: 0.42630366534109393 Test-Loss: 0.14121092973649968 Train-f1: 0.4961809795143129 Test-f1: 0.4526190476190476 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11556/20000] Train-Loss: 0.42217507629999407 Test-Loss: 0.13833782757807278 Train-f1: 0.490502228831785 Test-f1: 0.44649416182596957 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11557/20000] Train-Loss: 0.4221492637870155 Test-Loss: 0.14251840191050527 Train-f1: 0.4881562881562882 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11558/20000] Train-Loss: 0.4244375739507824 Test-Loss: 0.13690178733922942 Train-f1: 0.4851202325654881 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11559/20000] Train-Loss: 0.4239056961928134 Test-Loss: 0.13745327105531085 Train-f1: 0.49555681626035275 Test-f1: 0.4503312334891282 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11560/20000] Train-Loss: 0.42394215477739094 Test-Loss: 0.13700301744231352 Train-f1: 0.4942776815059388 Test-f1: 0.4647619047619048 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11561/20000] Train-Loss: 0.42355025678120206 Test-Loss: 0.1375760471428619 Train-f1: 0.4845222581285043 Test-f1: 0.45393483709273186 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11562/20000] Train-Loss: 0.42659136728933794 Test-Loss: 0.14066504466321547 Train-f1: 0.48392257168119235 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11563/20000] Train-Loss: 0.42233234459079777 Test-Loss: 0.13861705228496884 Train-f1: 0.4919276759276759 Test-f1: 0.44649416182596957 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11564/20000] Train-Loss: 0.4247098915454003 Test-Loss: 0.14631079202316635 Train-f1: 0.4783642176047239 Test-f1: 0.5141038802093091 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11565/20000] Train-Loss: 0.42801549764671154 Test-Loss: 0.13868934463845936 Train-f1: 0.4845660737380483 Test-f1: 0.4536974789915966 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11566/20000] Train-Loss: 0.4265014584922806 Test-Loss: 0.13892969313172612 Train-f1: 0.4816203404438698 Test-f1: 0.518795518207283 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11567/20000] Train-Loss: 0.42546303835889426 Test-Loss: 0.14072225121552623 Train-f1: 0.48076382159037206 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11568/20000] Train-Loss: 0.426329404978675 Test-Loss: 0.1424297073009165 Train-f1: 0.4919276759276759 Test-f1: 0.4554347826086957 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11569/20000] Train-Loss: 0.4203148953001406 Test-Loss: 0.14256417496449728 Train-f1: 0.47320030632001286 Test-f1: 0.44649416182596957 Train-acc: 0.7947019867549668 Test-acc: 0.7631578947368421\n",
      "Epoch [11570/20000] Train-Loss: 0.42515310193403966 Test-Loss: 0.13717523094927309 Train-f1: 0.4862899005756149 Test-f1: 0.45393483709273186 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11571/20000] Train-Loss: 0.415871756312816 Test-Loss: 0.1481898183434631 Train-f1: 0.5096369968469582 Test-f1: 0.4652641421224871 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11572/20000] Train-Loss: 0.4257074921074943 Test-Loss: 0.13963258202550488 Train-f1: 0.4885755753680282 Test-f1: 0.504984028699199 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11573/20000] Train-Loss: 0.422086793518079 Test-Loss: 0.14392517784236733 Train-f1: 0.49598062490295014 Test-f1: 0.44649416182596957 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11574/20000] Train-Loss: 0.4259512633430829 Test-Loss: 0.14114029589019497 Train-f1: 0.49153561685207253 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11575/20000] Train-Loss: 0.4222516241341096 Test-Loss: 0.14574798836361302 Train-f1: 0.4829598387619377 Test-f1: 0.5141038802093091 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11576/20000] Train-Loss: 0.42867168184277976 Test-Loss: 0.1402382194399052 Train-f1: 0.4853822512485594 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11577/20000] Train-Loss: 0.4241890415924036 Test-Loss: 0.13716720810679317 Train-f1: 0.49443509847696987 Test-f1: 0.4503312334891282 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11578/20000] Train-Loss: 0.42222716803950217 Test-Loss: 0.14575982980926985 Train-f1: 0.48982295482295485 Test-f1: 0.4726570048309179 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11579/20000] Train-Loss: 0.4215285256515769 Test-Loss: 0.13968596628675053 Train-f1: 0.48477315235263596 Test-f1: 0.5066513056835638 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [11580/20000] Train-Loss: 0.4261241159946302 Test-Loss: 0.13681241022255644 Train-f1: 0.5527761711972239 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11581/20000] Train-Loss: 0.4245535146566095 Test-Loss: 0.13948815757743718 Train-f1: 0.4928785103785104 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11582/20000] Train-Loss: 0.42549516467558446 Test-Loss: 0.1392763972906799 Train-f1: 0.4889095999257159 Test-f1: 0.504984984984985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11583/20000] Train-Loss: 0.42446015998847847 Test-Loss: 0.14665533158727204 Train-f1: 0.4904146558277983 Test-f1: 0.4751156336725254 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [11584/20000] Train-Loss: 0.42151688832550466 Test-Loss: 0.1406110621904916 Train-f1: 0.49195366037471305 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11585/20000] Train-Loss: 0.4251545532814217 Test-Loss: 0.13738109742891705 Train-f1: 0.48724746515629674 Test-f1: 0.4503312334891282 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11586/20000] Train-Loss: 0.42056963449006474 Test-Loss: 0.13882693841002927 Train-f1: 0.4867121889961542 Test-f1: 0.4956963725929244 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11587/20000] Train-Loss: 0.4244554943895212 Test-Loss: 0.13765710168360704 Train-f1: 0.4894398130605027 Test-f1: 0.4537451737451737 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11588/20000] Train-Loss: 0.4243572881047481 Test-Loss: 0.14260750120479823 Train-f1: 0.48177837834257364 Test-f1: 0.4558710801393729 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11589/20000] Train-Loss: 0.42510574124413325 Test-Loss: 0.13681236069655475 Train-f1: 0.4906454091432962 Test-f1: 0.460952380952381 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11590/20000] Train-Loss: 0.4270809094602988 Test-Loss: 0.13742014706051187 Train-f1: 0.5039014513467068 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11591/20000] Train-Loss: 0.426343396046375 Test-Loss: 0.13936270176445276 Train-f1: 0.47799089466255884 Test-f1: 0.4526190476190476 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11592/20000] Train-Loss: 0.42350303943448037 Test-Loss: 0.1411155287445754 Train-f1: 0.4893562577773104 Test-f1: 0.504984028699199 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11593/20000] Train-Loss: 0.42301643326108157 Test-Loss: 0.13918650219999573 Train-f1: 0.49356892672846797 Test-f1: 0.456831179005092 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11594/20000] Train-Loss: 0.4247464705414014 Test-Loss: 0.13706983630922354 Train-f1: 0.48932208142734457 Test-f1: 0.4602828448452552 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11595/20000] Train-Loss: 0.4195028610794513 Test-Loss: 0.14363634695231878 Train-f1: 0.4986436986436987 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11596/20000] Train-Loss: 0.4248714197737567 Test-Loss: 0.14299144953112597 Train-f1: 0.5513238439412935 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11597/20000] Train-Loss: 0.4299848018263191 Test-Loss: 0.13915829674994312 Train-f1: 0.4840930998825736 Test-f1: 0.46066734074823057 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11598/20000] Train-Loss: 0.42048879465054784 Test-Loss: 0.13734067714579698 Train-f1: 0.4924699461612213 Test-f1: 0.4503312334891282 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11599/20000] Train-Loss: 0.4267692935122436 Test-Loss: 0.14117303721954047 Train-f1: 0.486287056449706 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11600/20000] Train-Loss: 0.4245453626165582 Test-Loss: 0.1476926032363433 Train-f1: 0.49248847267172013 Test-f1: 0.4695652173913043 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11601/20000] Train-Loss: 0.4274336296956669 Test-Loss: 0.13810704086304215 Train-f1: 0.4763915822739352 Test-f1: 0.4568421052631579 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11602/20000] Train-Loss: 0.42166248258164757 Test-Loss: 0.13784365324173217 Train-f1: 0.4903150377602932 Test-f1: 0.518795518207283 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11603/20000] Train-Loss: 0.4224017781434094 Test-Loss: 0.14070900309617002 Train-f1: 0.4798561633014188 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11604/20000] Train-Loss: 0.42244093073405303 Test-Loss: 0.13768650656677256 Train-f1: 0.49008553753079304 Test-f1: 0.46815958815958825 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11605/20000] Train-Loss: 0.4242236902587192 Test-Loss: 0.1377639269399679 Train-f1: 0.48636182922507887 Test-f1: 0.4497330282227307 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11606/20000] Train-Loss: 0.41747270639226913 Test-Loss: 0.14636031524354767 Train-f1: 0.48423336807932305 Test-f1: 0.4695652173913043 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11607/20000] Train-Loss: 0.4246994061785678 Test-Loss: 0.14611762651911783 Train-f1: 0.4917426928792235 Test-f1: 0.4726570048309179 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11608/20000] Train-Loss: 0.4247620454090316 Test-Loss: 0.1462307055417229 Train-f1: 0.48258983650178466 Test-f1: 0.5628157349896481 Train-acc: 0.8079470198675497 Test-acc: 0.8157894736842105\n",
      "Epoch [11609/20000] Train-Loss: 0.426236006983729 Test-Loss: 0.14097694323363832 Train-f1: 0.48007930259585885 Test-f1: 0.45233400741489715 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11610/20000] Train-Loss: 0.42227114130431526 Test-Loss: 0.14544392912167362 Train-f1: 0.488513974609865 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11611/20000] Train-Loss: 0.4232552275195873 Test-Loss: 0.1398057684845966 Train-f1: 0.49816453745939854 Test-f1: 0.463768115942029 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11612/20000] Train-Loss: 0.4211905831509719 Test-Loss: 0.1377820167549995 Train-f1: 0.4828803024932057 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11613/20000] Train-Loss: 0.42989178027176966 Test-Loss: 0.13757861036634506 Train-f1: 0.4788958172063948 Test-f1: 0.46016451233842537 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11614/20000] Train-Loss: 0.4237466968526639 Test-Loss: 0.1433261912297262 Train-f1: 0.5166689157325537 Test-f1: 0.44649416182596957 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [11615/20000] Train-Loss: 0.4236690475254086 Test-Loss: 0.13736005131786855 Train-f1: 0.4866379996814779 Test-f1: 0.518795518207283 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11616/20000] Train-Loss: 0.42272088022197574 Test-Loss: 0.14137218216065064 Train-f1: 0.47501237501237503 Test-f1: 0.456831179005092 Train-acc: 0.7980132450331126 Test-acc: 0.7763157894736842\n",
      "Epoch [11617/20000] Train-Loss: 0.4262989242606635 Test-Loss: 0.14060122380531204 Train-f1: 0.48567434511960056 Test-f1: 0.4559090373512431 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11618/20000] Train-Loss: 0.42382623812679343 Test-Loss: 0.14458115124534193 Train-f1: 0.4822534163329789 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11619/20000] Train-Loss: 0.42228038350441266 Test-Loss: 0.13872552165213786 Train-f1: 0.4838734793187348 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11620/20000] Train-Loss: 0.42686852153833243 Test-Loss: 0.13753693623081392 Train-f1: 0.49155659307056465 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11621/20000] Train-Loss: 0.42368116172572257 Test-Loss: 0.14671703864306967 Train-f1: 0.4844353795484306 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11622/20000] Train-Loss: 0.42662401479205714 Test-Loss: 0.14091330256778187 Train-f1: 0.49153561685207253 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11623/20000] Train-Loss: 0.42270721033511405 Test-Loss: 0.14260547979546737 Train-f1: 0.49526739926739927 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11624/20000] Train-Loss: 0.4206348683481755 Test-Loss: 0.13925940568793604 Train-f1: 0.4792591002337675 Test-f1: 0.518795518207283 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11625/20000] Train-Loss: 0.424656648163626 Test-Loss: 0.13695436900723984 Train-f1: 0.47391019492930314 Test-f1: 0.518795518207283 Train-acc: 0.7980132450331126 Test-acc: 0.7894736842105263\n",
      "Epoch [11626/20000] Train-Loss: 0.4247076194836146 Test-Loss: 0.1370715174534295 Train-f1: 0.4881562881562882 Test-f1: 0.4503312334891282 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11627/20000] Train-Loss: 0.42560762054715673 Test-Loss: 0.13914813305388954 Train-f1: 0.48441305858570516 Test-f1: 0.4503227377907685 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11628/20000] Train-Loss: 0.42663846310602344 Test-Loss: 0.13901227745242484 Train-f1: 0.4890249475009444 Test-f1: 0.4570308123249299 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11629/20000] Train-Loss: 0.41914323484144433 Test-Loss: 0.13848320164233527 Train-f1: 0.5593882627439675 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11630/20000] Train-Loss: 0.42350886259334886 Test-Loss: 0.13694328463212724 Train-f1: 0.47965727090421506 Test-f1: 0.460952380952381 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11631/20000] Train-Loss: 0.42591881146019694 Test-Loss: 0.13876118103567522 Train-f1: 0.4918544533156795 Test-f1: 0.4568421052631579 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11632/20000] Train-Loss: 0.4301153214856789 Test-Loss: 0.1370818623419676 Train-f1: 0.4792591002337675 Test-f1: 0.45393483709273186 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11633/20000] Train-Loss: 0.42346392353744833 Test-Loss: 0.13684098280260973 Train-f1: 0.48699903831426283 Test-f1: 0.460952380952381 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11634/20000] Train-Loss: 0.4260250240995379 Test-Loss: 0.14497253384723163 Train-f1: 0.49247328921831635 Test-f1: 0.46679841897233204 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11635/20000] Train-Loss: 0.42311924851392213 Test-Loss: 0.13794965799621442 Train-f1: 0.4904782695804367 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11636/20000] Train-Loss: 0.4247098434895182 Test-Loss: 0.14918070056134547 Train-f1: 0.47766246099579435 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11637/20000] Train-Loss: 0.42513445581103065 Test-Loss: 0.13856663764405744 Train-f1: 0.48384443945499456 Test-f1: 0.449645003282331 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11638/20000] Train-Loss: 0.42218657295799267 Test-Loss: 0.14137302908819274 Train-f1: 0.48760704358228385 Test-f1: 0.4561490683229813 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11639/20000] Train-Loss: 0.4229542396039042 Test-Loss: 0.14011084203515564 Train-f1: 0.49129666380340237 Test-f1: 0.4554347826086957 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11640/20000] Train-Loss: 0.42072980012645406 Test-Loss: 0.13919624946995227 Train-f1: 0.49021089285989944 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11641/20000] Train-Loss: 0.4221503478080829 Test-Loss: 0.13699409295324938 Train-f1: 0.47886411833780257 Test-f1: 0.4647619047619048 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11642/20000] Train-Loss: 0.4249110936358995 Test-Loss: 0.1403054306208622 Train-f1: 0.4754388801626167 Test-f1: 0.44937417654808953 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11643/20000] Train-Loss: 0.42387818830841917 Test-Loss: 0.14045241379157652 Train-f1: 0.49473004879782084 Test-f1: 0.45233400741489715 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11644/20000] Train-Loss: 0.41935590182332366 Test-Loss: 0.13895690923480528 Train-f1: 0.4811294947660527 Test-f1: 0.518795518207283 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11645/20000] Train-Loss: 0.4213394647014358 Test-Loss: 0.13857194097682726 Train-f1: 0.4971984337501579 Test-f1: 0.5056027456027457 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11646/20000] Train-Loss: 0.42530781086446523 Test-Loss: 0.1374752345324342 Train-f1: 0.5008075163278483 Test-f1: 0.45675057208237985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11647/20000] Train-Loss: 0.4259728717203755 Test-Loss: 0.14079258500848268 Train-f1: 0.4866379996814779 Test-f1: 0.504984028699199 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11648/20000] Train-Loss: 0.4207214979833557 Test-Loss: 0.14403598227517245 Train-f1: 0.4926496400948956 Test-f1: 0.4624973437035148 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11649/20000] Train-Loss: 0.4250704598389482 Test-Loss: 0.14072677339332007 Train-f1: 0.4784449310076634 Test-f1: 0.4558710801393729 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11650/20000] Train-Loss: 0.42245222275755573 Test-Loss: 0.14072307556382482 Train-f1: 0.5564348828275067 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11651/20000] Train-Loss: 0.4227309235885204 Test-Loss: 0.14460974024786016 Train-f1: 0.4924699461612213 Test-f1: 0.4652641421224871 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11652/20000] Train-Loss: 0.4194759153127789 Test-Loss: 0.1389395259090699 Train-f1: 0.4839299239926292 Test-f1: 0.4642044134727062 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11653/20000] Train-Loss: 0.4228073383381795 Test-Loss: 0.13990336436568931 Train-f1: 0.4904954694428378 Test-f1: 0.518795518207283 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11654/20000] Train-Loss: 0.42580083677158503 Test-Loss: 0.13702360489299995 Train-f1: 0.47965727090421506 Test-f1: 0.45393483709273186 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11655/20000] Train-Loss: 0.41783210435363155 Test-Loss: 0.14239198628157684 Train-f1: 0.5513238439412935 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11656/20000] Train-Loss: 0.42739659704643396 Test-Loss: 0.14348318359002812 Train-f1: 0.5011803629113327 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11657/20000] Train-Loss: 0.4176453309519152 Test-Loss: 0.14533500191082785 Train-f1: 0.49418483526066403 Test-f1: 0.4594670406732118 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11658/20000] Train-Loss: 0.4231270433977954 Test-Loss: 0.1396037487960275 Train-f1: 0.48605985209758795 Test-f1: 0.46761904761904766 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11659/20000] Train-Loss: 0.4231186813616471 Test-Loss: 0.13866604580453246 Train-f1: 0.4946261674907778 Test-f1: 0.44649416182596957 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11660/20000] Train-Loss: 0.42692740895901554 Test-Loss: 0.13927934048470691 Train-f1: 0.4852388431243012 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11661/20000] Train-Loss: 0.42187850584752545 Test-Loss: 0.1416165614652078 Train-f1: 0.4918631540735102 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11662/20000] Train-Loss: 0.42479910032703966 Test-Loss: 0.14076703620234277 Train-f1: 0.4839299239926292 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11663/20000] Train-Loss: 0.4246728293972824 Test-Loss: 0.14030055770961364 Train-f1: 0.5044475002122061 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11664/20000] Train-Loss: 0.4212022943325005 Test-Loss: 0.138614201853384 Train-f1: 0.4867490913497633 Test-f1: 0.4642044134727062 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11665/20000] Train-Loss: 0.42332956382029685 Test-Loss: 0.14105051584573222 Train-f1: 0.4811294947660527 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11666/20000] Train-Loss: 0.4208195947404879 Test-Loss: 0.13738255003751731 Train-f1: 0.491372662425294 Test-f1: 0.518795518207283 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11667/20000] Train-Loss: 0.42001907233713803 Test-Loss: 0.15444702704400912 Train-f1: 0.4877772793244491 Test-f1: 0.4751156336725254 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [11668/20000] Train-Loss: 0.42701569431037045 Test-Loss: 0.14770976577095704 Train-f1: 0.4839299239926292 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11669/20000] Train-Loss: 0.42267499366826033 Test-Loss: 0.13973703581291705 Train-f1: 0.4970841050994864 Test-f1: 0.4526190476190476 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11670/20000] Train-Loss: 0.4247634216344177 Test-Loss: 0.13734124859330235 Train-f1: 0.481495697285171 Test-f1: 0.4497330282227307 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11671/20000] Train-Loss: 0.4235579670193185 Test-Loss: 0.13787026944004627 Train-f1: 0.4865415231587532 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11672/20000] Train-Loss: 0.42107243746552725 Test-Loss: 0.13933218233772998 Train-f1: 0.4890249475009444 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11673/20000] Train-Loss: 0.42315866246338557 Test-Loss: 0.14028626025150628 Train-f1: 0.4893788556628208 Test-f1: 0.449645003282331 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11674/20000] Train-Loss: 0.42394636992893187 Test-Loss: 0.14185869570980525 Train-f1: 0.49101397460986496 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11675/20000] Train-Loss: 0.4237270416543458 Test-Loss: 0.13774088042232988 Train-f1: 0.5111453335396997 Test-f1: 0.44649416182596957 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11676/20000] Train-Loss: 0.4220262801703197 Test-Loss: 0.13952120525273334 Train-f1: 0.48318081918081923 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11677/20000] Train-Loss: 0.42571649378032833 Test-Loss: 0.14714950762544424 Train-f1: 0.4958395387843241 Test-f1: 0.4726570048309179 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [11678/20000] Train-Loss: 0.42612631808802737 Test-Loss: 0.13882771587796003 Train-f1: 0.47198841749925335 Test-f1: 0.46761904761904766 Train-acc: 0.7947019867549668 Test-acc: 0.7763157894736842\n",
      "Epoch [11679/20000] Train-Loss: 0.422637633902931 Test-Loss: 0.14061533078454572 Train-f1: 0.4971826419194841 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11680/20000] Train-Loss: 0.42334871328689866 Test-Loss: 0.13971437380951404 Train-f1: 0.487031112347568 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11681/20000] Train-Loss: 0.42169171699706953 Test-Loss: 0.1368660053969057 Train-f1: 0.49236431784202483 Test-f1: 0.4647619047619048 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11682/20000] Train-Loss: 0.4180621568208961 Test-Loss: 0.14558282116417445 Train-f1: 0.48513416086172045 Test-f1: 0.5028754578754578 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11683/20000] Train-Loss: 0.4270711283663992 Test-Loss: 0.13937292031803714 Train-f1: 0.4951331438390262 Test-f1: 0.456831179005092 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11684/20000] Train-Loss: 0.4212414130277828 Test-Loss: 0.14003438850578223 Train-f1: 0.4893734793187348 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11685/20000] Train-Loss: 0.4242039556677678 Test-Loss: 0.14087643108946302 Train-f1: 0.4925563511935994 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11686/20000] Train-Loss: 0.4265448424915825 Test-Loss: 0.14061632556711576 Train-f1: 0.4867490913497633 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11687/20000] Train-Loss: 0.4261025456061912 Test-Loss: 0.13940428382037848 Train-f1: 0.48800877873996046 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11688/20000] Train-Loss: 0.42068564692484656 Test-Loss: 0.14198350692585213 Train-f1: 0.5001578132012915 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11689/20000] Train-Loss: 0.42266503912296677 Test-Loss: 0.13991461541308906 Train-f1: 0.4971826419194841 Test-f1: 0.4526190476190476 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11690/20000] Train-Loss: 0.42609697674699926 Test-Loss: 0.1385483142370265 Train-f1: 0.49159416368410513 Test-f1: 0.504984028699199 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11691/20000] Train-Loss: 0.42292785129761035 Test-Loss: 0.1374380211924515 Train-f1: 0.5070844598103008 Test-f1: 0.45675057208237985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11692/20000] Train-Loss: 0.41364303921026646 Test-Loss: 0.13767117763286307 Train-f1: 0.4801735113112893 Test-f1: 0.44472049689440996 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11693/20000] Train-Loss: 0.42454909789914175 Test-Loss: 0.13901796345936868 Train-f1: 0.4889343618573244 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11694/20000] Train-Loss: 0.4245234833825246 Test-Loss: 0.14002692710760917 Train-f1: 0.4845660737380483 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11695/20000] Train-Loss: 0.42185802300036795 Test-Loss: 0.1422933165527971 Train-f1: 0.4829651693475031 Test-f1: 0.45543478260869563 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11696/20000] Train-Loss: 0.42420451078354177 Test-Loss: 0.14091196376883774 Train-f1: 0.48165443789605 Test-f1: 0.4558710801393729 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11697/20000] Train-Loss: 0.42056591310010966 Test-Loss: 0.13682205081621127 Train-f1: 0.490914942025572 Test-f1: 0.4503312334891282 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11698/20000] Train-Loss: 0.42118064922650256 Test-Loss: 0.1366435654981703 Train-f1: 0.49943530866400365 Test-f1: 0.4503312334891282 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [11699/20000] Train-Loss: 0.42072041138168575 Test-Loss: 0.14093093149365873 Train-f1: 0.4904759999263207 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11700/20000] Train-Loss: 0.42288161104678984 Test-Loss: 0.15088243663361953 Train-f1: 0.48883550463519043 Test-f1: 0.4751156336725254 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [11701/20000] Train-Loss: 0.42302113460122126 Test-Loss: 0.13873249678258048 Train-f1: 0.5622532953802304 Test-f1: 0.4503227377907685 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11702/20000] Train-Loss: 0.4259222792875505 Test-Loss: 0.1415258996892069 Train-f1: 0.49101397460986496 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11703/20000] Train-Loss: 0.4189856887030046 Test-Loss: 0.13806590491299983 Train-f1: 0.48360462838344953 Test-f1: 0.44472049689440996 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11704/20000] Train-Loss: 0.42332380996495644 Test-Loss: 0.13881324238369594 Train-f1: 0.4885755753680282 Test-f1: 0.46761904761904766 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11705/20000] Train-Loss: 0.4237549451184743 Test-Loss: 0.14423214216581273 Train-f1: 0.4813940653940653 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11706/20000] Train-Loss: 0.4213580317856036 Test-Loss: 0.14591215530382515 Train-f1: 0.5586712073142676 Test-f1: 0.47645625692137317 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11707/20000] Train-Loss: 0.424788239950132 Test-Loss: 0.1384254030433656 Train-f1: 0.4852388431243012 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11708/20000] Train-Loss: 0.4225848018792752 Test-Loss: 0.14039639833227402 Train-f1: 0.48359411008417624 Test-f1: 0.5165079365079366 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11709/20000] Train-Loss: 0.428312255219392 Test-Loss: 0.14067137580714803 Train-f1: 0.48446997054680496 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11710/20000] Train-Loss: 0.4170623486584457 Test-Loss: 0.14464669404761543 Train-f1: 0.49659890799131307 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11711/20000] Train-Loss: 0.42143845428679155 Test-Loss: 0.13932366209789787 Train-f1: 0.48888521395333456 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11712/20000] Train-Loss: 0.42146217955049115 Test-Loss: 0.13974903905743064 Train-f1: 0.4940882386401849 Test-f1: 0.4526190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11713/20000] Train-Loss: 0.4200390491948906 Test-Loss: 0.1432456356816969 Train-f1: 0.48397579490935616 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11714/20000] Train-Loss: 0.41810336031027623 Test-Loss: 0.13889730256977856 Train-f1: 0.490502228831785 Test-f1: 0.44649416182596957 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11715/20000] Train-Loss: 0.42341847268346916 Test-Loss: 0.13878051271933237 Train-f1: 0.48386022190370015 Test-f1: 0.4461529397902675 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11716/20000] Train-Loss: 0.41997165377276985 Test-Loss: 0.13767044084324762 Train-f1: 0.4946261674907778 Test-f1: 0.45675057208237985 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11717/20000] Train-Loss: 0.42345852511793325 Test-Loss: 0.13850225287999487 Train-f1: 0.4835985658770469 Test-f1: 0.5183710801393728 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11718/20000] Train-Loss: 0.42493651689870926 Test-Loss: 0.13833127510690216 Train-f1: 0.4808958631743442 Test-f1: 0.5165079365079366 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11719/20000] Train-Loss: 0.42164578023425225 Test-Loss: 0.14551235871076357 Train-f1: 0.4866811923708475 Test-f1: 0.4726570048309179 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11720/20000] Train-Loss: 0.42165815065379914 Test-Loss: 0.1373492273917556 Train-f1: 0.5576857041870324 Test-f1: 0.5183710801393728 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11721/20000] Train-Loss: 0.4239842836956717 Test-Loss: 0.13927650012405024 Train-f1: 0.4891224717311674 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11722/20000] Train-Loss: 0.42459530500637743 Test-Loss: 0.13837881163179494 Train-f1: 0.4863865911566875 Test-f1: 0.4568421052631579 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11723/20000] Train-Loss: 0.4241957117063947 Test-Loss: 0.14011270051691158 Train-f1: 0.4888329141493698 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11724/20000] Train-Loss: 0.4266059145232229 Test-Loss: 0.14057383933408807 Train-f1: 0.48496448363998035 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11725/20000] Train-Loss: 0.42072073775773133 Test-Loss: 0.13750449791529856 Train-f1: 0.48423723035798255 Test-f1: 0.46815958815958825 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11726/20000] Train-Loss: 0.41911523616992474 Test-Loss: 0.14475795312828002 Train-f1: 0.4975238095238096 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11727/20000] Train-Loss: 0.42081575457641773 Test-Loss: 0.14236369927314502 Train-f1: 0.49222779648419746 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11728/20000] Train-Loss: 0.42469847358360197 Test-Loss: 0.13851052160770022 Train-f1: 0.4882486882486883 Test-f1: 0.4570308123249299 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11729/20000] Train-Loss: 0.42106345450188304 Test-Loss: 0.1399626022795481 Train-f1: 0.5065737514383618 Test-f1: 0.456831179005092 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11730/20000] Train-Loss: 0.4218882850235275 Test-Loss: 0.14411197156521047 Train-f1: 0.4834460910499967 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11731/20000] Train-Loss: 0.4221059130403936 Test-Loss: 0.1367714446868189 Train-f1: 0.4874577161110253 Test-f1: 0.4647619047619048 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11732/20000] Train-Loss: 0.41644443311751506 Test-Loss: 0.14580469231317722 Train-f1: 0.485293656904212 Test-f1: 0.5141038802093091 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11733/20000] Train-Loss: 0.42429437262006164 Test-Loss: 0.14010683479111322 Train-f1: 0.4877176351628907 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11734/20000] Train-Loss: 0.425835709977624 Test-Loss: 0.14480455803817754 Train-f1: 0.4834223897534743 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11735/20000] Train-Loss: 0.41550863199200805 Test-Loss: 0.14411222673780336 Train-f1: 0.4900572849649592 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11736/20000] Train-Loss: 0.4229770280915009 Test-Loss: 0.14143505693532687 Train-f1: 0.477021645021645 Test-f1: 0.46066734074823057 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [11737/20000] Train-Loss: 0.4205441239207816 Test-Loss: 0.13834978922536367 Train-f1: 0.48012874628117325 Test-f1: 0.504984984984985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11738/20000] Train-Loss: 0.41956539887751404 Test-Loss: 0.13853594091238336 Train-f1: 0.48423336807932305 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11739/20000] Train-Loss: 0.42143597146821216 Test-Loss: 0.13992286671898985 Train-f1: 0.4793156003347086 Test-f1: 0.504984984984985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11740/20000] Train-Loss: 0.4227330493216665 Test-Loss: 0.13733019755616693 Train-f1: 0.48581128809525326 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11741/20000] Train-Loss: 0.42209612549219916 Test-Loss: 0.13965261946415025 Train-f1: 0.48397579490935616 Test-f1: 0.5165823650034176 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11742/20000] Train-Loss: 0.4230151731135244 Test-Loss: 0.14066479556797393 Train-f1: 0.49012587412587416 Test-f1: 0.5031312595734653 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11743/20000] Train-Loss: 0.42326253918472273 Test-Loss: 0.13983970783315922 Train-f1: 0.49021089285989944 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11744/20000] Train-Loss: 0.4239301712329249 Test-Loss: 0.1465908902094109 Train-f1: 0.4901876887435742 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11745/20000] Train-Loss: 0.41830884431842047 Test-Loss: 0.14381802815138037 Train-f1: 0.4835669161756118 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11746/20000] Train-Loss: 0.4184336145595411 Test-Loss: 0.1370761654476323 Train-f1: 0.4946922358687065 Test-f1: 0.4488311688311688 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11747/20000] Train-Loss: 0.42170138744278945 Test-Loss: 0.1396416342385494 Train-f1: 0.48536985044281733 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11748/20000] Train-Loss: 0.4250652814311482 Test-Loss: 0.1388035783101826 Train-f1: 0.4949120885426618 Test-f1: 0.463768115942029 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11749/20000] Train-Loss: 0.4200081971156238 Test-Loss: 0.14459174355808804 Train-f1: 0.4909315633242013 Test-f1: 0.47645625692137317 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11750/20000] Train-Loss: 0.4243149938792254 Test-Loss: 0.13951506558961504 Train-f1: 0.4862536350059662 Test-f1: 0.4558710801393729 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11751/20000] Train-Loss: 0.42235504515038513 Test-Loss: 0.1375845344409146 Train-f1: 0.48932208142734457 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11752/20000] Train-Loss: 0.4183629837655761 Test-Loss: 0.13826023495346415 Train-f1: 0.49021089285989944 Test-f1: 0.4570308123249299 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11753/20000] Train-Loss: 0.4223340665712464 Test-Loss: 0.1419409533314199 Train-f1: 0.4813179237631792 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11754/20000] Train-Loss: 0.41861518654153923 Test-Loss: 0.14083201986682267 Train-f1: 0.4945852393220814 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11755/20000] Train-Loss: 0.4211690202551212 Test-Loss: 0.13871589579786833 Train-f1: 0.4863038590655703 Test-f1: 0.4642044134727062 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11756/20000] Train-Loss: 0.4169327470241816 Test-Loss: 0.14536398234318326 Train-f1: 0.49287225250196354 Test-f1: 0.4751156336725254 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [11757/20000] Train-Loss: 0.421067539868257 Test-Loss: 0.13703795998058585 Train-f1: 0.49014227986941455 Test-f1: 0.4647619047619048 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11758/20000] Train-Loss: 0.4204615388297079 Test-Loss: 0.14309433967385415 Train-f1: 0.49101397460986496 Test-f1: 0.5028754578754578 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11759/20000] Train-Loss: 0.42029354655771023 Test-Loss: 0.1420062901563299 Train-f1: 0.48492247982389464 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11760/20000] Train-Loss: 0.41991201663009015 Test-Loss: 0.1370045785247529 Train-f1: 0.5093306237141854 Test-f1: 0.45393483709273186 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11761/20000] Train-Loss: 0.4207173256294792 Test-Loss: 0.13900885171423233 Train-f1: 0.4928785103785104 Test-f1: 0.46761904761904766 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11762/20000] Train-Loss: 0.41963418961558935 Test-Loss: 0.14050361112504173 Train-f1: 0.487031112347568 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11763/20000] Train-Loss: 0.41940303787304856 Test-Loss: 0.13659213359246325 Train-f1: 0.4946922358687065 Test-f1: 0.4647619047619048 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11764/20000] Train-Loss: 0.4237928232628115 Test-Loss: 0.1431635815778832 Train-f1: 0.4866379996814779 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11765/20000] Train-Loss: 0.4198753126308665 Test-Loss: 0.13671966459617235 Train-f1: 0.4916656676656676 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11766/20000] Train-Loss: 0.42274080832735 Test-Loss: 0.14025434556702351 Train-f1: 0.4863865911566875 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11767/20000] Train-Loss: 0.4184050914339854 Test-Loss: 0.13695834898793505 Train-f1: 0.4878878497468177 Test-f1: 0.4503312334891282 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11768/20000] Train-Loss: 0.4179167803721827 Test-Loss: 0.13702789519266742 Train-f1: 0.5045937066501737 Test-f1: 0.4497330282227307 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [11769/20000] Train-Loss: 0.41908976982743207 Test-Loss: 0.13672977008311352 Train-f1: 0.4866379996814779 Test-f1: 0.45393483709273186 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11770/20000] Train-Loss: 0.4206191279585701 Test-Loss: 0.13899150896783613 Train-f1: 0.48201863292157726 Test-f1: 0.46761904761904766 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11771/20000] Train-Loss: 0.42042689653112 Test-Loss: 0.13874296996820734 Train-f1: 0.48392257168119235 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11772/20000] Train-Loss: 0.4231243645099966 Test-Loss: 0.13970870641153937 Train-f1: 0.5039701061645476 Test-f1: 0.46066734074823057 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11773/20000] Train-Loss: 0.42537168004314063 Test-Loss: 0.14111340172496364 Train-f1: 0.48699903831426283 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11774/20000] Train-Loss: 0.41947452758625875 Test-Loss: 0.1390989371897826 Train-f1: 0.49002654060013795 Test-f1: 0.46066734074823057 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11775/20000] Train-Loss: 0.41860551810497765 Test-Loss: 0.13734958448234752 Train-f1: 0.48275067230119095 Test-f1: 0.46815958815958825 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11776/20000] Train-Loss: 0.4198471401858377 Test-Loss: 0.13719978104792843 Train-f1: 0.5011803629113327 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11777/20000] Train-Loss: 0.42037484181291795 Test-Loss: 0.13878157824689716 Train-f1: 0.4938123870738426 Test-f1: 0.4568421052631579 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11778/20000] Train-Loss: 0.42054147526563973 Test-Loss: 0.14050377793906624 Train-f1: 0.47330672118297235 Test-f1: 0.4558710801393729 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11779/20000] Train-Loss: 0.42395601173718944 Test-Loss: 0.13926312707408312 Train-f1: 0.487031112347568 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11780/20000] Train-Loss: 0.4209990085877191 Test-Loss: 0.14405775939730822 Train-f1: 0.48908929385939015 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11781/20000] Train-Loss: 0.4199263940068694 Test-Loss: 0.14181986132013674 Train-f1: 0.48654014598540146 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11782/20000] Train-Loss: 0.4197890595217927 Test-Loss: 0.13691493934950671 Train-f1: 0.48505096849622403 Test-f1: 0.5077443609022556 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11783/20000] Train-Loss: 0.42430130396142657 Test-Loss: 0.14285973400747468 Train-f1: 0.5093626373626374 Test-f1: 0.4561490683229813 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11784/20000] Train-Loss: 0.42455587220621205 Test-Loss: 0.13982550729769552 Train-f1: 0.4889095999257159 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11785/20000] Train-Loss: 0.4189166587551448 Test-Loss: 0.1387884158280172 Train-f1: 0.4919595008241112 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11786/20000] Train-Loss: 0.4224510469645254 Test-Loss: 0.13790579973506292 Train-f1: 0.4898165471413879 Test-f1: 0.4568421052631579 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11787/20000] Train-Loss: 0.421168833477691 Test-Loss: 0.13786631684432715 Train-f1: 0.5042864621719203 Test-f1: 0.518795518207283 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11788/20000] Train-Loss: 0.4199843692120561 Test-Loss: 0.14133504227071675 Train-f1: 0.4852734706618504 Test-f1: 0.46679841897233204 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11789/20000] Train-Loss: 0.4199263782217678 Test-Loss: 0.1436775707662181 Train-f1: 0.49250335823105773 Test-f1: 0.47307359307359304 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11790/20000] Train-Loss: 0.42456571164803897 Test-Loss: 0.13874339698777524 Train-f1: 0.48684909861380443 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11791/20000] Train-Loss: 0.42337717665440866 Test-Loss: 0.13928524550956842 Train-f1: 0.49454014598540147 Test-f1: 0.4570308123249299 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11792/20000] Train-Loss: 0.4219406915373214 Test-Loss: 0.1429065981283811 Train-f1: 0.4863865911566875 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11793/20000] Train-Loss: 0.41448209006497416 Test-Loss: 0.1437870313114703 Train-f1: 0.49506702054472757 Test-f1: 0.45543478260869563 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11794/20000] Train-Loss: 0.4224306016140034 Test-Loss: 0.14104019139789956 Train-f1: 0.4829598387619377 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11795/20000] Train-Loss: 0.4259949695065126 Test-Loss: 0.13939569696911447 Train-f1: 0.48575382860068617 Test-f1: 0.5062301587301588 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11796/20000] Train-Loss: 0.4199387166990997 Test-Loss: 0.14010647497054665 Train-f1: 0.49153388159695943 Test-f1: 0.4526190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11797/20000] Train-Loss: 0.413945995841762 Test-Loss: 0.1368376110818562 Train-f1: 0.5518989782147676 Test-f1: 0.4602828448452552 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11798/20000] Train-Loss: 0.42059959563809923 Test-Loss: 0.1381909718253573 Train-f1: 0.500504773004773 Test-f1: 0.460952380952381 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11799/20000] Train-Loss: 0.41972643749809724 Test-Loss: 0.13948831415262267 Train-f1: 0.4777150777150777 Test-f1: 0.44937417654808953 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11800/20000] Train-Loss: 0.42231509172318404 Test-Loss: 0.140147338318065 Train-f1: 0.4868850794150642 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11801/20000] Train-Loss: 0.4206300910748624 Test-Loss: 0.1404930653210749 Train-f1: 0.48121611721611723 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11802/20000] Train-Loss: 0.42317993601514503 Test-Loss: 0.1391766738055455 Train-f1: 0.5060977825871237 Test-f1: 0.46066734074823057 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11803/20000] Train-Loss: 0.42175711727457144 Test-Loss: 0.13862068440540087 Train-f1: 0.5038663513116068 Test-f1: 0.46761904761904766 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11804/20000] Train-Loss: 0.4099743496829724 Test-Loss: 0.15318558725894965 Train-f1: 0.49400424137266247 Test-f1: 0.4326032176778446 Train-acc: 0.8211920529801324 Test-acc: 0.7368421052631579\n",
      "Epoch [11805/20000] Train-Loss: 0.4217167881865149 Test-Loss: 0.1403117394222102 Train-f1: 0.4856953183991946 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11806/20000] Train-Loss: 0.4189754164471117 Test-Loss: 0.14002884560677367 Train-f1: 0.48404552232948744 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11807/20000] Train-Loss: 0.42079317480341216 Test-Loss: 0.13927891264025585 Train-f1: 0.4894888173771606 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11808/20000] Train-Loss: 0.41856165966165076 Test-Loss: 0.14276052547792162 Train-f1: 0.5873495631884894 Test-f1: 0.4554347826086957 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [11809/20000] Train-Loss: 0.41943288196684214 Test-Loss: 0.13843738409446152 Train-f1: 0.4941419343847421 Test-f1: 0.4642044134727062 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11810/20000] Train-Loss: 0.4187196133014833 Test-Loss: 0.13691637353801606 Train-f1: 0.4953206706724432 Test-f1: 0.44649416182596957 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11811/20000] Train-Loss: 0.4202823007935678 Test-Loss: 0.13704583034483078 Train-f1: 0.4946512570965126 Test-f1: 0.45393483709273186 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11812/20000] Train-Loss: 0.4195892391618038 Test-Loss: 0.14509677824084857 Train-f1: 0.4889095999257159 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11813/20000] Train-Loss: 0.42370785287838997 Test-Loss: 0.13873980466394026 Train-f1: 0.4920463697376448 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11814/20000] Train-Loss: 0.4219450475603985 Test-Loss: 0.14588472937061184 Train-f1: 0.4853843188356852 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11815/20000] Train-Loss: 0.4229662643402693 Test-Loss: 0.13746552752895355 Train-f1: 0.4820930747105243 Test-f1: 0.5046089567828698 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11816/20000] Train-Loss: 0.4169545436343588 Test-Loss: 0.13742594454893975 Train-f1: 0.49926739926739927 Test-f1: 0.5066513056835638 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11817/20000] Train-Loss: 0.4221637788158706 Test-Loss: 0.13731639976020354 Train-f1: 0.49372017288059267 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11818/20000] Train-Loss: 0.4232915012450911 Test-Loss: 0.1422407638238007 Train-f1: 0.48918119237084756 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11819/20000] Train-Loss: 0.41968080718253303 Test-Loss: 0.14320034622492075 Train-f1: 0.4702264204952054 Test-f1: 0.44937417654808953 Train-acc: 0.7913907284768212 Test-acc: 0.7631578947368421\n",
      "Epoch [11820/20000] Train-Loss: 0.4221862292860644 Test-Loss: 0.13819892922337187 Train-f1: 0.49207785678373916 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11821/20000] Train-Loss: 0.4211061230685055 Test-Loss: 0.14119316060139891 Train-f1: 0.5006433205885761 Test-f1: 0.45543478260869563 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11822/20000] Train-Loss: 0.42090825359959044 Test-Loss: 0.14103085253382885 Train-f1: 0.48047536671224833 Test-f1: 0.4558710801393729 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11823/20000] Train-Loss: 0.42264022964345166 Test-Loss: 0.13803115149123757 Train-f1: 0.4842888343830965 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11824/20000] Train-Loss: 0.41470632399819934 Test-Loss: 0.15170867766092844 Train-f1: 0.49372017288059267 Test-f1: 0.5237166085946574 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [11825/20000] Train-Loss: 0.4182138587071618 Test-Loss: 0.1396568978984463 Train-f1: 0.47874584632532996 Test-f1: 0.5165823650034176 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [11826/20000] Train-Loss: 0.4168770497303409 Test-Loss: 0.1429891806504929 Train-f1: 0.48659570154095705 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11827/20000] Train-Loss: 0.4197633495252444 Test-Loss: 0.1392103984781347 Train-f1: 0.485884596997648 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11828/20000] Train-Loss: 0.4195866325300056 Test-Loss: 0.14549927281834243 Train-f1: 0.48961341739923014 Test-f1: 0.45675057208237985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11829/20000] Train-Loss: 0.42029527832951113 Test-Loss: 0.1377918429412979 Train-f1: 0.4938123870738426 Test-f1: 0.449645003282331 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11830/20000] Train-Loss: 0.4207635424627233 Test-Loss: 0.13697297759202304 Train-f1: 0.49127728736560305 Test-f1: 0.45393483709273186 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11831/20000] Train-Loss: 0.4237410187677335 Test-Loss: 0.13774942421173944 Train-f1: 0.4764662774228906 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11832/20000] Train-Loss: 0.41807566814334773 Test-Loss: 0.13700789029398808 Train-f1: 0.4823670810425778 Test-f1: 0.4526190476190476 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11833/20000] Train-Loss: 0.4231519505678238 Test-Loss: 0.14588374660346182 Train-f1: 0.4839153172966123 Test-f1: 0.5244444444444445 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [11834/20000] Train-Loss: 0.4230503660113847 Test-Loss: 0.13687016258260182 Train-f1: 0.4963063603063603 Test-f1: 0.45393483709273186 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11835/20000] Train-Loss: 0.4193053258679624 Test-Loss: 0.1371132595523126 Train-f1: 0.4836838884539848 Test-f1: 0.45393483709273186 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11836/20000] Train-Loss: 0.4160524844098836 Test-Loss: 0.13737472675211057 Train-f1: 0.48752546378352835 Test-f1: 0.4488311688311688 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11837/20000] Train-Loss: 0.422320702327143 Test-Loss: 0.14145039345313729 Train-f1: 0.4814936420519259 Test-f1: 0.45543478260869563 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11838/20000] Train-Loss: 0.4246143580803804 Test-Loss: 0.1392617210143369 Train-f1: 0.4772343755315892 Test-f1: 0.4558710801393729 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11839/20000] Train-Loss: 0.41795775671434815 Test-Loss: 0.13852180438674858 Train-f1: 0.4873828348280904 Test-f1: 0.4642044134727062 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11840/20000] Train-Loss: 0.4224081446360301 Test-Loss: 0.1385011641452855 Train-f1: 0.49159416368410513 Test-f1: 0.5148792270531402 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11841/20000] Train-Loss: 0.4218649516112946 Test-Loss: 0.13776656379571228 Train-f1: 0.48956275791718823 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11842/20000] Train-Loss: 0.41943404930167505 Test-Loss: 0.1373896389904394 Train-f1: 0.4878878497468177 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11843/20000] Train-Loss: 0.4193702803121113 Test-Loss: 0.13698953972115122 Train-f1: 0.48481873226398775 Test-f1: 0.5077443609022556 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11844/20000] Train-Loss: 0.4146807100792456 Test-Loss: 0.14498134893243966 Train-f1: 0.49402696069362734 Test-f1: 0.5237164363554656 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [11845/20000] Train-Loss: 0.4184353563725362 Test-Loss: 0.14143062599890066 Train-f1: 0.4878532222092685 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11846/20000] Train-Loss: 0.42312937417067137 Test-Loss: 0.14049718375176762 Train-f1: 0.47971807442006115 Test-f1: 0.4558710801393729 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11847/20000] Train-Loss: 0.41594034351955367 Test-Loss: 0.1375828989871812 Train-f1: 0.4881806774612629 Test-f1: 0.5067505720823798 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11848/20000] Train-Loss: 0.4160980497508056 Test-Loss: 0.14710030820386294 Train-f1: 0.49922980115781523 Test-f1: 0.5244444444444445 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [11849/20000] Train-Loss: 0.42278775336212787 Test-Loss: 0.1458919395831338 Train-f1: 0.4871033317604563 Test-f1: 0.5028754578754578 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11850/20000] Train-Loss: 0.42371046424443387 Test-Loss: 0.13853834043900423 Train-f1: 0.49111155511155513 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11851/20000] Train-Loss: 0.41854670031096664 Test-Loss: 0.13958933918143734 Train-f1: 0.5253570329042028 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11852/20000] Train-Loss: 0.423565666439104 Test-Loss: 0.1378415004769513 Train-f1: 0.48397579490935616 Test-f1: 0.463768115942029 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11853/20000] Train-Loss: 0.4172836822641651 Test-Loss: 0.13839817663105158 Train-f1: 0.49560382857596486 Test-f1: 0.4559090373512431 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11854/20000] Train-Loss: 0.41581352200051414 Test-Loss: 0.1460744908313633 Train-f1: 0.4886114379346277 Test-f1: 0.4695652173913043 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11855/20000] Train-Loss: 0.4230576589914861 Test-Loss: 0.1407151656622919 Train-f1: 0.4885755753680282 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11856/20000] Train-Loss: 0.4138348466157951 Test-Loss: 0.13719951734368285 Train-f1: 0.49649286782126667 Test-f1: 0.518795518207283 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11857/20000] Train-Loss: 0.4211637140694949 Test-Loss: 0.13843519747733624 Train-f1: 0.4873828348280904 Test-f1: 0.46761904761904766 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11858/20000] Train-Loss: 0.42049628233811776 Test-Loss: 0.13842157254705087 Train-f1: 0.4866144877510184 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11859/20000] Train-Loss: 0.41873521666463415 Test-Loss: 0.14464307090321413 Train-f1: 0.5165849496415534 Test-f1: 0.5244444444444445 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [11860/20000] Train-Loss: 0.41925089107593316 Test-Loss: 0.13836637764081997 Train-f1: 0.48592753337278893 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11861/20000] Train-Loss: 0.4149412749237043 Test-Loss: 0.1378117955246316 Train-f1: 0.49443509847696987 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11862/20000] Train-Loss: 0.41272096070393416 Test-Loss: 0.15085444670641315 Train-f1: 0.49435502768836104 Test-f1: 0.4695652173913043 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11863/20000] Train-Loss: 0.42200635611600046 Test-Loss: 0.14558573367916416 Train-f1: 0.4868862369804991 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11864/20000] Train-Loss: 0.42225020588652845 Test-Loss: 0.13992573848436884 Train-f1: 0.4814225716811924 Test-f1: 0.5031312595734653 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11865/20000] Train-Loss: 0.4163815609003458 Test-Loss: 0.13824905137297422 Train-f1: 0.49174629080236354 Test-f1: 0.518795518207283 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11866/20000] Train-Loss: 0.4209036262758575 Test-Loss: 0.14532923585770774 Train-f1: 0.4796864992330468 Test-f1: 0.475889328063241 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11867/20000] Train-Loss: 0.42016067449048683 Test-Loss: 0.13918939544942607 Train-f1: 0.49473004879782084 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11868/20000] Train-Loss: 0.41797600914527144 Test-Loss: 0.13892200797553594 Train-f1: 0.5013632488085042 Test-f1: 0.463768115942029 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [11869/20000] Train-Loss: 0.4214153884827322 Test-Loss: 0.1388039812153618 Train-f1: 0.48801383019592776 Test-f1: 0.4568421052631579 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11870/20000] Train-Loss: 0.4209707340261158 Test-Loss: 0.1388057822315 Train-f1: 0.48917859031512095 Test-f1: 0.46815958815958825 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11871/20000] Train-Loss: 0.42196675555477003 Test-Loss: 0.13674284374750975 Train-f1: 0.5603419477872033 Test-f1: 0.4497330282227307 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11872/20000] Train-Loss: 0.41962388811337303 Test-Loss: 0.14065739654621298 Train-f1: 0.4866811923708475 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11873/20000] Train-Loss: 0.42202743833783196 Test-Loss: 0.13681004163454738 Train-f1: 0.4897510162984389 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11874/20000] Train-Loss: 0.4221386126456498 Test-Loss: 0.1408800965646572 Train-f1: 0.5500183839879984 Test-f1: 0.504984028699199 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11875/20000] Train-Loss: 0.41879554627218274 Test-Loss: 0.13762965168230626 Train-f1: 0.48944450512658566 Test-f1: 0.4497330282227307 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "best score_dx!!\n",
      "Epoch [11876/20000] Train-Loss: 0.4193088542360743 Test-Loss: 0.13645141871502328 Train-f1: 0.5141865110927896 Test-f1: 0.4647619047619048 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11877/20000] Train-Loss: 0.42056499881719256 Test-Loss: 0.1376782640765217 Train-f1: 0.48367726546668965 Test-f1: 0.518795518207283 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11878/20000] Train-Loss: 0.4175235668498639 Test-Loss: 0.14047759737606924 Train-f1: 0.48990584359711875 Test-f1: 0.5156532356532357 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11879/20000] Train-Loss: 0.41908720103332947 Test-Loss: 0.13674792910645012 Train-f1: 0.4838140585244419 Test-f1: 0.5076190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11880/20000] Train-Loss: 0.42303488540568746 Test-Loss: 0.1405500793133651 Train-f1: 0.5084876984329539 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11881/20000] Train-Loss: 0.4195285411019228 Test-Loss: 0.13712694403190714 Train-f1: 0.49280829545730204 Test-f1: 0.4497330282227307 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11882/20000] Train-Loss: 0.41991789134850954 Test-Loss: 0.13933478596365884 Train-f1: 0.5571688954984517 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11883/20000] Train-Loss: 0.41939421934775495 Test-Loss: 0.13868107191030216 Train-f1: 0.4922838463525624 Test-f1: 0.4642044134727062 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11884/20000] Train-Loss: 0.4191421087090487 Test-Loss: 0.1390269864360631 Train-f1: 0.48481873226398775 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11885/20000] Train-Loss: 0.4146444245218537 Test-Loss: 0.1490345672537144 Train-f1: 0.4921326462004183 Test-f1: 0.4554347826086957 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11886/20000] Train-Loss: 0.42041373972879303 Test-Loss: 0.14181300882594705 Train-f1: 0.4925563511935994 Test-f1: 0.4554347826086957 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11887/20000] Train-Loss: 0.4176070012915229 Test-Loss: 0.1410602522178332 Train-f1: 0.48177837834257364 Test-f1: 0.5156532356532357 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11888/20000] Train-Loss: 0.42121703126600396 Test-Loss: 0.13763657855502773 Train-f1: 0.4898165471413879 Test-f1: 0.463768115942029 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11889/20000] Train-Loss: 0.42099965026741004 Test-Loss: 0.13868393859881017 Train-f1: 0.4823670810425778 Test-f1: 0.46066734074823057 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11890/20000] Train-Loss: 0.4143201498817921 Test-Loss: 0.13776557131523529 Train-f1: 0.47971807442006115 Test-f1: 0.5056027456027457 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11891/20000] Train-Loss: 0.4178853523369496 Test-Loss: 0.14836863127292355 Train-f1: 0.49136455971899007 Test-f1: 0.5028754578754578 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11892/20000] Train-Loss: 0.41850577503721115 Test-Loss: 0.14339643936460067 Train-f1: 0.484657177274627 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11893/20000] Train-Loss: 0.41417920620865767 Test-Loss: 0.14040196065138383 Train-f1: 0.49869164146217937 Test-f1: 0.5051919146036793 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11894/20000] Train-Loss: 0.4173145004024481 Test-Loss: 0.13770841410816617 Train-f1: 0.48564973310589243 Test-f1: 0.5066513056835638 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11895/20000] Train-Loss: 0.4162103605384565 Test-Loss: 0.13777961013027942 Train-f1: 0.47766246099579435 Test-f1: 0.5165079365079366 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11896/20000] Train-Loss: 0.41759598356541006 Test-Loss: 0.13836809012743115 Train-f1: 0.5234561495220947 Test-f1: 0.452815734989648 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11897/20000] Train-Loss: 0.4154191289023027 Test-Loss: 0.13754000082972137 Train-f1: 0.4878878497468177 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11898/20000] Train-Loss: 0.4204031676650076 Test-Loss: 0.13751844171791414 Train-f1: 0.5502954042883922 Test-f1: 0.5066513056835638 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11899/20000] Train-Loss: 0.4179646408718748 Test-Loss: 0.14538138225579272 Train-f1: 0.48697309410672035 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11900/20000] Train-Loss: 0.4158948433586758 Test-Loss: 0.14048721096672517 Train-f1: 0.4900085804929625 Test-f1: 0.5156532356532357 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11901/20000] Train-Loss: 0.41998432972706884 Test-Loss: 0.13828029539545547 Train-f1: 0.4919595008241112 Test-f1: 0.449645003282331 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11902/20000] Train-Loss: 0.4227156064679864 Test-Loss: 0.14137091677819102 Train-f1: 0.4844773750473054 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11903/20000] Train-Loss: 0.4150820472981353 Test-Loss: 0.1444319876640284 Train-f1: 0.47971807442006115 Test-f1: 0.4726570048309179 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11904/20000] Train-Loss: 0.4218887024760674 Test-Loss: 0.14279687665344099 Train-f1: 0.47952546378352834 Test-f1: 0.44937417654808953 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11905/20000] Train-Loss: 0.41923136764046376 Test-Loss: 0.14072020155428727 Train-f1: 0.4840577766752263 Test-f1: 0.4554347826086957 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11906/20000] Train-Loss: 0.4198961681858996 Test-Loss: 0.1378305062098118 Train-f1: 0.48932208142734457 Test-f1: 0.46815958815958825 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11907/20000] Train-Loss: 0.41547525486136466 Test-Loss: 0.14227378092384826 Train-f1: 0.4877176351628907 Test-f1: 0.4461529397902675 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11908/20000] Train-Loss: 0.4192394798981936 Test-Loss: 0.13722111916697474 Train-f1: 0.4917426928792235 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11909/20000] Train-Loss: 0.4145844960909456 Test-Loss: 0.14883316707432445 Train-f1: 0.496673548935408 Test-f1: 0.5237164363554656 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [11910/20000] Train-Loss: 0.42165992060180196 Test-Loss: 0.14742968242602567 Train-f1: 0.48760704358228385 Test-f1: 0.45543478260869563 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11911/20000] Train-Loss: 0.4227994677343602 Test-Loss: 0.14208447516072029 Train-f1: 0.5057825507825509 Test-f1: 0.45543478260869563 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11912/20000] Train-Loss: 0.4172039293251548 Test-Loss: 0.13784955515533154 Train-f1: 0.5505671033880603 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [11913/20000] Train-Loss: 0.41934301745218705 Test-Loss: 0.14115221800615685 Train-f1: 0.48099270170830033 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11914/20000] Train-Loss: 0.418326182511941 Test-Loss: 0.1410164734568796 Train-f1: 0.48260073260073255 Test-f1: 0.5156532356532357 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11915/20000] Train-Loss: 0.4205854695521817 Test-Loss: 0.13952154847387885 Train-f1: 0.4839299239926292 Test-f1: 0.5031312595734653 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11916/20000] Train-Loss: 0.4158641347233347 Test-Loss: 0.13716317311408577 Train-f1: 0.542376558208011 Test-f1: 0.4497330282227307 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [11917/20000] Train-Loss: 0.41971956883521633 Test-Loss: 0.14003504540817538 Train-f1: 0.4835827706301961 Test-f1: 0.4558710801393729 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11918/20000] Train-Loss: 0.4202119335679994 Test-Loss: 0.13995606457392318 Train-f1: 0.4897487466443229 Test-f1: 0.45543478260869563 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [11919/20000] Train-Loss: 0.4196373235666938 Test-Loss: 0.13989002895492453 Train-f1: 0.49659890799131307 Test-f1: 0.44951827242524917 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [11920/20000] Train-Loss: 0.42261698324950747 Test-Loss: 0.13778233694250125 Train-f1: 0.4876134902624969 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11921/20000] Train-Loss: 0.42156205146923204 Test-Loss: 0.13731178255345922 Train-f1: 0.4871138444386853 Test-f1: 0.5076190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11922/20000] Train-Loss: 0.4174777903909298 Test-Loss: 0.13716245834214671 Train-f1: 0.49820100779853105 Test-f1: 0.5183710801393728 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [11923/20000] Train-Loss: 0.4215113570298428 Test-Loss: 0.14074106657239663 Train-f1: 0.49159416368410513 Test-f1: 0.5031312595734653 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11924/20000] Train-Loss: 0.42294812682742455 Test-Loss: 0.13794769815391228 Train-f1: 0.5537618717763678 Test-f1: 0.4568421052631579 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11925/20000] Train-Loss: 0.4158316303178228 Test-Loss: 0.14333013546484674 Train-f1: 0.4958395387843241 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11926/20000] Train-Loss: 0.4168493387636321 Test-Loss: 0.1407995786642463 Train-f1: 0.4871138444386853 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11927/20000] Train-Loss: 0.422918610552744 Test-Loss: 0.13862983995732003 Train-f1: 0.48445432513793973 Test-f1: 0.4642044134727062 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11928/20000] Train-Loss: 0.41695172576584516 Test-Loss: 0.14012126245179532 Train-f1: 0.49506702054472757 Test-f1: 0.504984028699199 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11929/20000] Train-Loss: 0.4139945617331567 Test-Loss: 0.14326527510113496 Train-f1: 0.4792975444862031 Test-f1: 0.5149874686716792 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11930/20000] Train-Loss: 0.416517569175765 Test-Loss: 0.14170171827354192 Train-f1: 0.5087350215934195 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11931/20000] Train-Loss: 0.41939399161053365 Test-Loss: 0.13803214742707803 Train-f1: 0.4885141525141525 Test-f1: 0.456831179005092 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11932/20000] Train-Loss: 0.41345197753259627 Test-Loss: 0.13915010565622515 Train-f1: 0.4933768933768934 Test-f1: 0.46066734074823057 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11933/20000] Train-Loss: 0.42025130901621766 Test-Loss: 0.14256300460585758 Train-f1: 0.5004805479258034 Test-f1: 0.5031312595734653 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [11934/20000] Train-Loss: 0.416104852959403 Test-Loss: 0.1434528300647958 Train-f1: 0.49145737062635275 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11935/20000] Train-Loss: 0.42249620397954585 Test-Loss: 0.1412986552931083 Train-f1: 0.4854610080393235 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11936/20000] Train-Loss: 0.420052647766406 Test-Loss: 0.14157130128266154 Train-f1: 0.5084826029862946 Test-f1: 0.5031312595734653 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [11937/20000] Train-Loss: 0.4153526842007247 Test-Loss: 0.13639922416831482 Train-f1: 0.49926739926739927 Test-f1: 0.4647619047619048 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11938/20000] Train-Loss: 0.41897592419989993 Test-Loss: 0.13990509181305452 Train-f1: 0.502751762589656 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11939/20000] Train-Loss: 0.41593733790082965 Test-Loss: 0.14305252426627113 Train-f1: 0.49576561453926893 Test-f1: 0.5031312595734653 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11940/20000] Train-Loss: 0.42033206001905926 Test-Loss: 0.14538864667491466 Train-f1: 0.5522126522126523 Test-f1: 0.4554347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11941/20000] Train-Loss: 0.41720931831915653 Test-Loss: 0.13663559065868275 Train-f1: 0.5525609525609526 Test-f1: 0.44650793650793663 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11942/20000] Train-Loss: 0.4178786240876396 Test-Loss: 0.1415185102375396 Train-f1: 0.5060207131543394 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11943/20000] Train-Loss: 0.4133429254194291 Test-Loss: 0.14365095516350737 Train-f1: 0.4931166079167523 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11944/20000] Train-Loss: 0.4167664422537434 Test-Loss: 0.13852114260064416 Train-f1: 0.4823418346626044 Test-f1: 0.456831179005092 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11945/20000] Train-Loss: 0.41260191283067243 Test-Loss: 0.1397338714770749 Train-f1: 0.4948154919221827 Test-f1: 0.504984984984985 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11946/20000] Train-Loss: 0.4214899469395771 Test-Loss: 0.1367195999260972 Train-f1: 0.5498973096006297 Test-f1: 0.45675057208237985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11947/20000] Train-Loss: 0.4172191794734044 Test-Loss: 0.14166884048564862 Train-f1: 0.5017331526920568 Test-f1: 0.5037301587301587 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [11948/20000] Train-Loss: 0.41452655077736866 Test-Loss: 0.14790400583570157 Train-f1: 0.48901488863405723 Test-f1: 0.5244444444444445 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [11949/20000] Train-Loss: 0.42311407455569583 Test-Loss: 0.1367024047508005 Train-f1: 0.48654014598540146 Test-f1: 0.45393483709273186 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11950/20000] Train-Loss: 0.42162997398612273 Test-Loss: 0.13853453833920754 Train-f1: 0.4863446939533896 Test-f1: 0.46761904761904766 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11951/20000] Train-Loss: 0.42117726582647946 Test-Loss: 0.14074003098099197 Train-f1: 0.48760704358228385 Test-f1: 0.45233400741489715 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11952/20000] Train-Loss: 0.41828032502619455 Test-Loss: 0.1399543745292007 Train-f1: 0.48417016116649025 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11953/20000] Train-Loss: 0.4150247042740265 Test-Loss: 0.13777614558472698 Train-f1: 0.5022348131168062 Test-f1: 0.463768115942029 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [11954/20000] Train-Loss: 0.4196827999016311 Test-Loss: 0.14199516143100308 Train-f1: 0.49125370063726226 Test-f1: 0.44951827242524917 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11955/20000] Train-Loss: 0.41554942094721964 Test-Loss: 0.1426552189332614 Train-f1: 0.4867246788299419 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11956/20000] Train-Loss: 0.41692564979586133 Test-Loss: 0.1450315673008667 Train-f1: 0.4874394422781519 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [11957/20000] Train-Loss: 0.41493295631434984 Test-Loss: 0.141366848993007 Train-f1: 0.4981063087023352 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11958/20000] Train-Loss: 0.4160535253266505 Test-Loss: 0.14378376109200652 Train-f1: 0.48412727623253937 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11959/20000] Train-Loss: 0.41934129495015027 Test-Loss: 0.14234306301522537 Train-f1: 0.4958874081892949 Test-f1: 0.5031312595734653 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11960/20000] Train-Loss: 0.41693168999364844 Test-Loss: 0.14683178587766746 Train-f1: 0.4843100239121939 Test-f1: 0.5244444444444445 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [11961/20000] Train-Loss: 0.4149725207096015 Test-Loss: 0.13687738529543647 Train-f1: 0.5551220369544347 Test-f1: 0.45393483709273186 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11962/20000] Train-Loss: 0.4202414595538391 Test-Loss: 0.1382392887585249 Train-f1: 0.4813658214285265 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11963/20000] Train-Loss: 0.41639792710144935 Test-Loss: 0.14213580098369744 Train-f1: 0.48159570154095704 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [11964/20000] Train-Loss: 0.4161379665683948 Test-Loss: 0.13984325260353875 Train-f1: 0.5568775595265661 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11965/20000] Train-Loss: 0.417622307759864 Test-Loss: 0.13652792710492867 Train-f1: 0.55177857917796 Test-f1: 0.45369747899159674 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11966/20000] Train-Loss: 0.4187757272333361 Test-Loss: 0.14034754507162345 Train-f1: 0.48956275791718823 Test-f1: 0.5031312595734653 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11967/20000] Train-Loss: 0.4183737210933966 Test-Loss: 0.1401066473896068 Train-f1: 0.49566051667962496 Test-f1: 0.5165079365079366 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [11968/20000] Train-Loss: 0.4176075165018252 Test-Loss: 0.14723494175760937 Train-f1: 0.48158893547681975 Test-f1: 0.475889328063241 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [11969/20000] Train-Loss: 0.41773933778258165 Test-Loss: 0.1388559366732619 Train-f1: 0.49057212103569714 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11970/20000] Train-Loss: 0.4218807024582796 Test-Loss: 0.1369698061787245 Train-f1: 0.555279156415687 Test-f1: 0.5067505720823798 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11971/20000] Train-Loss: 0.4205165161208563 Test-Loss: 0.13716200751495733 Train-f1: 0.4786520146520147 Test-f1: 0.5056027456027457 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11972/20000] Train-Loss: 0.4163870176160985 Test-Loss: 0.13981314101088596 Train-f1: 0.491212559778505 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11973/20000] Train-Loss: 0.4184804031632298 Test-Loss: 0.14316038201966197 Train-f1: 0.48968740254580057 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [11974/20000] Train-Loss: 0.417569470078006 Test-Loss: 0.14998243496031916 Train-f1: 0.49017175840858346 Test-f1: 0.475889328063241 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [11975/20000] Train-Loss: 0.4154922691001591 Test-Loss: 0.1486160060330548 Train-f1: 0.573424060216513 Test-f1: 0.47645625692137317 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [11976/20000] Train-Loss: 0.41860997818999873 Test-Loss: 0.1415353683391964 Train-f1: 0.4794085109521351 Test-f1: 0.4558710801393729 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [11977/20000] Train-Loss: 0.4175429046550898 Test-Loss: 0.13736832672395843 Train-f1: 0.48620848916914366 Test-f1: 0.5076190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11978/20000] Train-Loss: 0.4120687818187254 Test-Loss: 0.15386886252891352 Train-f1: 0.47985691075540365 Test-f1: 0.4634616025320673 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11979/20000] Train-Loss: 0.4228924480881098 Test-Loss: 0.1419691842946288 Train-f1: 0.4764188142312233 Test-f1: 0.45543478260869563 Train-acc: 0.7980132450331126 Test-acc: 0.7631578947368421\n",
      "Epoch [11980/20000] Train-Loss: 0.4171681537279982 Test-Loss: 0.141023229733147 Train-f1: 0.4817376490199082 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11981/20000] Train-Loss: 0.4180744488158085 Test-Loss: 0.14761663848516401 Train-f1: 0.49236431784202483 Test-f1: 0.5028754578754578 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11982/20000] Train-Loss: 0.4146792221920468 Test-Loss: 0.13706961456318134 Train-f1: 0.5011171023593384 Test-f1: 0.4644611528822056 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11983/20000] Train-Loss: 0.41802176483388676 Test-Loss: 0.1385727630386108 Train-f1: 0.4942776815059388 Test-f1: 0.5156532356532357 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11984/20000] Train-Loss: 0.41945331697659405 Test-Loss: 0.142400650789639 Train-f1: 0.49370895770895773 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [11985/20000] Train-Loss: 0.41687619580922974 Test-Loss: 0.13739935413713036 Train-f1: 0.5458383915994366 Test-f1: 0.4644611528822056 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [11986/20000] Train-Loss: 0.41587693919596014 Test-Loss: 0.1423625259256153 Train-f1: 0.49356892672846797 Test-f1: 0.4554347826086957 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [11987/20000] Train-Loss: 0.4190645103009513 Test-Loss: 0.13857183595193331 Train-f1: 0.4824631337015238 Test-f1: 0.504984984984985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [11988/20000] Train-Loss: 0.42134134137033163 Test-Loss: 0.1399913953622952 Train-f1: 0.4837802197802198 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [11989/20000] Train-Loss: 0.4184322306364137 Test-Loss: 0.13636658295548937 Train-f1: 0.49397118046816013 Test-f1: 0.45393483709273186 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [11990/20000] Train-Loss: 0.4167263272042977 Test-Loss: 0.13772291652578103 Train-f1: 0.48481873226398775 Test-f1: 0.456831179005092 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11991/20000] Train-Loss: 0.41554065037590904 Test-Loss: 0.14125716426112522 Train-f1: 0.4886151030288155 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [11992/20000] Train-Loss: 0.4173226755842433 Test-Loss: 0.14019350915867576 Train-f1: 0.493446024240627 Test-f1: 0.5175180375180375 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [11993/20000] Train-Loss: 0.42003763738992844 Test-Loss: 0.13845971801484008 Train-f1: 0.48500867745393295 Test-f1: 0.4642044134727062 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [11994/20000] Train-Loss: 0.4188607962297558 Test-Loss: 0.1369014381193835 Train-f1: 0.4871366372134716 Test-f1: 0.45393483709273186 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [11995/20000] Train-Loss: 0.41814973984146897 Test-Loss: 0.13991673094975463 Train-f1: 0.49682649814313135 Test-f1: 0.5056027456027457 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [11996/20000] Train-Loss: 0.4150942722186206 Test-Loss: 0.1492245173081523 Train-f1: 0.5031391424409879 Test-f1: 0.47645625692137317 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [11997/20000] Train-Loss: 0.4106466874913847 Test-Loss: 0.14592883505396406 Train-f1: 0.5650339637990645 Test-f1: 0.46679841897233204 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [11998/20000] Train-Loss: 0.41850376372705034 Test-Loss: 0.13755906126921 Train-f1: 0.49434432234432235 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [11999/20000] Train-Loss: 0.41923933379857675 Test-Loss: 0.14146421161291703 Train-f1: 0.48299854472951453 Test-f1: 0.45233400741489715 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12000/20000] Train-Loss: 0.41685726361750747 Test-Loss: 0.13781897313324945 Train-f1: 0.48397579490935616 Test-f1: 0.4533333333333333 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12001/20000] Train-Loss: 0.4189315089173379 Test-Loss: 0.13684212708160867 Train-f1: 0.5013626373626374 Test-f1: 0.4488311688311688 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12002/20000] Train-Loss: 0.4198026643891955 Test-Loss: 0.1378375017862845 Train-f1: 0.49401775223877653 Test-f1: 0.4568421052631579 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12003/20000] Train-Loss: 0.4148405838093362 Test-Loss: 0.13671144373497277 Train-f1: 0.4950912769736185 Test-f1: 0.518795518207283 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12004/20000] Train-Loss: 0.4107328733874517 Test-Loss: 0.14339864003667877 Train-f1: 0.49285207546077114 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12005/20000] Train-Loss: 0.4148156449454141 Test-Loss: 0.13955804167385474 Train-f1: 0.4969084249084249 Test-f1: 0.5156532356532357 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12006/20000] Train-Loss: 0.4172235779421559 Test-Loss: 0.13796857580592337 Train-f1: 0.5037278037278037 Test-f1: 0.4568421052631579 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12007/20000] Train-Loss: 0.4194432855137783 Test-Loss: 0.14102558851216346 Train-f1: 0.5635358602749907 Test-f1: 0.5037301587301587 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12008/20000] Train-Loss: 0.4139893069986645 Test-Loss: 0.14166029420362286 Train-f1: 0.525968005796823 Test-f1: 0.4554347826086957 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12009/20000] Train-Loss: 0.4162852124327117 Test-Loss: 0.14704281743090453 Train-f1: 0.4884770801412481 Test-f1: 0.47645625692137317 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12010/20000] Train-Loss: 0.4132110931120706 Test-Loss: 0.1418499677968374 Train-f1: 0.4940882386401849 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12011/20000] Train-Loss: 0.42035213450774417 Test-Loss: 0.1389385209164354 Train-f1: 0.48477277561148524 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12012/20000] Train-Loss: 0.41953517391412565 Test-Loss: 0.13965787906133337 Train-f1: 0.5074753099823504 Test-f1: 0.463768115942029 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12013/20000] Train-Loss: 0.4162402081281968 Test-Loss: 0.14001812873738304 Train-f1: 0.49545730207981864 Test-f1: 0.46016451233842537 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12014/20000] Train-Loss: 0.41433840528886773 Test-Loss: 0.1445976080194416 Train-f1: 0.508122541544527 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12015/20000] Train-Loss: 0.41800217959479674 Test-Loss: 0.13774960528019106 Train-f1: 0.48752546378352835 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12016/20000] Train-Loss: 0.41687154683158945 Test-Loss: 0.13721047741407397 Train-f1: 0.4927683339048645 Test-f1: 0.460952380952381 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12017/20000] Train-Loss: 0.41499958687723276 Test-Loss: 0.1371661762024899 Train-f1: 0.4953534207727756 Test-f1: 0.4573487773487773 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12018/20000] Train-Loss: 0.4180607147968294 Test-Loss: 0.13756753269303926 Train-f1: 0.556854355410241 Test-f1: 0.4568421052631579 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12019/20000] Train-Loss: 0.4178922113343418 Test-Loss: 0.142915643015432 Train-f1: 0.49649286782126667 Test-f1: 0.5149874686716792 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12020/20000] Train-Loss: 0.4164171791957841 Test-Loss: 0.1363922489669766 Train-f1: 0.48245356589882143 Test-f1: 0.45369747899159674 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12021/20000] Train-Loss: 0.421540407272115 Test-Loss: 0.13970309865264796 Train-f1: 0.4829598387619377 Test-f1: 0.4558710801393729 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12022/20000] Train-Loss: 0.4140808311314403 Test-Loss: 0.1417835896539537 Train-f1: 0.4971826419194841 Test-f1: 0.5066513056835638 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12023/20000] Train-Loss: 0.41938397931640337 Test-Loss: 0.139356566975411 Train-f1: 0.49236431784202483 Test-f1: 0.5051919146036793 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12024/20000] Train-Loss: 0.4190676126077008 Test-Loss: 0.1377327180938689 Train-f1: 0.48752546378352835 Test-f1: 0.4568421052631579 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12025/20000] Train-Loss: 0.4155891811707491 Test-Loss: 0.13906486835288803 Train-f1: 0.48420051565667493 Test-f1: 0.46761904761904766 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12026/20000] Train-Loss: 0.4172425085410929 Test-Loss: 0.14128301770798465 Train-f1: 0.48386022190370015 Test-f1: 0.4558710801393729 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12027/20000] Train-Loss: 0.4183469563476156 Test-Loss: 0.14244033367788964 Train-f1: 0.5245516994379597 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12028/20000] Train-Loss: 0.4144235977526103 Test-Loss: 0.14201695843301185 Train-f1: 0.48423336807932305 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12029/20000] Train-Loss: 0.414902731833179 Test-Loss: 0.14368376034734578 Train-f1: 0.49927122887955677 Test-f1: 0.47645625692137317 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12030/20000] Train-Loss: 0.4131098907225396 Test-Loss: 0.13651415347937007 Train-f1: 0.48699903831426283 Test-f1: 0.45369747899159674 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12031/20000] Train-Loss: 0.41794088130001145 Test-Loss: 0.13687990467210934 Train-f1: 0.49224499634659863 Test-f1: 0.4573487773487773 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12032/20000] Train-Loss: 0.4200542761698099 Test-Loss: 0.1370442094401342 Train-f1: 0.48201863292157726 Test-f1: 0.4573487773487773 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12033/20000] Train-Loss: 0.4171235433832795 Test-Loss: 0.13943919808567484 Train-f1: 0.49248847267172013 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12034/20000] Train-Loss: 0.4133801333640143 Test-Loss: 0.15745096990903926 Train-f1: 0.5814953390425088 Test-f1: 0.43814823474397946 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [12035/20000] Train-Loss: 0.4070861283916334 Test-Loss: 0.14231468637976458 Train-f1: 0.5597656098598719 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12036/20000] Train-Loss: 0.416866760041721 Test-Loss: 0.1380376840737095 Train-f1: 0.48948363957790164 Test-f1: 0.504984984984985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12037/20000] Train-Loss: 0.41753872806146325 Test-Loss: 0.13858357378868963 Train-f1: 0.48047536671224833 Test-f1: 0.504984984984985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12038/20000] Train-Loss: 0.4115546017983919 Test-Loss: 0.1408767373434943 Train-f1: 0.49002630479140546 Test-f1: 0.5149874686716792 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12039/20000] Train-Loss: 0.41674609171983384 Test-Loss: 0.1383051002549608 Train-f1: 0.5093917105282412 Test-f1: 0.5051190476190477 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12040/20000] Train-Loss: 0.4155455982372293 Test-Loss: 0.14091327495657396 Train-f1: 0.492590407355508 Test-f1: 0.45543478260869563 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12041/20000] Train-Loss: 0.41750497461458075 Test-Loss: 0.1453154215970004 Train-f1: 0.48684909861380443 Test-f1: 0.475889328063241 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12042/20000] Train-Loss: 0.41826357469352426 Test-Loss: 0.14549394349476924 Train-f1: 0.5113247882944048 Test-f1: 0.4695652173913043 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12043/20000] Train-Loss: 0.4160059630513309 Test-Loss: 0.15423890761274078 Train-f1: 0.495992437669857 Test-f1: 0.4751156336725254 Train-acc: 0.804635761589404 Test-acc: 0.8026315789473685\n",
      "Epoch [12044/20000] Train-Loss: 0.4127032245008177 Test-Loss: 0.13805765424673686 Train-f1: 0.48762995912584756 Test-f1: 0.4559090373512431 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12045/20000] Train-Loss: 0.4198546556699238 Test-Loss: 0.13750112887016488 Train-f1: 0.49715125709651264 Test-f1: 0.45 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12046/20000] Train-Loss: 0.41322432198078507 Test-Loss: 0.14644202103943132 Train-f1: 0.48085071044839295 Test-f1: 0.4652641421224871 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [12047/20000] Train-Loss: 0.4163946440501365 Test-Loss: 0.13719145110165845 Train-f1: 0.48684909861380443 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12048/20000] Train-Loss: 0.414155630444049 Test-Loss: 0.13976600327157612 Train-f1: 0.5056904956357512 Test-f1: 0.456831179005092 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12049/20000] Train-Loss: 0.41646743702691574 Test-Loss: 0.14311309140467632 Train-f1: 0.5062705627705627 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12050/20000] Train-Loss: 0.41351269352973635 Test-Loss: 0.13641928924489394 Train-f1: 0.4946922358687065 Test-f1: 0.4647619047619048 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12051/20000] Train-Loss: 0.41485127857234133 Test-Loss: 0.13881298226529026 Train-f1: 0.4886124897490204 Test-f1: 0.4604347826086957 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12052/20000] Train-Loss: 0.4151519406933242 Test-Loss: 0.13811693576860717 Train-f1: 0.49545730207981864 Test-f1: 0.4604347826086957 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12053/20000] Train-Loss: 0.4131610860438821 Test-Loss: 0.13985625530680512 Train-f1: 0.49351549351549356 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12054/20000] Train-Loss: 0.41482895864379865 Test-Loss: 0.1441659545592404 Train-f1: 0.568422062442141 Test-f1: 0.47645625692137317 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12055/20000] Train-Loss: 0.4144813040481673 Test-Loss: 0.13887820248927799 Train-f1: 0.5144000144000145 Test-f1: 0.5041022846972504 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12056/20000] Train-Loss: 0.4192048053208657 Test-Loss: 0.1409696347425369 Train-f1: 0.4802136358265391 Test-f1: 0.5037301587301587 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12057/20000] Train-Loss: 0.41560906464327163 Test-Loss: 0.1367031069813421 Train-f1: 0.4840930998825736 Test-f1: 0.45393483709273186 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12058/20000] Train-Loss: 0.4180732125033257 Test-Loss: 0.13968144484536732 Train-f1: 0.47971807442006115 Test-f1: 0.5165823650034176 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [12059/20000] Train-Loss: 0.4097753573685425 Test-Loss: 0.14363295110296181 Train-f1: 0.5041881153799312 Test-f1: 0.5044444444444445 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12060/20000] Train-Loss: 0.4194581551753077 Test-Loss: 0.1411206960513065 Train-f1: 0.508796365691942 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12061/20000] Train-Loss: 0.4180456372987694 Test-Loss: 0.14208307389054997 Train-f1: 0.49576561453926893 Test-f1: 0.5031312595734653 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12062/20000] Train-Loss: 0.4148295620584721 Test-Loss: 0.13794835478719755 Train-f1: 0.4916776556776557 Test-f1: 0.5154197675936807 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12063/20000] Train-Loss: 0.41774699293453654 Test-Loss: 0.13773692591454723 Train-f1: 0.5064305064305065 Test-f1: 0.449645003282331 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12064/20000] Train-Loss: 0.4180252011410677 Test-Loss: 0.14044845179729298 Train-f1: 0.4780345225550705 Test-f1: 0.4561490683229813 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12065/20000] Train-Loss: 0.41541082071739177 Test-Loss: 0.13968734052880294 Train-f1: 0.490502228831785 Test-f1: 0.5156532356532357 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12066/20000] Train-Loss: 0.4190412109813506 Test-Loss: 0.13829153153548396 Train-f1: 0.4855626472936171 Test-f1: 0.518795518207283 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12067/20000] Train-Loss: 0.41505569654288627 Test-Loss: 0.13892441528439903 Train-f1: 0.4818906791811359 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12068/20000] Train-Loss: 0.4161952916198476 Test-Loss: 0.1381541541603771 Train-f1: 0.4784449310076634 Test-f1: 0.504984984984985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12069/20000] Train-Loss: 0.4169304899790266 Test-Loss: 0.14398636262002623 Train-f1: 0.48629148629148633 Test-f1: 0.5149874686716792 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12070/20000] Train-Loss: 0.4162007496954607 Test-Loss: 0.13759891888577383 Train-f1: 0.4843100239121939 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12071/20000] Train-Loss: 0.41393779142308673 Test-Loss: 0.13822620221877827 Train-f1: 0.4871138444386853 Test-f1: 0.5076190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12072/20000] Train-Loss: 0.4137246157783008 Test-Loss: 0.14375256907260334 Train-f1: 0.4911373179665862 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12073/20000] Train-Loss: 0.4195770282380951 Test-Loss: 0.1500457892992147 Train-f1: 0.48071548436308154 Test-f1: 0.4554347826086957 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12074/20000] Train-Loss: 0.41682272389450137 Test-Loss: 0.13722978931979354 Train-f1: 0.48200105593637765 Test-f1: 0.46815958815958825 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12075/20000] Train-Loss: 0.41592047685125544 Test-Loss: 0.1375428394865908 Train-f1: 0.48968645682260703 Test-f1: 0.46815958815958825 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12076/20000] Train-Loss: 0.4139011343786981 Test-Loss: 0.1407442520030168 Train-f1: 0.48177837834257364 Test-f1: 0.5156532356532357 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12077/20000] Train-Loss: 0.41308598868348706 Test-Loss: 0.14396264268449058 Train-f1: 0.4882486882486883 Test-f1: 0.4661751152073733 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12078/20000] Train-Loss: 0.4146986632063759 Test-Loss: 0.14299590801118978 Train-f1: 0.4880147453395861 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12079/20000] Train-Loss: 0.4110270048370734 Test-Loss: 0.1492765416623761 Train-f1: 0.48669050247997614 Test-f1: 0.5237166085946574 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12080/20000] Train-Loss: 0.4166159783879444 Test-Loss: 0.14350656402146764 Train-f1: 0.4960426348396629 Test-f1: 0.5028754578754578 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12081/20000] Train-Loss: 0.41539301200554357 Test-Loss: 0.1366282319692947 Train-f1: 0.4901876887435742 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12082/20000] Train-Loss: 0.41882744652610193 Test-Loss: 0.13707207912319108 Train-f1: 0.488513974609865 Test-f1: 0.5077443609022556 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12083/20000] Train-Loss: 0.411375691859208 Test-Loss: 0.1504179093529943 Train-f1: 0.4975365803575569 Test-f1: 0.5237164363554656 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12084/20000] Train-Loss: 0.41301827821579123 Test-Loss: 0.13716972962639326 Train-f1: 0.5609990009990009 Test-f1: 0.4573487773487773 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12085/20000] Train-Loss: 0.4119925275423753 Test-Loss: 0.14739903546496017 Train-f1: 0.5447955682814838 Test-f1: 0.4695652173913043 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12086/20000] Train-Loss: 0.41631573931248417 Test-Loss: 0.14177290082360244 Train-f1: 0.4926867541061089 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12087/20000] Train-Loss: 0.4167301550295904 Test-Loss: 0.14062873887550387 Train-f1: 0.48990584359711875 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12088/20000] Train-Loss: 0.4121093821392083 Test-Loss: 0.14123354760920034 Train-f1: 0.4916776556776557 Test-f1: 0.4561490683229813 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12089/20000] Train-Loss: 0.41714074295938064 Test-Loss: 0.1399859726539177 Train-f1: 0.4860218380677656 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12090/20000] Train-Loss: 0.414313032588169 Test-Loss: 0.14360178311174468 Train-f1: 0.4844773750473054 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12091/20000] Train-Loss: 0.4158098584454911 Test-Loss: 0.1411334447334545 Train-f1: 0.5093917105282412 Test-f1: 0.46604809770217487 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12092/20000] Train-Loss: 0.41444339251100837 Test-Loss: 0.14247577965059655 Train-f1: 0.49434432234432235 Test-f1: 0.5037301587301587 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12093/20000] Train-Loss: 0.41949326961189504 Test-Loss: 0.142206280782171 Train-f1: 0.488513974609865 Test-f1: 0.5031312595734653 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12094/20000] Train-Loss: 0.4143328509294487 Test-Loss: 0.1471387955862178 Train-f1: 0.48388278388278383 Test-f1: 0.5244444444444445 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [12095/20000] Train-Loss: 0.4191844373408203 Test-Loss: 0.13701594487574617 Train-f1: 0.48279428098782934 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12096/20000] Train-Loss: 0.4120579390938621 Test-Loss: 0.14249524851470366 Train-f1: 0.5627462627462627 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12097/20000] Train-Loss: 0.41159298256134064 Test-Loss: 0.14428439165048415 Train-f1: 0.5113880488463991 Test-f1: 0.5028754578754578 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12098/20000] Train-Loss: 0.41292929564006703 Test-Loss: 0.14006383694429425 Train-f1: 0.4907358337749603 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12099/20000] Train-Loss: 0.4135441869378262 Test-Loss: 0.14117793510438403 Train-f1: 0.5008548009415011 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12100/20000] Train-Loss: 0.4184862208653984 Test-Loss: 0.14497063645791833 Train-f1: 0.4953534207727756 Test-f1: 0.4652641421224871 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12101/20000] Train-Loss: 0.4195684350743309 Test-Loss: 0.13774759874389975 Train-f1: 0.4853822512485594 Test-f1: 0.5154197675936807 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12102/20000] Train-Loss: 0.41395769910863933 Test-Loss: 0.1364878625828372 Train-f1: 0.48990584359711875 Test-f1: 0.5076190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12103/20000] Train-Loss: 0.4156471857741852 Test-Loss: 0.14132859630614616 Train-f1: 0.5537805111053519 Test-f1: 0.5161904761904762 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12104/20000] Train-Loss: 0.41685444133871524 Test-Loss: 0.13774432398039385 Train-f1: 0.4819037339788103 Test-f1: 0.46815958815958825 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12105/20000] Train-Loss: 0.4135478467280802 Test-Loss: 0.14209072655921945 Train-f1: 0.4851202325654881 Test-f1: 0.44602036819428126 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12106/20000] Train-Loss: 0.4201065233996295 Test-Loss: 0.13651618881246672 Train-f1: 0.492590407355508 Test-f1: 0.4647619047619048 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12107/20000] Train-Loss: 0.4111807228352944 Test-Loss: 0.14281795979763254 Train-f1: 0.4937537006372623 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12108/20000] Train-Loss: 0.41237109389845 Test-Loss: 0.13872361652389437 Train-f1: 0.49356892672846797 Test-f1: 0.5148792270531402 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12109/20000] Train-Loss: 0.4187113834692816 Test-Loss: 0.13935862314772607 Train-f1: 0.4845660737380483 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12110/20000] Train-Loss: 0.4118298621401622 Test-Loss: 0.14177477285298082 Train-f1: 0.4929722496285964 Test-f1: 0.44649416182596957 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12111/20000] Train-Loss: 0.42102594115857556 Test-Loss: 0.13639672005208417 Train-f1: 0.46397884676030376 Test-f1: 0.4503312334891282 Train-acc: 0.7847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [12112/20000] Train-Loss: 0.409270728018951 Test-Loss: 0.14213952549254066 Train-f1: 0.49280829545730204 Test-f1: 0.5149874686716792 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12113/20000] Train-Loss: 0.41568894658411076 Test-Loss: 0.14488287248261034 Train-f1: 0.48990584359711875 Test-f1: 0.5244444444444445 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [12114/20000] Train-Loss: 0.41442538028210796 Test-Loss: 0.14455591952403762 Train-f1: 0.4926496400948956 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12115/20000] Train-Loss: 0.4108988297641429 Test-Loss: 0.15514572438567634 Train-f1: 0.5482041601321742 Test-f1: 0.4554347826086957 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [12116/20000] Train-Loss: 0.4167858372408684 Test-Loss: 0.13780975216671698 Train-f1: 0.5543929582629273 Test-f1: 0.4568421052631579 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12117/20000] Train-Loss: 0.41170644931278266 Test-Loss: 0.13865425810033216 Train-f1: 0.5146167729016936 Test-f1: 0.46761904761904766 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12118/20000] Train-Loss: 0.41455958273672244 Test-Loss: 0.14047808549260535 Train-f1: 0.4867246788299419 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12119/20000] Train-Loss: 0.40917869260342654 Test-Loss: 0.14923070584245418 Train-f1: 0.48328211204036836 Test-f1: 0.5244444444444445 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [12120/20000] Train-Loss: 0.4165797030590807 Test-Loss: 0.13936311376594473 Train-f1: 0.48654014598540146 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12121/20000] Train-Loss: 0.41617500589491185 Test-Loss: 0.13771423352735399 Train-f1: 0.4817176448088828 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12122/20000] Train-Loss: 0.41472899333191215 Test-Loss: 0.14307837585722732 Train-f1: 0.4885755753680282 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12123/20000] Train-Loss: 0.4123072287273389 Test-Loss: 0.13672861276520218 Train-f1: 0.5079738646446721 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12124/20000] Train-Loss: 0.4144648657488531 Test-Loss: 0.1406984973836747 Train-f1: 0.49649342666465957 Test-f1: 0.5051919146036793 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12125/20000] Train-Loss: 0.41632261879399746 Test-Loss: 0.1468392002065594 Train-f1: 0.4812487417374755 Test-f1: 0.5244444444444445 Train-acc: 0.8079470198675497 Test-acc: 0.8026315789473685\n",
      "Epoch [12126/20000] Train-Loss: 0.4154282105457616 Test-Loss: 0.13649637067885947 Train-f1: 0.48395911034673544 Test-f1: 0.518795518207283 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12127/20000] Train-Loss: 0.4166868087349312 Test-Loss: 0.13927906853602087 Train-f1: 0.485293656904212 Test-f1: 0.5148792270531402 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12128/20000] Train-Loss: 0.4170577507696873 Test-Loss: 0.1375525635210885 Train-f1: 0.4843100239121939 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [12129/20000] Train-Loss: 0.41088283932998415 Test-Loss: 0.13634781839065746 Train-f1: 0.49327698809066467 Test-f1: 0.44650793650793663 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12130/20000] Train-Loss: 0.4170246066907318 Test-Loss: 0.1367219401918406 Train-f1: 0.5034210983663538 Test-f1: 0.5076190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12131/20000] Train-Loss: 0.41341069069041064 Test-Loss: 0.13994631533726284 Train-f1: 0.48956275791718823 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12132/20000] Train-Loss: 0.4138242025025391 Test-Loss: 0.14771090497440317 Train-f1: 0.4860218380677656 Test-f1: 0.46679841897233204 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12133/20000] Train-Loss: 0.4170488749354304 Test-Loss: 0.13996245638717056 Train-f1: 0.48367726546668965 Test-f1: 0.4558710801393729 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12134/20000] Train-Loss: 0.41196145951046825 Test-Loss: 0.14363205055047376 Train-f1: 0.5793797920505994 Test-f1: 0.46679841897233204 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12135/20000] Train-Loss: 0.4132296488525815 Test-Loss: 0.1394726859533321 Train-f1: 0.4898352627582253 Test-f1: 0.449645003282331 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12136/20000] Train-Loss: 0.41408043399234123 Test-Loss: 0.1365840157043076 Train-f1: 0.5692902825625937 Test-f1: 0.4573487773487773 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12137/20000] Train-Loss: 0.4152677705532569 Test-Loss: 0.14043855909089517 Train-f1: 0.4867121889961542 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12138/20000] Train-Loss: 0.4145004564511728 Test-Loss: 0.13876934944096211 Train-f1: 0.48944450512658566 Test-f1: 0.46761904761904766 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [12139/20000] Train-Loss: 0.4088698984430533 Test-Loss: 0.1362588998925455 Train-f1: 0.4881648700472116 Test-f1: 0.45369747899159674 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12140/20000] Train-Loss: 0.4195334745261356 Test-Loss: 0.13764613673773052 Train-f1: 0.4861302114466672 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12141/20000] Train-Loss: 0.4148744050513085 Test-Loss: 0.13928793086863214 Train-f1: 0.48956275791718823 Test-f1: 0.5041022846972504 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12142/20000] Train-Loss: 0.41366524121786197 Test-Loss: 0.1392935029692713 Train-f1: 0.4876560351381432 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12143/20000] Train-Loss: 0.4137519366057378 Test-Loss: 0.13774499781308405 Train-f1: 0.5091866423461836 Test-f1: 0.5154197675936807 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12144/20000] Train-Loss: 0.4099549578113833 Test-Loss: 0.13719064166095393 Train-f1: 0.4979340659340659 Test-f1: 0.5076190476190476 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12145/20000] Train-Loss: 0.41316848584567606 Test-Loss: 0.14334734267024074 Train-f1: 0.5596743121626077 Test-f1: 0.456831179005092 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12146/20000] Train-Loss: 0.41426328617454916 Test-Loss: 0.13683437027767653 Train-f1: 0.48669050247997614 Test-f1: 0.5077443609022556 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12147/20000] Train-Loss: 0.4203374268106581 Test-Loss: 0.1442626682551581 Train-f1: 0.4888329141493698 Test-f1: 0.47645625692137317 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12148/20000] Train-Loss: 0.41451846719732993 Test-Loss: 0.15732955367463947 Train-f1: 0.49800891512196 Test-f1: 0.4751156336725254 Train-acc: 0.8079470198675497 Test-acc: 0.8026315789473685\n",
      "Epoch [12149/20000] Train-Loss: 0.4161455696083453 Test-Loss: 0.1401304646195386 Train-f1: 0.4946261674907778 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12150/20000] Train-Loss: 0.412875458680474 Test-Loss: 0.13718293048467742 Train-f1: 0.553077150748966 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12151/20000] Train-Loss: 0.41358432444941234 Test-Loss: 0.13693033944346883 Train-f1: 0.48477277561148524 Test-f1: 0.4573487773487773 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12152/20000] Train-Loss: 0.41456225872262953 Test-Loss: 0.1384809799285295 Train-f1: 0.5543227018048098 Test-f1: 0.47010295757098824 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12153/20000] Train-Loss: 0.41630054700678826 Test-Loss: 0.13801747023179664 Train-f1: 0.4924699461612213 Test-f1: 0.4568421052631579 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12154/20000] Train-Loss: 0.4159826326653172 Test-Loss: 0.14332853326261644 Train-f1: 0.5574794574794575 Test-f1: 0.4594670406732118 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12155/20000] Train-Loss: 0.41384191369681994 Test-Loss: 0.1409442538337229 Train-f1: 0.49526739926739927 Test-f1: 0.5037301587301587 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12156/20000] Train-Loss: 0.41868951701794993 Test-Loss: 0.1401020078435503 Train-f1: 0.4772343755315892 Test-f1: 0.5156532356532357 Train-acc: 0.8013245033112583 Test-acc: 0.7894736842105263\n",
      "Epoch [12157/20000] Train-Loss: 0.4126115894159323 Test-Loss: 0.14115950962738472 Train-f1: 0.4939292929292929 Test-f1: 0.5037301587301587 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12158/20000] Train-Loss: 0.41156939403305615 Test-Loss: 0.13818369088832186 Train-f1: 0.48801383019592776 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12159/20000] Train-Loss: 0.41248988686324906 Test-Loss: 0.14154506055374783 Train-f1: 0.4975365803575569 Test-f1: 0.5148792270531402 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12160/20000] Train-Loss: 0.4180755722009616 Test-Loss: 0.13853078167797064 Train-f1: 0.490357870543629 Test-f1: 0.46761904761904766 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12161/20000] Train-Loss: 0.4165716377492076 Test-Loss: 0.14561584015498838 Train-f1: 0.4871138444386853 Test-f1: 0.47645625692137317 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12162/20000] Train-Loss: 0.4098489151876973 Test-Loss: 0.14495064216417866 Train-f1: 0.5605451554029661 Test-f1: 0.5244444444444445 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12163/20000] Train-Loss: 0.4131068870062693 Test-Loss: 0.1376809563746989 Train-f1: 0.49337492614911965 Test-f1: 0.5076190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12164/20000] Train-Loss: 0.4131441298772441 Test-Loss: 0.13805793583116427 Train-f1: 0.4873828348280904 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12165/20000] Train-Loss: 0.4128671023581254 Test-Loss: 0.1401964596401665 Train-f1: 0.49129666380340237 Test-f1: 0.5031312595734653 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12166/20000] Train-Loss: 0.4128485172332581 Test-Loss: 0.13990760879912156 Train-f1: 0.4873417410330162 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12167/20000] Train-Loss: 0.4081438166170569 Test-Loss: 0.14187213541057098 Train-f1: 0.5658430891272708 Test-f1: 0.5051919146036793 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12168/20000] Train-Loss: 0.4161844990800308 Test-Loss: 0.13858134465560962 Train-f1: 0.5168456556778127 Test-f1: 0.46761904761904766 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12169/20000] Train-Loss: 0.414307943566851 Test-Loss: 0.1398684604593197 Train-f1: 0.5899397725204177 Test-f1: 0.45543478260869563 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [12170/20000] Train-Loss: 0.41801416570933553 Test-Loss: 0.14373822680588505 Train-f1: 0.4845367160803402 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12171/20000] Train-Loss: 0.41528208751152357 Test-Loss: 0.142627962577375 Train-f1: 0.5016106360124133 Test-f1: 0.44649416182596957 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12172/20000] Train-Loss: 0.4183992922751319 Test-Loss: 0.1372170361117518 Train-f1: 0.49034067068122794 Test-f1: 0.460952380952381 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12173/20000] Train-Loss: 0.40992241833717963 Test-Loss: 0.1461873939660558 Train-f1: 0.4890463929834682 Test-f1: 0.5149874686716792 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12174/20000] Train-Loss: 0.41415561551449975 Test-Loss: 0.13968012641972283 Train-f1: 0.4852388431243012 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12175/20000] Train-Loss: 0.4151004538447279 Test-Loss: 0.1396868862663256 Train-f1: 0.4931512354543017 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12176/20000] Train-Loss: 0.4125926600642629 Test-Loss: 0.13760018334980795 Train-f1: 0.5595970457649052 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12177/20000] Train-Loss: 0.42246065194285215 Test-Loss: 0.13907407553577703 Train-f1: 0.48360462838344953 Test-f1: 0.5156532356532357 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12178/20000] Train-Loss: 0.4150120297467103 Test-Loss: 0.136933865700208 Train-f1: 0.4864018577870384 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12179/20000] Train-Loss: 0.40504348085604186 Test-Loss: 0.1487192266801905 Train-f1: 0.5045937066501737 Test-f1: 0.5244444444444445 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [12180/20000] Train-Loss: 0.40806915270471494 Test-Loss: 0.14055904931124918 Train-f1: 0.6210345079910298 Test-f1: 0.44602036819428126 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12181/20000] Train-Loss: 0.4137605527615404 Test-Loss: 0.13856283676097592 Train-f1: 0.5134239150026672 Test-f1: 0.46761904761904766 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12182/20000] Train-Loss: 0.41576698874244017 Test-Loss: 0.14366637569500051 Train-f1: 0.4879159644053054 Test-f1: 0.4594670406732118 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12183/20000] Train-Loss: 0.4113771841337225 Test-Loss: 0.15946312974428575 Train-f1: 0.5576855272893375 Test-f1: 0.4354060018656291 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [12184/20000] Train-Loss: 0.4195338081070522 Test-Loss: 0.14197303534397723 Train-f1: 0.4920710962905453 Test-f1: 0.44937417654808953 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12185/20000] Train-Loss: 0.4184892546190768 Test-Loss: 0.14842274591770563 Train-f1: 0.5463847410867279 Test-f1: 0.4751156336725254 Train-acc: 0.8079470198675497 Test-acc: 0.8026315789473685\n",
      "Epoch [12186/20000] Train-Loss: 0.415929233746143 Test-Loss: 0.1391247109328428 Train-f1: 0.48290148433972774 Test-f1: 0.46066734074823057 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12187/20000] Train-Loss: 0.4164949277794446 Test-Loss: 0.13868471486895004 Train-f1: 0.4867246788299419 Test-f1: 0.46761904761904766 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12188/20000] Train-Loss: 0.41142030683454683 Test-Loss: 0.14415998311297407 Train-f1: 0.4893788556628208 Test-f1: 0.5161904761904762 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12189/20000] Train-Loss: 0.41715820637618406 Test-Loss: 0.14697064828956452 Train-f1: 0.5825561300013856 Test-f1: 0.4652641421224871 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12190/20000] Train-Loss: 0.4106020343278592 Test-Loss: 0.1378043989466709 Train-f1: 0.555105059025742 Test-f1: 0.5065132139812447 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12191/20000] Train-Loss: 0.4160352664650039 Test-Loss: 0.1409088457416206 Train-f1: 0.48845393908884976 Test-f1: 0.5031312595734653 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12192/20000] Train-Loss: 0.4106021488128186 Test-Loss: 0.1364909771349426 Train-f1: 0.491372662425294 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12193/20000] Train-Loss: 0.4101502605625935 Test-Loss: 0.13809768075228215 Train-f1: 0.5416790416790417 Test-f1: 0.44602036819428126 Train-acc: 0.8013245033112583 Test-acc: 0.7631578947368421\n",
      "Epoch [12194/20000] Train-Loss: 0.4096756033694459 Test-Loss: 0.14284017431512097 Train-f1: 0.4853903081691935 Test-f1: 0.4661751152073733 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12195/20000] Train-Loss: 0.4174349247115114 Test-Loss: 0.13631043588038583 Train-f1: 0.49550105227524577 Test-f1: 0.4469172932330827 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12196/20000] Train-Loss: 0.4124401696139231 Test-Loss: 0.1410168736049045 Train-f1: 0.49195366037471305 Test-f1: 0.5051919146036793 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12197/20000] Train-Loss: 0.41435576055502077 Test-Loss: 0.13809126409615116 Train-f1: 0.49845927447595856 Test-f1: 0.5165079365079366 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12198/20000] Train-Loss: 0.4141718641233837 Test-Loss: 0.1371471006145173 Train-f1: 0.4896306172616313 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12199/20000] Train-Loss: 0.4126883301643115 Test-Loss: 0.13799835824134557 Train-f1: 0.4871138444386853 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12200/20000] Train-Loss: 0.4156600918082015 Test-Loss: 0.1421912496545973 Train-f1: 0.4874394422781519 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12201/20000] Train-Loss: 0.41111115315948354 Test-Loss: 0.13742027738918966 Train-f1: 0.48852532858803377 Test-f1: 0.4565132139812447 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12202/20000] Train-Loss: 0.412428516478219 Test-Loss: 0.1382171903823682 Train-f1: 0.4856174406713919 Test-f1: 0.5548792270531402 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12203/20000] Train-Loss: 0.4110377548922007 Test-Loss: 0.13681502131463524 Train-f1: 0.5541061089448186 Test-f1: 0.5076190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12204/20000] Train-Loss: 0.41649474040942186 Test-Loss: 0.13744182729964113 Train-f1: 0.5199713330148112 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12205/20000] Train-Loss: 0.4125341482881707 Test-Loss: 0.13726480046574188 Train-f1: 0.49438215772701677 Test-f1: 0.46815958815958825 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12206/20000] Train-Loss: 0.4139035123110086 Test-Loss: 0.1396020472227439 Train-f1: 0.48577424858246776 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12207/20000] Train-Loss: 0.4129251827578968 Test-Loss: 0.1366221623738556 Train-f1: 0.48324336223224196 Test-f1: 0.45369747899159674 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12208/20000] Train-Loss: 0.41012699881838305 Test-Loss: 0.14973333443366013 Train-f1: 0.4829093745760412 Test-f1: 0.5028754578754578 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12209/20000] Train-Loss: 0.4138868199638111 Test-Loss: 0.13787435544195056 Train-f1: 0.5121860595544806 Test-f1: 0.5076190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12210/20000] Train-Loss: 0.4069008805565605 Test-Loss: 0.15194627049581572 Train-f1: 0.49612274517934896 Test-f1: 0.5244444444444445 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12211/20000] Train-Loss: 0.4185681872784074 Test-Loss: 0.14620697364901075 Train-f1: 0.4902916073406732 Test-f1: 0.4695652173913043 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12212/20000] Train-Loss: 0.4131534016613346 Test-Loss: 0.13725826023466545 Train-f1: 0.4878532222092685 Test-f1: 0.5077443609022556 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12213/20000] Train-Loss: 0.410533825347953 Test-Loss: 0.14665996553097208 Train-f1: 0.4975365803575569 Test-f1: 0.5028754578754578 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12214/20000] Train-Loss: 0.413757501111804 Test-Loss: 0.13934115287617282 Train-f1: 0.5688395854092649 Test-f1: 0.46761904761904766 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12215/20000] Train-Loss: 0.41638106696869437 Test-Loss: 0.13759320318998194 Train-f1: 0.48756188623738295 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12216/20000] Train-Loss: 0.41501669283506987 Test-Loss: 0.13748708192419332 Train-f1: 0.4941419343847421 Test-f1: 0.4359678460364959 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [12217/20000] Train-Loss: 0.4125968664521266 Test-Loss: 0.14010202729318852 Train-f1: 0.4951545099196106 Test-f1: 0.5156532356532357 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12218/20000] Train-Loss: 0.4112044615411103 Test-Loss: 0.14588759067302717 Train-f1: 0.48275067230119095 Test-f1: 0.46604809770217487 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12219/20000] Train-Loss: 0.41411982450413004 Test-Loss: 0.14026688395270623 Train-f1: 0.49999971597283566 Test-f1: 0.46066734074823057 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12220/20000] Train-Loss: 0.4126172630031093 Test-Loss: 0.13742794916434387 Train-f1: 0.4850947126488922 Test-f1: 0.5067505720823798 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12221/20000] Train-Loss: 0.4115102342604234 Test-Loss: 0.14490802430020092 Train-f1: 0.4901876887435742 Test-f1: 0.5244444444444445 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [12222/20000] Train-Loss: 0.4117097996765443 Test-Loss: 0.13699121779470783 Train-f1: 0.5541062963881755 Test-f1: 0.45369747899159674 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12223/20000] Train-Loss: 0.4117470741273799 Test-Loss: 0.1431883120608023 Train-f1: 0.4938123870738426 Test-f1: 0.4554347826086957 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12224/20000] Train-Loss: 0.4121486649919939 Test-Loss: 0.1443840366352433 Train-f1: 0.5522293139602837 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12225/20000] Train-Loss: 0.4129567194624924 Test-Loss: 0.13946998839506966 Train-f1: 0.5582008872425177 Test-f1: 0.46066734074823057 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12226/20000] Train-Loss: 0.41222780351936356 Test-Loss: 0.1376256623620514 Train-f1: 0.4830567642648179 Test-f1: 0.5062301587301588 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12227/20000] Train-Loss: 0.4146081603328509 Test-Loss: 0.13813351648387548 Train-f1: 0.532733237776716 Test-f1: 0.4602828448452552 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12228/20000] Train-Loss: 0.41561400400752874 Test-Loss: 0.13674809115699724 Train-f1: 0.48932208142734457 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12229/20000] Train-Loss: 0.41436252565491566 Test-Loss: 0.13918635145859587 Train-f1: 0.48944450512658566 Test-f1: 0.5156532356532357 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12230/20000] Train-Loss: 0.41090296248403846 Test-Loss: 0.1372242259157913 Train-f1: 0.4897682161608401 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12231/20000] Train-Loss: 0.41091965348812587 Test-Loss: 0.1512799611753857 Train-f1: 0.48956275791718823 Test-f1: 0.5237166085946574 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [12232/20000] Train-Loss: 0.41433861777007225 Test-Loss: 0.143959410429499 Train-f1: 0.5120340331558242 Test-f1: 0.4594670406732118 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12233/20000] Train-Loss: 0.4135197747907944 Test-Loss: 0.1380168252562675 Train-f1: 0.494017270168408 Test-f1: 0.4602828448452552 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12234/20000] Train-Loss: 0.4100639711544345 Test-Loss: 0.13911396790544492 Train-f1: 0.558017270168408 Test-f1: 0.46761904761904766 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12235/20000] Train-Loss: 0.4088384450902657 Test-Loss: 0.14181274349174505 Train-f1: 0.48645725171072646 Test-f1: 0.45233400741489715 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12236/20000] Train-Loss: 0.4089405308705605 Test-Loss: 0.14071153374659048 Train-f1: 0.6068864468864469 Test-f1: 0.4561490683229813 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12237/20000] Train-Loss: 0.41619646361245555 Test-Loss: 0.14243311651586238 Train-f1: 0.4806953183991946 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12238/20000] Train-Loss: 0.41363140108379326 Test-Loss: 0.13674937413542326 Train-f1: 0.48684909861380443 Test-f1: 0.49677440206851975 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12239/20000] Train-Loss: 0.41521817540559014 Test-Loss: 0.14089357876099473 Train-f1: 0.4843100239121939 Test-f1: 0.4558710801393729 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12240/20000] Train-Loss: 0.4116394813309955 Test-Loss: 0.13933450414137394 Train-f1: 0.48662480153187265 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12241/20000] Train-Loss: 0.40925697164745006 Test-Loss: 0.1457786526285044 Train-f1: 0.49929329929329924 Test-f1: 0.5244444444444445 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [12242/20000] Train-Loss: 0.41610290083220075 Test-Loss: 0.13940503763647716 Train-f1: 0.485204215586752 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12243/20000] Train-Loss: 0.41306761931208447 Test-Loss: 0.1366884866137707 Train-f1: 0.4951428809987325 Test-f1: 0.45393483709273186 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12244/20000] Train-Loss: 0.416673903325771 Test-Loss: 0.13655808740712996 Train-f1: 0.4908122564086314 Test-f1: 0.45369747899159674 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12245/20000] Train-Loss: 0.41113931870006126 Test-Loss: 0.14260346862566603 Train-f1: 0.5557891383978341 Test-f1: 0.5149874686716792 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12246/20000] Train-Loss: 0.4114780557819092 Test-Loss: 0.13964075327001516 Train-f1: 0.5091866423461836 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12247/20000] Train-Loss: 0.4126110492282558 Test-Loss: 0.1501017399235079 Train-f1: 0.5554028910834407 Test-f1: 0.5237164363554656 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [12248/20000] Train-Loss: 0.41616430853475767 Test-Loss: 0.14020951163762918 Train-f1: 0.4935996826816561 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12249/20000] Train-Loss: 0.4113091753811632 Test-Loss: 0.14263763587116526 Train-f1: 0.4843100239121939 Test-f1: 0.44937417654808953 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12250/20000] Train-Loss: 0.4131405362903649 Test-Loss: 0.13773483366785322 Train-f1: 0.4960762323481527 Test-f1: 0.49677440206851975 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12251/20000] Train-Loss: 0.4104600750200022 Test-Loss: 0.13787957525925984 Train-f1: 0.48471147465673015 Test-f1: 0.4568421052631579 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12252/20000] Train-Loss: 0.41195535162142577 Test-Loss: 0.14086843013747546 Train-f1: 0.507067604535959 Test-f1: 0.5156532356532357 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [12253/20000] Train-Loss: 0.410957185678743 Test-Loss: 0.13809124598626385 Train-f1: 0.49473004879782084 Test-f1: 0.4568421052631579 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12254/20000] Train-Loss: 0.4151651983980999 Test-Loss: 0.13812121505884867 Train-f1: 0.48532571181577805 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12255/20000] Train-Loss: 0.40965715605552216 Test-Loss: 0.137362017299209 Train-f1: 0.4889095999257159 Test-f1: 0.5066513056835638 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12256/20000] Train-Loss: 0.4135888216327007 Test-Loss: 0.14271951826098264 Train-f1: 0.5239643689643689 Test-f1: 0.5149874686716792 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12257/20000] Train-Loss: 0.4097062825584464 Test-Loss: 0.13663815289093947 Train-f1: 0.4907792491321789 Test-f1: 0.45369747899159674 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12258/20000] Train-Loss: 0.4134387729769215 Test-Loss: 0.13898849418494336 Train-f1: 0.5544029405655901 Test-f1: 0.504984984984985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12259/20000] Train-Loss: 0.40923756142263407 Test-Loss: 0.13992190434949628 Train-f1: 0.4929722496285964 Test-f1: 0.5062301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12260/20000] Train-Loss: 0.41382342860386373 Test-Loss: 0.1370445913350249 Train-f1: 0.48426929458952833 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12261/20000] Train-Loss: 0.40455319584579713 Test-Loss: 0.1384674438211778 Train-f1: 0.5048408972681558 Test-f1: 0.46728722258134026 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [12262/20000] Train-Loss: 0.4108439465237394 Test-Loss: 0.14284406652462903 Train-f1: 0.4953534207727756 Test-f1: 0.44937417654808953 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12263/20000] Train-Loss: 0.4136955709727031 Test-Loss: 0.13734307363073972 Train-f1: 0.4892068126520681 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12264/20000] Train-Loss: 0.41093209347989856 Test-Loss: 0.1430587256893805 Train-f1: 0.4814936420519259 Test-f1: 0.4661751152073733 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12265/20000] Train-Loss: 0.41119978327043194 Test-Loss: 0.14156418052145664 Train-f1: 0.4894888173771606 Test-f1: 0.5031312595734653 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12266/20000] Train-Loss: 0.4167398201713083 Test-Loss: 0.13792210691285972 Train-f1: 0.4852388431243012 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12267/20000] Train-Loss: 0.4121202576319892 Test-Loss: 0.13864074978299828 Train-f1: 0.49009191250250855 Test-f1: 0.46728722258134026 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12268/20000] Train-Loss: 0.4104943230315556 Test-Loss: 0.14273479030931122 Train-f1: 0.4795519269971825 Test-f1: 0.5037301587301587 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12269/20000] Train-Loss: 0.41288655477629393 Test-Loss: 0.1463978824282203 Train-f1: 0.48918119237084756 Test-f1: 0.4594670406732118 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12270/20000] Train-Loss: 0.41110477981269405 Test-Loss: 0.13782774961205976 Train-f1: 0.48946347769877185 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12271/20000] Train-Loss: 0.41003203192340354 Test-Loss: 0.14094141303617058 Train-f1: 0.4819993178447065 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12272/20000] Train-Loss: 0.41094260179834113 Test-Loss: 0.141729795531815 Train-f1: 0.48219213045019493 Test-f1: 0.44937417654808953 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12273/20000] Train-Loss: 0.41477254135964076 Test-Loss: 0.1381075278254814 Train-f1: 0.5606839368350747 Test-f1: 0.4568421052631579 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12274/20000] Train-Loss: 0.4172007958985425 Test-Loss: 0.1397887162547346 Train-f1: 0.48700902061692564 Test-f1: 0.5148792270531402 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12275/20000] Train-Loss: 0.40907245488696986 Test-Loss: 0.13935169117624446 Train-f1: 0.4942221139186323 Test-f1: 0.5062301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12276/20000] Train-Loss: 0.41239764939073587 Test-Loss: 0.14313540442590772 Train-f1: 0.4936574682766368 Test-f1: 0.4661751152073733 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12277/20000] Train-Loss: 0.40998505224074916 Test-Loss: 0.1432271418843563 Train-f1: 0.4879443543567004 Test-f1: 0.4661751152073733 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12278/20000] Train-Loss: 0.40942006270875714 Test-Loss: 0.13943146060286649 Train-f1: 0.4915065024141767 Test-f1: 0.5076190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12279/20000] Train-Loss: 0.4093363264490008 Test-Loss: 0.1469838991362578 Train-f1: 0.49228039221394315 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12280/20000] Train-Loss: 0.4176110003399633 Test-Loss: 0.1373837501857107 Train-f1: 0.48797471843829454 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12281/20000] Train-Loss: 0.4107158186541259 Test-Loss: 0.13788728143339118 Train-f1: 0.489375179959689 Test-f1: 0.5062301587301588 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12282/20000] Train-Loss: 0.4133647499238554 Test-Loss: 0.13810095373151932 Train-f1: 0.48219213045019493 Test-f1: 0.5051919146036793 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12283/20000] Train-Loss: 0.41020035068535166 Test-Loss: 0.14158956833837205 Train-f1: 0.49101496676210904 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12284/20000] Train-Loss: 0.4070192128839161 Test-Loss: 0.1408279987889884 Train-f1: 0.5656532656532656 Test-f1: 0.5031312595734653 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12285/20000] Train-Loss: 0.40839113005526395 Test-Loss: 0.14327247736783608 Train-f1: 0.48428093645484954 Test-f1: 0.5141038802093091 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12286/20000] Train-Loss: 0.411275321217699 Test-Loss: 0.14477597606325612 Train-f1: 0.49510412189423636 Test-f1: 0.5628157349896481 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [12287/20000] Train-Loss: 0.41158133596543134 Test-Loss: 0.14523311548646797 Train-f1: 0.4950201182413405 Test-f1: 0.5156685910608086 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12288/20000] Train-Loss: 0.4090112353237044 Test-Loss: 0.13776953122306396 Train-f1: 0.5072730827001927 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12289/20000] Train-Loss: 0.4120967422023474 Test-Loss: 0.13997369126973505 Train-f1: 0.5718959101312043 Test-f1: 0.4670201484623542 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12290/20000] Train-Loss: 0.41453051949315733 Test-Loss: 0.13680466178911266 Train-f1: 0.491042291042291 Test-f1: 0.4573487773487773 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12291/20000] Train-Loss: 0.41074552344964466 Test-Loss: 0.1419286548653 Train-f1: 0.5586064797271694 Test-f1: 0.5037301587301587 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12292/20000] Train-Loss: 0.41295076221718446 Test-Loss: 0.14441967239291498 Train-f1: 0.4877176351628907 Test-f1: 0.5028754578754578 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12293/20000] Train-Loss: 0.4096716356002008 Test-Loss: 0.14367756245891594 Train-f1: 0.48873622441677406 Test-f1: 0.46679841897233204 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12294/20000] Train-Loss: 0.41183090521474536 Test-Loss: 0.14186487440384885 Train-f1: 0.4889279873487185 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12295/20000] Train-Loss: 0.40684575545299856 Test-Loss: 0.1444076968761493 Train-f1: 0.49747746977633617 Test-f1: 0.5141038802093091 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12296/20000] Train-Loss: 0.41363119173882756 Test-Loss: 0.13648465056646567 Train-f1: 0.4801309066209728 Test-f1: 0.45393483709273186 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [12297/20000] Train-Loss: 0.4116942981320389 Test-Loss: 0.14223821934589057 Train-f1: 0.48629148629148633 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12298/20000] Train-Loss: 0.4074150632807562 Test-Loss: 0.1473168322891786 Train-f1: 0.48961341739923014 Test-f1: 0.4554347826086957 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12299/20000] Train-Loss: 0.4074485000816767 Test-Loss: 0.13862141755549193 Train-f1: 0.490357870543629 Test-f1: 0.5066513056835638 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12300/20000] Train-Loss: 0.4165513577340486 Test-Loss: 0.13919711714814959 Train-f1: 0.48349428349428347 Test-f1: 0.46761904761904766 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12301/20000] Train-Loss: 0.4112881311644035 Test-Loss: 0.13947680987644046 Train-f1: 0.4903150377602932 Test-f1: 0.46761904761904766 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12302/20000] Train-Loss: 0.41362540466929926 Test-Loss: 0.1416639451248747 Train-f1: 0.4935995057839612 Test-f1: 0.44937417654808953 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12303/20000] Train-Loss: 0.4073878602349384 Test-Loss: 0.14123931981049123 Train-f1: 0.557761302625913 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12304/20000] Train-Loss: 0.41063412964960105 Test-Loss: 0.1367810494413124 Train-f1: 0.4881100881100881 Test-f1: 0.5077443609022556 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12305/20000] Train-Loss: 0.4139822363017224 Test-Loss: 0.1391149477372546 Train-f1: 0.48177837834257364 Test-f1: 0.5156532356532357 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12306/20000] Train-Loss: 0.4136679044612814 Test-Loss: 0.13987366969569678 Train-f1: 0.5118852425628784 Test-f1: 0.4526190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12307/20000] Train-Loss: 0.4109662446399507 Test-Loss: 0.13988807773734702 Train-f1: 0.48990584359711875 Test-f1: 0.46761904761904766 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12308/20000] Train-Loss: 0.4106774896958705 Test-Loss: 0.13693373457480323 Train-f1: 0.553068265068265 Test-f1: 0.44650793650793663 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12309/20000] Train-Loss: 0.4177318744813159 Test-Loss: 0.13851565991560066 Train-f1: 0.48423336807932305 Test-f1: 0.4568421052631579 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12310/20000] Train-Loss: 0.41147859264455117 Test-Loss: 0.13696968347495556 Train-f1: 0.48124536023423986 Test-f1: 0.5056027456027457 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12311/20000] Train-Loss: 0.41048923789526953 Test-Loss: 0.13944047869407672 Train-f1: 0.4866379996814779 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12312/20000] Train-Loss: 0.4133139784234344 Test-Loss: 0.13772987139107745 Train-f1: 0.4916776556776557 Test-f1: 0.4568421052631579 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12313/20000] Train-Loss: 0.41394918182779367 Test-Loss: 0.13946520338536125 Train-f1: 0.4924699461612213 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12314/20000] Train-Loss: 0.4067863745871755 Test-Loss: 0.1377503995307711 Train-f1: 0.5575816086922387 Test-f1: 0.463768115942029 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12315/20000] Train-Loss: 0.4102277303359539 Test-Loss: 0.1423506711869334 Train-f1: 0.496689705085379 Test-f1: 0.45233400741489715 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12316/20000] Train-Loss: 0.4091017406647095 Test-Loss: 0.14222902152244926 Train-f1: 0.4843674593668944 Test-f1: 0.4554347826086957 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12317/20000] Train-Loss: 0.4093281089514705 Test-Loss: 0.1370911006028794 Train-f1: 0.4871033317604563 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12318/20000] Train-Loss: 0.4111064960428514 Test-Loss: 0.13790108799537817 Train-f1: 0.49555681626035275 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12319/20000] Train-Loss: 0.41173461847426407 Test-Loss: 0.14516772615030668 Train-f1: 0.48546697463894917 Test-f1: 0.5141038802093091 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12320/20000] Train-Loss: 0.41097643369302767 Test-Loss: 0.14374160061903873 Train-f1: 0.4874394422781519 Test-f1: 0.44937417654808953 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12321/20000] Train-Loss: 0.41368701922853235 Test-Loss: 0.13911475990269562 Train-f1: 0.4878878497468177 Test-f1: 0.5155103444886726 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12322/20000] Train-Loss: 0.4100195185091721 Test-Loss: 0.14215129879656502 Train-f1: 0.4866811923708475 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12323/20000] Train-Loss: 0.40577908685778913 Test-Loss: 0.13643578711369003 Train-f1: 0.5124412472647653 Test-f1: 0.45000000000000007 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12324/20000] Train-Loss: 0.4122416516060161 Test-Loss: 0.13652586877612363 Train-f1: 0.4906237250608111 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12325/20000] Train-Loss: 0.4073333896487461 Test-Loss: 0.1376791070076143 Train-f1: 0.4853601144991873 Test-f1: 0.46815958815958825 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12326/20000] Train-Loss: 0.4128684107597712 Test-Loss: 0.13867101017858266 Train-f1: 0.4915750915750916 Test-f1: 0.46761904761904766 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12327/20000] Train-Loss: 0.4130181181914997 Test-Loss: 0.13743521497415134 Train-f1: 0.48885400313971744 Test-f1: 0.4644611528822056 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12328/20000] Train-Loss: 0.40544984873864814 Test-Loss: 0.1388598985421709 Train-f1: 0.5665703424092686 Test-f1: 0.5066513056835638 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12329/20000] Train-Loss: 0.4113134884927359 Test-Loss: 0.14635195907179377 Train-f1: 0.5175324216835595 Test-f1: 0.4726570048309179 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12330/20000] Train-Loss: 0.41578538525853925 Test-Loss: 0.14348539222624754 Train-f1: 0.4822090497885335 Test-f1: 0.5028754578754578 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12331/20000] Train-Loss: 0.4113704558212124 Test-Loss: 0.13945444936582713 Train-f1: 0.4947445234504058 Test-f1: 0.449645003282331 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12332/20000] Train-Loss: 0.40988009355205224 Test-Loss: 0.1404094546794696 Train-f1: 0.4976147912453645 Test-f1: 0.5156532356532357 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12333/20000] Train-Loss: 0.4109288392021205 Test-Loss: 0.1389985061010693 Train-f1: 0.4926867541061089 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12334/20000] Train-Loss: 0.4141333329238272 Test-Loss: 0.13878774452125014 Train-f1: 0.4814225716811924 Test-f1: 0.504984984984985 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12335/20000] Train-Loss: 0.41454416732906635 Test-Loss: 0.13935793621035178 Train-f1: 0.5602661724506279 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12336/20000] Train-Loss: 0.413139652233587 Test-Loss: 0.1414045174178143 Train-f1: 0.5706462824387353 Test-f1: 0.5037301587301587 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12337/20000] Train-Loss: 0.41129268783146883 Test-Loss: 0.1411227445253693 Train-f1: 0.5871539617817164 Test-f1: 0.5037301587301587 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [12338/20000] Train-Loss: 0.415732269359264 Test-Loss: 0.1434555103077429 Train-f1: 0.5070211992583065 Test-f1: 0.45233400741489715 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12339/20000] Train-Loss: 0.41048535661493124 Test-Loss: 0.13744318862633212 Train-f1: 0.48219213045019493 Test-f1: 0.46815958815958825 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12340/20000] Train-Loss: 0.4119717688580721 Test-Loss: 0.13784901090977877 Train-f1: 0.48367726546668965 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12341/20000] Train-Loss: 0.4100321806033753 Test-Loss: 0.14922378028485025 Train-f1: 0.6118277737134482 Test-f1: 0.4554347826086957 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12342/20000] Train-Loss: 0.41009683900069294 Test-Loss: 0.14115023653367476 Train-f1: 0.5628468176119182 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12343/20000] Train-Loss: 0.4078398125932651 Test-Loss: 0.1385953668005818 Train-f1: 0.4885804754912663 Test-f1: 0.46728722258134026 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12344/20000] Train-Loss: 0.4112994583529233 Test-Loss: 0.13822134347740112 Train-f1: 0.6103397221426387 Test-f1: 0.45 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12345/20000] Train-Loss: 0.4115570958580434 Test-Loss: 0.13718321575631115 Train-f1: 0.4945852393220814 Test-f1: 0.4644611528822056 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12346/20000] Train-Loss: 0.40818763560964477 Test-Loss: 0.14269795572202246 Train-f1: 0.5061261725385187 Test-f1: 0.4661751152073733 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12347/20000] Train-Loss: 0.41049774427747343 Test-Loss: 0.14003962161106423 Train-f1: 0.5741062189708293 Test-f1: 0.46066734074823057 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12348/20000] Train-Loss: 0.41051586176707905 Test-Loss: 0.13866952783080838 Train-f1: 0.5749324904830091 Test-f1: 0.4561490683229813 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12349/20000] Train-Loss: 0.4141868759530553 Test-Loss: 0.14049845511016998 Train-f1: 0.49129666380340237 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12350/20000] Train-Loss: 0.40892472415209896 Test-Loss: 0.13976057299307587 Train-f1: 0.49236431784202483 Test-f1: 0.46761904761904766 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12351/20000] Train-Loss: 0.40870004029966844 Test-Loss: 0.13762051813980544 Train-f1: 0.5115587115587116 Test-f1: 0.46815958815958825 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12352/20000] Train-Loss: 0.40643700948919825 Test-Loss: 0.14042644135299212 Train-f1: 0.4916370645600271 Test-f1: 0.5066513056835638 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12353/20000] Train-Loss: 0.40736355002589486 Test-Loss: 0.14671954254675262 Train-f1: 0.4858084524171481 Test-f1: 0.5244444444444445 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12354/20000] Train-Loss: 0.4119654501933063 Test-Loss: 0.13722998522900698 Train-f1: 0.48219213045019493 Test-f1: 0.5165079365079366 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12355/20000] Train-Loss: 0.4129636084706839 Test-Loss: 0.14229272476023674 Train-f1: 0.49443981306050266 Test-f1: 0.5037301587301587 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12356/20000] Train-Loss: 0.41182043465679913 Test-Loss: 0.1431336352008577 Train-f1: 0.4813658214285265 Test-f1: 0.5037301587301587 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12357/20000] Train-Loss: 0.41023747854838477 Test-Loss: 0.13688981843391915 Train-f1: 0.507223881211486 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12358/20000] Train-Loss: 0.4130331679560523 Test-Loss: 0.13864794279689444 Train-f1: 0.4898165471413879 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12359/20000] Train-Loss: 0.40870831192532564 Test-Loss: 0.1421003215917585 Train-f1: 0.48629148629148633 Test-f1: 0.5037301587301587 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12360/20000] Train-Loss: 0.410337094750569 Test-Loss: 0.14477033290531596 Train-f1: 0.4871366372134716 Test-f1: 0.4594670406732118 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12361/20000] Train-Loss: 0.4116631597185171 Test-Loss: 0.13972271390539923 Train-f1: 0.4961801509452516 Test-f1: 0.5170201484623542 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12362/20000] Train-Loss: 0.4104927646439126 Test-Loss: 0.14118203317564645 Train-f1: 0.4963756243756244 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12363/20000] Train-Loss: 0.4130569848386002 Test-Loss: 0.14642335828465447 Train-f1: 0.5084549709032883 Test-f1: 0.47307359307359304 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12364/20000] Train-Loss: 0.41098156362034094 Test-Loss: 0.14180468667692472 Train-f1: 0.48731433953378855 Test-f1: 0.5155103444886726 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12365/20000] Train-Loss: 0.40597078573335316 Test-Loss: 0.13704256288205816 Train-f1: 0.5609893152662194 Test-f1: 0.4642044134727062 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12366/20000] Train-Loss: 0.4133031158015203 Test-Loss: 0.14190543818552423 Train-f1: 0.4872205552133219 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12367/20000] Train-Loss: 0.417876423098856 Test-Loss: 0.13683096425255384 Train-f1: 0.484657177274627 Test-f1: 0.45369747899159674 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12368/20000] Train-Loss: 0.4104386350023047 Test-Loss: 0.1400689644876145 Train-f1: 0.4898352627582253 Test-f1: 0.449645003282331 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12369/20000] Train-Loss: 0.4097431723236606 Test-Loss: 0.1408691704289875 Train-f1: 0.4843100239121939 Test-f1: 0.46679841897233204 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12370/20000] Train-Loss: 0.4090330480620776 Test-Loss: 0.14014265121412714 Train-f1: 0.49659890799131307 Test-f1: 0.504984984984985 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12371/20000] Train-Loss: 0.4111050444375786 Test-Loss: 0.14499020367603244 Train-f1: 0.4993409726451793 Test-f1: 0.5134352946414658 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12372/20000] Train-Loss: 0.4106162173892134 Test-Loss: 0.1381184953145986 Train-f1: 0.4843665106823001 Test-f1: 0.4568421052631579 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12373/20000] Train-Loss: 0.4139517497917895 Test-Loss: 0.1408711872634442 Train-f1: 0.4877176351628907 Test-f1: 0.449645003282331 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12374/20000] Train-Loss: 0.41173971895207206 Test-Loss: 0.14097402638451006 Train-f1: 0.4923825952458074 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12375/20000] Train-Loss: 0.4079686276886161 Test-Loss: 0.13885982659452956 Train-f1: 0.5547767547767548 Test-f1: 0.46728722258134026 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12376/20000] Train-Loss: 0.40893930547753343 Test-Loss: 0.13856354111215086 Train-f1: 0.5664355708220028 Test-f1: 0.504984984984985 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12377/20000] Train-Loss: 0.40680006571527266 Test-Loss: 0.14109549387159132 Train-f1: 0.5800635965341847 Test-f1: 0.5044444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12378/20000] Train-Loss: 0.40654801124363193 Test-Loss: 0.13953867944783707 Train-f1: 0.4950201182413405 Test-f1: 0.5061490683229813 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12379/20000] Train-Loss: 0.41087662864132274 Test-Loss: 0.13755629542746783 Train-f1: 0.4969084249084249 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12380/20000] Train-Loss: 0.4061464727158913 Test-Loss: 0.14819600080171216 Train-f1: 0.4863038590655703 Test-f1: 0.5244444444444445 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12381/20000] Train-Loss: 0.4077292701966955 Test-Loss: 0.1400535199533188 Train-f1: 0.48925258187003157 Test-f1: 0.4558710801393729 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12382/20000] Train-Loss: 0.40869175148356324 Test-Loss: 0.1376357427686075 Train-f1: 0.49433976726272977 Test-f1: 0.4531469684787762 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12383/20000] Train-Loss: 0.4111647858816523 Test-Loss: 0.14280350023922886 Train-f1: 0.49725998994543835 Test-f1: 0.4594670406732118 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12384/20000] Train-Loss: 0.41058855358837965 Test-Loss: 0.13708620699288762 Train-f1: 0.5502887227252644 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12385/20000] Train-Loss: 0.4108509492792149 Test-Loss: 0.14135902257923308 Train-f1: 0.5091537279924376 Test-f1: 0.515048830342948 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12386/20000] Train-Loss: 0.41070139583130905 Test-Loss: 0.13855910028417182 Train-f1: 0.5624980418429008 Test-f1: 0.504984984984985 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12387/20000] Train-Loss: 0.4128124123641372 Test-Loss: 0.1387202274508536 Train-f1: 0.4909615521909375 Test-f1: 0.46761904761904766 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12388/20000] Train-Loss: 0.40989689973695353 Test-Loss: 0.13759270445614968 Train-f1: 0.5560636068645671 Test-f1: 0.46815958815958825 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12389/20000] Train-Loss: 0.40825115446837 Test-Loss: 0.1413795150437631 Train-f1: 0.5082825507825508 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12390/20000] Train-Loss: 0.41157669810072556 Test-Loss: 0.14033996283440667 Train-f1: 0.5094788716892278 Test-f1: 0.44951827242524917 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12391/20000] Train-Loss: 0.4072997592953491 Test-Loss: 0.13801653220549526 Train-f1: 0.4927215960773007 Test-f1: 0.44602036819428126 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12392/20000] Train-Loss: 0.40488662429676486 Test-Loss: 0.14129667066613355 Train-f1: 0.49209440348680855 Test-f1: 0.5155103444886726 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12393/20000] Train-Loss: 0.4044502438847176 Test-Loss: 0.13838172386731945 Train-f1: 0.48261793246313367 Test-f1: 0.4944577129097253 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12394/20000] Train-Loss: 0.4088746896810272 Test-Loss: 0.13890410367990036 Train-f1: 0.48918119237084756 Test-f1: 0.5051919146036793 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12395/20000] Train-Loss: 0.41101158600542115 Test-Loss: 0.13716042789650587 Train-f1: 0.48990584359711875 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12396/20000] Train-Loss: 0.4070015890126063 Test-Loss: 0.13906510855743923 Train-f1: 0.4815159518982856 Test-f1: 0.5051919146036793 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12397/20000] Train-Loss: 0.41285835243846447 Test-Loss: 0.14239933586610315 Train-f1: 0.4894888173771606 Test-f1: 0.5044444444444445 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12398/20000] Train-Loss: 0.40839779164601003 Test-Loss: 0.13859515684138482 Train-f1: 0.5100143011720257 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12399/20000] Train-Loss: 0.40774878817198074 Test-Loss: 0.13792958126877547 Train-f1: 0.48367726546668965 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12400/20000] Train-Loss: 0.41168386770964904 Test-Loss: 0.13725707866713135 Train-f1: 0.5524448884448885 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12401/20000] Train-Loss: 0.40954900410344275 Test-Loss: 0.14036633486655506 Train-f1: 0.4979340659340659 Test-f1: 0.5156532356532357 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12402/20000] Train-Loss: 0.4077966201751575 Test-Loss: 0.13666358568214118 Train-f1: 0.49153422057585117 Test-f1: 0.4430769230769231 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [12403/20000] Train-Loss: 0.4097580897353191 Test-Loss: 0.1371402579633172 Train-f1: 0.4951545099196106 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12404/20000] Train-Loss: 0.41110269777536357 Test-Loss: 0.14656905415504942 Train-f1: 0.49160694378085684 Test-f1: 0.5244444444444445 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12405/20000] Train-Loss: 0.4113134581134885 Test-Loss: 0.14140135437295442 Train-f1: 0.5270280667835232 Test-f1: 0.5037301587301587 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12406/20000] Train-Loss: 0.4129019467076823 Test-Loss: 0.14239976552138944 Train-f1: 0.4894888173771606 Test-f1: 0.5141038802093091 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12407/20000] Train-Loss: 0.4125978953679014 Test-Loss: 0.14111828334238843 Train-f1: 0.49521374265899815 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12408/20000] Train-Loss: 0.4084430914386681 Test-Loss: 0.14782332760978728 Train-f1: 0.4969084249084249 Test-f1: 0.5258893280632411 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12409/20000] Train-Loss: 0.41127486670678864 Test-Loss: 0.13734480784843428 Train-f1: 0.5722025632643482 Test-f1: 0.4503312334891282 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12410/20000] Train-Loss: 0.41150132396385164 Test-Loss: 0.14611586495082585 Train-f1: 0.48577424858246776 Test-f1: 0.5141038802093091 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12411/20000] Train-Loss: 0.40454257782939623 Test-Loss: 0.13869152147235472 Train-f1: 0.4897693163116655 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12412/20000] Train-Loss: 0.4114928499112003 Test-Loss: 0.14226188089249137 Train-f1: 0.48700902061692564 Test-f1: 0.5149874686716792 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12413/20000] Train-Loss: 0.40853021906791814 Test-Loss: 0.13928939559269599 Train-f1: 0.48025247771721113 Test-f1: 0.5161904761904762 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [12414/20000] Train-Loss: 0.4078799306752156 Test-Loss: 0.14506624945221863 Train-f1: 0.5307140478895027 Test-f1: 0.47645625692137317 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12415/20000] Train-Loss: 0.41336959823254515 Test-Loss: 0.1370518037694955 Train-f1: 0.4963965858702701 Test-f1: 0.49677440206851975 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [12416/20000] Train-Loss: 0.4127049045650255 Test-Loss: 0.14656706923368498 Train-f1: 0.4944272566376128 Test-f1: 0.5393814080656185 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12417/20000] Train-Loss: 0.40637993424556207 Test-Loss: 0.13742714767406267 Train-f1: 0.517199209312541 Test-f1: 0.4573487773487773 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12418/20000] Train-Loss: 0.40987834893333674 Test-Loss: 0.13900646548415752 Train-f1: 0.5059668797158543 Test-f1: 0.46761904761904766 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12419/20000] Train-Loss: 0.4082240807566575 Test-Loss: 0.13660403817587422 Train-f1: 0.5124412062293766 Test-f1: 0.4430769230769231 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [12420/20000] Train-Loss: 0.40546435459330493 Test-Loss: 0.14604606043492624 Train-f1: 0.4968241634908301 Test-f1: 0.47645625692137317 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12421/20000] Train-Loss: 0.40752586658342393 Test-Loss: 0.14343442965968814 Train-f1: 0.4979673650559726 Test-f1: 0.4661751152073733 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12422/20000] Train-Loss: 0.4081884041234907 Test-Loss: 0.14474604120215875 Train-f1: 0.6141893900283162 Test-f1: 0.5244444444444445 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [12423/20000] Train-Loss: 0.40850425827567266 Test-Loss: 0.14294271458835353 Train-f1: 0.5060282633531041 Test-f1: 0.5141038802093091 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12424/20000] Train-Loss: 0.4116603973026856 Test-Loss: 0.13871852844545618 Train-f1: 0.48347792558318875 Test-f1: 0.5165079365079366 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12425/20000] Train-Loss: 0.4094485665991819 Test-Loss: 0.13828989007472653 Train-f1: 0.49236431784202483 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12426/20000] Train-Loss: 0.41116888748208863 Test-Loss: 0.14133014451600723 Train-f1: 0.5245986707470083 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12427/20000] Train-Loss: 0.4078118466869383 Test-Loss: 0.14288669611889107 Train-f1: 0.4843100239121939 Test-f1: 0.4661751152073733 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12428/20000] Train-Loss: 0.4082922691103365 Test-Loss: 0.14691254713917104 Train-f1: 0.4878638904954694 Test-f1: 0.4695652173913043 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12429/20000] Train-Loss: 0.411711291291497 Test-Loss: 0.1379305637286531 Train-f1: 0.48771642819185573 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12430/20000] Train-Loss: 0.40545437200922885 Test-Loss: 0.14333978037962827 Train-f1: 0.48901098901098905 Test-f1: 0.5028754578754578 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12431/20000] Train-Loss: 0.4104509082797271 Test-Loss: 0.13805364744166884 Train-f1: 0.5573408573408574 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12432/20000] Train-Loss: 0.40578478395359074 Test-Loss: 0.1410886671234652 Train-f1: 0.506854782046598 Test-f1: 0.5051919146036793 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12433/20000] Train-Loss: 0.4037142841711306 Test-Loss: 0.15589597215531734 Train-f1: 0.4918734793187348 Test-f1: 0.4695652173913043 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12434/20000] Train-Loss: 0.40822842080952243 Test-Loss: 0.13769617142696722 Train-f1: 0.4881276143211627 Test-f1: 0.5076190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12435/20000] Train-Loss: 0.40944682399442844 Test-Loss: 0.14096819518637071 Train-f1: 0.4897487466443229 Test-f1: 0.4561490683229813 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12436/20000] Train-Loss: 0.4076034827408049 Test-Loss: 0.1365114262834669 Train-f1: 0.5152691986775985 Test-f1: 0.4503312334891282 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12437/20000] Train-Loss: 0.4063026794933843 Test-Loss: 0.13994805869363322 Train-f1: 0.487064935064935 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12438/20000] Train-Loss: 0.4099236964525818 Test-Loss: 0.13875847754802093 Train-f1: 0.48684909861380443 Test-f1: 0.5155103444886726 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12439/20000] Train-Loss: 0.405291748342832 Test-Loss: 0.1365358161623166 Train-f1: 0.491042291042291 Test-f1: 0.45393483709273186 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12440/20000] Train-Loss: 0.40951201138485344 Test-Loss: 0.1395591845524694 Train-f1: 0.49117299622694743 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12441/20000] Train-Loss: 0.4046937051725883 Test-Loss: 0.1419259544007919 Train-f1: 0.4962024836477391 Test-f1: 0.5051919146036793 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12442/20000] Train-Loss: 0.41026798225636796 Test-Loss: 0.1408668415003416 Train-f1: 0.4850705443082527 Test-f1: 0.5037301587301587 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12443/20000] Train-Loss: 0.40877993165267923 Test-Loss: 0.14545973948273033 Train-f1: 0.485966096406299 Test-f1: 0.5133179723502305 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12444/20000] Train-Loss: 0.4099351336788102 Test-Loss: 0.13710662985892536 Train-f1: 0.5480113224821347 Test-f1: 0.4503312334891282 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [12445/20000] Train-Loss: 0.40874771896360856 Test-Loss: 0.13681620163956645 Train-f1: 0.5622457736910291 Test-f1: 0.4573487773487773 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12446/20000] Train-Loss: 0.4102165853642462 Test-Loss: 0.13842884535083366 Train-f1: 0.5261844470726105 Test-f1: 0.4568421052631579 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12447/20000] Train-Loss: 0.40548894975415434 Test-Loss: 0.14401640550719472 Train-f1: 0.5027893143625513 Test-f1: 0.5134352946414658 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12448/20000] Train-Loss: 0.41004666741023577 Test-Loss: 0.14232158366028425 Train-f1: 0.48505096849622403 Test-f1: 0.4661751152073733 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12449/20000] Train-Loss: 0.40729853033452434 Test-Loss: 0.13733383984832556 Train-f1: 0.4906454091432962 Test-f1: 0.46815958815958825 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12450/20000] Train-Loss: 0.40612675207347293 Test-Loss: 0.13843432956512375 Train-f1: 0.48477277561148524 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12451/20000] Train-Loss: 0.41234215230411686 Test-Loss: 0.14063181967931754 Train-f1: 0.4920463697376448 Test-f1: 0.5165132139812446 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12452/20000] Train-Loss: 0.4081669749098001 Test-Loss: 0.14053124173741824 Train-f1: 0.511774318396835 Test-f1: 0.4526190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12453/20000] Train-Loss: 0.4085105930483981 Test-Loss: 0.13684350148234634 Train-f1: 0.49526739926739927 Test-f1: 0.4642044134727062 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12454/20000] Train-Loss: 0.4089739018854748 Test-Loss: 0.13748534036244153 Train-f1: 0.487031112347568 Test-f1: 0.49677440206851975 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12455/20000] Train-Loss: 0.4089131396007693 Test-Loss: 0.13776827198932012 Train-f1: 0.48496448363998035 Test-f1: 0.4568421052631579 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12456/20000] Train-Loss: 0.4053210959808077 Test-Loss: 0.14454121120139318 Train-f1: 0.4938123870738426 Test-f1: 0.5133179723502305 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12457/20000] Train-Loss: 0.4053091489241011 Test-Loss: 0.14256060638935614 Train-f1: 0.48345517866514004 Test-f1: 0.44649416182596957 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12458/20000] Train-Loss: 0.40524856900031175 Test-Loss: 0.13665667283200036 Train-f1: 0.49413919413919416 Test-f1: 0.4430769230769231 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [12459/20000] Train-Loss: 0.40521759657528517 Test-Loss: 0.1429387312252118 Train-f1: 0.5022453954220671 Test-f1: 0.5141038802093091 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12460/20000] Train-Loss: 0.40695092748958217 Test-Loss: 0.14067681194159137 Train-f1: 0.5102077966183927 Test-f1: 0.4558710801393729 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12461/20000] Train-Loss: 0.41041237985720325 Test-Loss: 0.1373817781793244 Train-f1: 0.5172933927386483 Test-f1: 0.45369747899159674 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12462/20000] Train-Loss: 0.4046792473336661 Test-Loss: 0.14372056775496347 Train-f1: 0.49807973822440366 Test-f1: 0.5412987012987013 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12463/20000] Train-Loss: 0.40658131423409655 Test-Loss: 0.14346143887994195 Train-f1: 0.48468545550508246 Test-f1: 0.5134352946414658 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12464/20000] Train-Loss: 0.4110724958933396 Test-Loss: 0.139276706596013 Train-f1: 0.5432258052160457 Test-f1: 0.5051919146036793 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [12465/20000] Train-Loss: 0.40446250223280916 Test-Loss: 0.14388928207934829 Train-f1: 0.4882029315012823 Test-f1: 0.5412987012987013 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12466/20000] Train-Loss: 0.40843160871477274 Test-Loss: 0.14098038835812937 Train-f1: 0.4976147912453645 Test-f1: 0.5037301587301587 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12467/20000] Train-Loss: 0.40603387324430595 Test-Loss: 0.13913560865610686 Train-f1: 0.508994608994609 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12468/20000] Train-Loss: 0.41142301793087555 Test-Loss: 0.14223879648371462 Train-f1: 0.5121990883502262 Test-f1: 0.46604809770217487 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12469/20000] Train-Loss: 0.4043212012549794 Test-Loss: 0.13773779246196335 Train-f1: 0.4904782695804367 Test-f1: 0.5076190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12470/20000] Train-Loss: 0.4069335653775504 Test-Loss: 0.13711241870287524 Train-f1: 0.4769448186222379 Test-f1: 0.5076190476190476 Train-acc: 0.8013245033112583 Test-acc: 0.7763157894736842\n",
      "Epoch [12471/20000] Train-Loss: 0.4090156161889422 Test-Loss: 0.1417111638126996 Train-f1: 0.4885458023400984 Test-f1: 0.5156532356532357 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12472/20000] Train-Loss: 0.407379527677484 Test-Loss: 0.14120416038854203 Train-f1: 0.5660509338732946 Test-f1: 0.5156532356532357 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12473/20000] Train-Loss: 0.4116253502788647 Test-Loss: 0.13842576012055655 Train-f1: 0.48726877644075095 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12474/20000] Train-Loss: 0.40686480250820806 Test-Loss: 0.14081208229435754 Train-f1: 0.5766703740897289 Test-f1: 0.5156532356532357 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12475/20000] Train-Loss: 0.40825415935226433 Test-Loss: 0.1389826079480832 Train-f1: 0.49473004879782084 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12476/20000] Train-Loss: 0.41018642203068634 Test-Loss: 0.14477173672449084 Train-f1: 0.48918119237084756 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12477/20000] Train-Loss: 0.4075545914872469 Test-Loss: 0.13884465376538205 Train-f1: 0.5608324661898862 Test-f1: 0.504984984984985 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12478/20000] Train-Loss: 0.4079762990592703 Test-Loss: 0.1419563004540436 Train-f1: 0.5163865412401523 Test-f1: 0.4594670406732118 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12479/20000] Train-Loss: 0.4109074138720237 Test-Loss: 0.14442770169512456 Train-f1: 0.5167342817934265 Test-f1: 0.4652641421224871 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12480/20000] Train-Loss: 0.4064112005295112 Test-Loss: 0.1445444932921166 Train-f1: 0.4897487466443229 Test-f1: 0.4652641421224871 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12481/20000] Train-Loss: 0.41033323642226477 Test-Loss: 0.1397611499616856 Train-f1: 0.49560382857596486 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12482/20000] Train-Loss: 0.4089986861101903 Test-Loss: 0.1377273445494305 Train-f1: 0.5047219641672196 Test-f1: 0.49677440206851975 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12483/20000] Train-Loss: 0.4058866414232747 Test-Loss: 0.13779167598097447 Train-f1: 0.4867490913497633 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12484/20000] Train-Loss: 0.40829522796569134 Test-Loss: 0.14217490205071898 Train-f1: 0.49725998994543835 Test-f1: 0.5141038802093091 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12485/20000] Train-Loss: 0.4107061373105783 Test-Loss: 0.14337267028533024 Train-f1: 0.555279156415687 Test-f1: 0.5141038802093091 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12486/20000] Train-Loss: 0.41018899718047047 Test-Loss: 0.1381934798849884 Train-f1: 0.556279133901999 Test-f1: 0.46727688787185356 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12487/20000] Train-Loss: 0.40505594729972916 Test-Loss: 0.1530715607252841 Train-f1: 0.4852046283293032 Test-f1: 0.4751156336725254 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [12488/20000] Train-Loss: 0.40598233157250163 Test-Loss: 0.1370807118654446 Train-f1: 0.49926739926739927 Test-f1: 0.4503312334891282 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12489/20000] Train-Loss: 0.40698825443756 Test-Loss: 0.1476215644185205 Train-f1: 0.4969084249084249 Test-f1: 0.5412987012987013 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12490/20000] Train-Loss: 0.4046633741935913 Test-Loss: 0.13757793723039624 Train-f1: 0.5048086433156838 Test-f1: 0.5066513056835638 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12491/20000] Train-Loss: 0.4104707807761432 Test-Loss: 0.1392599102701384 Train-f1: 0.5958930985757999 Test-f1: 0.5041022846972504 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12492/20000] Train-Loss: 0.41206432652791586 Test-Loss: 0.13814096100313064 Train-f1: 0.4919595008241112 Test-f1: 0.4568421052631579 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12493/20000] Train-Loss: 0.40918614894613353 Test-Loss: 0.13677518102004751 Train-f1: 0.5855273462170014 Test-f1: 0.4430769230769231 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [12494/20000] Train-Loss: 0.4082334963552127 Test-Loss: 0.1382327001338651 Train-f1: 0.4843100239121939 Test-f1: 0.4568421052631579 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12495/20000] Train-Loss: 0.41006477357076204 Test-Loss: 0.14146138332613822 Train-f1: 0.4795519269971825 Test-f1: 0.4661751152073733 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12496/20000] Train-Loss: 0.40735564503579963 Test-Loss: 0.13758555699377192 Train-f1: 0.489490451129255 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12497/20000] Train-Loss: 0.4055057039414094 Test-Loss: 0.14947552719548973 Train-f1: 0.4950201182413405 Test-f1: 0.4726570048309179 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12498/20000] Train-Loss: 0.41021737154569066 Test-Loss: 0.14158630042128603 Train-f1: 0.4918544533156795 Test-f1: 0.5421972621972622 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12499/20000] Train-Loss: 0.4107946854972787 Test-Loss: 0.13684009845164125 Train-f1: 0.5176456309500709 Test-f1: 0.49677440206851975 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12500/20000] Train-Loss: 0.4053839897812837 Test-Loss: 0.14404161768972704 Train-f1: 0.5023754411250445 Test-f1: 0.46679841897233204 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12501/20000] Train-Loss: 0.4071530898924366 Test-Loss: 0.13681303149780602 Train-f1: 0.4856885199150797 Test-f1: 0.4430769230769231 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [12502/20000] Train-Loss: 0.406751838166918 Test-Loss: 0.138612362079765 Train-f1: 0.48141045539948973 Test-f1: 0.504984984984985 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12503/20000] Train-Loss: 0.404090533637769 Test-Loss: 0.1451229187670833 Train-f1: 0.4881632497861149 Test-f1: 0.44649416182596957 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12504/20000] Train-Loss: 0.40851147511361663 Test-Loss: 0.14120973765972702 Train-f1: 0.48932208142734457 Test-f1: 0.4558710801393729 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12505/20000] Train-Loss: 0.4077082102106654 Test-Loss: 0.13788835843063313 Train-f1: 0.48272830921837545 Test-f1: 0.5165079365079366 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12506/20000] Train-Loss: 0.4087458208082615 Test-Loss: 0.13673079000618046 Train-f1: 0.5527408479898765 Test-f1: 0.4430769230769231 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [12507/20000] Train-Loss: 0.4060438945637973 Test-Loss: 0.14928865624244256 Train-f1: 0.49220312581624076 Test-f1: 0.4695652173913043 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12508/20000] Train-Loss: 0.4042720991773177 Test-Loss: 0.1437573515574764 Train-f1: 0.4871194373510706 Test-f1: 0.5412987012987013 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12509/20000] Train-Loss: 0.40898786204272525 Test-Loss: 0.1370101051594505 Train-f1: 0.571037894322076 Test-f1: 0.5077443609022556 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [12510/20000] Train-Loss: 0.40858193008549754 Test-Loss: 0.138187019865453 Train-f1: 0.4945202639180411 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12511/20000] Train-Loss: 0.40713156267744866 Test-Loss: 0.13775696301582985 Train-f1: 0.5712953712953712 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12512/20000] Train-Loss: 0.40375098175459745 Test-Loss: 0.13781802280417504 Train-f1: 0.4999036757426019 Test-f1: 0.5076190476190476 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12513/20000] Train-Loss: 0.40925590897303943 Test-Loss: 0.1429846423980948 Train-f1: 0.4890886890886891 Test-f1: 0.4661751152073733 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12514/20000] Train-Loss: 0.3983169474904664 Test-Loss: 0.13872155832491365 Train-f1: 0.5712481481051683 Test-f1: 0.5066513056835638 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [12515/20000] Train-Loss: 0.4070945814478212 Test-Loss: 0.13833951328592117 Train-f1: 0.5436114852889047 Test-f1: 0.4939348370927319 Train-acc: 0.804635761589404 Test-acc: 0.7631578947368421\n",
      "Epoch [12516/20000] Train-Loss: 0.4046790526575237 Test-Loss: 0.14558451134097083 Train-f1: 0.4992237502058541 Test-f1: 0.5149874686716792 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12517/20000] Train-Loss: 0.4062377978961495 Test-Loss: 0.14117509290316346 Train-f1: 0.49200454064760085 Test-f1: 0.4661751152073733 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12518/20000] Train-Loss: 0.40488047475227984 Test-Loss: 0.14088714330090746 Train-f1: 0.5653103653103653 Test-f1: 0.46066734074823057 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12519/20000] Train-Loss: 0.40785928597162086 Test-Loss: 0.13754951672106613 Train-f1: 0.4880147453395861 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12520/20000] Train-Loss: 0.40607256032329464 Test-Loss: 0.13991195255125452 Train-f1: 0.5787819687950636 Test-f1: 0.46761904761904766 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12521/20000] Train-Loss: 0.40692205493222344 Test-Loss: 0.138883138450293 Train-f1: 0.49322112765821374 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12522/20000] Train-Loss: 0.4076295120336515 Test-Loss: 0.14115978109166621 Train-f1: 0.5634891159101685 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12523/20000] Train-Loss: 0.4048450862161921 Test-Loss: 0.139249118845839 Train-f1: 0.4912294821137828 Test-f1: 0.504984984984985 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12524/20000] Train-Loss: 0.4036806613136528 Test-Loss: 0.14014946794790445 Train-f1: 0.49034067068122794 Test-f1: 0.5051919146036793 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12525/20000] Train-Loss: 0.40460658641125474 Test-Loss: 0.1407052652090498 Train-f1: 0.5137761040345126 Test-f1: 0.4558710801393729 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12526/20000] Train-Loss: 0.4097591804306728 Test-Loss: 0.1397413472807142 Train-f1: 0.48760704358228385 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12527/20000] Train-Loss: 0.4066586533670224 Test-Loss: 0.14175846578194407 Train-f1: 0.5147093135406342 Test-f1: 0.4594670406732118 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12528/20000] Train-Loss: 0.4092422621252302 Test-Loss: 0.14155341536502952 Train-f1: 0.5551115551115551 Test-f1: 0.5156532356532357 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12529/20000] Train-Loss: 0.40462845597492914 Test-Loss: 0.13720428884700195 Train-f1: 0.49454014598540147 Test-f1: 0.4573487773487773 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12530/20000] Train-Loss: 0.40738523235135676 Test-Loss: 0.1417530453501919 Train-f1: 0.5085458635382889 Test-f1: 0.45233400741489715 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12531/20000] Train-Loss: 0.4064034510640201 Test-Loss: 0.13673060816554847 Train-f1: 0.48465006255889415 Test-f1: 0.4430769230769231 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [12532/20000] Train-Loss: 0.4072621854955842 Test-Loss: 0.1444272023812343 Train-f1: 0.5557891383978341 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12533/20000] Train-Loss: 0.41206672085621326 Test-Loss: 0.14000645122763708 Train-f1: 0.4871366372134716 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12534/20000] Train-Loss: 0.40740789061363286 Test-Loss: 0.14049275791053933 Train-f1: 0.5628468176119182 Test-f1: 0.47043478260869565 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12535/20000] Train-Loss: 0.40686288037453117 Test-Loss: 0.14853229335666143 Train-f1: 0.5015335355031498 Test-f1: 0.5393814080656185 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12536/20000] Train-Loss: 0.4094689999698683 Test-Loss: 0.14118655412773432 Train-f1: 0.5118921106774497 Test-f1: 0.46604809770217487 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12537/20000] Train-Loss: 0.40945757523479903 Test-Loss: 0.1373945872344443 Train-f1: 0.4904782695804367 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12538/20000] Train-Loss: 0.40290847586339146 Test-Loss: 0.13927313733198543 Train-f1: 0.6081653728712553 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12539/20000] Train-Loss: 0.4063873500708488 Test-Loss: 0.13746944993088392 Train-f1: 0.6127460960092539 Test-f1: 0.4526190476190476 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12540/20000] Train-Loss: 0.41310847971626247 Test-Loss: 0.13689359359091804 Train-f1: 0.4953365475104605 Test-f1: 0.49677440206851975 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12541/20000] Train-Loss: 0.4054698370515128 Test-Loss: 0.14236300071565572 Train-f1: 0.5711806831317962 Test-f1: 0.5141038802093091 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [12542/20000] Train-Loss: 0.4015458929440576 Test-Loss: 0.15605820438590906 Train-f1: 0.5031891019380765 Test-f1: 0.4751156336725254 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12543/20000] Train-Loss: 0.41177534421645856 Test-Loss: 0.14253399644588946 Train-f1: 0.5580393290919606 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12544/20000] Train-Loss: 0.4069589344824709 Test-Loss: 0.13969692746254045 Train-f1: 0.5603756243756244 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12545/20000] Train-Loss: 0.40391335225450886 Test-Loss: 0.13714438700196457 Train-f1: 0.4934428378638905 Test-f1: 0.45000000000000007 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12546/20000] Train-Loss: 0.4042940041772057 Test-Loss: 0.14546449467138725 Train-f1: 0.49469843375015793 Test-f1: 0.47645625692137317 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12547/20000] Train-Loss: 0.4064055546682447 Test-Loss: 0.1394452978736387 Train-f1: 0.5558734793187348 Test-f1: 0.5155103444886726 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12548/20000] Train-Loss: 0.4072590528584989 Test-Loss: 0.13677435259005274 Train-f1: 0.48653854137725105 Test-f1: 0.45393483709273186 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12549/20000] Train-Loss: 0.40647096458619486 Test-Loss: 0.13689825147264784 Train-f1: 0.4909434935925002 Test-f1: 0.4573487773487773 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12550/20000] Train-Loss: 0.4038018588896141 Test-Loss: 0.15585727988486398 Train-f1: 0.4960969102088324 Test-f1: 0.475 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12551/20000] Train-Loss: 0.4076618202047371 Test-Loss: 0.13913071220468542 Train-f1: 0.5584129574690302 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12552/20000] Train-Loss: 0.4063192036472126 Test-Loss: 0.1431738522982779 Train-f1: 0.49545289766325384 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12553/20000] Train-Loss: 0.40800390991799973 Test-Loss: 0.13749208369743618 Train-f1: 0.485232311548101 Test-f1: 0.46815958815958825 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12554/20000] Train-Loss: 0.4051850843484804 Test-Loss: 0.14733852642750572 Train-f1: 0.4918734793187348 Test-f1: 0.47645625692137317 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12555/20000] Train-Loss: 0.4111704696512724 Test-Loss: 0.14071446606767038 Train-f1: 0.49195366037471305 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12556/20000] Train-Loss: 0.4049732432651778 Test-Loss: 0.14877798314184526 Train-f1: 0.48711417432285115 Test-f1: 0.5244444444444445 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12557/20000] Train-Loss: 0.4039298358596097 Test-Loss: 0.13709711800985888 Train-f1: 0.4897363766808033 Test-f1: 0.5076190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12558/20000] Train-Loss: 0.40694490430100844 Test-Loss: 0.13851796751999254 Train-f1: 0.4938438611987501 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12559/20000] Train-Loss: 0.4041981918111856 Test-Loss: 0.1460561458850562 Train-f1: 0.5169575457046074 Test-f1: 0.4751156336725254 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [12560/20000] Train-Loss: 0.4071716300528656 Test-Loss: 0.13953418842129298 Train-f1: 0.5608329031147823 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12561/20000] Train-Loss: 0.4082280144618713 Test-Loss: 0.1391429884835394 Train-f1: 0.49506702054472757 Test-f1: 0.46761904761904766 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12562/20000] Train-Loss: 0.40573624749446197 Test-Loss: 0.13862357752127433 Train-f1: 0.49999971597283566 Test-f1: 0.46728722258134026 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12563/20000] Train-Loss: 0.4059258522154439 Test-Loss: 0.14582167742337981 Train-f1: 0.4885804754912663 Test-f1: 0.549891398923657 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12564/20000] Train-Loss: 0.40688771914880434 Test-Loss: 0.14401774965869754 Train-f1: 0.5740454625866085 Test-f1: 0.4661751152073733 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12565/20000] Train-Loss: 0.40758438289718046 Test-Loss: 0.137016444495192 Train-f1: 0.5070223833507299 Test-f1: 0.5076190476190476 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12566/20000] Train-Loss: 0.40364891779307627 Test-Loss: 0.14458058601511534 Train-f1: 0.5434849561812092 Test-f1: 0.4792719919110212 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12567/20000] Train-Loss: 0.4017455345110853 Test-Loss: 0.14264540295122455 Train-f1: 0.5155764500400262 Test-f1: 0.4628528560711606 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12568/20000] Train-Loss: 0.4070800206526695 Test-Loss: 0.13660496147190732 Train-f1: 0.4884448884448885 Test-f1: 0.4395848595848596 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [12569/20000] Train-Loss: 0.4071993956042137 Test-Loss: 0.1392710768391883 Train-f1: 0.5030877400004916 Test-f1: 0.46761904761904766 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [12570/20000] Train-Loss: 0.4060348182219596 Test-Loss: 0.1395213074593104 Train-f1: 0.4791606683326428 Test-f1: 0.46761904761904766 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12571/20000] Train-Loss: 0.40352767311269383 Test-Loss: 0.145929116006538 Train-f1: 0.5111078742793093 Test-f1: 0.4652641421224871 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12572/20000] Train-Loss: 0.4025686500721879 Test-Loss: 0.1442370385576394 Train-f1: 0.49473004879782084 Test-f1: 0.5051190476190477 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12573/20000] Train-Loss: 0.40193783815358874 Test-Loss: 0.1384854796487557 Train-f1: 0.5174801341018602 Test-f1: 0.5061904761904762 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12574/20000] Train-Loss: 0.40420026888098565 Test-Loss: 0.14612318219955975 Train-f1: 0.4940914158305462 Test-f1: 0.5258893280632411 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12575/20000] Train-Loss: 0.40647267374834767 Test-Loss: 0.13710353592924046 Train-f1: 0.5609297369297369 Test-f1: 0.4573487773487773 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12576/20000] Train-Loss: 0.4040567854946975 Test-Loss: 0.13990698198275547 Train-f1: 0.48873622441677406 Test-f1: 0.47043478260869565 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12577/20000] Train-Loss: 0.40764861602836555 Test-Loss: 0.13910982915009948 Train-f1: 0.48918119237084756 Test-f1: 0.5156532356532357 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12578/20000] Train-Loss: 0.40404956306310347 Test-Loss: 0.14663620274023234 Train-f1: 0.4879714879714879 Test-f1: 0.5133179723502305 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12579/20000] Train-Loss: 0.4067274651248793 Test-Loss: 0.13987431956630825 Train-f1: 0.5035098429550985 Test-f1: 0.5156532356532357 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12580/20000] Train-Loss: 0.41042097080791545 Test-Loss: 0.13755422164145048 Train-f1: 0.49821820766093217 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12581/20000] Train-Loss: 0.405444253000771 Test-Loss: 0.14069108486630294 Train-f1: 0.5505598158132905 Test-f1: 0.47043478260869565 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12582/20000] Train-Loss: 0.4080829425045819 Test-Loss: 0.1411703439937575 Train-f1: 0.4886124897490204 Test-f1: 0.515048830342948 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12583/20000] Train-Loss: 0.40216092256839564 Test-Loss: 0.14944116558681042 Train-f1: 0.5693589025353731 Test-f1: 0.5237166085946574 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [12584/20000] Train-Loss: 0.39917585843998904 Test-Loss: 0.1384961259941461 Train-f1: 0.5853000922085754 Test-f1: 0.5076190476190476 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [12585/20000] Train-Loss: 0.40483723133239025 Test-Loss: 0.1383113812632816 Train-f1: 0.4951545099196106 Test-f1: 0.46815958815958825 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12586/20000] Train-Loss: 0.4076591271484058 Test-Loss: 0.1434198855796281 Train-f1: 0.48990584359711875 Test-f1: 0.5028754578754578 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12587/20000] Train-Loss: 0.4056424911080121 Test-Loss: 0.13686669409166238 Train-f1: 0.49034067068122794 Test-f1: 0.49677440206851975 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12588/20000] Train-Loss: 0.4092639115609656 Test-Loss: 0.13794033236514955 Train-f1: 0.4874394422781519 Test-f1: 0.4458926580665711 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [12589/20000] Train-Loss: 0.40591523042855054 Test-Loss: 0.1376061331143602 Train-f1: 0.5089688913178181 Test-f1: 0.45675057208237985 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12590/20000] Train-Loss: 0.4050582797507485 Test-Loss: 0.14082844144500856 Train-f1: 0.48173493173493176 Test-f1: 0.546221198156682 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12591/20000] Train-Loss: 0.4062720070150567 Test-Loss: 0.14526937029097467 Train-f1: 0.5131286326691626 Test-f1: 0.5141038802093091 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12592/20000] Train-Loss: 0.40821503597213316 Test-Loss: 0.13727236739465368 Train-f1: 0.48113752913752916 Test-f1: 0.5056027456027457 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12593/20000] Train-Loss: 0.4042070398908538 Test-Loss: 0.1426800195529014 Train-f1: 0.5569725469178024 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12594/20000] Train-Loss: 0.40882958043720374 Test-Loss: 0.13987673929991554 Train-f1: 0.550502505341215 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12595/20000] Train-Loss: 0.4082424211979818 Test-Loss: 0.1413050374881756 Train-f1: 0.5071606206594916 Test-f1: 0.4586868151290209 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12596/20000] Train-Loss: 0.40089683578751595 Test-Loss: 0.1388303156341178 Train-f1: 0.4876370592499624 Test-f1: 0.5076190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12597/20000] Train-Loss: 0.40810746790546415 Test-Loss: 0.13705021878786822 Train-f1: 0.5873539793539794 Test-f1: 0.4573487773487773 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [12598/20000] Train-Loss: 0.4070604244908046 Test-Loss: 0.13864750178105092 Train-f1: 0.4980335428245441 Test-f1: 0.504984984984985 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12599/20000] Train-Loss: 0.4031801340890184 Test-Loss: 0.13724668033965978 Train-f1: 0.5625752337046998 Test-f1: 0.4503312334891282 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12600/20000] Train-Loss: 0.40611620607146076 Test-Loss: 0.14210709113721323 Train-f1: 0.49402696069362734 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12601/20000] Train-Loss: 0.4026326900915353 Test-Loss: 0.14283384997464998 Train-f1: 0.6047811837285522 Test-f1: 0.4689908501970213 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12602/20000] Train-Loss: 0.4014401041539597 Test-Loss: 0.14804654016656016 Train-f1: 0.5167655903778471 Test-f1: 0.4751156336725254 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [12603/20000] Train-Loss: 0.40981999753212367 Test-Loss: 0.1438123625374278 Train-f1: 0.5659340659340659 Test-f1: 0.4554347826086957 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12604/20000] Train-Loss: 0.39797313339134 Test-Loss: 0.14105364810651375 Train-f1: 0.5621239687464853 Test-f1: 0.4558710801393729 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12605/20000] Train-Loss: 0.39816355428334615 Test-Loss: 0.15003247965996744 Train-f1: 0.5193736752433353 Test-f1: 0.4792719919110212 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12606/20000] Train-Loss: 0.4104817396158546 Test-Loss: 0.13763271654439913 Train-f1: 0.555053055053055 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12607/20000] Train-Loss: 0.4062740356733144 Test-Loss: 0.13775471490676477 Train-f1: 0.4850263984209936 Test-f1: 0.46815958815958825 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12608/20000] Train-Loss: 0.41035894695853126 Test-Loss: 0.13774902845646805 Train-f1: 0.4904879281792033 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12609/20000] Train-Loss: 0.4038950086083689 Test-Loss: 0.14274258182358443 Train-f1: 0.4931166079167523 Test-f1: 0.5519803072744249 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12610/20000] Train-Loss: 0.40546322103126436 Test-Loss: 0.1429146334346791 Train-f1: 0.615188959925802 Test-f1: 0.4630985798349032 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12611/20000] Train-Loss: 0.40561978797606274 Test-Loss: 0.14601339212679196 Train-f1: 0.5606901954166591 Test-f1: 0.5028754578754578 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12612/20000] Train-Loss: 0.4011577339889206 Test-Loss: 0.14890764703624992 Train-f1: 0.4878033644208141 Test-f1: 0.4751156336725254 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12613/20000] Train-Loss: 0.4085211925174806 Test-Loss: 0.14316857921803222 Train-f1: 0.48496448363998035 Test-f1: 0.5141038802093091 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12614/20000] Train-Loss: 0.408296963948937 Test-Loss: 0.13712795746484233 Train-f1: 0.5618817783523665 Test-f1: 0.46815958815958825 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12615/20000] Train-Loss: 0.4037707988753783 Test-Loss: 0.1418272709652829 Train-f1: 0.560078607523863 Test-f1: 0.46679841897233204 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12616/20000] Train-Loss: 0.40582036479103534 Test-Loss: 0.13998836291392652 Train-f1: 0.4824803335639249 Test-f1: 0.5051919146036793 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12617/20000] Train-Loss: 0.4050903345535734 Test-Loss: 0.1428193606608859 Train-f1: 0.4975365803575569 Test-f1: 0.5141038802093091 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12618/20000] Train-Loss: 0.4075731140219747 Test-Loss: 0.1458057984171005 Train-f1: 0.5223117418051386 Test-f1: 0.5134352946414658 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [12619/20000] Train-Loss: 0.4057756842809108 Test-Loss: 0.1416718772765813 Train-f1: 0.5095081439099212 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12620/20000] Train-Loss: 0.41037058566444123 Test-Loss: 0.14235972573568148 Train-f1: 0.6013872637798877 Test-f1: 0.5412987012987013 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12621/20000] Train-Loss: 0.40667652155701844 Test-Loss: 0.14651523026207885 Train-f1: 0.4961801509452516 Test-f1: 0.5133179723502305 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12622/20000] Train-Loss: 0.40822002160214155 Test-Loss: 0.147359813529078 Train-f1: 0.5177723905341017 Test-f1: 0.5156685910608086 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12623/20000] Train-Loss: 0.40495718801900105 Test-Loss: 0.14404198543837346 Train-f1: 0.5094113002956011 Test-f1: 0.5532874968169086 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12624/20000] Train-Loss: 0.40523762450481915 Test-Loss: 0.1371241721731038 Train-f1: 0.508994608994609 Test-f1: 0.49677440206851975 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12625/20000] Train-Loss: 0.40509865682284907 Test-Loss: 0.13671699438516247 Train-f1: 0.48852532858803377 Test-f1: 0.4537451737451737 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12626/20000] Train-Loss: 0.4057957046134246 Test-Loss: 0.13890046236583548 Train-f1: 0.5659649825867087 Test-f1: 0.4561490683229813 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12627/20000] Train-Loss: 0.4053209958147129 Test-Loss: 0.1428753103948153 Train-f1: 0.5111121972800567 Test-f1: 0.46679841897233204 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12628/20000] Train-Loss: 0.40755888454652317 Test-Loss: 0.1373877933038584 Train-f1: 0.4881276143211627 Test-f1: 0.4526190476190476 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12629/20000] Train-Loss: 0.40510875189647694 Test-Loss: 0.1436337859353112 Train-f1: 0.5571142766415924 Test-f1: 0.4554347826086957 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12630/20000] Train-Loss: 0.40527811901544575 Test-Loss: 0.14281591939454477 Train-f1: 0.5739629739629739 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12631/20000] Train-Loss: 0.4036056675528764 Test-Loss: 0.14051925180717617 Train-f1: 0.5595107595107595 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12632/20000] Train-Loss: 0.4050553002165186 Test-Loss: 0.13954679805598555 Train-f1: 0.5015704295704295 Test-f1: 0.5156532356532357 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12633/20000] Train-Loss: 0.39870075853964515 Test-Loss: 0.14825412475126623 Train-f1: 0.5243590912777087 Test-f1: 0.4726570048309179 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [12634/20000] Train-Loss: 0.4118392273950965 Test-Loss: 0.14447292132507647 Train-f1: 0.5551115551115551 Test-f1: 0.47645625692137317 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12635/20000] Train-Loss: 0.4061967609221946 Test-Loss: 0.13834566277505603 Train-f1: 0.4929722496285964 Test-f1: 0.5193236714975845 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12636/20000] Train-Loss: 0.4039645513844386 Test-Loss: 0.13920325694919078 Train-f1: 0.49295504976619525 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12637/20000] Train-Loss: 0.4047016819345954 Test-Loss: 0.13918302053898107 Train-f1: 0.493603218699109 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12638/20000] Train-Loss: 0.40555339982056154 Test-Loss: 0.14155414978867759 Train-f1: 0.5118570993023548 Test-f1: 0.4661751152073733 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12639/20000] Train-Loss: 0.4066816309722692 Test-Loss: 0.13749329662608237 Train-f1: 0.48360462838344953 Test-f1: 0.4944577129097253 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12640/20000] Train-Loss: 0.4048893627914736 Test-Loss: 0.14044139962448593 Train-f1: 0.5143619691270698 Test-f1: 0.5161904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12641/20000] Train-Loss: 0.4073953637951063 Test-Loss: 0.14292898571561527 Train-f1: 0.4881100881100881 Test-f1: 0.4661751152073733 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12642/20000] Train-Loss: 0.4086553981941638 Test-Loss: 0.13786688180136153 Train-f1: 0.5711383854240998 Test-f1: 0.47043478260869565 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12643/20000] Train-Loss: 0.40153644467191846 Test-Loss: 0.1397589719410158 Train-f1: 0.4873193806293451 Test-f1: 0.4939348370927319 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12644/20000] Train-Loss: 0.4064026790689851 Test-Loss: 0.14014248800618884 Train-f1: 0.49062863933452167 Test-f1: 0.47043478260869565 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12645/20000] Train-Loss: 0.4049972130321413 Test-Loss: 0.15304966520261368 Train-f1: 0.49321572595371793 Test-f1: 0.4751156336725254 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12646/20000] Train-Loss: 0.4034561464789575 Test-Loss: 0.14077578361521537 Train-f1: 0.5725998834722489 Test-f1: 0.5149874686716792 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12647/20000] Train-Loss: 0.4034662137472952 Test-Loss: 0.1387139543837106 Train-f1: 0.5065737514383618 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12648/20000] Train-Loss: 0.4013389080929384 Test-Loss: 0.13787766583802666 Train-f1: 0.50295225896521 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12649/20000] Train-Loss: 0.40416313951570615 Test-Loss: 0.14373368279584456 Train-f1: 0.5556656676656677 Test-f1: 0.4661751152073733 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12650/20000] Train-Loss: 0.40075887963761336 Test-Loss: 0.13681979585572115 Train-f1: 0.6041674926380809 Test-f1: 0.4430769230769231 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [12651/20000] Train-Loss: 0.4024736224977038 Test-Loss: 0.14218226487073665 Train-f1: 0.4999290348742904 Test-f1: 0.4661751152073733 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12652/20000] Train-Loss: 0.40090443620083027 Test-Loss: 0.14243556723087086 Train-f1: 0.5589281405245156 Test-f1: 0.5037301587301587 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12653/20000] Train-Loss: 0.40754306395075995 Test-Loss: 0.1448318795131924 Train-f1: 0.5774283659332241 Test-f1: 0.47645625692137317 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12654/20000] Train-Loss: 0.4021309741771716 Test-Loss: 0.13670047326714754 Train-f1: 0.48990584359711875 Test-f1: 0.4430769230769231 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [12655/20000] Train-Loss: 0.40914628526171204 Test-Loss: 0.1399847076976136 Train-f1: 0.4845660737380483 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12656/20000] Train-Loss: 0.40621389427464366 Test-Loss: 0.1377079349964959 Train-f1: 0.49527497111389734 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12657/20000] Train-Loss: 0.4030294856363833 Test-Loss: 0.14246146336226478 Train-f1: 0.5827431663047301 Test-f1: 0.4561490683229813 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12658/20000] Train-Loss: 0.4074477865861752 Test-Loss: 0.14030357901543095 Train-f1: 0.48496448363998035 Test-f1: 0.504984984984985 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12659/20000] Train-Loss: 0.40753532104494017 Test-Loss: 0.14035616973243933 Train-f1: 0.5763242428978403 Test-f1: 0.4558710801393729 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12660/20000] Train-Loss: 0.40168676541474113 Test-Loss: 0.1365907361261164 Train-f1: 0.49432264859955277 Test-f1: 0.4430769230769231 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [12661/20000] Train-Loss: 0.40497059169814087 Test-Loss: 0.14036353333316454 Train-f1: 0.49434432234432235 Test-f1: 0.5172768878718536 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12662/20000] Train-Loss: 0.40250145259370523 Test-Loss: 0.14710389366567084 Train-f1: 0.4921928948244737 Test-f1: 0.5532874968169086 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12663/20000] Train-Loss: 0.40011933874696687 Test-Loss: 0.13731999334874634 Train-f1: 0.50220963086294 Test-f1: 0.5066513056835638 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12664/20000] Train-Loss: 0.4066951905942558 Test-Loss: 0.1452788367775038 Train-f1: 0.4894398130605027 Test-f1: 0.5244444444444445 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [12665/20000] Train-Loss: 0.4061301853767057 Test-Loss: 0.14757271774031724 Train-f1: 0.5765805622948481 Test-f1: 0.4751156336725254 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12666/20000] Train-Loss: 0.4083840273568914 Test-Loss: 0.13741717518360358 Train-f1: 0.589269165739754 Test-f1: 0.4497330282227307 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12667/20000] Train-Loss: 0.39896639041728676 Test-Loss: 0.1466073447976514 Train-f1: 0.5513211836959397 Test-f1: 0.47307359307359304 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12668/20000] Train-Loss: 0.40708572376601226 Test-Loss: 0.13736683995544482 Train-f1: 0.4919595008241112 Test-f1: 0.4644611528822056 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12669/20000] Train-Loss: 0.40153614603151283 Test-Loss: 0.14032688050873063 Train-f1: 0.48649402655673174 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12670/20000] Train-Loss: 0.4043229078836089 Test-Loss: 0.14120506682323913 Train-f1: 0.482464902301906 Test-f1: 0.5170201484623542 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12671/20000] Train-Loss: 0.40279665799454817 Test-Loss: 0.14374006109140783 Train-f1: 0.5711383854240998 Test-f1: 0.5156685910608086 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12672/20000] Train-Loss: 0.4034642548579993 Test-Loss: 0.13952265693246446 Train-f1: 0.6072061213996698 Test-f1: 0.4670201484623542 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12673/20000] Train-Loss: 0.40698133012130683 Test-Loss: 0.14002209393846124 Train-f1: 0.49506702054472757 Test-f1: 0.46761904761904766 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12674/20000] Train-Loss: 0.40766461815028043 Test-Loss: 0.13678313471340245 Train-f1: 0.5198923826878306 Test-f1: 0.4430769230769231 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [12675/20000] Train-Loss: 0.4029358105854577 Test-Loss: 0.14126633903448155 Train-f1: 0.49222854340455785 Test-f1: 0.4628528560711606 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12676/20000] Train-Loss: 0.40173019335506943 Test-Loss: 0.14342007519980374 Train-f1: 0.4918734793187348 Test-f1: 0.46554019457245266 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12677/20000] Train-Loss: 0.40793796095437657 Test-Loss: 0.1426776371264637 Train-f1: 0.5577089577089577 Test-f1: 0.4661751152073733 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12678/20000] Train-Loss: 0.4042697875964575 Test-Loss: 0.13667557669490624 Train-f1: 0.4871138444386853 Test-f1: 0.4430769230769231 Train-acc: 0.8145695364238411 Test-acc: 0.75\n",
      "Epoch [12679/20000] Train-Loss: 0.3980096591626269 Test-Loss: 0.1385270269943949 Train-f1: 0.5017279491732046 Test-f1: 0.504984984984985 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12680/20000] Train-Loss: 0.40076377154893356 Test-Loss: 0.1411318816471824 Train-f1: 0.49208969891896714 Test-f1: 0.5149874686716792 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12681/20000] Train-Loss: 0.4019852616396117 Test-Loss: 0.14541264874098375 Train-f1: 0.5653103653103653 Test-f1: 0.5244444444444445 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [12682/20000] Train-Loss: 0.4058085155152785 Test-Loss: 0.14061513317051041 Train-f1: 0.49960323416943775 Test-f1: 0.5044444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12683/20000] Train-Loss: 0.4047111503699886 Test-Loss: 0.13708428236961992 Train-f1: 0.560162253853529 Test-f1: 0.46815958815958825 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12684/20000] Train-Loss: 0.4050392538968285 Test-Loss: 0.14366972198287806 Train-f1: 0.4835061147973253 Test-f1: 0.4689908501970213 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12685/20000] Train-Loss: 0.4029163226515032 Test-Loss: 0.13799093379900068 Train-f1: 0.5313051344216229 Test-f1: 0.46815958815958825 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12686/20000] Train-Loss: 0.40690602552860516 Test-Loss: 0.13746218663415447 Train-f1: 0.48801383019592776 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12687/20000] Train-Loss: 0.40302928281030675 Test-Loss: 0.1367734862691474 Train-f1: 0.5575816086922387 Test-f1: 0.4537451737451737 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12688/20000] Train-Loss: 0.40176340478739486 Test-Loss: 0.13717356044644527 Train-f1: 0.5701308323599344 Test-f1: 0.4573487773487773 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [12689/20000] Train-Loss: 0.40415305373167526 Test-Loss: 0.1368158784709098 Train-f1: 0.504046483620455 Test-f1: 0.4573487773487773 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12690/20000] Train-Loss: 0.4067729266349763 Test-Loss: 0.13820100075637673 Train-f1: 0.49999971597283566 Test-f1: 0.4568421052631579 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12691/20000] Train-Loss: 0.40151990543699 Test-Loss: 0.13933404188376974 Train-f1: 0.5581731690808434 Test-f1: 0.5062301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12692/20000] Train-Loss: 0.4075184214335188 Test-Loss: 0.1379143684065855 Train-f1: 0.5331289016148172 Test-f1: 0.4568421052631579 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [12693/20000] Train-Loss: 0.40105791693491327 Test-Loss: 0.14242719183169153 Train-f1: 0.4916370645600271 Test-f1: 0.4359678460364959 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [12694/20000] Train-Loss: 0.4060507164172961 Test-Loss: 0.1381381351119573 Train-f1: 0.5580393290919606 Test-f1: 0.4568421052631579 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12695/20000] Train-Loss: 0.4034945066881772 Test-Loss: 0.1374082858726262 Train-f1: 0.4883919332565436 Test-f1: 0.4644611528822056 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12696/20000] Train-Loss: 0.40782272100718786 Test-Loss: 0.14119477343958672 Train-f1: 0.5563282818059888 Test-f1: 0.5044444444444445 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12697/20000] Train-Loss: 0.4035155290368845 Test-Loss: 0.1391898095343753 Train-f1: 0.4929792555445459 Test-f1: 0.47010295757098824 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12698/20000] Train-Loss: 0.40646677556039273 Test-Loss: 0.13706887712436544 Train-f1: 0.5064305064305065 Test-f1: 0.4537451737451737 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12699/20000] Train-Loss: 0.40165717231258247 Test-Loss: 0.13657605561744968 Train-f1: 0.5822654297106852 Test-f1: 0.4395848595848596 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [12700/20000] Train-Loss: 0.4064227930228832 Test-Loss: 0.14226255916660097 Train-f1: 0.48423723035798255 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12701/20000] Train-Loss: 0.404459684091537 Test-Loss: 0.14106415263481273 Train-f1: 0.485232311548101 Test-f1: 0.5161904761904762 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12702/20000] Train-Loss: 0.4040531754341675 Test-Loss: 0.13741035523865633 Train-f1: 0.6140411674943369 Test-f1: 0.5076190476190476 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [12703/20000] Train-Loss: 0.40146083326760595 Test-Loss: 0.14309152464071756 Train-f1: 0.4930945597612264 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12704/20000] Train-Loss: 0.40396322216987857 Test-Loss: 0.14884778352399886 Train-f1: 0.494017270168408 Test-f1: 0.5258893280632411 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12705/20000] Train-Loss: 0.405431138081253 Test-Loss: 0.14014068323575088 Train-f1: 0.550998548332114 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12706/20000] Train-Loss: 0.40308512556039916 Test-Loss: 0.13778059366727238 Train-f1: 0.5268675220905779 Test-f1: 0.4568421052631579 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12707/20000] Train-Loss: 0.4024324413950086 Test-Loss: 0.14163707344614979 Train-f1: 0.4903150377602932 Test-f1: 0.5044444444444445 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12708/20000] Train-Loss: 0.40488524799226355 Test-Loss: 0.13883165181858237 Train-f1: 0.48873622441677406 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12709/20000] Train-Loss: 0.3989498188001488 Test-Loss: 0.14658191994859274 Train-f1: 0.4979164453617009 Test-f1: 0.46679841897233204 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12710/20000] Train-Loss: 0.40519658593855945 Test-Loss: 0.14370615887878913 Train-f1: 0.49127728736560305 Test-f1: 0.5134352946414658 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12711/20000] Train-Loss: 0.4037748418115209 Test-Loss: 0.1372781926757485 Train-f1: 0.5507990947481394 Test-f1: 0.5056027456027457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12712/20000] Train-Loss: 0.40393221991902195 Test-Loss: 0.13813740405037137 Train-f1: 0.4955866287135636 Test-f1: 0.46727688787185356 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12713/20000] Train-Loss: 0.4002867452153934 Test-Loss: 0.14673885881320936 Train-f1: 0.5693583360467579 Test-f1: 0.5258893280632411 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [12714/20000] Train-Loss: 0.40284351516225697 Test-Loss: 0.14553242809567674 Train-f1: 0.49021089285989944 Test-f1: 0.5133179723502305 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12715/20000] Train-Loss: 0.4006041451416589 Test-Loss: 0.1394011476968632 Train-f1: 0.5075838607381054 Test-f1: 0.43909729127120434 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [12716/20000] Train-Loss: 0.3935720437814847 Test-Loss: 0.144434754827813 Train-f1: 0.5542529342453595 Test-f1: 0.5141038802093091 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12717/20000] Train-Loss: 0.4038589437629986 Test-Loss: 0.1381495821310852 Train-f1: 0.5121358277982319 Test-f1: 0.46727688787185356 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12718/20000] Train-Loss: 0.4013057003415308 Test-Loss: 0.13757025656115018 Train-f1: 0.4857782217782217 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12719/20000] Train-Loss: 0.40152861883988217 Test-Loss: 0.14429205324688624 Train-f1: 0.49510412189423636 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12720/20000] Train-Loss: 0.40329630944135664 Test-Loss: 0.14014069726853476 Train-f1: 0.567499253412566 Test-f1: 0.5161904761904762 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12721/20000] Train-Loss: 0.4055803322073493 Test-Loss: 0.14224613189196672 Train-f1: 0.49545730207981864 Test-f1: 0.5149874686716792 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12722/20000] Train-Loss: 0.4015177546587851 Test-Loss: 0.14201581760662854 Train-f1: 0.4957242276127725 Test-f1: 0.546221198156682 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12723/20000] Train-Loss: 0.3994824750749248 Test-Loss: 0.13765544356749493 Train-f1: 0.4991849083948698 Test-f1: 0.4602828448452552 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12724/20000] Train-Loss: 0.40354785950533917 Test-Loss: 0.1372584484900804 Train-f1: 0.5679901648900928 Test-f1: 0.5076190476190476 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [12725/20000] Train-Loss: 0.4039220128245602 Test-Loss: 0.1407514803297414 Train-f1: 0.49272966892833664 Test-f1: 0.5308287764866713 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12726/20000] Train-Loss: 0.4052272240662873 Test-Loss: 0.1376147574087386 Train-f1: 0.5614111901170724 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12727/20000] Train-Loss: 0.4019254764396354 Test-Loss: 0.1423625024298388 Train-f1: 0.5673871129954409 Test-f1: 0.5156532356532357 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12728/20000] Train-Loss: 0.40020649027086685 Test-Loss: 0.14865837465256654 Train-f1: 0.5333805715173163 Test-f1: 0.47645625692137317 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12729/20000] Train-Loss: 0.4025837902879236 Test-Loss: 0.13654254775443467 Train-f1: 0.5288568032409092 Test-f1: 0.45000000000000007 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12730/20000] Train-Loss: 0.4049934608482002 Test-Loss: 0.14777867974175168 Train-f1: 0.4949120885426618 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12731/20000] Train-Loss: 0.4039260973805501 Test-Loss: 0.1419130927788394 Train-f1: 0.4948154919221827 Test-f1: 0.46679841897233204 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12732/20000] Train-Loss: 0.40208526521404614 Test-Loss: 0.13914005998152904 Train-f1: 0.48540738540738537 Test-f1: 0.4568421052631579 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [12733/20000] Train-Loss: 0.39998900010415067 Test-Loss: 0.13876252836984562 Train-f1: 0.5173236926256014 Test-f1: 0.504984984984985 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12734/20000] Train-Loss: 0.40167743813613677 Test-Loss: 0.13968449974030245 Train-f1: 0.508994608994609 Test-f1: 0.46651321398124457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12735/20000] Train-Loss: 0.4020546830436067 Test-Loss: 0.13820112247520547 Train-f1: 0.4897510162984389 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12736/20000] Train-Loss: 0.4006828809205087 Test-Loss: 0.14529673153535402 Train-f1: 0.5140525538355556 Test-f1: 0.5516976716976717 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12737/20000] Train-Loss: 0.4046169427034329 Test-Loss: 0.14095309707399767 Train-f1: 0.5315642647238059 Test-f1: 0.4781271777003485 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12738/20000] Train-Loss: 0.4031865582449726 Test-Loss: 0.1417005772979977 Train-f1: 0.5011002343122453 Test-f1: 0.5141038802093091 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12739/20000] Train-Loss: 0.40359375021155813 Test-Loss: 0.13962059640726943 Train-f1: 0.4949819901667797 Test-f1: 0.45965784025280587 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12740/20000] Train-Loss: 0.3995751633181478 Test-Loss: 0.1401803864172812 Train-f1: 0.5173582406424224 Test-f1: 0.5051919146036793 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12741/20000] Train-Loss: 0.40316845849712885 Test-Loss: 0.14599437528426276 Train-f1: 0.500068225927177 Test-f1: 0.549891398923657 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [12742/20000] Train-Loss: 0.4079546468307845 Test-Loss: 0.13885782239782749 Train-f1: 0.4873534207727756 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12743/20000] Train-Loss: 0.4038643348229954 Test-Loss: 0.14098373917831603 Train-f1: 0.4852046283293032 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12744/20000] Train-Loss: 0.4048599531486882 Test-Loss: 0.13949870432109338 Train-f1: 0.5581424247160222 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12745/20000] Train-Loss: 0.40067246795978545 Test-Loss: 0.14680327564245402 Train-f1: 0.5949298186987138 Test-f1: 0.4652641421224871 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12746/20000] Train-Loss: 0.4013417230197159 Test-Loss: 0.14319468633661012 Train-f1: 0.492590407355508 Test-f1: 0.5149874686716792 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12747/20000] Train-Loss: 0.40448060328229685 Test-Loss: 0.1415771420541312 Train-f1: 0.4941832271983676 Test-f1: 0.47043478260869565 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12748/20000] Train-Loss: 0.4052483223580796 Test-Loss: 0.1417156444810123 Train-f1: 0.5034754750860302 Test-f1: 0.5141038802093091 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12749/20000] Train-Loss: 0.4061166953918956 Test-Loss: 0.14077170339528383 Train-f1: 0.559136612827888 Test-f1: 0.5044444444444445 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12750/20000] Train-Loss: 0.4036293222551108 Test-Loss: 0.13749278930952735 Train-f1: 0.4960795960795961 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12751/20000] Train-Loss: 0.3988963759677407 Test-Loss: 0.15243711009452424 Train-f1: 0.48711417432285115 Test-f1: 0.4751156336725254 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12752/20000] Train-Loss: 0.40272815462990963 Test-Loss: 0.13972477432873678 Train-f1: 0.48698088828796393 Test-f1: 0.46761904761904766 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12753/20000] Train-Loss: 0.40229068300416243 Test-Loss: 0.14265040083836486 Train-f1: 0.48801383019592776 Test-f1: 0.515048830342948 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12754/20000] Train-Loss: 0.40314252653054655 Test-Loss: 0.13792630429411692 Train-f1: 0.5223117418051386 Test-f1: 0.5165079365079366 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [12755/20000] Train-Loss: 0.40506831070344684 Test-Loss: 0.13951654618926884 Train-f1: 0.4828873669708285 Test-f1: 0.5051919146036793 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [12756/20000] Train-Loss: 0.39747190319264025 Test-Loss: 0.1379403743984376 Train-f1: 0.5659517310108757 Test-f1: 0.5066513056835638 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12757/20000] Train-Loss: 0.4027179935403276 Test-Loss: 0.14221353611410645 Train-f1: 0.48994693739219297 Test-f1: 0.46679841897233204 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12758/20000] Train-Loss: 0.3996597484078561 Test-Loss: 0.13716666854706078 Train-f1: 0.616184692500482 Test-f1: 0.4944577129097253 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [12759/20000] Train-Loss: 0.39646445735561203 Test-Loss: 0.15060833128752715 Train-f1: 0.5610067429413675 Test-f1: 0.5244444444444445 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12760/20000] Train-Loss: 0.4067419310719841 Test-Loss: 0.1392211857064707 Train-f1: 0.5721158841158841 Test-f1: 0.5155103444886726 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12761/20000] Train-Loss: 0.4026996230445688 Test-Loss: 0.14707569669633297 Train-f1: 0.48883550463519043 Test-f1: 0.5156685910608086 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12762/20000] Train-Loss: 0.40070146223346303 Test-Loss: 0.1373879406519783 Train-f1: 0.6047811837285522 Test-f1: 0.4642044134727062 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12763/20000] Train-Loss: 0.4061938447173505 Test-Loss: 0.1377545148774393 Train-f1: 0.4948154919221827 Test-f1: 0.4944577129097253 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12764/20000] Train-Loss: 0.40166072910928446 Test-Loss: 0.1480948036593114 Train-f1: 0.5091233817941893 Test-f1: 0.4792719919110212 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12765/20000] Train-Loss: 0.39898713130525093 Test-Loss: 0.14526281760741352 Train-f1: 0.5695100104412509 Test-f1: 0.4792719919110212 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12766/20000] Train-Loss: 0.40419535357531955 Test-Loss: 0.13788211192536676 Train-f1: 0.49668393683507467 Test-f1: 0.4568421052631579 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12767/20000] Train-Loss: 0.4090416742695229 Test-Loss: 0.14102913132843214 Train-f1: 0.48852532858803377 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12768/20000] Train-Loss: 0.4016831077279639 Test-Loss: 0.13879447025326452 Train-f1: 0.6317483758477548 Test-f1: 0.4561490683229813 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [12769/20000] Train-Loss: 0.4008623552173992 Test-Loss: 0.13974502573304834 Train-f1: 0.4861218453318067 Test-f1: 0.5156532356532357 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12770/20000] Train-Loss: 0.40096903519926314 Test-Loss: 0.13787192107435872 Train-f1: 0.5199380717105688 Test-f1: 0.5165079365079366 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12771/20000] Train-Loss: 0.40501420345451383 Test-Loss: 0.14212196048361153 Train-f1: 0.5628468176119182 Test-f1: 0.4774585921325052 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12772/20000] Train-Loss: 0.4021806786406702 Test-Loss: 0.1415998415261265 Train-f1: 0.48756188623738295 Test-f1: 0.5156532356532357 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12773/20000] Train-Loss: 0.4067698975054086 Test-Loss: 0.1379512529498273 Train-f1: 0.6041674926380809 Test-f1: 0.46815958815958825 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12774/20000] Train-Loss: 0.4022154040860381 Test-Loss: 0.13929676707070598 Train-f1: 0.4950201182413405 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12775/20000] Train-Loss: 0.39651887028682176 Test-Loss: 0.13793414807511206 Train-f1: 0.5301616034294856 Test-f1: 0.44650793650793663 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12776/20000] Train-Loss: 0.4058981397239856 Test-Loss: 0.1435750646294306 Train-f1: 0.5180646830154624 Test-f1: 0.4652641421224871 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12777/20000] Train-Loss: 0.40500168150122523 Test-Loss: 0.13777166959214018 Train-f1: 0.5568775595265661 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12778/20000] Train-Loss: 0.40361548200893455 Test-Loss: 0.14133373715786396 Train-f1: 0.4969716552220693 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12779/20000] Train-Loss: 0.402711805904675 Test-Loss: 0.13727510484598165 Train-f1: 0.5590675619257006 Test-f1: 0.4395848595848596 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [12780/20000] Train-Loss: 0.40118773822284853 Test-Loss: 0.1422076974059741 Train-f1: 0.493513974609865 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12781/20000] Train-Loss: 0.39937639978506445 Test-Loss: 0.14738175553640986 Train-f1: 0.4955392910655612 Test-f1: 0.5237166085946574 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12782/20000] Train-Loss: 0.4011720176585958 Test-Loss: 0.1388281618277185 Train-f1: 0.49300764549594095 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12783/20000] Train-Loss: 0.40527972464562395 Test-Loss: 0.14080083037304478 Train-f1: 0.5709086435151893 Test-f1: 0.5156532356532357 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [12784/20000] Train-Loss: 0.40208333735979096 Test-Loss: 0.13705083021874356 Train-f1: 0.5293771764497611 Test-f1: 0.4944577129097253 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12785/20000] Train-Loss: 0.40300825176144905 Test-Loss: 0.13856577106767495 Train-f1: 0.6156934644303066 Test-f1: 0.5185803851321092 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12786/20000] Train-Loss: 0.4023367638393699 Test-Loss: 0.1415496193606401 Train-f1: 0.5135603285603285 Test-f1: 0.5056027456027457 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12787/20000] Train-Loss: 0.40041187114766424 Test-Loss: 0.14118028848438058 Train-f1: 0.4917689041553361 Test-f1: 0.4628528560711606 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12788/20000] Train-Loss: 0.39935399370305324 Test-Loss: 0.14106633834721866 Train-f1: 0.4890510637265605 Test-f1: 0.4661751152073733 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [12789/20000] Train-Loss: 0.4045874956846021 Test-Loss: 0.13719820232326363 Train-f1: 0.5116874843582918 Test-f1: 0.46815958815958825 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12790/20000] Train-Loss: 0.4002087627115851 Test-Loss: 0.1506695014044808 Train-f1: 0.4971984337501579 Test-f1: 0.5237166085946574 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12791/20000] Train-Loss: 0.3982408201959908 Test-Loss: 0.13741127809691686 Train-f1: 0.5798983867300354 Test-f1: 0.4944577129097253 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12792/20000] Train-Loss: 0.40589827917973037 Test-Loss: 0.13910165107302366 Train-f1: 0.5036428262211416 Test-f1: 0.46728722258134026 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12793/20000] Train-Loss: 0.40017156673559556 Test-Loss: 0.14349156461405194 Train-f1: 0.4930945597612264 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12794/20000] Train-Loss: 0.40414729172203345 Test-Loss: 0.14497181246323249 Train-f1: 0.56082217570336 Test-f1: 0.47645625692137317 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12795/20000] Train-Loss: 0.39828048058509036 Test-Loss: 0.15230819130905845 Train-f1: 0.4892054466931629 Test-f1: 0.549891398923657 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [12796/20000] Train-Loss: 0.4046520745663084 Test-Loss: 0.13913735516367354 Train-f1: 0.49298170442695993 Test-f1: 0.5155103444886726 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12797/20000] Train-Loss: 0.3948270155207882 Test-Loss: 0.15121885766372564 Train-f1: 0.5313360580597595 Test-f1: 0.4751156336725254 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [12798/20000] Train-Loss: 0.40434148979092244 Test-Loss: 0.14189378919903847 Train-f1: 0.498973026973027 Test-f1: 0.4628528560711606 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12799/20000] Train-Loss: 0.4034616313601976 Test-Loss: 0.1366111182442467 Train-f1: 0.5556656676656677 Test-f1: 0.4395848595848596 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [12800/20000] Train-Loss: 0.3983980611546463 Test-Loss: 0.14017935678846039 Train-f1: 0.4904879281792033 Test-f1: 0.46761904761904766 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12801/20000] Train-Loss: 0.39703599974440096 Test-Loss: 0.14060910630072404 Train-f1: 0.49295504976619525 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12802/20000] Train-Loss: 0.4004339308581729 Test-Loss: 0.13814805024679322 Train-f1: 0.5240754428530966 Test-f1: 0.4565853658536586 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12803/20000] Train-Loss: 0.39738132795387116 Test-Loss: 0.14469569601680007 Train-f1: 0.4957473715233383 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [12804/20000] Train-Loss: 0.39967760606720554 Test-Loss: 0.1476071977782493 Train-f1: 0.5621127690353455 Test-f1: 0.5244444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12805/20000] Train-Loss: 0.40691579428585 Test-Loss: 0.13769397501567615 Train-f1: 0.5667690788911635 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12806/20000] Train-Loss: 0.39827641729149443 Test-Loss: 0.15324128225515396 Train-f1: 0.4954110651253508 Test-f1: 0.4726570048309179 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12807/20000] Train-Loss: 0.4030505869591054 Test-Loss: 0.14146805190664158 Train-f1: 0.5769786292139234 Test-f1: 0.5262301587301588 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12808/20000] Train-Loss: 0.3985477912308995 Test-Loss: 0.13750276441153886 Train-f1: 0.5269127530610906 Test-f1: 0.49677440206851975 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12809/20000] Train-Loss: 0.40352317341466226 Test-Loss: 0.1474409353429319 Train-f1: 0.5342855895004391 Test-f1: 0.46604809770217487 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12810/20000] Train-Loss: 0.3967712050966115 Test-Loss: 0.13956316411553682 Train-f1: 0.6065118617750198 Test-f1: 0.5155103444886726 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12811/20000] Train-Loss: 0.4012629620271483 Test-Loss: 0.13829119553565072 Train-f1: 0.5005147110953562 Test-f1: 0.5165079365079366 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12812/20000] Train-Loss: 0.4019515323633534 Test-Loss: 0.14212824869261753 Train-f1: 0.49298170442695993 Test-f1: 0.5412987012987013 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12813/20000] Train-Loss: 0.40053227555619 Test-Loss: 0.13921663004710938 Train-f1: 0.4955791070243626 Test-f1: 0.5155103444886726 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12814/20000] Train-Loss: 0.40109838184768953 Test-Loss: 0.14222365771861512 Train-f1: 0.4867490913497633 Test-f1: 0.4774585921325052 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12815/20000] Train-Loss: 0.40054653454102973 Test-Loss: 0.139027718398767 Train-f1: 0.49805470467722124 Test-f1: 0.5062301587301588 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12816/20000] Train-Loss: 0.40213257779520606 Test-Loss: 0.13823931519236382 Train-f1: 0.5087443342220412 Test-f1: 0.46727688787185356 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12817/20000] Train-Loss: 0.4012764769788965 Test-Loss: 0.13689246107708986 Train-f1: 0.5117191028149932 Test-f1: 0.4395848595848596 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [12818/20000] Train-Loss: 0.4025818226944955 Test-Loss: 0.13822928570500284 Train-f1: 0.4918631540735102 Test-f1: 0.4533333333333333 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12819/20000] Train-Loss: 0.4006577928822254 Test-Loss: 0.13837226345342188 Train-f1: 0.4896306172616313 Test-f1: 0.4944577129097253 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12820/20000] Train-Loss: 0.40126278479241934 Test-Loss: 0.13888618599680216 Train-f1: 0.4959096850861556 Test-f1: 0.47097532314923624 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12821/20000] Train-Loss: 0.40152428598845163 Test-Loss: 0.15551211844359494 Train-f1: 0.5063070268761664 Test-f1: 0.475 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [12822/20000] Train-Loss: 0.39896260197557687 Test-Loss: 0.13744379594302333 Train-f1: 0.4916021338840131 Test-f1: 0.4644611528822056 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12823/20000] Train-Loss: 0.40084266248960754 Test-Loss: 0.13927565796748143 Train-f1: 0.4963063603063603 Test-f1: 0.5155103444886726 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12824/20000] Train-Loss: 0.40208606060209373 Test-Loss: 0.14080478500742144 Train-f1: 0.49160694378085684 Test-f1: 0.4781271777003485 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12825/20000] Train-Loss: 0.40458717219777457 Test-Loss: 0.1370548882172215 Train-f1: 0.4983558065601409 Test-f1: 0.5056027456027457 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12826/20000] Train-Loss: 0.40327990090381904 Test-Loss: 0.14539682846167384 Train-f1: 0.4951545099196106 Test-f1: 0.5133179723502305 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12827/20000] Train-Loss: 0.3987775438208711 Test-Loss: 0.14261187335180892 Train-f1: 0.5721158841158841 Test-f1: 0.546221198156682 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12828/20000] Train-Loss: 0.41003673686149084 Test-Loss: 0.14094561589389668 Train-f1: 0.4802774562941404 Test-f1: 0.5156532356532357 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [12829/20000] Train-Loss: 0.4001895712995678 Test-Loss: 0.14607397866233937 Train-f1: 0.5249366498318597 Test-f1: 0.5134352946414658 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [12830/20000] Train-Loss: 0.40371601293858655 Test-Loss: 0.13973368816886253 Train-f1: 0.49255171508837137 Test-f1: 0.5161904761904762 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12831/20000] Train-Loss: 0.40108022088384454 Test-Loss: 0.13745260380981408 Train-f1: 0.4926496400948956 Test-f1: 0.5053968253968255 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12832/20000] Train-Loss: 0.40176216642159523 Test-Loss: 0.14222612275839291 Train-f1: 0.4951545099196106 Test-f1: 0.5149874686716792 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12833/20000] Train-Loss: 0.4001492392057487 Test-Loss: 0.13915634681194056 Train-f1: 0.5039931062770714 Test-f1: 0.46651321398124457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12834/20000] Train-Loss: 0.3986751674813439 Test-Loss: 0.1434554936593663 Train-f1: 0.49846753687813294 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12835/20000] Train-Loss: 0.40493422069002205 Test-Loss: 0.13697127787487567 Train-f1: 0.5071621361095044 Test-f1: 0.4430769230769231 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [12836/20000] Train-Loss: 0.4044007160856706 Test-Loss: 0.13876866164458374 Train-f1: 0.5121881153799313 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12837/20000] Train-Loss: 0.40328931145101365 Test-Loss: 0.1379570227046704 Train-f1: 0.4798487546165565 Test-f1: 0.46815958815958825 Train-acc: 0.804635761589404 Test-acc: 0.7763157894736842\n",
      "Epoch [12838/20000] Train-Loss: 0.40512961843300554 Test-Loss: 0.13797987340968956 Train-f1: 0.48797471843829454 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12839/20000] Train-Loss: 0.3952669530384351 Test-Loss: 0.13957403752377945 Train-f1: 0.5224003369698733 Test-f1: 0.5066513056835638 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [12840/20000] Train-Loss: 0.39786900280420784 Test-Loss: 0.13780733493248543 Train-f1: 0.4954461023686788 Test-f1: 0.4944577129097253 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12841/20000] Train-Loss: 0.40169515359609026 Test-Loss: 0.14687317419794182 Train-f1: 0.4901061089448186 Test-f1: 0.5237166085946574 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [12842/20000] Train-Loss: 0.40259292429659443 Test-Loss: 0.1369956289221021 Train-f1: 0.5775640694303775 Test-f1: 0.4944577129097253 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12843/20000] Train-Loss: 0.3970744527904944 Test-Loss: 0.15021703325406233 Train-f1: 0.550891078478178 Test-f1: 0.5272601794340925 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12844/20000] Train-Loss: 0.4038176961205407 Test-Loss: 0.13729082987423466 Train-f1: 0.5490203780526361 Test-f1: 0.4944577129097253 Train-acc: 0.8079470198675497 Test-acc: 0.7631578947368421\n",
      "Epoch [12845/20000] Train-Loss: 0.40380860129869195 Test-Loss: 0.13902290504765288 Train-f1: 0.4867490913497633 Test-f1: 0.504984984984985 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12846/20000] Train-Loss: 0.40234600712686647 Test-Loss: 0.14169863852928613 Train-f1: 0.5108761824009834 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12847/20000] Train-Loss: 0.400758102948004 Test-Loss: 0.13760363333264308 Train-f1: 0.6072330191078048 Test-f1: 0.4395848595848596 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [12848/20000] Train-Loss: 0.4022721870434464 Test-Loss: 0.141322236621319 Train-f1: 0.5626076626076626 Test-f1: 0.4661751152073733 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12849/20000] Train-Loss: 0.39944055220644087 Test-Loss: 0.14046779094813128 Train-f1: 0.5660855614108439 Test-f1: 0.5421972621972622 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12850/20000] Train-Loss: 0.4006428325216931 Test-Loss: 0.13956554113304276 Train-f1: 0.5321995535578525 Test-f1: 0.46761904761904766 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12851/20000] Train-Loss: 0.4007352897500405 Test-Loss: 0.1396967899051601 Train-f1: 0.5266809678386923 Test-f1: 0.4642044134727062 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12852/20000] Train-Loss: 0.4009875731836708 Test-Loss: 0.1395885166497633 Train-f1: 0.49298170442695993 Test-f1: 0.46761904761904766 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12853/20000] Train-Loss: 0.3994348746944484 Test-Loss: 0.1401088472411854 Train-f1: 0.6031778376149237 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12854/20000] Train-Loss: 0.4010995972662432 Test-Loss: 0.14374241559517223 Train-f1: 0.4823418346626044 Test-f1: 0.5133179723502305 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12855/20000] Train-Loss: 0.40216843885618997 Test-Loss: 0.14167690787463508 Train-f1: 0.5826631186262629 Test-f1: 0.46679841897233204 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12856/20000] Train-Loss: 0.4020056089571666 Test-Loss: 0.14158052951342473 Train-f1: 0.5653103653103653 Test-f1: 0.4781271777003485 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12857/20000] Train-Loss: 0.39870443593852767 Test-Loss: 0.13672363027166873 Train-f1: 0.5006009650304952 Test-f1: 0.4430769230769231 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [12858/20000] Train-Loss: 0.3942346354716336 Test-Loss: 0.13929147863023958 Train-f1: 0.5557858532052081 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12859/20000] Train-Loss: 0.4027816523753997 Test-Loss: 0.14211076001682632 Train-f1: 0.5603419477872033 Test-f1: 0.5156532356532357 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12860/20000] Train-Loss: 0.40044591088631554 Test-Loss: 0.143152438081367 Train-f1: 0.5609628466771324 Test-f1: 0.5149874686716792 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12861/20000] Train-Loss: 0.4001359457077881 Test-Loss: 0.1384676442137323 Train-f1: 0.6072584256555832 Test-f1: 0.47097532314923624 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12862/20000] Train-Loss: 0.4035229915108628 Test-Loss: 0.13768453369172456 Train-f1: 0.490502228831785 Test-f1: 0.45000000000000007 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12863/20000] Train-Loss: 0.4055712861810992 Test-Loss: 0.13916612291368272 Train-f1: 0.6089074236725244 Test-f1: 0.4939348370927319 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12864/20000] Train-Loss: 0.4003589642457138 Test-Loss: 0.14000454331298712 Train-f1: 0.5132056349447653 Test-f1: 0.5156532356532357 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12865/20000] Train-Loss: 0.39545990436835415 Test-Loss: 0.14604818536847972 Train-f1: 0.49526739926739927 Test-f1: 0.5133179723502305 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12866/20000] Train-Loss: 0.3997840053216677 Test-Loss: 0.13914768410702535 Train-f1: 0.5143999618452174 Test-f1: 0.46761904761904766 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12867/20000] Train-Loss: 0.39807543645027504 Test-Loss: 0.13733015030122422 Train-f1: 0.49337492614911965 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12868/20000] Train-Loss: 0.39865487254570636 Test-Loss: 0.1396570754700705 Train-f1: 0.5321937988127246 Test-f1: 0.5422818042172881 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12869/20000] Train-Loss: 0.400209838491724 Test-Loss: 0.13786681320520747 Train-f1: 0.5189141525305161 Test-f1: 0.4939348370927319 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [12870/20000] Train-Loss: 0.400956984132678 Test-Loss: 0.14156754775877775 Train-f1: 0.589715046857904 Test-f1: 0.4995434517173648 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [12871/20000] Train-Loss: 0.4018024416512288 Test-Loss: 0.13897913921507465 Train-f1: 0.48392257168119235 Test-f1: 0.5155103444886726 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12872/20000] Train-Loss: 0.4018103611037627 Test-Loss: 0.1405897401379579 Train-f1: 0.4871138444386853 Test-f1: 0.5155103444886726 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12873/20000] Train-Loss: 0.40356058044588144 Test-Loss: 0.13910005316900015 Train-f1: 0.4901061089448186 Test-f1: 0.4636974789915966 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12874/20000] Train-Loss: 0.39881788562330683 Test-Loss: 0.14026632007727766 Train-f1: 0.4988691503144057 Test-f1: 0.5156532356532357 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12875/20000] Train-Loss: 0.3986836733670001 Test-Loss: 0.142031035927108 Train-f1: 0.5582417582417583 Test-f1: 0.46604809770217487 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12876/20000] Train-Loss: 0.4021216467027827 Test-Loss: 0.13801077277376572 Train-f1: 0.49805470467722124 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12877/20000] Train-Loss: 0.39750716997111946 Test-Loss: 0.1380607865943231 Train-f1: 0.5788720135984773 Test-f1: 0.4528157349896481 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12878/20000] Train-Loss: 0.3993839340288388 Test-Loss: 0.1393726886129456 Train-f1: 0.5586268950636926 Test-f1: 0.5308287764866713 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12879/20000] Train-Loss: 0.39398420157724773 Test-Loss: 0.14161908144665655 Train-f1: 0.580186700321154 Test-f1: 0.4561490683229813 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12880/20000] Train-Loss: 0.40005260079247984 Test-Loss: 0.14358571687057434 Train-f1: 0.5198974056559196 Test-f1: 0.4656685910608086 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [12881/20000] Train-Loss: 0.39833377839666817 Test-Loss: 0.1409950489985387 Train-f1: 0.5096393780365356 Test-f1: 0.5161904761904762 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12882/20000] Train-Loss: 0.3993715174289625 Test-Loss: 0.1368192192466634 Train-f1: 0.5589121247545378 Test-f1: 0.45369747899159674 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12883/20000] Train-Loss: 0.40051464323190034 Test-Loss: 0.1383370015718118 Train-f1: 0.4972191119842126 Test-f1: 0.5076190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12884/20000] Train-Loss: 0.4035585060500385 Test-Loss: 0.14033870690222147 Train-f1: 0.4919398130605027 Test-f1: 0.46761904761904766 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12885/20000] Train-Loss: 0.4012550694955444 Test-Loss: 0.14166861722898144 Train-f1: 0.548077995523251 Test-f1: 0.5262301587301588 Train-acc: 0.8079470198675497 Test-acc: 0.8026315789473685\n",
      "Epoch [12886/20000] Train-Loss: 0.4009826204391649 Test-Loss: 0.14126990077541862 Train-f1: 0.49681480614573614 Test-f1: 0.4670201484623542 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12887/20000] Train-Loss: 0.4005816852717671 Test-Loss: 0.1434460530583365 Train-f1: 0.5841335491926939 Test-f1: 0.4661751152073733 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [12888/20000] Train-Loss: 0.39731882646587435 Test-Loss: 0.137256296211203 Train-f1: 0.509652778205675 Test-f1: 0.4537451737451737 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12889/20000] Train-Loss: 0.4018693779296389 Test-Loss: 0.14342848832324437 Train-f1: 0.4917426928792235 Test-f1: 0.4661751152073733 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12890/20000] Train-Loss: 0.3974354936103943 Test-Loss: 0.14208475294253203 Train-f1: 0.5099397493668594 Test-f1: 0.46604809770217487 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12891/20000] Train-Loss: 0.4012284527932575 Test-Loss: 0.14071809780102623 Train-f1: 0.5657585657585658 Test-f1: 0.5155103444886726 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12892/20000] Train-Loss: 0.3982504404390734 Test-Loss: 0.1431364887359314 Train-f1: 0.560078607523863 Test-f1: 0.5133179723502305 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12893/20000] Train-Loss: 0.3988173457683128 Test-Loss: 0.1413013179647355 Train-f1: 0.5124448884448884 Test-f1: 0.5145474020154326 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12894/20000] Train-Loss: 0.4010682492999244 Test-Loss: 0.1372343432086615 Train-f1: 0.4955866287135636 Test-f1: 0.4573487773487773 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12895/20000] Train-Loss: 0.4005687131314439 Test-Loss: 0.13766659919258703 Train-f1: 0.4878532222092685 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12896/20000] Train-Loss: 0.4018273849845688 Test-Loss: 0.1380687129245429 Train-f1: 0.5654274627732085 Test-f1: 0.4939348370927319 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12897/20000] Train-Loss: 0.3996080220320287 Test-Loss: 0.1369407082181974 Train-f1: 0.49510412189423636 Test-f1: 0.45369747899159674 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12898/20000] Train-Loss: 0.39951477979733085 Test-Loss: 0.1410391953295839 Train-f1: 0.5575145049597604 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12899/20000] Train-Loss: 0.3980766968463424 Test-Loss: 0.13875854801001342 Train-f1: 0.5541816683752168 Test-f1: 0.46727688787185356 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12900/20000] Train-Loss: 0.39750398910803453 Test-Loss: 0.14301084479926943 Train-f1: 0.5863535392959766 Test-f1: 0.4661751152073733 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [12901/20000] Train-Loss: 0.39644186544500465 Test-Loss: 0.13985524541694802 Train-f1: 0.5622059577322279 Test-f1: 0.4642044134727062 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12902/20000] Train-Loss: 0.39245825337766166 Test-Loss: 0.14152260549534365 Train-f1: 0.49866666666666665 Test-f1: 0.5156532356532357 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12903/20000] Train-Loss: 0.40047078692867455 Test-Loss: 0.13955326216014546 Train-f1: 0.546642124083484 Test-f1: 0.5156532356532357 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12904/20000] Train-Loss: 0.3922278536698873 Test-Loss: 0.13998911389396201 Train-f1: 0.4938438611987501 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12905/20000] Train-Loss: 0.3991298204743072 Test-Loss: 0.1426025794979512 Train-f1: 0.4955791070243626 Test-f1: 0.46604809770217487 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12906/20000] Train-Loss: 0.39854522886234656 Test-Loss: 0.14387667372336896 Train-f1: 0.4941419343847421 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12907/20000] Train-Loss: 0.40377782498899384 Test-Loss: 0.14475114304432357 Train-f1: 0.49416579952321954 Test-f1: 0.5274585921325052 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12908/20000] Train-Loss: 0.39949267387953424 Test-Loss: 0.13949169614225657 Train-f1: 0.5580393290919606 Test-f1: 0.5161904761904762 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12909/20000] Train-Loss: 0.3973771149249409 Test-Loss: 0.14045225688463742 Train-f1: 0.49353581698107246 Test-f1: 0.5498412698412698 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12910/20000] Train-Loss: 0.3993023638080036 Test-Loss: 0.13986984228092877 Train-f1: 0.5345055231272492 Test-f1: 0.46651321398124457 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [12911/20000] Train-Loss: 0.3938515143290991 Test-Loss: 0.13681114263571933 Train-f1: 0.5038213227476194 Test-f1: 0.4428237259816208 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [12912/20000] Train-Loss: 0.4013534518440307 Test-Loss: 0.138596146975533 Train-f1: 0.5289430677622657 Test-f1: 0.46728722258134026 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12913/20000] Train-Loss: 0.3973667646099339 Test-Loss: 0.1371667715347431 Train-f1: 0.6021724765300924 Test-f1: 0.4644611528822056 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [12914/20000] Train-Loss: 0.40236770136740463 Test-Loss: 0.1373012514559236 Train-f1: 0.5586038122950875 Test-f1: 0.5056027456027457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12915/20000] Train-Loss: 0.4006550010653543 Test-Loss: 0.1397252159125772 Train-f1: 0.49981651458161525 Test-f1: 0.5155103444886726 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12916/20000] Train-Loss: 0.4019470749640209 Test-Loss: 0.13752818204570766 Train-f1: 0.555558789995876 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12917/20000] Train-Loss: 0.4016059491145622 Test-Loss: 0.14335165169620012 Train-f1: 0.49406726242169274 Test-f1: 0.5519803072744249 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12918/20000] Train-Loss: 0.4009338242249449 Test-Loss: 0.13850652627674376 Train-f1: 0.5601462994094574 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12919/20000] Train-Loss: 0.3991195949399432 Test-Loss: 0.1372686182828952 Train-f1: 0.49322112765821374 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12920/20000] Train-Loss: 0.40013673686275264 Test-Loss: 0.1378877333435694 Train-f1: 0.5141466695360543 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12921/20000] Train-Loss: 0.39094236750462313 Test-Loss: 0.14948748458770444 Train-f1: 0.5120130137346361 Test-f1: 0.5237166085946574 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12922/20000] Train-Loss: 0.40279742621971043 Test-Loss: 0.13777905974249033 Train-f1: 0.6060034446789414 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12923/20000] Train-Loss: 0.3983608127068537 Test-Loss: 0.1390322643317201 Train-f1: 0.5799149466233601 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12924/20000] Train-Loss: 0.40183023417973296 Test-Loss: 0.1371128511453683 Train-f1: 0.5118907758907759 Test-f1: 0.4573487773487773 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [12925/20000] Train-Loss: 0.40023237149356794 Test-Loss: 0.13897104367259897 Train-f1: 0.49594099594099594 Test-f1: 0.5155103444886726 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12926/20000] Train-Loss: 0.39914133169620564 Test-Loss: 0.13767737391442594 Train-f1: 0.49545289766325384 Test-f1: 0.5076190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12927/20000] Train-Loss: 0.3995738646017275 Test-Loss: 0.1416495581926813 Train-f1: 0.5526327630729657 Test-f1: 0.4661751152073733 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12928/20000] Train-Loss: 0.39787300120822694 Test-Loss: 0.14726742482860516 Train-f1: 0.5144965414530632 Test-f1: 0.4782608695652174 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12929/20000] Train-Loss: 0.40045560888187554 Test-Loss: 0.1408685077831536 Train-f1: 0.5146746860030849 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12930/20000] Train-Loss: 0.39793710527352877 Test-Loss: 0.13855966928379507 Train-f1: 0.49280829545730204 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12931/20000] Train-Loss: 0.4032785471525848 Test-Loss: 0.14433712010327376 Train-f1: 0.5002429323631106 Test-f1: 0.5412987012987013 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12932/20000] Train-Loss: 0.3904271859269138 Test-Loss: 0.14970334163351598 Train-f1: 0.5102715171007854 Test-f1: 0.5272601794340925 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12933/20000] Train-Loss: 0.3942288994061693 Test-Loss: 0.13862326873893993 Train-f1: 0.577700070349077 Test-f1: 0.5183710801393728 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12934/20000] Train-Loss: 0.4023378125066187 Test-Loss: 0.1405580882192616 Train-f1: 0.4947442666749981 Test-f1: 0.5156532356532357 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12935/20000] Train-Loss: 0.39401870169572856 Test-Loss: 0.1389645330487961 Train-f1: 0.48551586342469505 Test-f1: 0.5161904761904762 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12936/20000] Train-Loss: 0.397311940123539 Test-Loss: 0.137623487588484 Train-f1: 0.562973026973027 Test-f1: 0.4944577129097253 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12937/20000] Train-Loss: 0.4052497882771967 Test-Loss: 0.14104962365196724 Train-f1: 0.49200454064760085 Test-f1: 0.5262301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12938/20000] Train-Loss: 0.3969438615628978 Test-Loss: 0.14271461920038303 Train-f1: 0.49545730207981864 Test-f1: 0.5308287764866713 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12939/20000] Train-Loss: 0.4044759286506385 Test-Loss: 0.13794736708457733 Train-f1: 0.5262077966183927 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12940/20000] Train-Loss: 0.3986841737561458 Test-Loss: 0.14295276437180823 Train-f1: 0.4957473715233383 Test-f1: 0.5256318681318681 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12941/20000] Train-Loss: 0.39836917042060543 Test-Loss: 0.14098945241164096 Train-f1: 0.4949120885426618 Test-f1: 0.515048830342948 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12942/20000] Train-Loss: 0.3909687193397756 Test-Loss: 0.13924354637181321 Train-f1: 0.5154628832922448 Test-f1: 0.5051919146036793 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12943/20000] Train-Loss: 0.3983889796713803 Test-Loss: 0.13763009217172417 Train-f1: 0.493050968496224 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12944/20000] Train-Loss: 0.40226358495611864 Test-Loss: 0.1385086294706676 Train-f1: 0.5316879505617775 Test-f1: 0.46728722258134026 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12945/20000] Train-Loss: 0.3990922662325902 Test-Loss: 0.1399951882924511 Train-f1: 0.5767856304769057 Test-f1: 0.4642044134727062 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12946/20000] Train-Loss: 0.3994420279897844 Test-Loss: 0.14438255406057088 Train-f1: 0.48918119237084756 Test-f1: 0.5141038802093091 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12947/20000] Train-Loss: 0.3984237133580363 Test-Loss: 0.1388043551604136 Train-f1: 0.4881648700472116 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12948/20000] Train-Loss: 0.4015648077190271 Test-Loss: 0.14338751458763868 Train-f1: 0.5867707832726425 Test-f1: 0.4656685910608086 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [12949/20000] Train-Loss: 0.39767413800098056 Test-Loss: 0.13950524225488387 Train-f1: 0.4830690879446021 Test-f1: 0.5161904761904762 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [12950/20000] Train-Loss: 0.3985642901105756 Test-Loss: 0.14285665997647226 Train-f1: 0.49755176381096067 Test-f1: 0.4774585921325052 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12951/20000] Train-Loss: 0.39545575601998684 Test-Loss: 0.14601400552777036 Train-f1: 0.4976147912453645 Test-f1: 0.5244444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12952/20000] Train-Loss: 0.3954931144453833 Test-Loss: 0.13852566607301603 Train-f1: 0.6436922466926415 Test-f1: 0.4533333333333333 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12953/20000] Train-Loss: 0.3937777387927413 Test-Loss: 0.13742649377040697 Train-f1: 0.5529444109109087 Test-f1: 0.4642044134727062 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [12954/20000] Train-Loss: 0.39862911388416405 Test-Loss: 0.14093143481652493 Train-f1: 0.5073465490124957 Test-f1: 0.5156532356532357 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [12955/20000] Train-Loss: 0.3990744546123783 Test-Loss: 0.14911254272836 Train-f1: 0.49195366037471305 Test-f1: 0.5516976716976717 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12956/20000] Train-Loss: 0.39812833653343205 Test-Loss: 0.1469395763417049 Train-f1: 0.5631585253688816 Test-f1: 0.5519803072744249 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12957/20000] Train-Loss: 0.3955245416988807 Test-Loss: 0.14140692097561966 Train-f1: 0.5112034739071379 Test-f1: 0.5044444444444445 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12958/20000] Train-Loss: 0.39871023200347666 Test-Loss: 0.14778130016957863 Train-f1: 0.5668254466828616 Test-f1: 0.5237166085946574 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [12959/20000] Train-Loss: 0.39976173719457203 Test-Loss: 0.13880726130010998 Train-f1: 0.5802767235301265 Test-f1: 0.504984984984985 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [12960/20000] Train-Loss: 0.40034514176150754 Test-Loss: 0.13711840573937076 Train-f1: 0.5088564032767788 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12961/20000] Train-Loss: 0.39530800395354365 Test-Loss: 0.1491415052388008 Train-f1: 0.5636498462304914 Test-f1: 0.5244444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12962/20000] Train-Loss: 0.4028121193854987 Test-Loss: 0.13787237189242288 Train-f1: 0.4860695577501074 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [12963/20000] Train-Loss: 0.3942986352189847 Test-Loss: 0.1442420395800581 Train-f1: 0.5595970457649052 Test-f1: 0.4656685910608086 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12964/20000] Train-Loss: 0.39486783038705797 Test-Loss: 0.13868034043106503 Train-f1: 0.5153278398082042 Test-f1: 0.4944577129097253 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12965/20000] Train-Loss: 0.4007955923786193 Test-Loss: 0.1400960500722125 Train-f1: 0.5844298558584273 Test-f1: 0.5156532356532357 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [12966/20000] Train-Loss: 0.38798514199415285 Test-Loss: 0.1497000438590696 Train-f1: 0.5229722957146393 Test-f1: 0.5244444444444445 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [12967/20000] Train-Loss: 0.3990453966074608 Test-Loss: 0.14183936906732786 Train-f1: 0.5638857786508793 Test-f1: 0.5149874686716792 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12968/20000] Train-Loss: 0.3944058801481869 Test-Loss: 0.14594058213260608 Train-f1: 0.5995234699075758 Test-f1: 0.5256318681318681 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [12969/20000] Train-Loss: 0.39733858389297877 Test-Loss: 0.14004754293212016 Train-f1: 0.4985837721960289 Test-f1: 0.5155103444886726 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12970/20000] Train-Loss: 0.4025410054707904 Test-Loss: 0.1373112754150097 Train-f1: 0.611504826269927 Test-f1: 0.46815958815958825 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12971/20000] Train-Loss: 0.39938414003526357 Test-Loss: 0.14253211261017124 Train-f1: 0.5597769534075266 Test-f1: 0.5262301587301588 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12972/20000] Train-Loss: 0.39766518873045426 Test-Loss: 0.13707281278034 Train-f1: 0.5150893450700696 Test-f1: 0.49677440206851975 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12973/20000] Train-Loss: 0.393736230126582 Test-Loss: 0.13829687778321412 Train-f1: 0.5566867541061089 Test-f1: 0.4568421052631579 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [12974/20000] Train-Loss: 0.39917677226699827 Test-Loss: 0.1379175301574643 Train-f1: 0.4926867541061089 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [12975/20000] Train-Loss: 0.396154399145384 Test-Loss: 0.1407264945906511 Train-f1: 0.5572291827068897 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12976/20000] Train-Loss: 0.3971297651349557 Test-Loss: 0.14221914227768062 Train-f1: 0.5173582406424224 Test-f1: 0.5262301587301588 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [12977/20000] Train-Loss: 0.3919009214168556 Test-Loss: 0.13820300066529154 Train-f1: 0.6033367780122748 Test-f1: 0.5062301587301588 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [12978/20000] Train-Loss: 0.397924770633063 Test-Loss: 0.14032376954967793 Train-f1: 0.5259457798223448 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [12979/20000] Train-Loss: 0.4019128357300364 Test-Loss: 0.14185819825195495 Train-f1: 0.49510412189423636 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12980/20000] Train-Loss: 0.39904423715895393 Test-Loss: 0.13748815243495016 Train-f1: 0.5666810141262697 Test-f1: 0.4644611528822056 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [12981/20000] Train-Loss: 0.398194187701181 Test-Loss: 0.13849269925814128 Train-f1: 0.49236431784202483 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12982/20000] Train-Loss: 0.40318936669099814 Test-Loss: 0.13790809175712032 Train-f1: 0.4933768933768934 Test-f1: 0.4644611528822056 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [12983/20000] Train-Loss: 0.39508963661340446 Test-Loss: 0.14496862400474547 Train-f1: 0.5107825507825507 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12984/20000] Train-Loss: 0.39801539485585385 Test-Loss: 0.14524050741662403 Train-f1: 0.5773969298675181 Test-f1: 0.5516976716976717 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [12985/20000] Train-Loss: 0.3988701811782458 Test-Loss: 0.13903612655315 Train-f1: 0.6337011727768032 Test-f1: 0.5155103444886726 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [12986/20000] Train-Loss: 0.3984346565951842 Test-Loss: 0.14113071959171092 Train-f1: 0.5168456556778127 Test-f1: 0.4781271777003485 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [12987/20000] Train-Loss: 0.39349129120816323 Test-Loss: 0.14925870845849706 Train-f1: 0.5648975980571393 Test-f1: 0.5237166085946574 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [12988/20000] Train-Loss: 0.40024733831521137 Test-Loss: 0.1380735406395069 Train-f1: 0.5072730827001927 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [12989/20000] Train-Loss: 0.3962261259497975 Test-Loss: 0.13770305395336874 Train-f1: 0.4980138301959278 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [12990/20000] Train-Loss: 0.3990570761021864 Test-Loss: 0.14117778683201299 Train-f1: 0.48756188623738295 Test-f1: 0.5519803072744249 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [12991/20000] Train-Loss: 0.3966213264648811 Test-Loss: 0.1454963511413866 Train-f1: 0.5802051810175544 Test-f1: 0.4670201484623542 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12992/20000] Train-Loss: 0.39952200222523626 Test-Loss: 0.1425257282659827 Train-f1: 0.49083057160916044 Test-f1: 0.5516976716976717 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12993/20000] Train-Loss: 0.3963463221737001 Test-Loss: 0.1410968538479688 Train-f1: 0.49467890655369223 Test-f1: 0.5262301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [12994/20000] Train-Loss: 0.3969088639773441 Test-Loss: 0.140837585893961 Train-f1: 0.5538044040309638 Test-f1: 0.4740245261984392 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [12995/20000] Train-Loss: 0.39265980953937146 Test-Loss: 0.13849338292018448 Train-f1: 0.5679208209009533 Test-f1: 0.4528157349896481 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [12996/20000] Train-Loss: 0.3947911830493267 Test-Loss: 0.14100712093029413 Train-f1: 0.5798516860713657 Test-f1: 0.47285467739587883 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [12997/20000] Train-Loss: 0.3968084628187331 Test-Loss: 0.13783068978122046 Train-f1: 0.4988647147036408 Test-f1: 0.49677440206851975 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [12998/20000] Train-Loss: 0.4020575669705249 Test-Loss: 0.14358187648042833 Train-f1: 0.4924699461612213 Test-f1: 0.5149874686716792 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [12999/20000] Train-Loss: 0.39732799349807435 Test-Loss: 0.14515214395415293 Train-f1: 0.4854610080393235 Test-f1: 0.5141038802093091 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [13000/20000] Train-Loss: 0.39636805292572924 Test-Loss: 0.14223452140402904 Train-f1: 0.6356717322615941 Test-f1: 0.5141038802093091 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13001/20000] Train-Loss: 0.39919673698200636 Test-Loss: 0.13759552149344914 Train-f1: 0.5094788716892278 Test-f1: 0.4430769230769231 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [13002/20000] Train-Loss: 0.40013496330723397 Test-Loss: 0.13810155486267925 Train-f1: 0.5758816183816184 Test-f1: 0.4533333333333333 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13003/20000] Train-Loss: 0.4014648910711618 Test-Loss: 0.14250487306450332 Train-f1: 0.5003174939480672 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13004/20000] Train-Loss: 0.402057720331341 Test-Loss: 0.14163743273799373 Train-f1: 0.5581731690808434 Test-f1: 0.5519803072744249 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13005/20000] Train-Loss: 0.397041555703597 Test-Loss: 0.14465918809653128 Train-f1: 0.5958880645186377 Test-f1: 0.5133179723502305 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13006/20000] Train-Loss: 0.404895606186742 Test-Loss: 0.13710216118616492 Train-f1: 0.5445601961107147 Test-f1: 0.4944577129097253 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [13007/20000] Train-Loss: 0.3967471886199482 Test-Loss: 0.1401555303118372 Train-f1: 0.5111894636777592 Test-f1: 0.5155103444886726 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13008/20000] Train-Loss: 0.4011982640324736 Test-Loss: 0.1377468637657062 Train-f1: 0.5532052080439177 Test-f1: 0.4944577129097253 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [13009/20000] Train-Loss: 0.3974183193551272 Test-Loss: 0.14464796890461654 Train-f1: 0.6016054011348129 Test-f1: 0.4689908501970213 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13010/20000] Train-Loss: 0.39743066912697383 Test-Loss: 0.14729455321819093 Train-f1: 0.6095008259714142 Test-f1: 0.5156685910608086 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13011/20000] Train-Loss: 0.39788506171179505 Test-Loss: 0.13817318613450866 Train-f1: 0.5744609520838172 Test-f1: 0.4568421052631579 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13012/20000] Train-Loss: 0.39839067863248295 Test-Loss: 0.14254794236414076 Train-f1: 0.4979164453617009 Test-f1: 0.5141038802093091 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13013/20000] Train-Loss: 0.40005839501196977 Test-Loss: 0.13793728344814254 Train-f1: 0.517294980090428 Test-f1: 0.4644611528822056 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13014/20000] Train-Loss: 0.3947031366560385 Test-Loss: 0.1419309023389951 Train-f1: 0.4983799936242363 Test-f1: 0.5149874686716792 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13015/20000] Train-Loss: 0.3971254534663968 Test-Loss: 0.1398932210032787 Train-f1: 0.5606839368350747 Test-f1: 0.47010295757098824 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13016/20000] Train-Loss: 0.3990424550065157 Test-Loss: 0.1397658542201542 Train-f1: 0.4934955871868623 Test-f1: 0.5156532356532357 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13017/20000] Train-Loss: 0.3949081902731499 Test-Loss: 0.13693969394110242 Train-f1: 0.4955866287135636 Test-f1: 0.4430769230769231 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13018/20000] Train-Loss: 0.39742915161085995 Test-Loss: 0.1507240994787605 Train-f1: 0.5074886918466991 Test-f1: 0.5603463203463204 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13019/20000] Train-Loss: 0.4005066315308846 Test-Loss: 0.13792081550501747 Train-f1: 0.5445850952700267 Test-f1: 0.46815958815958825 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13020/20000] Train-Loss: 0.39297051894699997 Test-Loss: 0.1472201583795422 Train-f1: 0.5645057403446665 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13021/20000] Train-Loss: 0.4002147110047556 Test-Loss: 0.14159294090730443 Train-f1: 0.566840305041769 Test-f1: 0.5412987012987013 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13022/20000] Train-Loss: 0.39296021965439576 Test-Loss: 0.14274690038240964 Train-f1: 0.5075328263618393 Test-f1: 0.5607665764187504 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13023/20000] Train-Loss: 0.39907292825145063 Test-Loss: 0.1432299271238282 Train-f1: 0.49044760997492576 Test-f1: 0.5624077238550923 Train-acc: 0.8178807947019867 Test-acc: 0.8157894736842105\n",
      "Epoch [13024/20000] Train-Loss: 0.39418497341972836 Test-Loss: 0.14088448618152138 Train-f1: 0.5606709080393291 Test-f1: 0.5532874968169086 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13025/20000] Train-Loss: 0.39698944559706983 Test-Loss: 0.1409782278195911 Train-f1: 0.5115587115587116 Test-f1: 0.5156532356532357 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13026/20000] Train-Loss: 0.3980045339121072 Test-Loss: 0.13681828330490178 Train-f1: 0.5823905442762187 Test-f1: 0.4463157894736843 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13027/20000] Train-Loss: 0.39960120526985643 Test-Loss: 0.13970072397212038 Train-f1: 0.5573408573408574 Test-f1: 0.46761904761904766 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13028/20000] Train-Loss: 0.39656211624797777 Test-Loss: 0.14212239468706822 Train-f1: 0.4976147912453645 Test-f1: 0.5141038802093091 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13029/20000] Train-Loss: 0.4001505736995775 Test-Loss: 0.14030070054164256 Train-f1: 0.49280829545730204 Test-f1: 0.5161904761904762 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13030/20000] Train-Loss: 0.39573461871660776 Test-Loss: 0.13976328865508586 Train-f1: 0.5803724398176954 Test-f1: 0.46651321398124457 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [13031/20000] Train-Loss: 0.4001750518124353 Test-Loss: 0.13763282726978063 Train-f1: 0.562553777318878 Test-f1: 0.46815958815958825 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13032/20000] Train-Loss: 0.39600034038150317 Test-Loss: 0.1369823853277369 Train-f1: 0.49526739926739927 Test-f1: 0.4463157894736843 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13033/20000] Train-Loss: 0.40125005704556754 Test-Loss: 0.14400281801567566 Train-f1: 0.4837802197802198 Test-f1: 0.5519803072744249 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [13034/20000] Train-Loss: 0.3957941463884184 Test-Loss: 0.14090768188968622 Train-f1: 0.5795844183697573 Test-f1: 0.46651321398124457 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13035/20000] Train-Loss: 0.39866005996214327 Test-Loss: 0.14789265730102122 Train-f1: 0.4979340659340659 Test-f1: 0.5519803072744249 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13036/20000] Train-Loss: 0.3976867946193484 Test-Loss: 0.14296328828077987 Train-f1: 0.6048935788066223 Test-f1: 0.5141038802093091 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13037/20000] Train-Loss: 0.4018460642978767 Test-Loss: 0.13694528694742586 Train-f1: 0.494120881499144 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13038/20000] Train-Loss: 0.39942932582467283 Test-Loss: 0.13975716028539603 Train-f1: 0.5114654226574153 Test-f1: 0.5155103444886726 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13039/20000] Train-Loss: 0.40060279367190804 Test-Loss: 0.1464704401484524 Train-f1: 0.5938796585855408 Test-f1: 0.5156685910608086 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13040/20000] Train-Loss: 0.3981468683779786 Test-Loss: 0.14252255652531934 Train-f1: 0.6118834757534357 Test-f1: 0.5262301587301588 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13041/20000] Train-Loss: 0.3951414591186284 Test-Loss: 0.13829370614170722 Train-f1: 0.5149966243275543 Test-f1: 0.4644611528822056 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13042/20000] Train-Loss: 0.40247994765947565 Test-Loss: 0.13836006141936832 Train-f1: 0.5797152461427508 Test-f1: 0.4712087912087912 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13043/20000] Train-Loss: 0.39528327365962 Test-Loss: 0.15107587037187664 Train-f1: 0.5019220395444053 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13044/20000] Train-Loss: 0.4016468377817227 Test-Loss: 0.13688044342100508 Train-f1: 0.5597312071764626 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13045/20000] Train-Loss: 0.40013999300683933 Test-Loss: 0.14058566249928545 Train-f1: 0.5025822377935054 Test-f1: 0.5262301587301588 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [13046/20000] Train-Loss: 0.39350531324520666 Test-Loss: 0.14594015212836997 Train-f1: 0.4977483443708609 Test-f1: 0.5133179723502305 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13047/20000] Train-Loss: 0.39727379691578435 Test-Loss: 0.1375669390502763 Train-f1: 0.6337216867018192 Test-f1: 0.5165079365079366 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13048/20000] Train-Loss: 0.400152554824148 Test-Loss: 0.13853155186031638 Train-f1: 0.5934729749331883 Test-f1: 0.4636974789915966 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13049/20000] Train-Loss: 0.39728207479069694 Test-Loss: 0.14553800631846212 Train-f1: 0.5062268089830166 Test-f1: 0.5133179723502305 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13050/20000] Train-Loss: 0.3976745039177645 Test-Loss: 0.14590015184726815 Train-f1: 0.5764235764235764 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13051/20000] Train-Loss: 0.39665145373475513 Test-Loss: 0.14463088935943155 Train-f1: 0.610778582106981 Test-f1: 0.4656685910608086 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13052/20000] Train-Loss: 0.39175408826150193 Test-Loss: 0.1388815670383663 Train-f1: 0.510271040097333 Test-f1: 0.5062301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13053/20000] Train-Loss: 0.39791587510899873 Test-Loss: 0.1411652931782689 Train-f1: 0.5928258602711158 Test-f1: 0.47285467739587883 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13054/20000] Train-Loss: 0.39599658867122484 Test-Loss: 0.14123999222839614 Train-f1: 0.5023137568832933 Test-f1: 0.5421972621972622 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13055/20000] Train-Loss: 0.39374352027586057 Test-Loss: 0.1399027664697352 Train-f1: 0.5584878584037021 Test-f1: 0.46727688787185356 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13056/20000] Train-Loss: 0.3984578661156967 Test-Loss: 0.1385424445411245 Train-f1: 0.5578961487804495 Test-f1: 0.5051919146036793 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13057/20000] Train-Loss: 0.39256279742254874 Test-Loss: 0.14306985598682942 Train-f1: 0.53404486044902 Test-f1: 0.5170201484623542 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13058/20000] Train-Loss: 0.4001575258969805 Test-Loss: 0.13869213187890642 Train-f1: 0.5841595103265115 Test-f1: 0.46728722258134026 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13059/20000] Train-Loss: 0.3971282610230648 Test-Loss: 0.13663527324396532 Train-f1: 0.5619340659340659 Test-f1: 0.4463157894736843 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13060/20000] Train-Loss: 0.4037230529878983 Test-Loss: 0.14486103934011846 Train-f1: 0.4866858844369305 Test-f1: 0.5516976716976717 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [13061/20000] Train-Loss: 0.4016964524877681 Test-Loss: 0.13773916117888885 Train-f1: 0.4881276143211627 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [13062/20000] Train-Loss: 0.39634549872037184 Test-Loss: 0.14106702072054153 Train-f1: 0.573909758285635 Test-f1: 0.4781271777003485 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13063/20000] Train-Loss: 0.39058769048467273 Test-Loss: 0.14300955971828047 Train-f1: 0.6206708816297858 Test-f1: 0.4670201484623542 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13064/20000] Train-Loss: 0.39799308657436233 Test-Loss: 0.13786328678535337 Train-f1: 0.5148082492100265 Test-f1: 0.46815958815958825 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13065/20000] Train-Loss: 0.39814302218022923 Test-Loss: 0.13708843652447783 Train-f1: 0.56082217570336 Test-f1: 0.4944577129097253 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13066/20000] Train-Loss: 0.3993181263310795 Test-Loss: 0.15172778607869783 Train-f1: 0.5666922358687064 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13067/20000] Train-Loss: 0.40009660286298265 Test-Loss: 0.14133599439441816 Train-f1: 0.4796664703949472 Test-f1: 0.515048830342948 Train-acc: 0.804635761589404 Test-acc: 0.7894736842105263\n",
      "Epoch [13068/20000] Train-Loss: 0.39614309202521564 Test-Loss: 0.14640420957084066 Train-f1: 0.49236431784202483 Test-f1: 0.5237166085946574 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13069/20000] Train-Loss: 0.39097953233962557 Test-Loss: 0.13690577868382803 Train-f1: 0.5887205659794861 Test-f1: 0.4944577129097253 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [13070/20000] Train-Loss: 0.4000572350159575 Test-Loss: 0.1391673907324561 Train-f1: 0.5599502711422638 Test-f1: 0.4561490683229813 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13071/20000] Train-Loss: 0.398881634281359 Test-Loss: 0.14010560799952745 Train-f1: 0.48468013212538763 Test-f1: 0.5422818042172881 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [13072/20000] Train-Loss: 0.39698499505972706 Test-Loss: 0.14282699850486588 Train-f1: 0.5191328656347249 Test-f1: 0.5412987012987013 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13073/20000] Train-Loss: 0.39980468485739606 Test-Loss: 0.13890224261282827 Train-f1: 0.5746839368350747 Test-f1: 0.46728722258134026 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13074/20000] Train-Loss: 0.3938528445911128 Test-Loss: 0.1426684306174966 Train-f1: 0.5143619691270698 Test-f1: 0.4628528560711606 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13075/20000] Train-Loss: 0.39355490518907243 Test-Loss: 0.14437091716647735 Train-f1: 0.5089176519567784 Test-f1: 0.4689908501970213 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13076/20000] Train-Loss: 0.3983879784125211 Test-Loss: 0.1373965134860438 Train-f1: 0.5611523611523611 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13077/20000] Train-Loss: 0.3973036056183287 Test-Loss: 0.13979233527561835 Train-f1: 0.5794994664559882 Test-f1: 0.5066513056835638 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13078/20000] Train-Loss: 0.4010915668085434 Test-Loss: 0.13941112691447008 Train-f1: 0.5117813239657795 Test-f1: 0.5421972621972622 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13079/20000] Train-Loss: 0.3951875041240766 Test-Loss: 0.14486268992745543 Train-f1: 0.5115587115587116 Test-f1: 0.4656685910608086 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13080/20000] Train-Loss: 0.3964533142914205 Test-Loss: 0.1369885707824703 Train-f1: 0.5126675126675126 Test-f1: 0.4463157894736843 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13081/20000] Train-Loss: 0.4004533493916431 Test-Loss: 0.13895640201978332 Train-f1: 0.5920506026648723 Test-f1: 0.4636974789915966 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [13082/20000] Train-Loss: 0.3897689184078376 Test-Loss: 0.15064799850960542 Train-f1: 0.49866666666666665 Test-f1: 0.5603463203463204 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13083/20000] Train-Loss: 0.3962308004963109 Test-Loss: 0.1386012181331273 Train-f1: 0.5097069471091191 Test-f1: 0.4939348370927319 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13084/20000] Train-Loss: 0.39765995452964353 Test-Loss: 0.14242083775620978 Train-f1: 0.5504863597150547 Test-f1: 0.4656685910608086 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13085/20000] Train-Loss: 0.3957792018498105 Test-Loss: 0.14547898594760564 Train-f1: 0.5605105278654168 Test-f1: 0.5519803072744249 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13086/20000] Train-Loss: 0.39925431914450993 Test-Loss: 0.13857315850697352 Train-f1: 0.4951545099196106 Test-f1: 0.46728722258134026 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13087/20000] Train-Loss: 0.39568957272080235 Test-Loss: 0.1374515786469423 Train-f1: 0.5765463045987167 Test-f1: 0.4395848595848596 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13088/20000] Train-Loss: 0.3983274008470478 Test-Loss: 0.14335097707860367 Train-f1: 0.49771861248371313 Test-f1: 0.5256318681318681 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13089/20000] Train-Loss: 0.3953636132145404 Test-Loss: 0.13992248745448996 Train-f1: 0.5096367624356972 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13090/20000] Train-Loss: 0.39675323887331815 Test-Loss: 0.14987247630725822 Train-f1: 0.568742942676228 Test-f1: 0.46835592956210065 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13091/20000] Train-Loss: 0.3964362684052717 Test-Loss: 0.14238484592066616 Train-f1: 0.5168643248797062 Test-f1: 0.5262301587301588 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13092/20000] Train-Loss: 0.39921430584857515 Test-Loss: 0.13799840875890823 Train-f1: 0.5797315558826938 Test-f1: 0.46815958815958825 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13093/20000] Train-Loss: 0.3976225257277315 Test-Loss: 0.13758067804324403 Train-f1: 0.49583137517623416 Test-f1: 0.46815958815958825 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13094/20000] Train-Loss: 0.4029386949529282 Test-Loss: 0.1402083858876675 Train-f1: 0.5622283505737246 Test-f1: 0.46651321398124457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13095/20000] Train-Loss: 0.39663795359751897 Test-Loss: 0.1385307573391822 Train-f1: 0.4935995057839612 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13096/20000] Train-Loss: 0.3939177087375912 Test-Loss: 0.13871897823735563 Train-f1: 0.5766703740897289 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13097/20000] Train-Loss: 0.3942963108378232 Test-Loss: 0.14235345269092384 Train-f1: 0.5010290002467528 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13098/20000] Train-Loss: 0.399216342038392 Test-Loss: 0.15041731614646292 Train-f1: 0.5840722350543389 Test-f1: 0.47972222222222227 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13099/20000] Train-Loss: 0.39572591603970564 Test-Loss: 0.13954530659956374 Train-f1: 0.4931098485278052 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13100/20000] Train-Loss: 0.39521139217675466 Test-Loss: 0.13812656152635472 Train-f1: 0.5033011138971404 Test-f1: 0.504984984984985 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13101/20000] Train-Loss: 0.39999592086357527 Test-Loss: 0.13947436492345405 Train-f1: 0.5147764326971613 Test-f1: 0.46761904761904766 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13102/20000] Train-Loss: 0.3964371804534578 Test-Loss: 0.140152406573247 Train-f1: 0.5117343731537279 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13103/20000] Train-Loss: 0.39695002520197487 Test-Loss: 0.1458215706259967 Train-f1: 0.508994608994609 Test-f1: 0.5156685910608086 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13104/20000] Train-Loss: 0.39522744123235026 Test-Loss: 0.13970175937085766 Train-f1: 0.5652090438297335 Test-f1: 0.47285467739587883 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13105/20000] Train-Loss: 0.3963608686802923 Test-Loss: 0.13852800894980485 Train-f1: 0.5557349317349317 Test-f1: 0.5183710801393728 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13106/20000] Train-Loss: 0.39920706352565943 Test-Loss: 0.1407500686436998 Train-f1: 0.5495099495099496 Test-f1: 0.5262301587301588 Train-acc: 0.8112582781456954 Test-acc: 0.8026315789473685\n",
      "Epoch [13107/20000] Train-Loss: 0.397684151377878 Test-Loss: 0.13865202473905497 Train-f1: 0.49909160706590505 Test-f1: 0.46728722258134026 Train-acc: 0.8079470198675497 Test-acc: 0.7763157894736842\n",
      "Epoch [13108/20000] Train-Loss: 0.3955560712023593 Test-Loss: 0.14291845107264253 Train-f1: 0.5766703740897289 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13109/20000] Train-Loss: 0.39556966933798127 Test-Loss: 0.14167416828093793 Train-f1: 0.5799500343426582 Test-f1: 0.515048830342948 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13110/20000] Train-Loss: 0.39748248460900276 Test-Loss: 0.14069150925954615 Train-f1: 0.5604739616104923 Test-f1: 0.5149874686716792 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13111/20000] Train-Loss: 0.3895495679197187 Test-Loss: 0.1371284970777431 Train-f1: 0.5235700179626419 Test-f1: 0.45000000000000007 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13112/20000] Train-Loss: 0.3973982370795094 Test-Loss: 0.14223608785276323 Train-f1: 0.5609628466771324 Test-f1: 0.4809429126899965 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13113/20000] Train-Loss: 0.39877033574580845 Test-Loss: 0.14109108160664174 Train-f1: 0.5611082251082251 Test-f1: 0.5149874686716792 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13114/20000] Train-Loss: 0.3938681163943954 Test-Loss: 0.14067683992542043 Train-f1: 0.4949120885426618 Test-f1: 0.5262301587301588 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13115/20000] Train-Loss: 0.39660870133819454 Test-Loss: 0.14273990355289412 Train-f1: 0.5643997951082085 Test-f1: 0.4995434517173648 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13116/20000] Train-Loss: 0.391027813734597 Test-Loss: 0.15028708182650044 Train-f1: 0.5136427179388614 Test-f1: 0.5603463203463204 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13117/20000] Train-Loss: 0.4039390099931448 Test-Loss: 0.13955070874868125 Train-f1: 0.5725159569030993 Test-f1: 0.5161904761904762 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13118/20000] Train-Loss: 0.39254465940747707 Test-Loss: 0.1420950634716522 Train-f1: 0.5067105687846107 Test-f1: 0.5141038802093091 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13119/20000] Train-Loss: 0.3948124506350487 Test-Loss: 0.14388042021268818 Train-f1: 0.5265401459854016 Test-f1: 0.4689908501970213 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13120/20000] Train-Loss: 0.40210409465642044 Test-Loss: 0.13899063200495976 Train-f1: 0.490914942025572 Test-f1: 0.515048830342948 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13121/20000] Train-Loss: 0.3942468796998664 Test-Loss: 0.1441632175803838 Train-f1: 0.6284236734567861 Test-f1: 0.5165132139812446 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13122/20000] Train-Loss: 0.39856409733341447 Test-Loss: 0.14091971534537026 Train-f1: 0.5173582406424224 Test-f1: 0.5262301587301588 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13123/20000] Train-Loss: 0.39819957896084107 Test-Loss: 0.1398707359715865 Train-f1: 0.492859899482416 Test-f1: 0.5155103444886726 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13124/20000] Train-Loss: 0.3935005806767091 Test-Loss: 0.13685422489774862 Train-f1: 0.5827263791782572 Test-f1: 0.4944577129097253 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13125/20000] Train-Loss: 0.3970833339220594 Test-Loss: 0.1434251970919218 Train-f1: 0.570145733685096 Test-f1: 0.5133179723502305 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13126/20000] Train-Loss: 0.39545067438740183 Test-Loss: 0.14156919981822438 Train-f1: 0.6041547169872402 Test-f1: 0.5262301587301588 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13127/20000] Train-Loss: 0.3946556510674679 Test-Loss: 0.13893664564460637 Train-f1: 0.5195053164052443 Test-f1: 0.5155103444886726 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13128/20000] Train-Loss: 0.39441050345121353 Test-Loss: 0.13728751487991095 Train-f1: 0.6143112834877541 Test-f1: 0.4644611528822056 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13129/20000] Train-Loss: 0.3953623068419053 Test-Loss: 0.13992830951397178 Train-f1: 0.5140131933580523 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13130/20000] Train-Loss: 0.3974002874949531 Test-Loss: 0.13657426473977352 Train-f1: 0.5696461607406187 Test-f1: 0.4428237259816208 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [13131/20000] Train-Loss: 0.39574388801866506 Test-Loss: 0.1385418751838703 Train-f1: 0.573035760114923 Test-f1: 0.46728722258134026 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13132/20000] Train-Loss: 0.39850311308754377 Test-Loss: 0.14069423726443522 Train-f1: 0.4951545099196106 Test-f1: 0.5262301587301588 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13133/20000] Train-Loss: 0.39451169545070847 Test-Loss: 0.14195749380484401 Train-f1: 0.498973026973027 Test-f1: 0.5141038802093091 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13134/20000] Train-Loss: 0.38650334543054127 Test-Loss: 0.1374766141113458 Train-f1: 0.5603238104445627 Test-f1: 0.5183710801393728 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13135/20000] Train-Loss: 0.39496398738024246 Test-Loss: 0.13854609986394278 Train-f1: 0.4929533169533169 Test-f1: 0.4528157349896481 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13136/20000] Train-Loss: 0.39682652485590486 Test-Loss: 0.14391602024428438 Train-f1: 0.519200561302174 Test-f1: 0.5141038802093091 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13137/20000] Train-Loss: 0.39702103176812215 Test-Loss: 0.1370722602442676 Train-f1: 0.5099711854325653 Test-f1: 0.46815958815958825 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13138/20000] Train-Loss: 0.39484942042048343 Test-Loss: 0.14347619134444178 Train-f1: 0.5161128526645768 Test-f1: 0.47682367149758453 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13139/20000] Train-Loss: 0.3978989717440912 Test-Loss: 0.1430392141228933 Train-f1: 0.5197310966438483 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13140/20000] Train-Loss: 0.3924247860438794 Test-Loss: 0.1416678436402015 Train-f1: 0.584981637797344 Test-f1: 0.4661751152073733 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13141/20000] Train-Loss: 0.38891737437439183 Test-Loss: 0.14065597345591863 Train-f1: 0.5724389704255476 Test-f1: 0.5308287764866713 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13142/20000] Train-Loss: 0.39619247885752407 Test-Loss: 0.14046082299492768 Train-f1: 0.5284033938486494 Test-f1: 0.5149874686716792 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13143/20000] Train-Loss: 0.4023783944572449 Test-Loss: 0.13856873648670337 Train-f1: 0.6098473981927721 Test-f1: 0.504984984984985 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13144/20000] Train-Loss: 0.39336743376597083 Test-Loss: 0.13874943596013936 Train-f1: 0.6222161400243593 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13145/20000] Train-Loss: 0.3927188262887011 Test-Loss: 0.14126791357290513 Train-f1: 0.5063338272465788 Test-f1: 0.5516976716976717 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13146/20000] Train-Loss: 0.39733664748760017 Test-Loss: 0.13845250460154876 Train-f1: 0.6473981265304959 Test-f1: 0.504984984984985 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13147/20000] Train-Loss: 0.3894682533680664 Test-Loss: 0.13673276818676672 Train-f1: 0.5770306823843616 Test-f1: 0.49677440206851975 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13148/20000] Train-Loss: 0.39863915045134446 Test-Loss: 0.13788130863324685 Train-f1: 0.4896306172616313 Test-f1: 0.4939348370927319 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [13149/20000] Train-Loss: 0.3934347711389325 Test-Loss: 0.14474663691564996 Train-f1: 0.6437486427747672 Test-f1: 0.46835592956210065 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13150/20000] Train-Loss: 0.3989803420692633 Test-Loss: 0.1373342389699927 Train-f1: 0.4932962754679946 Test-f1: 0.46815958815958825 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13151/20000] Train-Loss: 0.39860742353355305 Test-Loss: 0.14097709219906912 Train-f1: 0.5556656676656677 Test-f1: 0.5149874686716792 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13152/20000] Train-Loss: 0.39309340768242834 Test-Loss: 0.1449188402756397 Train-f1: 0.5107825507825507 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13153/20000] Train-Loss: 0.38954655433397944 Test-Loss: 0.13753025457822077 Train-f1: 0.5714529474696316 Test-f1: 0.4939348370927319 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13154/20000] Train-Loss: 0.3942880736708813 Test-Loss: 0.13654662601771794 Train-f1: 0.49002008743944225 Test-f1: 0.4428237259816208 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [13155/20000] Train-Loss: 0.3979889701554318 Test-Loss: 0.13810509138950838 Train-f1: 0.4944025344025344 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13156/20000] Train-Loss: 0.39924344685387886 Test-Loss: 0.13894240630368257 Train-f1: 0.4844111417359825 Test-f1: 0.4939348370927319 Train-acc: 0.8112582781456954 Test-acc: 0.7631578947368421\n",
      "Epoch [13157/20000] Train-Loss: 0.39165278408788595 Test-Loss: 0.15513787116059685 Train-f1: 0.5166294386593671 Test-f1: 0.45818513813909173 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13158/20000] Train-Loss: 0.3950843653984209 Test-Loss: 0.14225375413609903 Train-f1: 0.5224156604022376 Test-f1: 0.5141038802093091 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13159/20000] Train-Loss: 0.39553160277958405 Test-Loss: 0.13758857673435032 Train-f1: 0.5650313124765679 Test-f1: 0.45000000000000007 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13160/20000] Train-Loss: 0.39384331164388886 Test-Loss: 0.14128434288811295 Train-f1: 0.560162253853529 Test-f1: 0.5308287764866713 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13161/20000] Train-Loss: 0.3893593495599662 Test-Loss: 0.1441767671659902 Train-f1: 0.650187081979477 Test-f1: 0.4656685910608086 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13162/20000] Train-Loss: 0.3927366731165019 Test-Loss: 0.13631947521701435 Train-f1: 0.6105773911446135 Test-f1: 0.4428237259816208 Train-acc: 0.847682119205298 Test-acc: 0.75\n",
      "Epoch [13163/20000] Train-Loss: 0.3943545522148224 Test-Loss: 0.14509553720494583 Train-f1: 0.4844111417359825 Test-f1: 0.5624077238550923 Train-acc: 0.8112582781456954 Test-acc: 0.8157894736842105\n",
      "Epoch [13164/20000] Train-Loss: 0.4003529273403322 Test-Loss: 0.14081061447687773 Train-f1: 0.5803490769715935 Test-f1: 0.5256318681318681 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13165/20000] Train-Loss: 0.3916223264496033 Test-Loss: 0.14701962479097996 Train-f1: 0.577093942936356 Test-f1: 0.475889328063241 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13166/20000] Train-Loss: 0.3938418139583154 Test-Loss: 0.13723076822816752 Train-f1: 0.49185020939946245 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13167/20000] Train-Loss: 0.39250116237965565 Test-Loss: 0.1380613246139251 Train-f1: 0.4960046929109714 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13168/20000] Train-Loss: 0.3944567209887997 Test-Loss: 0.14290474630939157 Train-f1: 0.49002008743944225 Test-f1: 0.5141038802093091 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13169/20000] Train-Loss: 0.40029816350575465 Test-Loss: 0.13832282812353056 Train-f1: 0.4960426348396629 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13170/20000] Train-Loss: 0.39422430061180763 Test-Loss: 0.13842730426511 Train-f1: 0.5816117396258745 Test-f1: 0.5161904761904762 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13171/20000] Train-Loss: 0.3950408437343021 Test-Loss: 0.14476420173108426 Train-f1: 0.4955866287135636 Test-f1: 0.5256318681318681 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13172/20000] Train-Loss: 0.39431618389669487 Test-Loss: 0.14130705012890882 Train-f1: 0.5095081439099212 Test-f1: 0.5149874686716792 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13173/20000] Train-Loss: 0.39261502603696 Test-Loss: 0.13994636270358532 Train-f1: 0.6045452661295425 Test-f1: 0.5262301587301588 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13174/20000] Train-Loss: 0.39168418502332336 Test-Loss: 0.13749888767652432 Train-f1: 0.5912828122239888 Test-f1: 0.46761904761904766 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13175/20000] Train-Loss: 0.3961690758740188 Test-Loss: 0.14371671898690902 Train-f1: 0.5578961487804495 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13176/20000] Train-Loss: 0.39301296972903815 Test-Loss: 0.13968511521354807 Train-f1: 0.5970157436382602 Test-f1: 0.515048830342948 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13177/20000] Train-Loss: 0.39286546412460643 Test-Loss: 0.1401113730799553 Train-f1: 0.5657550739594084 Test-f1: 0.5161904761904762 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13178/20000] Train-Loss: 0.3992916416917797 Test-Loss: 0.14000447696985208 Train-f1: 0.5092764541410645 Test-f1: 0.5161904761904762 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13179/20000] Train-Loss: 0.3854938971652536 Test-Loss: 0.13749606248700363 Train-f1: 0.5582077966183927 Test-f1: 0.5076190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13180/20000] Train-Loss: 0.39299905116737915 Test-Loss: 0.14046366728295165 Train-f1: 0.560162253853529 Test-f1: 0.4670201484623542 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13181/20000] Train-Loss: 0.39228729258229117 Test-Loss: 0.14001547782376123 Train-f1: 0.5554077501445922 Test-f1: 0.46727688787185356 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13182/20000] Train-Loss: 0.3949916759144441 Test-Loss: 0.1397061271400843 Train-f1: 0.5588222592917429 Test-f1: 0.5156532356532357 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13183/20000] Train-Loss: 0.39648720582929825 Test-Loss: 0.14024194628514494 Train-f1: 0.5946016806658619 Test-f1: 0.4774585921325052 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13184/20000] Train-Loss: 0.3978961919872934 Test-Loss: 0.13983874455394013 Train-f1: 0.56082217570336 Test-f1: 0.515048830342948 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13185/20000] Train-Loss: 0.3907739162180233 Test-Loss: 0.1507687490129001 Train-f1: 0.5691237211064797 Test-f1: 0.5603463203463204 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13186/20000] Train-Loss: 0.39986356435470066 Test-Loss: 0.14172589609325995 Train-f1: 0.5909639379113063 Test-f1: 0.5256318681318681 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13187/20000] Train-Loss: 0.3923669798965299 Test-Loss: 0.1371539294197343 Train-f1: 0.5745534404573307 Test-f1: 0.4428237259816208 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [13188/20000] Train-Loss: 0.39423437946197476 Test-Loss: 0.14558744537704785 Train-f1: 0.49200454064760085 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13189/20000] Train-Loss: 0.392948768336881 Test-Loss: 0.13909619936673118 Train-f1: 0.5111894636777592 Test-f1: 0.4642044134727062 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13190/20000] Train-Loss: 0.39348742296574324 Test-Loss: 0.1400655882438391 Train-f1: 0.563298315920042 Test-f1: 0.5161904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13191/20000] Train-Loss: 0.396510877903906 Test-Loss: 0.1390130206956616 Train-f1: 0.5744553691922112 Test-f1: 0.504984984984985 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13192/20000] Train-Loss: 0.39481879772354944 Test-Loss: 0.14160071011596886 Train-f1: 0.5622283505737246 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13193/20000] Train-Loss: 0.39907883606089767 Test-Loss: 0.13781521231278265 Train-f1: 0.49526739926739927 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13194/20000] Train-Loss: 0.3955062333008135 Test-Loss: 0.1374768201594593 Train-f1: 0.49908926129961745 Test-f1: 0.4944577129097253 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13195/20000] Train-Loss: 0.3954030693899002 Test-Loss: 0.13942438173538182 Train-f1: 0.5734917025357881 Test-f1: 0.46651321398124457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13196/20000] Train-Loss: 0.39291729732573316 Test-Loss: 0.14450411536731436 Train-f1: 0.4916021338840131 Test-f1: 0.5422818042172881 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13197/20000] Train-Loss: 0.396859622193451 Test-Loss: 0.14307418676890316 Train-f1: 0.49293037909823856 Test-f1: 0.4656685910608086 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13198/20000] Train-Loss: 0.39459792345041267 Test-Loss: 0.13760654763311028 Train-f1: 0.49322112765821374 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13199/20000] Train-Loss: 0.3978484046768488 Test-Loss: 0.14008395779170543 Train-f1: 0.5554026986886249 Test-f1: 0.4746428571428572 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13200/20000] Train-Loss: 0.39189685323105944 Test-Loss: 0.13709040292586774 Train-f1: 0.5800141120037463 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13201/20000] Train-Loss: 0.39426512047253776 Test-Loss: 0.1397249476046767 Train-f1: 0.5091866423461836 Test-f1: 0.5262301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13202/20000] Train-Loss: 0.39293679871935744 Test-Loss: 0.14709432433052697 Train-f1: 0.5605105278654168 Test-f1: 0.5244444444444445 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13203/20000] Train-Loss: 0.3936721540825589 Test-Loss: 0.14596472571333455 Train-f1: 0.5605105278654168 Test-f1: 0.46835592956210065 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13204/20000] Train-Loss: 0.394641102911037 Test-Loss: 0.1403138526780155 Train-f1: 0.5070844598103008 Test-f1: 0.5149874686716792 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13205/20000] Train-Loss: 0.39316169803901885 Test-Loss: 0.1439074850111035 Train-f1: 0.5167342817934265 Test-f1: 0.4656685910608086 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13206/20000] Train-Loss: 0.3922945797021552 Test-Loss: 0.13685581155948817 Train-f1: 0.505053055053055 Test-f1: 0.46815958815958825 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [13207/20000] Train-Loss: 0.39497497145678634 Test-Loss: 0.14175867742464265 Train-f1: 0.5312343308865047 Test-f1: 0.4656685910608086 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13208/20000] Train-Loss: 0.3934827624629314 Test-Loss: 0.13976965575875905 Train-f1: 0.49805470467722124 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13209/20000] Train-Loss: 0.39140579042161283 Test-Loss: 0.144809239546185 Train-f1: 0.48944450512658566 Test-f1: 0.4656685910608086 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13210/20000] Train-Loss: 0.3892198593302966 Test-Loss: 0.14260036981160445 Train-f1: 0.5961016517728848 Test-f1: 0.5141038802093091 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13211/20000] Train-Loss: 0.39297090710091087 Test-Loss: 0.13768621144048446 Train-f1: 0.5177217917992616 Test-f1: 0.46815958815958825 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13212/20000] Train-Loss: 0.39358164349454827 Test-Loss: 0.1398788946471058 Train-f1: 0.5820675968326975 Test-f1: 0.5262301587301588 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13213/20000] Train-Loss: 0.3949565825701228 Test-Loss: 0.1388319367346837 Train-f1: 0.5585506072874493 Test-f1: 0.5155103444886726 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13214/20000] Train-Loss: 0.39719873539848344 Test-Loss: 0.13979605780562346 Train-f1: 0.5578765590130896 Test-f1: 0.46651321398124457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13215/20000] Train-Loss: 0.3976612573879943 Test-Loss: 0.13945886704528324 Train-f1: 0.5850141889861076 Test-f1: 0.4781271777003485 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13216/20000] Train-Loss: 0.39160092391129087 Test-Loss: 0.14445237318134643 Train-f1: 0.49526739926739927 Test-f1: 0.5133179723502305 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13217/20000] Train-Loss: 0.3969261901796942 Test-Loss: 0.13657293400358717 Train-f1: 0.5794994664559882 Test-f1: 0.4463157894736843 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13218/20000] Train-Loss: 0.3903021874807078 Test-Loss: 0.14079355981824343 Train-f1: 0.5343639797055946 Test-f1: 0.4781271777003485 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13219/20000] Train-Loss: 0.3988233046103881 Test-Loss: 0.1386657636261693 Train-f1: 0.4906454091432962 Test-f1: 0.46728722258134026 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13220/20000] Train-Loss: 0.39462571129935003 Test-Loss: 0.14005258435049642 Train-f1: 0.5678744678744678 Test-f1: 0.5262301587301588 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13221/20000] Train-Loss: 0.39784230258422926 Test-Loss: 0.14409706470318187 Train-f1: 0.4944272566376128 Test-f1: 0.4774585921325052 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13222/20000] Train-Loss: 0.3953044171985109 Test-Loss: 0.1374911888055991 Train-f1: 0.4983558065601409 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13223/20000] Train-Loss: 0.3943207851778802 Test-Loss: 0.14067042327000276 Train-f1: 0.508994608994609 Test-f1: 0.5532874968169086 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13224/20000] Train-Loss: 0.3914638797655485 Test-Loss: 0.1470309170079954 Train-f1: 0.5861192084094717 Test-f1: 0.5272601794340925 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13225/20000] Train-Loss: 0.3916177633053566 Test-Loss: 0.1500764776483055 Train-f1: 0.5222578663572452 Test-f1: 0.475889328063241 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13226/20000] Train-Loss: 0.391351629935439 Test-Loss: 0.13800050831531332 Train-f1: 0.5676287150739705 Test-f1: 0.5051919146036793 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13227/20000] Train-Loss: 0.3859671594082376 Test-Loss: 0.1436621003900103 Train-f1: 0.5391611176909242 Test-f1: 0.5149874686716792 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13228/20000] Train-Loss: 0.3946934511324515 Test-Loss: 0.1427863450038919 Train-f1: 0.5839190087726197 Test-f1: 0.4628528560711606 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13229/20000] Train-Loss: 0.3921767896738936 Test-Loss: 0.13827075335035582 Train-f1: 0.49327698809066467 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13230/20000] Train-Loss: 0.39357137062927555 Test-Loss: 0.14205875519263475 Train-f1: 0.5089176519567784 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13231/20000] Train-Loss: 0.390557419301011 Test-Loss: 0.14364019610754142 Train-f1: 0.6064764607966893 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13232/20000] Train-Loss: 0.39715823335039535 Test-Loss: 0.1365987910952788 Train-f1: 0.5691173604560377 Test-f1: 0.4944577129097253 Train-acc: 0.8145695364238411 Test-acc: 0.7631578947368421\n",
      "Epoch [13233/20000] Train-Loss: 0.3916827595923293 Test-Loss: 0.1435367908972842 Train-f1: 0.5193553113553113 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13234/20000] Train-Loss: 0.39403113306121273 Test-Loss: 0.14740579489199374 Train-f1: 0.6359536943387876 Test-f1: 0.549891398923657 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13235/20000] Train-Loss: 0.38244637125221703 Test-Loss: 0.15357184618586367 Train-f1: 0.5734892886764797 Test-f1: 0.5603463203463204 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13236/20000] Train-Loss: 0.3916035785357343 Test-Loss: 0.1442657067113186 Train-f1: 0.5719108814362425 Test-f1: 0.5308287764866713 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13237/20000] Train-Loss: 0.3961494261520226 Test-Loss: 0.13807500623028468 Train-f1: 0.5338652215364544 Test-f1: 0.5165079365079366 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13238/20000] Train-Loss: 0.3951569042998159 Test-Loss: 0.13852977577875367 Train-f1: 0.5099711854325653 Test-f1: 0.4939348370927319 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13239/20000] Train-Loss: 0.39474193542212005 Test-Loss: 0.14063774733376655 Train-f1: 0.5113247882944048 Test-f1: 0.5262301587301588 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13240/20000] Train-Loss: 0.3926002440678346 Test-Loss: 0.1372177229381149 Train-f1: 0.5285340222866874 Test-f1: 0.4428237259816208 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13241/20000] Train-Loss: 0.3928954620189091 Test-Loss: 0.1423345900381438 Train-f1: 0.4982457736910292 Test-f1: 0.5262301587301588 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13242/20000] Train-Loss: 0.39244468413490685 Test-Loss: 0.13735574014748292 Train-f1: 0.533819693048388 Test-f1: 0.46815958815958825 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13243/20000] Train-Loss: 0.3928503179101201 Test-Loss: 0.14353119085275903 Train-f1: 0.570145733685096 Test-f1: 0.4774585921325052 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13244/20000] Train-Loss: 0.39745800707511697 Test-Loss: 0.1383131618208574 Train-f1: 0.5640255692020397 Test-f1: 0.5161904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13245/20000] Train-Loss: 0.3884761739334907 Test-Loss: 0.13696961588210144 Train-f1: 0.6515978769934145 Test-f1: 0.4644611528822056 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13246/20000] Train-Loss: 0.3919824071649777 Test-Loss: 0.14141503839952466 Train-f1: 0.49280829545730204 Test-f1: 0.4781271777003485 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13247/20000] Train-Loss: 0.3936616686701121 Test-Loss: 0.13714518687183191 Train-f1: 0.6360381295241145 Test-f1: 0.4642044134727062 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13248/20000] Train-Loss: 0.39241192057918944 Test-Loss: 0.14542000542558256 Train-f1: 0.5723185092312609 Test-f1: 0.4656685910608086 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13249/20000] Train-Loss: 0.39356895383988305 Test-Loss: 0.13874924183429743 Train-f1: 0.5279350104821804 Test-f1: 0.5161904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13250/20000] Train-Loss: 0.3932714072470905 Test-Loss: 0.13759629735377396 Train-f1: 0.5066267066267066 Test-f1: 0.5165079365079366 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13251/20000] Train-Loss: 0.3922948027575093 Test-Loss: 0.1402973566753513 Train-f1: 0.4945202639180411 Test-f1: 0.5412987012987013 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13252/20000] Train-Loss: 0.3968576961655545 Test-Loss: 0.1368843275280284 Train-f1: 0.5988864468864469 Test-f1: 0.5056027456027457 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [13253/20000] Train-Loss: 0.3958057108850514 Test-Loss: 0.1382373153596686 Train-f1: 0.5313919810521668 Test-f1: 0.4636974789915966 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13254/20000] Train-Loss: 0.39353853291191193 Test-Loss: 0.13863399126738596 Train-f1: 0.49947252747252746 Test-f1: 0.5161904761904762 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13255/20000] Train-Loss: 0.3919455325710618 Test-Loss: 0.13637069448888206 Train-f1: 0.5067969212106337 Test-f1: 0.4395848595848596 Train-acc: 0.8178807947019867 Test-acc: 0.75\n",
      "Epoch [13256/20000] Train-Loss: 0.3934053563630041 Test-Loss: 0.13904500543481516 Train-f1: 0.4982695218038214 Test-f1: 0.46651321398124457 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13257/20000] Train-Loss: 0.39540270405909644 Test-Loss: 0.13969583694404455 Train-f1: 0.4924699461612213 Test-f1: 0.5262301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13258/20000] Train-Loss: 0.39544829504675816 Test-Loss: 0.14332724873081132 Train-f1: 0.556279133901999 Test-f1: 0.4774585921325052 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13259/20000] Train-Loss: 0.39309236775688433 Test-Loss: 0.13752392721607698 Train-f1: 0.5177850523512559 Test-f1: 0.5165079365079366 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13260/20000] Train-Loss: 0.3908918066312975 Test-Loss: 0.14090686477537318 Train-f1: 0.5094113002956011 Test-f1: 0.5532874968169086 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13261/20000] Train-Loss: 0.3931471670541724 Test-Loss: 0.13992108605657042 Train-f1: 0.531799877141492 Test-f1: 0.515048830342948 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13262/20000] Train-Loss: 0.39148700575756273 Test-Loss: 0.14061470032877213 Train-f1: 0.5737642073768251 Test-f1: 0.5262301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13263/20000] Train-Loss: 0.3926041022474559 Test-Loss: 0.13947071839958047 Train-f1: 0.49342825575469557 Test-f1: 0.4528157349896481 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13264/20000] Train-Loss: 0.39742035958459826 Test-Loss: 0.14181638397686724 Train-f1: 0.5223117418051386 Test-f1: 0.4774585921325052 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13265/20000] Train-Loss: 0.3902022453519122 Test-Loss: 0.15158344024116477 Train-f1: 0.49526739926739927 Test-f1: 0.5304950558968786 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13266/20000] Train-Loss: 0.3938072397964348 Test-Loss: 0.14642233921572576 Train-f1: 0.5507075116664157 Test-f1: 0.5244444444444445 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13267/20000] Train-Loss: 0.39779746979259045 Test-Loss: 0.13678994893738836 Train-f1: 0.5664191664191663 Test-f1: 0.4944577129097253 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13268/20000] Train-Loss: 0.390644053300344 Test-Loss: 0.13824814076706937 Train-f1: 0.517294980090428 Test-f1: 0.46727688787185356 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13269/20000] Train-Loss: 0.39375842542960254 Test-Loss: 0.1371760312409562 Train-f1: 0.5288801288801289 Test-f1: 0.4944577129097253 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13270/20000] Train-Loss: 0.39173849458533283 Test-Loss: 0.1423479284877277 Train-f1: 0.502654640277006 Test-f1: 0.515048830342948 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13271/20000] Train-Loss: 0.3916052659131824 Test-Loss: 0.14709084856675395 Train-f1: 0.5820236977638842 Test-f1: 0.47972222222222227 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13272/20000] Train-Loss: 0.3922989016179688 Test-Loss: 0.13656377378155635 Train-f1: 0.5798983867300354 Test-f1: 0.4395848595848596 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13273/20000] Train-Loss: 0.3955136507409281 Test-Loss: 0.13739234800365482 Train-f1: 0.4906237250608111 Test-f1: 0.4944577129097253 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [13274/20000] Train-Loss: 0.39404808803196323 Test-Loss: 0.15152296086750616 Train-f1: 0.5786596481633397 Test-f1: 0.5237166085946574 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13275/20000] Train-Loss: 0.39747251159054936 Test-Loss: 0.1404811484570091 Train-f1: 0.5941904512371352 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13276/20000] Train-Loss: 0.39307161462918017 Test-Loss: 0.1384045864329775 Train-f1: 0.4923825952458074 Test-f1: 0.4636974789915966 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13277/20000] Train-Loss: 0.3919368043444985 Test-Loss: 0.13951648476209003 Train-f1: 0.5797589653288503 Test-f1: 0.5161904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13278/20000] Train-Loss: 0.39179615644480703 Test-Loss: 0.14031994147300164 Train-f1: 0.5846343311860553 Test-f1: 0.4781271777003485 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13279/20000] Train-Loss: 0.3966446894319487 Test-Loss: 0.14030920227547292 Train-f1: 0.5120762742866305 Test-f1: 0.46651321398124457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13280/20000] Train-Loss: 0.39750385003445404 Test-Loss: 0.1388268547231872 Train-f1: 0.5838601026740441 Test-f1: 0.5155103444886726 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13281/20000] Train-Loss: 0.39357848447400695 Test-Loss: 0.1375104572535112 Train-f1: 0.6335243993835837 Test-f1: 0.4488311688311688 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [13282/20000] Train-Loss: 0.3937569786260906 Test-Loss: 0.13994954641440085 Train-f1: 0.6248691289675257 Test-f1: 0.515048830342948 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13283/20000] Train-Loss: 0.3900789835698158 Test-Loss: 0.13980274119129296 Train-f1: 0.5998038925907629 Test-f1: 0.4528157349896481 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13284/20000] Train-Loss: 0.3897389722773157 Test-Loss: 0.14536487422650546 Train-f1: 0.5810446009389671 Test-f1: 0.5244444444444445 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13285/20000] Train-Loss: 0.3864242905847868 Test-Loss: 0.13681152920090128 Train-f1: 0.5070223374859136 Test-f1: 0.4944577129097253 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [13286/20000] Train-Loss: 0.3955093342079254 Test-Loss: 0.13900560973474058 Train-f1: 0.49947252747252746 Test-f1: 0.46651321398124457 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13287/20000] Train-Loss: 0.39788733491767964 Test-Loss: 0.1390580234569229 Train-f1: 0.5115587115587116 Test-f1: 0.5155103444886726 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13288/20000] Train-Loss: 0.3910157883016386 Test-Loss: 0.13894243745145937 Train-f1: 0.5661938061938062 Test-f1: 0.515048830342948 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13289/20000] Train-Loss: 0.3948786096873165 Test-Loss: 0.143269523866508 Train-f1: 0.5152691986775985 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13290/20000] Train-Loss: 0.3931579379923925 Test-Loss: 0.14258435833876007 Train-f1: 0.49545289766325384 Test-f1: 0.5516976716976717 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13291/20000] Train-Loss: 0.3975695127100475 Test-Loss: 0.1376682459815029 Train-f1: 0.5999096570858585 Test-f1: 0.4939348370927319 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13292/20000] Train-Loss: 0.3946926463286677 Test-Loss: 0.1377362969040802 Train-f1: 0.5565463108320251 Test-f1: 0.46815958815958825 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13293/20000] Train-Loss: 0.39109037873025226 Test-Loss: 0.13847929185071103 Train-f1: 0.5025010783400046 Test-f1: 0.5155103444886726 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13294/20000] Train-Loss: 0.3952431303817285 Test-Loss: 0.144503026191265 Train-f1: 0.5078682750044252 Test-f1: 0.48005599472990773 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13295/20000] Train-Loss: 0.38753585090643866 Test-Loss: 0.1427694788945385 Train-f1: 0.5595216430521253 Test-f1: 0.5308287764866713 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13296/20000] Train-Loss: 0.3931744924797807 Test-Loss: 0.1405632026843751 Train-f1: 0.5846531197696082 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13297/20000] Train-Loss: 0.3910463732284818 Test-Loss: 0.13990171503827104 Train-f1: 0.5627462627462627 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13298/20000] Train-Loss: 0.3881105219255955 Test-Loss: 0.1542599704202993 Train-f1: 0.49659890799131307 Test-f1: 0.5603463203463204 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13299/20000] Train-Loss: 0.3935910182892683 Test-Loss: 0.14165167268587667 Train-f1: 0.6078900877163808 Test-f1: 0.5308287764866713 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13300/20000] Train-Loss: 0.3953432210863764 Test-Loss: 0.14199475073941847 Train-f1: 0.4996984337501579 Test-f1: 0.5141038802093091 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13301/20000] Train-Loss: 0.3914278758933238 Test-Loss: 0.1393262160394422 Train-f1: 0.5127761711972239 Test-f1: 0.5308287764866713 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13302/20000] Train-Loss: 0.3870321965054678 Test-Loss: 0.1462990225905827 Train-f1: 0.5219106434125027 Test-f1: 0.4656685910608086 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13303/20000] Train-Loss: 0.3949191978790396 Test-Loss: 0.14279921854132094 Train-f1: 0.6075928486524512 Test-f1: 0.4774585921325052 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13304/20000] Train-Loss: 0.3944349722029756 Test-Loss: 0.1363910353470857 Train-f1: 0.5174691611315653 Test-f1: 0.4428237259816208 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13305/20000] Train-Loss: 0.39041029661773435 Test-Loss: 0.14293188027580211 Train-f1: 0.6093061915340557 Test-f1: 0.4774585921325052 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [13306/20000] Train-Loss: 0.39434465426141097 Test-Loss: 0.14177811535861423 Train-f1: 0.623067342662508 Test-f1: 0.5262301587301588 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13307/20000] Train-Loss: 0.3933909849252233 Test-Loss: 0.14270969191194344 Train-f1: 0.5772833676759916 Test-f1: 0.5256318681318681 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13308/20000] Train-Loss: 0.38777991411199403 Test-Loss: 0.14176137718293472 Train-f1: 0.5290787887061179 Test-f1: 0.5155103444886726 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13309/20000] Train-Loss: 0.39343300787134916 Test-Loss: 0.1445396605991667 Train-f1: 0.5097921996068601 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13310/20000] Train-Loss: 0.38674202222334864 Test-Loss: 0.1503559351940081 Train-f1: 0.4992237502058541 Test-f1: 0.5244444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13311/20000] Train-Loss: 0.3932700179973862 Test-Loss: 0.1424657641573199 Train-f1: 0.5822654297106852 Test-f1: 0.47285467739587883 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13312/20000] Train-Loss: 0.39348062917576493 Test-Loss: 0.14668878769542523 Train-f1: 0.5172658267085511 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13313/20000] Train-Loss: 0.3926943499030762 Test-Loss: 0.14360178699613896 Train-f1: 0.5299454327996528 Test-f1: 0.5133179723502305 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13314/20000] Train-Loss: 0.392474948703506 Test-Loss: 0.15148586134072486 Train-f1: 0.5658904168725207 Test-f1: 0.4792719919110212 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13315/20000] Train-Loss: 0.3916621782212088 Test-Loss: 0.1406720389550962 Train-f1: 0.5241223517266704 Test-f1: 0.4732919254658385 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13316/20000] Train-Loss: 0.3913355417751324 Test-Loss: 0.14703476481298292 Train-f1: 0.5140131933580523 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13317/20000] Train-Loss: 0.39589371340494245 Test-Loss: 0.14066064115531596 Train-f1: 0.6125573684989398 Test-f1: 0.5256318681318681 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13318/20000] Train-Loss: 0.39357466966607624 Test-Loss: 0.1412924928838242 Train-f1: 0.5226402069982142 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13319/20000] Train-Loss: 0.3910291013497078 Test-Loss: 0.14624287518867193 Train-f1: 0.5168255168255168 Test-f1: 0.5244444444444445 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13320/20000] Train-Loss: 0.39266564457029424 Test-Loss: 0.13630821034186022 Train-f1: 0.53279643889158 Test-f1: 0.4395848595848596 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13321/20000] Train-Loss: 0.3911756891584231 Test-Loss: 0.15161654499819513 Train-f1: 0.5037444654754353 Test-f1: 0.5603463203463204 Train-acc: 0.8145695364238411 Test-acc: 0.8157894736842105\n",
      "Epoch [13322/20000] Train-Loss: 0.39284343942148475 Test-Loss: 0.13680414536262836 Train-f1: 0.5970423016367638 Test-f1: 0.4944577129097253 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13323/20000] Train-Loss: 0.3912676213987214 Test-Loss: 0.1372996127474511 Train-f1: 0.5490414923498224 Test-f1: 0.5183710801393728 Train-acc: 0.8079470198675497 Test-acc: 0.7894736842105263\n",
      "Epoch [13324/20000] Train-Loss: 0.3976888439210372 Test-Loss: 0.13631489735577132 Train-f1: 0.5627462627462627 Test-f1: 0.45000000000000007 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13325/20000] Train-Loss: 0.3958465905493224 Test-Loss: 0.15352978815145993 Train-f1: 0.5642032470242235 Test-f1: 0.511705299066967 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13326/20000] Train-Loss: 0.3948481929874321 Test-Loss: 0.14049269834667696 Train-f1: 0.6257746668342695 Test-f1: 0.5519803072744249 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13327/20000] Train-Loss: 0.3874987726620011 Test-Loss: 0.14201381902454216 Train-f1: 0.5174801341018602 Test-f1: 0.5519803072744249 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13328/20000] Train-Loss: 0.3896808275758513 Test-Loss: 0.1422430755648604 Train-f1: 0.4868416694818142 Test-f1: 0.5256318681318681 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [13329/20000] Train-Loss: 0.39299889218877637 Test-Loss: 0.13693224458335265 Train-f1: 0.5342385812627841 Test-f1: 0.4939348370927319 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13330/20000] Train-Loss: 0.39417374151872553 Test-Loss: 0.14061823317836347 Train-f1: 0.5115587115587116 Test-f1: 0.4774585921325052 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13331/20000] Train-Loss: 0.39566941940670103 Test-Loss: 0.14548614259738468 Train-f1: 0.4926867541061089 Test-f1: 0.5133179723502305 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13332/20000] Train-Loss: 0.3924412499066965 Test-Loss: 0.14219411710684077 Train-f1: 0.4935995057839612 Test-f1: 0.5421972621972622 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13333/20000] Train-Loss: 0.3927823657658785 Test-Loss: 0.138623873177052 Train-f1: 0.5796941154084012 Test-f1: 0.46728722258134026 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13334/20000] Train-Loss: 0.3913681355205946 Test-Loss: 0.14468033495195584 Train-f1: 0.5584277469688929 Test-f1: 0.4774585921325052 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13335/20000] Train-Loss: 0.38990598817228833 Test-Loss: 0.1382189680334881 Train-f1: 0.5637719037719038 Test-f1: 0.46728722258134026 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13336/20000] Train-Loss: 0.38863170103970945 Test-Loss: 0.14281196343050137 Train-f1: 0.562973026973027 Test-f1: 0.5141038802093091 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13337/20000] Train-Loss: 0.3911212895775571 Test-Loss: 0.13757309049408192 Train-f1: 0.5570830470167818 Test-f1: 0.5183710801393728 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13338/20000] Train-Loss: 0.3911134350415298 Test-Loss: 0.14594983860708013 Train-f1: 0.5759397834869533 Test-f1: 0.5244444444444445 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13339/20000] Train-Loss: 0.39701642371112045 Test-Loss: 0.13723929322549097 Train-f1: 0.5146736768840331 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13340/20000] Train-Loss: 0.39112025438424847 Test-Loss: 0.14245964600941968 Train-f1: 0.4988389094349359 Test-f1: 0.5519803072744249 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13341/20000] Train-Loss: 0.38991609572331454 Test-Loss: 0.14793176350415996 Train-f1: 0.4982691810901576 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13342/20000] Train-Loss: 0.3910921979601461 Test-Loss: 0.14201174493356183 Train-f1: 0.5718293731364488 Test-f1: 0.5141038802093091 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13343/20000] Train-Loss: 0.39027519456708676 Test-Loss: 0.13699325285571753 Train-f1: 0.6068341593047476 Test-f1: 0.5183710801393728 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13344/20000] Train-Loss: 0.3918682465451481 Test-Loss: 0.142106890148936 Train-f1: 0.5765463045987167 Test-f1: 0.46651321398124457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13345/20000] Train-Loss: 0.38707703766102536 Test-Loss: 0.1437981486982644 Train-f1: 0.5682318541451667 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13346/20000] Train-Loss: 0.39076504175567667 Test-Loss: 0.14163731058705323 Train-f1: 0.6024994727396706 Test-f1: 0.5262301587301588 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13347/20000] Train-Loss: 0.3848347224095405 Test-Loss: 0.1530074133740635 Train-f1: 0.4976147912453645 Test-f1: 0.5108045977011495 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13348/20000] Train-Loss: 0.39271103348941194 Test-Loss: 0.14360045696665522 Train-f1: 0.6272354686223701 Test-f1: 0.4661751152073733 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13349/20000] Train-Loss: 0.38849975194420794 Test-Loss: 0.13758298544925704 Train-f1: 0.5635964035964036 Test-f1: 0.4526190476190476 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13350/20000] Train-Loss: 0.39748668471889864 Test-Loss: 0.13715332140400743 Train-f1: 0.4911373179665862 Test-f1: 0.46815958815958825 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13351/20000] Train-Loss: 0.38775448603486823 Test-Loss: 0.1365983042256788 Train-f1: 0.5799149466233601 Test-f1: 0.45000000000000007 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13352/20000] Train-Loss: 0.393324497160107 Test-Loss: 0.14293554265212327 Train-f1: 0.6322631007490163 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13353/20000] Train-Loss: 0.3929980016205644 Test-Loss: 0.14133901756976244 Train-f1: 0.5794701942352949 Test-f1: 0.5532874968169086 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13354/20000] Train-Loss: 0.3937360833149236 Test-Loss: 0.14317491385899403 Train-f1: 0.5815405894847188 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13355/20000] Train-Loss: 0.3906883002755954 Test-Loss: 0.13895258902978208 Train-f1: 0.5006396125143981 Test-f1: 0.5051919146036793 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13356/20000] Train-Loss: 0.3960415662630978 Test-Loss: 0.1383369131161048 Train-f1: 0.5758816183816184 Test-f1: 0.5155103444886726 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13357/20000] Train-Loss: 0.3923497084158299 Test-Loss: 0.13940982468299135 Train-f1: 0.48990584359711875 Test-f1: 0.5519803072744249 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13358/20000] Train-Loss: 0.3953521710016887 Test-Loss: 0.13639237355097827 Train-f1: 0.6208596422033063 Test-f1: 0.49677440206851975 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13359/20000] Train-Loss: 0.3957763372201396 Test-Loss: 0.1375522889805561 Train-f1: 0.49526739926739927 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13360/20000] Train-Loss: 0.3908912421765795 Test-Loss: 0.14505683607586284 Train-f1: 0.5342855895004391 Test-f1: 0.5133179723502305 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13361/20000] Train-Loss: 0.391958273801098 Test-Loss: 0.145971892708931 Train-f1: 0.5141865110927896 Test-f1: 0.5244444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13362/20000] Train-Loss: 0.3914625113200116 Test-Loss: 0.14346774722395764 Train-f1: 0.5684229201954173 Test-f1: 0.5519803072744249 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13363/20000] Train-Loss: 0.3906052808534657 Test-Loss: 0.13862715775174458 Train-f1: 0.5100143011720257 Test-f1: 0.46727688787185356 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13364/20000] Train-Loss: 0.39275685819973977 Test-Loss: 0.1396962053033855 Train-f1: 0.4934955871868623 Test-f1: 0.5262301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13365/20000] Train-Loss: 0.3880652563209983 Test-Loss: 0.15289321625244992 Train-f1: 0.4948745351173428 Test-f1: 0.5512035634085419 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13366/20000] Train-Loss: 0.39436022962961464 Test-Loss: 0.14264399279252457 Train-f1: 0.5087443342220412 Test-f1: 0.4628528560711606 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13367/20000] Train-Loss: 0.39490118290857246 Test-Loss: 0.13655178996889145 Train-f1: 0.493942893071497 Test-f1: 0.4395848595848596 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [13368/20000] Train-Loss: 0.38910071509676375 Test-Loss: 0.13661165634127903 Train-f1: 0.48654014598540146 Test-f1: 0.5076190476190476 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [13369/20000] Train-Loss: 0.39105403783870374 Test-Loss: 0.13838721966816553 Train-f1: 0.5767856304769057 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13370/20000] Train-Loss: 0.3934024241762593 Test-Loss: 0.1387640831943212 Train-f1: 0.5090967602073901 Test-f1: 0.515048830342948 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13371/20000] Train-Loss: 0.3923029943818597 Test-Loss: 0.13774075748646405 Train-f1: 0.4955866287135636 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13372/20000] Train-Loss: 0.39247072556395896 Test-Loss: 0.14097300482587727 Train-f1: 0.5906221997180902 Test-f1: 0.5262301587301588 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13373/20000] Train-Loss: 0.3912855558616865 Test-Loss: 0.1371301093671961 Train-f1: 0.49649286782126667 Test-f1: 0.4644611528822056 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13374/20000] Train-Loss: 0.3927637585651074 Test-Loss: 0.1431508749824633 Train-f1: 0.5118570993023548 Test-f1: 0.4656685910608086 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13375/20000] Train-Loss: 0.3900204234515376 Test-Loss: 0.137685623077681 Train-f1: 0.5972889502301266 Test-f1: 0.5076190476190476 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13376/20000] Train-Loss: 0.3924861991129133 Test-Loss: 0.13696382392549195 Train-f1: 0.5118570993023548 Test-f1: 0.4944577129097253 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13377/20000] Train-Loss: 0.3893389829285839 Test-Loss: 0.13739295775192123 Train-f1: 0.5115587115587116 Test-f1: 0.4944577129097253 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13378/20000] Train-Loss: 0.38142481735146677 Test-Loss: 0.14714076575052973 Train-f1: 0.48648108188268396 Test-f1: 0.5022040816326532 Train-acc: 0.8112582781456954 Test-acc: 0.7763157894736842\n",
      "Epoch [13379/20000] Train-Loss: 0.40240532429893267 Test-Loss: 0.1370796109084979 Train-f1: 0.4955791070243626 Test-f1: 0.4939348370927319 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13380/20000] Train-Loss: 0.39212299707728354 Test-Loss: 0.13978220802528038 Train-f1: 0.492859899482416 Test-f1: 0.515048830342948 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13381/20000] Train-Loss: 0.3895396352994587 Test-Loss: 0.14445534000417953 Train-f1: 0.5593541483456903 Test-f1: 0.5133179723502305 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13382/20000] Train-Loss: 0.39269356716391157 Test-Loss: 0.14274766040329312 Train-f1: 0.5370697346942317 Test-f1: 0.4774585921325052 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13383/20000] Train-Loss: 0.3920696275960952 Test-Loss: 0.14242933550894518 Train-f1: 0.5109244378809596 Test-f1: 0.4661751152073733 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13384/20000] Train-Loss: 0.3882421701554839 Test-Loss: 0.13700547535347912 Train-f1: 0.49805470467722124 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13385/20000] Train-Loss: 0.39001865522941276 Test-Loss: 0.15105183204220232 Train-f1: 0.49160694378085684 Test-f1: 0.5237166085946574 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13386/20000] Train-Loss: 0.3894206524689187 Test-Loss: 0.13642538804881416 Train-f1: 0.537914814771835 Test-f1: 0.4395848595848596 Train-acc: 0.8377483443708609 Test-acc: 0.75\n",
      "Epoch [13387/20000] Train-Loss: 0.39536477696103006 Test-Loss: 0.1431158417960611 Train-f1: 0.4894888173771606 Test-f1: 0.5256318681318681 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13388/20000] Train-Loss: 0.38831761848281293 Test-Loss: 0.1366857632929102 Train-f1: 0.6210345079910298 Test-f1: 0.5076190476190476 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13389/20000] Train-Loss: 0.39035532579345106 Test-Loss: 0.1378310675905761 Train-f1: 0.5635269492412349 Test-f1: 0.46728722258134026 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13390/20000] Train-Loss: 0.39109007482094343 Test-Loss: 0.13743015607582498 Train-f1: 0.5795930082988906 Test-f1: 0.4395848595848596 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13391/20000] Train-Loss: 0.3940828116504026 Test-Loss: 0.1390400934836708 Train-f1: 0.5583323343323343 Test-f1: 0.5183710801393728 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13392/20000] Train-Loss: 0.39201407768020424 Test-Loss: 0.14337303625304412 Train-f1: 0.48938002668191344 Test-f1: 0.4774585921325052 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13393/20000] Train-Loss: 0.39072444216374824 Test-Loss: 0.13684650399671938 Train-f1: 0.5295520858489671 Test-f1: 0.5062301587301588 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13394/20000] Train-Loss: 0.3906251491288682 Test-Loss: 0.13752340738084057 Train-f1: 0.5064022933588151 Test-f1: 0.4644611528822056 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13395/20000] Train-Loss: 0.3899225121413542 Test-Loss: 0.14203787303129906 Train-f1: 0.5008431762884318 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13396/20000] Train-Loss: 0.3925047849799661 Test-Loss: 0.1364440209795258 Train-f1: 0.585465721159085 Test-f1: 0.4944577129097253 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13397/20000] Train-Loss: 0.39554537675087786 Test-Loss: 0.14114585783060152 Train-f1: 0.6266292979321149 Test-f1: 0.5256318681318681 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13398/20000] Train-Loss: 0.3879199772787935 Test-Loss: 0.14115184491534286 Train-f1: 0.5645484450190332 Test-f1: 0.5262301587301588 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13399/20000] Train-Loss: 0.39332275900826813 Test-Loss: 0.1402624995553321 Train-f1: 0.5296021509604499 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13400/20000] Train-Loss: 0.3932182464020645 Test-Loss: 0.14055404159959659 Train-f1: 0.49941889474417717 Test-f1: 0.5262301587301588 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13401/20000] Train-Loss: 0.3967081929658579 Test-Loss: 0.14093016273036876 Train-f1: 0.59727807813849 Test-f1: 0.5256318681318681 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13402/20000] Train-Loss: 0.393188877816947 Test-Loss: 0.13722301570701037 Train-f1: 0.5092241092241092 Test-f1: 0.5308287764866713 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13403/20000] Train-Loss: 0.39265743492633765 Test-Loss: 0.13918281854469486 Train-f1: 0.5992414375380426 Test-f1: 0.5262301587301588 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13404/20000] Train-Loss: 0.387763877661745 Test-Loss: 0.13695986491538834 Train-f1: 0.5633024869866976 Test-f1: 0.4939348370927319 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13405/20000] Train-Loss: 0.3889406808650005 Test-Loss: 0.13720826986816262 Train-f1: 0.5854494498789801 Test-f1: 0.4428237259816208 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [13406/20000] Train-Loss: 0.3859169581322596 Test-Loss: 0.13903590588718087 Train-f1: 0.5854306334397867 Test-f1: 0.5183710801393728 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13407/20000] Train-Loss: 0.3915992803876429 Test-Loss: 0.14368146808596977 Train-f1: 0.574266662781908 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13408/20000] Train-Loss: 0.3907173656171223 Test-Loss: 0.14833541029811184 Train-f1: 0.49999971597283566 Test-f1: 0.549891398923657 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13409/20000] Train-Loss: 0.3928470365225158 Test-Loss: 0.13882980730015074 Train-f1: 0.5151933179038612 Test-f1: 0.5532874968169086 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13410/20000] Train-Loss: 0.3838843107338753 Test-Loss: 0.15030543681943154 Train-f1: 0.5812332845351713 Test-f1: 0.5272601794340925 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13411/20000] Train-Loss: 0.39207410033160356 Test-Loss: 0.13794701481370314 Train-f1: 0.6233883654902368 Test-f1: 0.47501831501831504 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13412/20000] Train-Loss: 0.3897975669797655 Test-Loss: 0.1376214598134554 Train-f1: 0.4963756243756244 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13413/20000] Train-Loss: 0.39290446614555075 Test-Loss: 0.14551408085685155 Train-f1: 0.508994608994609 Test-f1: 0.475889328063241 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13414/20000] Train-Loss: 0.39107164724449117 Test-Loss: 0.13924758195016515 Train-f1: 0.5199380717105688 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13415/20000] Train-Loss: 0.3885633022861511 Test-Loss: 0.14401240986220826 Train-f1: 0.5628468176119182 Test-f1: 0.5133179723502305 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13416/20000] Train-Loss: 0.3851624688543635 Test-Loss: 0.14044122428273237 Train-f1: 0.6304415584415585 Test-f1: 0.5422818042172881 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13417/20000] Train-Loss: 0.39441251006962225 Test-Loss: 0.1403780223062003 Train-f1: 0.6117445358062801 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [13418/20000] Train-Loss: 0.3923018720047991 Test-Loss: 0.1361978505431027 Train-f1: 0.5177217917992616 Test-f1: 0.45000000000000007 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13419/20000] Train-Loss: 0.39052812432167855 Test-Loss: 0.13982438526323185 Train-f1: 0.5551115551115551 Test-f1: 0.5262301587301588 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13420/20000] Train-Loss: 0.39033261007762005 Test-Loss: 0.14486917206555258 Train-f1: 0.49725998994543835 Test-f1: 0.5244444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13421/20000] Train-Loss: 0.39126501336034564 Test-Loss: 0.14344153054530562 Train-f1: 0.5352283510672773 Test-f1: 0.4766304347826087 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13422/20000] Train-Loss: 0.39055826318054443 Test-Loss: 0.13709935455800928 Train-f1: 0.5773273763834491 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13423/20000] Train-Loss: 0.392994250942007 Test-Loss: 0.14088617539116585 Train-f1: 0.5152691986775985 Test-f1: 0.4774585921325052 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13424/20000] Train-Loss: 0.3893274872519735 Test-Loss: 0.13983255785557078 Train-f1: 0.5097027928623341 Test-f1: 0.46651321398124457 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13425/20000] Train-Loss: 0.3918362405107075 Test-Loss: 0.14051471958460401 Train-f1: 0.4926496400948956 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [13426/20000] Train-Loss: 0.39121994163628604 Test-Loss: 0.14883519221721872 Train-f1: 0.49327698809066467 Test-f1: 0.5519803072744249 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13427/20000] Train-Loss: 0.39420091123695766 Test-Loss: 0.14243437498422487 Train-f1: 0.5955720601237843 Test-f1: 0.5256318681318681 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13428/20000] Train-Loss: 0.3894762434507452 Test-Loss: 0.14167234794544475 Train-f1: 0.5654274627732085 Test-f1: 0.5141038802093091 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13429/20000] Train-Loss: 0.3882213896590433 Test-Loss: 0.14614883493381534 Train-f1: 0.5690545177420093 Test-f1: 0.549891398923657 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [13430/20000] Train-Loss: 0.38993830908918403 Test-Loss: 0.1411231356627791 Train-f1: 0.5845343671150123 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13431/20000] Train-Loss: 0.3864149694894377 Test-Loss: 0.14047013617169604 Train-f1: 0.6258306055482221 Test-f1: 0.4740245261984392 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13432/20000] Train-Loss: 0.39156734100251844 Test-Loss: 0.13798722070039862 Train-f1: 0.48465006255889415 Test-f1: 0.5165079365079366 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [13433/20000] Train-Loss: 0.3909574370027374 Test-Loss: 0.14311243358662867 Train-f1: 0.5001068695763691 Test-f1: 0.5641558441558441 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13434/20000] Train-Loss: 0.3911229022043907 Test-Loss: 0.1380840395802461 Train-f1: 0.5941904512371352 Test-f1: 0.46727688787185356 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13435/20000] Train-Loss: 0.3911529618149761 Test-Loss: 0.13939056484577395 Train-f1: 0.4906454091432962 Test-f1: 0.48343865857768786 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13436/20000] Train-Loss: 0.39097316833159174 Test-Loss: 0.1410065688225748 Train-f1: 0.4980335428245441 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13437/20000] Train-Loss: 0.39324275887137683 Test-Loss: 0.13638904213244699 Train-f1: 0.5119884877027734 Test-f1: 0.4537451737451737 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13438/20000] Train-Loss: 0.3864148555487212 Test-Loss: 0.14466772245692666 Train-f1: 0.500068225927177 Test-f1: 0.5133179723502305 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13439/20000] Train-Loss: 0.3876321614840781 Test-Loss: 0.1422989911178229 Train-f1: 0.5558855558855559 Test-f1: 0.5532874968169086 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13440/20000] Train-Loss: 0.3871420628690386 Test-Loss: 0.14973965507988876 Train-f1: 0.510721162895076 Test-f1: 0.5237166085946574 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13441/20000] Train-Loss: 0.3907660839253727 Test-Loss: 0.13734671396355716 Train-f1: 0.6403750463797577 Test-f1: 0.4456394609712688 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13442/20000] Train-Loss: 0.39151479396460614 Test-Loss: 0.142506576769612 Train-f1: 0.49908926129961745 Test-f1: 0.4774585921325052 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13443/20000] Train-Loss: 0.3970373735511035 Test-Loss: 0.13719531490002726 Train-f1: 0.4909384143836698 Test-f1: 0.5051190476190477 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13444/20000] Train-Loss: 0.3872259928489113 Test-Loss: 0.13792222258683243 Train-f1: 0.49771861248371313 Test-f1: 0.5272243672243672 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13445/20000] Train-Loss: 0.3912433552039954 Test-Loss: 0.13832582285833847 Train-f1: 0.6457747897160917 Test-f1: 0.4944577129097253 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13446/20000] Train-Loss: 0.39057510292759096 Test-Loss: 0.1367992824732954 Train-f1: 0.5353095096479411 Test-f1: 0.4537451737451737 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13447/20000] Train-Loss: 0.38679402456776063 Test-Loss: 0.14170082834395287 Train-f1: 0.5335740718259498 Test-f1: 0.5141038802093091 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13448/20000] Train-Loss: 0.3824809235196478 Test-Loss: 0.13709656955622468 Train-f1: 0.608077508077508 Test-f1: 0.5183710801393728 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13449/20000] Train-Loss: 0.39444657572454184 Test-Loss: 0.14320244148000338 Train-f1: 0.6064287143577567 Test-f1: 0.4628528560711606 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [13450/20000] Train-Loss: 0.39051841967335127 Test-Loss: 0.13907082733480713 Train-f1: 0.5670182841068917 Test-f1: 0.4939348370927319 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13451/20000] Train-Loss: 0.387010209172071 Test-Loss: 0.14680517864017134 Train-f1: 0.5004805479258034 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13452/20000] Train-Loss: 0.39404778883971386 Test-Loss: 0.1380414248885206 Train-f1: 0.6260961130526348 Test-f1: 0.5051919146036793 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13453/20000] Train-Loss: 0.39014293559116464 Test-Loss: 0.13819897372412668 Train-f1: 0.5677982095633325 Test-f1: 0.4636974789915966 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13454/20000] Train-Loss: 0.3881808543727852 Test-Loss: 0.14503675864308865 Train-f1: 0.6204928849757344 Test-f1: 0.46894664842681255 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13455/20000] Train-Loss: 0.388425562656909 Test-Loss: 0.1367221485248834 Train-f1: 0.5318149089812814 Test-f1: 0.4428237259816208 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13456/20000] Train-Loss: 0.39008920204618525 Test-Loss: 0.1382908154627088 Train-f1: 0.5593163067615622 Test-f1: 0.4636974789915966 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13457/20000] Train-Loss: 0.38717816409616534 Test-Loss: 0.13767827193600082 Train-f1: 0.599099234404683 Test-f1: 0.5165079365079366 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13458/20000] Train-Loss: 0.3919196635589734 Test-Loss: 0.1389728356193641 Train-f1: 0.5851499316854007 Test-f1: 0.526984126984127 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13459/20000] Train-Loss: 0.3882376690519046 Test-Loss: 0.14897533159047932 Train-f1: 0.5654539720764886 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13460/20000] Train-Loss: 0.38602386706086544 Test-Loss: 0.15122202283917843 Train-f1: 0.6161445104643779 Test-f1: 0.5265323435843053 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13461/20000] Train-Loss: 0.3913176496352757 Test-Loss: 0.13805730621293896 Train-f1: 0.5805140477664079 Test-f1: 0.46727688787185356 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13462/20000] Train-Loss: 0.3880200236654537 Test-Loss: 0.138030252477734 Train-f1: 0.5238638904954694 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13463/20000] Train-Loss: 0.39376745115542044 Test-Loss: 0.14486485111381714 Train-f1: 0.49964697138610187 Test-f1: 0.48343865857768786 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13464/20000] Train-Loss: 0.3884730675998479 Test-Loss: 0.14505537061685086 Train-f1: 0.5035098429550985 Test-f1: 0.475889328063241 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13465/20000] Train-Loss: 0.39221220763810727 Test-Loss: 0.13930344377525367 Train-f1: 0.5856621346599384 Test-f1: 0.5262301587301588 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13466/20000] Train-Loss: 0.38869160359614446 Test-Loss: 0.14358528173642296 Train-f1: 0.5651361537578798 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13467/20000] Train-Loss: 0.3799807252605968 Test-Loss: 0.13690364817454825 Train-f1: 0.5807862309965872 Test-f1: 0.4395848595848596 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13468/20000] Train-Loss: 0.3891618834074521 Test-Loss: 0.13971720973399399 Train-f1: 0.5282055145979196 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13469/20000] Train-Loss: 0.38777032780987064 Test-Loss: 0.1426914743025812 Train-f1: 0.5765526856007617 Test-f1: 0.4628528560711606 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [13470/20000] Train-Loss: 0.3925956735107767 Test-Loss: 0.13696910776532287 Train-f1: 0.5795930082988906 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13471/20000] Train-Loss: 0.3868563679156564 Test-Loss: 0.14818303014049475 Train-f1: 0.5309832923756975 Test-f1: 0.475889328063241 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13472/20000] Train-Loss: 0.39160196715877477 Test-Loss: 0.14064593410873455 Train-f1: 0.5590849896008488 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [13473/20000] Train-Loss: 0.3871141178478113 Test-Loss: 0.1385280938633855 Train-f1: 0.5685656448814344 Test-f1: 0.5155103444886726 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13474/20000] Train-Loss: 0.3879954204032383 Test-Loss: 0.13886171716780468 Train-f1: 0.6043137978533472 Test-f1: 0.46727688787185356 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13475/20000] Train-Loss: 0.3877133181781968 Test-Loss: 0.13968782446924818 Train-f1: 0.5082860962376808 Test-f1: 0.5262301587301588 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13476/20000] Train-Loss: 0.389402381015941 Test-Loss: 0.13887917205141484 Train-f1: 0.5830445700010917 Test-f1: 0.4842443064182195 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13477/20000] Train-Loss: 0.3856305217121026 Test-Loss: 0.13721836748323626 Train-f1: 0.5771099587063337 Test-f1: 0.4644611528822056 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13478/20000] Train-Loss: 0.38779416375840464 Test-Loss: 0.1372577058592247 Train-f1: 0.49199434279530296 Test-f1: 0.4944577129097253 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [13479/20000] Train-Loss: 0.38881184912185357 Test-Loss: 0.13894468841143237 Train-f1: 0.6192554112554113 Test-f1: 0.46651321398124457 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13480/20000] Train-Loss: 0.3931546506752743 Test-Loss: 0.13803058713690738 Train-f1: 0.5367776367776368 Test-f1: 0.4636974789915966 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13481/20000] Train-Loss: 0.38991041430049794 Test-Loss: 0.1375901143664073 Train-f1: 0.6091787670909315 Test-f1: 0.4456394609712688 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13482/20000] Train-Loss: 0.3900596002691019 Test-Loss: 0.14723259112550324 Train-f1: 0.5923782221918868 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [13483/20000] Train-Loss: 0.3928739399608679 Test-Loss: 0.14176468997763744 Train-f1: 0.5847995717560935 Test-f1: 0.5519803072744249 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13484/20000] Train-Loss: 0.3875525122963099 Test-Loss: 0.14623084074138187 Train-f1: 0.6050219487676867 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13485/20000] Train-Loss: 0.39059601363668806 Test-Loss: 0.1374488055135555 Train-f1: 0.6367362244167741 Test-f1: 0.4456394609712688 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13486/20000] Train-Loss: 0.3886294831144835 Test-Loss: 0.14238864308806884 Train-f1: 0.5710372992803846 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13487/20000] Train-Loss: 0.3865584720809789 Test-Loss: 0.14531232522404025 Train-f1: 0.5727736588048602 Test-f1: 0.42983543819364717 Train-acc: 0.8178807947019867 Test-acc: 0.7368421052631579\n",
      "Epoch [13488/20000] Train-Loss: 0.3897589200987369 Test-Loss: 0.13837305582162407 Train-f1: 0.5008325867458995 Test-f1: 0.5155103444886726 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13489/20000] Train-Loss: 0.3945877659949632 Test-Loss: 0.1376092958835621 Train-f1: 0.5200178046927273 Test-f1: 0.5161904761904762 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13490/20000] Train-Loss: 0.384223562552824 Test-Loss: 0.1386820686859072 Train-f1: 0.5202150920109626 Test-f1: 0.5641558441558441 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13491/20000] Train-Loss: 0.38405241322912803 Test-Loss: 0.14059786547458672 Train-f1: 0.5031832506285061 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13492/20000] Train-Loss: 0.3878134979593505 Test-Loss: 0.13722496084794478 Train-f1: 0.49342825575469557 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13493/20000] Train-Loss: 0.38363018879936384 Test-Loss: 0.16104533788175665 Train-f1: 0.5167023298258527 Test-f1: 0.4341384863123993 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13494/20000] Train-Loss: 0.3994809299121742 Test-Loss: 0.1388149804099793 Train-f1: 0.5775640694303775 Test-f1: 0.4746428571428572 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13495/20000] Train-Loss: 0.3819734055596182 Test-Loss: 0.13868765632796617 Train-f1: 0.5881605606584477 Test-f1: 0.5616954528992294 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13496/20000] Train-Loss: 0.39046525353290257 Test-Loss: 0.1415717800872746 Train-f1: 0.5774283659332241 Test-f1: 0.5532874968169086 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13497/20000] Train-Loss: 0.39316503058594976 Test-Loss: 0.13827541512623104 Train-f1: 0.5745534404573307 Test-f1: 0.46727688787185356 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13498/20000] Train-Loss: 0.391862172510352 Test-Loss: 0.13846289579164817 Train-f1: 0.5290787887061179 Test-f1: 0.4740245261984392 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13499/20000] Train-Loss: 0.39123971605705377 Test-Loss: 0.15000206733708793 Train-f1: 0.5135183656922787 Test-f1: 0.4766304347826087 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13500/20000] Train-Loss: 0.3914650266229152 Test-Loss: 0.1420306310147344 Train-f1: 0.49280829545730204 Test-f1: 0.5262301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13501/20000] Train-Loss: 0.39144680736396426 Test-Loss: 0.13824012725243923 Train-f1: 0.6166742992549444 Test-f1: 0.5161904761904762 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13502/20000] Train-Loss: 0.3909679602666127 Test-Loss: 0.1371168113638825 Train-f1: 0.5771008670577836 Test-f1: 0.4644611528822056 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13503/20000] Train-Loss: 0.3903521900256238 Test-Loss: 0.14034985440919093 Train-f1: 0.5792900432900433 Test-f1: 0.5256318681318681 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13504/20000] Train-Loss: 0.38921288548289756 Test-Loss: 0.13803567628520155 Train-f1: 0.6015740718259499 Test-f1: 0.5165079365079366 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13505/20000] Train-Loss: 0.39027278395010057 Test-Loss: 0.1430000041122758 Train-f1: 0.5582077966183927 Test-f1: 0.5256318681318681 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13506/20000] Train-Loss: 0.3865361971063278 Test-Loss: 0.1392978998600171 Train-f1: 0.5174168735498659 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13507/20000] Train-Loss: 0.3861885584692052 Test-Loss: 0.14739565966451346 Train-f1: 0.601705033953241 Test-f1: 0.475889328063241 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13508/20000] Train-Loss: 0.3912508682189465 Test-Loss: 0.14578767303679557 Train-f1: 0.576104248549504 Test-f1: 0.47158825279442385 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13509/20000] Train-Loss: 0.39030339703424816 Test-Loss: 0.1409241314301719 Train-f1: 0.5840722350543389 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13510/20000] Train-Loss: 0.38881536029576624 Test-Loss: 0.14812336071616225 Train-f1: 0.5075215612128364 Test-f1: 0.475889328063241 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13511/20000] Train-Loss: 0.3907887045609627 Test-Loss: 0.13708073503272147 Train-f1: 0.5178388564605825 Test-f1: 0.4644611528822056 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [13512/20000] Train-Loss: 0.3880671592614195 Test-Loss: 0.13618298655900038 Train-f1: 0.5654274627732085 Test-f1: 0.4395848595848596 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13513/20000] Train-Loss: 0.3849240865669226 Test-Loss: 0.138686493472897 Train-f1: 0.5657739135729007 Test-f1: 0.5308287764866713 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13514/20000] Train-Loss: 0.39009477114916496 Test-Loss: 0.1370786324429062 Train-f1: 0.528904299612713 Test-f1: 0.4644611528822056 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13515/20000] Train-Loss: 0.3865431340345125 Test-Loss: 0.14088089626657563 Train-f1: 0.6111459968602826 Test-f1: 0.4746428571428572 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13516/20000] Train-Loss: 0.3893952479529493 Test-Loss: 0.14217712495854898 Train-f1: 0.5111894636777592 Test-f1: 0.4628528560711606 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13517/20000] Train-Loss: 0.3918153277622977 Test-Loss: 0.14028678939883918 Train-f1: 0.5748855171673963 Test-f1: 0.4774585921325052 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13518/20000] Train-Loss: 0.38860618078478276 Test-Loss: 0.14530174181511551 Train-f1: 0.5137216406781624 Test-f1: 0.516919615198957 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13519/20000] Train-Loss: 0.3873037859225445 Test-Loss: 0.14445361134841792 Train-f1: 0.5801875209734042 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13520/20000] Train-Loss: 0.3854855141725502 Test-Loss: 0.1361934274870352 Train-f1: 0.608077508077508 Test-f1: 0.4428237259816208 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13521/20000] Train-Loss: 0.3861823496337834 Test-Loss: 0.15021772871614109 Train-f1: 0.5111894636777592 Test-f1: 0.549891398923657 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13522/20000] Train-Loss: 0.387382553156906 Test-Loss: 0.1469207719258522 Train-f1: 0.5143148474743887 Test-f1: 0.5519803072744249 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13523/20000] Train-Loss: 0.3947223972844273 Test-Loss: 0.13810197541937036 Train-f1: 0.5175151559257519 Test-f1: 0.5421972621972622 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13524/20000] Train-Loss: 0.38849697770773 Test-Loss: 0.13907918387945975 Train-f1: 0.5855289817110794 Test-f1: 0.5616954528992294 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13525/20000] Train-Loss: 0.3856872580787033 Test-Loss: 0.1429185523095209 Train-f1: 0.5684065313581159 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13526/20000] Train-Loss: 0.38673650310878893 Test-Loss: 0.1431563259111162 Train-f1: 0.5671628241040005 Test-f1: 0.5308287764866713 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13527/20000] Train-Loss: 0.3881367249249691 Test-Loss: 0.1387451534152973 Train-f1: 0.6282612475244054 Test-f1: 0.5155103444886726 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13528/20000] Train-Loss: 0.3851675343395503 Test-Loss: 0.15219857445843957 Train-f1: 0.5195053164052443 Test-f1: 0.5603463203463204 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13529/20000] Train-Loss: 0.3879652565198516 Test-Loss: 0.13755007852595388 Train-f1: 0.5266809678386923 Test-f1: 0.4939348370927319 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13530/20000] Train-Loss: 0.3900855040076394 Test-Loss: 0.14280959269201884 Train-f1: 0.4871138444386853 Test-f1: 0.5624077238550923 Train-acc: 0.8145695364238411 Test-acc: 0.8157894736842105\n",
      "Epoch [13531/20000] Train-Loss: 0.38800214564260643 Test-Loss: 0.14539806118440785 Train-f1: 0.6582546337607516 Test-f1: 0.5133179723502305 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [13532/20000] Train-Loss: 0.3911741151517682 Test-Loss: 0.1455317734636486 Train-f1: 0.6047208895928372 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13533/20000] Train-Loss: 0.39381411190072746 Test-Loss: 0.14344167981993178 Train-f1: 0.555558789995876 Test-f1: 0.5532874968169086 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13534/20000] Train-Loss: 0.3871443190028215 Test-Loss: 0.1378379486239632 Train-f1: 0.5752103226555783 Test-f1: 0.4630985798349032 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13535/20000] Train-Loss: 0.38755255996979326 Test-Loss: 0.13626593158129555 Train-f1: 0.6098728047405505 Test-f1: 0.4428237259816208 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13536/20000] Train-Loss: 0.3873194766333384 Test-Loss: 0.1464120499555332 Train-f1: 0.5189136978113122 Test-f1: 0.5237166085946574 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13537/20000] Train-Loss: 0.38981523987031297 Test-Loss: 0.14979580871749085 Train-f1: 0.5568775595265661 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [13538/20000] Train-Loss: 0.3894786193955075 Test-Loss: 0.1365597561406289 Train-f1: 0.6558574582150741 Test-f1: 0.4939348370927319 Train-acc: 0.847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [13539/20000] Train-Loss: 0.38717466522467486 Test-Loss: 0.14061175292446354 Train-f1: 0.5395463833730367 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13540/20000] Train-Loss: 0.38714107390494 Test-Loss: 0.13688552457992667 Train-f1: 0.5092933732933733 Test-f1: 0.4939348370927319 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13541/20000] Train-Loss: 0.38811862530677516 Test-Loss: 0.14433465901111459 Train-f1: 0.6045452661295425 Test-f1: 0.4656685910608086 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "best score_dx!!\n",
      "Epoch [13542/20000] Train-Loss: 0.3856764003802879 Test-Loss: 0.13615680327463822 Train-f1: 0.5621473720926276 Test-f1: 0.45000000000000007 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13543/20000] Train-Loss: 0.3910803078345605 Test-Loss: 0.14083811904793078 Train-f1: 0.5788720135984773 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13544/20000] Train-Loss: 0.38346501833323393 Test-Loss: 0.13768904570322138 Train-f1: 0.6148866793162096 Test-f1: 0.4528157349896481 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13545/20000] Train-Loss: 0.387935305124017 Test-Loss: 0.140209344246153 Train-f1: 0.4994463850931045 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13546/20000] Train-Loss: 0.39206662891235267 Test-Loss: 0.13991113332510952 Train-f1: 0.6002454919878969 Test-f1: 0.46651321398124457 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13547/20000] Train-Loss: 0.38332981405700317 Test-Loss: 0.13709117287415884 Train-f1: 0.5096607412571162 Test-f1: 0.4644611528822056 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13548/20000] Train-Loss: 0.3866213248702462 Test-Loss: 0.13927948791979108 Train-f1: 0.517294980090428 Test-f1: 0.4746428571428572 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13549/20000] Train-Loss: 0.3884421075196148 Test-Loss: 0.13959693375230814 Train-f1: 0.59076396677312 Test-f1: 0.515048830342948 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13550/20000] Train-Loss: 0.39583568608108805 Test-Loss: 0.13938066330484206 Train-f1: 0.5172658267085511 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13551/20000] Train-Loss: 0.3887757217260582 Test-Loss: 0.14015307734965313 Train-f1: 0.5320888818408516 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13552/20000] Train-Loss: 0.39087189616558704 Test-Loss: 0.14071274157474753 Train-f1: 0.5844298558584273 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13553/20000] Train-Loss: 0.38726274143437606 Test-Loss: 0.1406009781450795 Train-f1: 0.5801674835231883 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13554/20000] Train-Loss: 0.3886397909487366 Test-Loss: 0.1378430895439878 Train-f1: 0.5652504388626955 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13555/20000] Train-Loss: 0.3842265232905475 Test-Loss: 0.14692638208150735 Train-f1: 0.6143597627937624 Test-f1: 0.475889328063241 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [13556/20000] Train-Loss: 0.39274808861353067 Test-Loss: 0.13996590152697744 Train-f1: 0.4931098485278052 Test-f1: 0.526984126984127 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13557/20000] Train-Loss: 0.3887856095096101 Test-Loss: 0.13639674819733694 Train-f1: 0.5871509442938014 Test-f1: 0.4428237259816208 Train-acc: 0.8377483443708609 Test-acc: 0.75\n",
      "Epoch [13558/20000] Train-Loss: 0.37690627726037507 Test-Loss: 0.14318796385719346 Train-f1: 0.5596393780365355 Test-f1: 0.5519803072744249 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13559/20000] Train-Loss: 0.3886680584523712 Test-Loss: 0.1410902691645365 Train-f1: 0.6509677660572801 Test-f1: 0.4746428571428572 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13560/20000] Train-Loss: 0.39116004493311585 Test-Loss: 0.1386452999538409 Train-f1: 0.5893131658577883 Test-f1: 0.4746428571428572 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13561/20000] Train-Loss: 0.3837873396167558 Test-Loss: 0.14735992870486908 Train-f1: 0.5161128526645768 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13562/20000] Train-Loss: 0.3839110094705699 Test-Loss: 0.13967534565230832 Train-f1: 0.5825816132900268 Test-f1: 0.4740245261984392 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13563/20000] Train-Loss: 0.388799482346804 Test-Loss: 0.13722286164309327 Train-f1: 0.5597769534075266 Test-f1: 0.4644611528822056 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13564/20000] Train-Loss: 0.3824658308304024 Test-Loss: 0.1499439018310599 Train-f1: 0.5865998287014413 Test-f1: 0.5237166085946574 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13565/20000] Train-Loss: 0.39519799292355473 Test-Loss: 0.13828766133415268 Train-f1: 0.5613562328521213 Test-f1: 0.4746428571428572 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13566/20000] Train-Loss: 0.38467567732792174 Test-Loss: 0.13692134889022697 Train-f1: 0.5000534553166132 Test-f1: 0.5076190476190476 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13567/20000] Train-Loss: 0.38786228716092513 Test-Loss: 0.14380164019635272 Train-f1: 0.5310843881572602 Test-f1: 0.4656685910608086 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13568/20000] Train-Loss: 0.3849545801442344 Test-Loss: 0.13794438794348907 Train-f1: 0.5968568032409092 Test-f1: 0.44240059457450764 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13569/20000] Train-Loss: 0.3872762723077561 Test-Loss: 0.14467262467903336 Train-f1: 0.5138393645865069 Test-f1: 0.4656685910608086 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13570/20000] Train-Loss: 0.3858293746429056 Test-Loss: 0.13638565477593215 Train-f1: 0.5848740540505246 Test-f1: 0.4712087912087912 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13571/20000] Train-Loss: 0.39117136282379694 Test-Loss: 0.14023600327161614 Train-f1: 0.50153927414582 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13572/20000] Train-Loss: 0.3828959925468803 Test-Loss: 0.13743406017002635 Train-f1: 0.5585108225108225 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13573/20000] Train-Loss: 0.3873008484425826 Test-Loss: 0.14887894884890432 Train-f1: 0.5786606544077968 Test-f1: 0.475889328063241 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13574/20000] Train-Loss: 0.385999486402093 Test-Loss: 0.1394343077415426 Train-f1: 0.5004961574373339 Test-f1: 0.5519803072744249 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13575/20000] Train-Loss: 0.384186439180254 Test-Loss: 0.14535007702478037 Train-f1: 0.53800333000333 Test-f1: 0.5532874968169086 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13576/20000] Train-Loss: 0.38965084938302447 Test-Loss: 0.13725972154866786 Train-f1: 0.5225769464462199 Test-f1: 0.4537451737451737 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [13577/20000] Train-Loss: 0.3904694340789721 Test-Loss: 0.13652454400809272 Train-f1: 0.5102684244964947 Test-f1: 0.4395848595848596 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [13578/20000] Train-Loss: 0.3815371302212736 Test-Loss: 0.14762715833136433 Train-f1: 0.6189952904238619 Test-f1: 0.5519803072744249 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13579/20000] Train-Loss: 0.39298068829951466 Test-Loss: 0.14009767322309524 Train-f1: 0.5480096420066216 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13580/20000] Train-Loss: 0.38504364574767436 Test-Loss: 0.14095149317077257 Train-f1: 0.5646702290997593 Test-f1: 0.5167494824016563 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13581/20000] Train-Loss: 0.3908688450749976 Test-Loss: 0.14100815963692775 Train-f1: 0.5104858641090525 Test-f1: 0.4746428571428572 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13582/20000] Train-Loss: 0.39084069908547986 Test-Loss: 0.14561323868273837 Train-f1: 0.5223117418051386 Test-f1: 0.5133179723502305 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13583/20000] Train-Loss: 0.38721282532742546 Test-Loss: 0.1408712752514252 Train-f1: 0.561559719471884 Test-f1: 0.4774585921325052 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13584/20000] Train-Loss: 0.38939559325773676 Test-Loss: 0.1384567949204515 Train-f1: 0.5992414375380426 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13585/20000] Train-Loss: 0.3859219930655373 Test-Loss: 0.1470818648688319 Train-f1: 0.570805975394088 Test-f1: 0.475889328063241 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13586/20000] Train-Loss: 0.3945208355409775 Test-Loss: 0.1371743652616886 Train-f1: 0.5414212454212455 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13587/20000] Train-Loss: 0.38461377366953353 Test-Loss: 0.14963931673358027 Train-f1: 0.5987368952886194 Test-f1: 0.5244444444444445 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13588/20000] Train-Loss: 0.38258057537270723 Test-Loss: 0.1384744737076096 Train-f1: 0.5915177708968711 Test-f1: 0.5155103444886726 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13589/20000] Train-Loss: 0.39119467627279936 Test-Loss: 0.13786481011082463 Train-f1: 0.49081125891652205 Test-f1: 0.5165079365079366 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [13590/20000] Train-Loss: 0.3860968477924616 Test-Loss: 0.1380169812388541 Train-f1: 0.5251264977330437 Test-f1: 0.5165079365079366 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13591/20000] Train-Loss: 0.38563348392234287 Test-Loss: 0.14472287078916518 Train-f1: 0.611830115216942 Test-f1: 0.4656685910608086 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13592/20000] Train-Loss: 0.38878882049342933 Test-Loss: 0.1374129181656476 Train-f1: 0.5197513303473569 Test-f1: 0.5073487773487774 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13593/20000] Train-Loss: 0.3869619201499424 Test-Loss: 0.13844077121236434 Train-f1: 0.5582077966183927 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13594/20000] Train-Loss: 0.3912065230141617 Test-Loss: 0.14132615031460222 Train-f1: 0.5107825507825507 Test-f1: 0.5519803072744249 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13595/20000] Train-Loss: 0.3878956218079003 Test-Loss: 0.13958240199440253 Train-f1: 0.6029669707213091 Test-f1: 0.5161904761904762 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13596/20000] Train-Loss: 0.3888281706823633 Test-Loss: 0.1432396861210806 Train-f1: 0.5823296252277183 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13597/20000] Train-Loss: 0.38747941218679044 Test-Loss: 0.15073789553356332 Train-f1: 0.5166689157325537 Test-f1: 0.5272601794340925 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13598/20000] Train-Loss: 0.38430989685245115 Test-Loss: 0.13854668391087946 Train-f1: 0.5174248366013072 Test-f1: 0.526984126984127 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13599/20000] Train-Loss: 0.3870566506386134 Test-Loss: 0.14034913572076685 Train-f1: 0.5597635852412923 Test-f1: 0.4774585921325052 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13600/20000] Train-Loss: 0.3781450945955624 Test-Loss: 0.15014924239485153 Train-f1: 0.5974742298099771 Test-f1: 0.5244444444444445 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [13601/20000] Train-Loss: 0.3872692231359038 Test-Loss: 0.14175232938029397 Train-f1: 0.5967594515457462 Test-f1: 0.4493236714975846 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13602/20000] Train-Loss: 0.3850426990110919 Test-Loss: 0.1461536040739121 Train-f1: 0.4923825952458074 Test-f1: 0.5718776875298615 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13603/20000] Train-Loss: 0.39297583488505394 Test-Loss: 0.14738005112032876 Train-f1: 0.5790705987671172 Test-f1: 0.5603463203463204 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13604/20000] Train-Loss: 0.3844130873953547 Test-Loss: 0.13693061243372787 Train-f1: 0.5614111901170724 Test-f1: 0.5076190476190476 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13605/20000] Train-Loss: 0.38831084861423837 Test-Loss: 0.13673080875297586 Train-f1: 0.49869164146217937 Test-f1: 0.5076190476190476 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13606/20000] Train-Loss: 0.39083633223917663 Test-Loss: 0.13941753394215062 Train-f1: 0.6013009978527221 Test-f1: 0.46727688787185356 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13607/20000] Train-Loss: 0.38955316352715713 Test-Loss: 0.14738815891879575 Train-f1: 0.5180927152317881 Test-f1: 0.46539589442815255 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [13608/20000] Train-Loss: 0.38752527962621464 Test-Loss: 0.14495373292546324 Train-f1: 0.5790965790965792 Test-f1: 0.4656685910608086 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13609/20000] Train-Loss: 0.38749307976261155 Test-Loss: 0.14209485122932625 Train-f1: 0.5146104163320387 Test-f1: 0.5256318681318681 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13610/20000] Train-Loss: 0.38391614645882666 Test-Loss: 0.1363919887432336 Train-f1: 0.5900971867007673 Test-f1: 0.4395848595848596 Train-acc: 0.8410596026490066 Test-acc: 0.75\n",
      "Epoch [13611/20000] Train-Loss: 0.3874389047877768 Test-Loss: 0.13821175998493818 Train-f1: 0.5168255168255168 Test-f1: 0.4636974789915966 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13612/20000] Train-Loss: 0.3849798058751949 Test-Loss: 0.1370566627901014 Train-f1: 0.5826200625177608 Test-f1: 0.4644611528822056 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13613/20000] Train-Loss: 0.39021841934798757 Test-Loss: 0.1365674761753461 Train-f1: 0.5266158841158841 Test-f1: 0.5056027456027457 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [13614/20000] Train-Loss: 0.3870168531179387 Test-Loss: 0.14729562541726707 Train-f1: 0.5893028667657176 Test-f1: 0.4815458937198067 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13615/20000] Train-Loss: 0.38987179009638684 Test-Loss: 0.14464643021542753 Train-f1: 0.5640255692020397 Test-f1: 0.5133179723502305 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13616/20000] Train-Loss: 0.3886791582281738 Test-Loss: 0.14159278424530497 Train-f1: 0.49443085456075275 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [13617/20000] Train-Loss: 0.37840845449351923 Test-Loss: 0.142201250596157 Train-f1: 0.5287594515457461 Test-f1: 0.5422818042172881 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13618/20000] Train-Loss: 0.3857502322403427 Test-Loss: 0.13917587719036806 Train-f1: 0.6119011993464548 Test-f1: 0.46727688787185356 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13619/20000] Train-Loss: 0.386115532400481 Test-Loss: 0.13836229481298415 Train-f1: 0.49771861248371313 Test-f1: 0.5155103444886726 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13620/20000] Train-Loss: 0.38414218983545817 Test-Loss: 0.13790951386326303 Train-f1: 0.6263515897786998 Test-f1: 0.5183710801393728 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13621/20000] Train-Loss: 0.38932602577081304 Test-Loss: 0.14427758675733582 Train-f1: 0.6086882486882488 Test-f1: 0.5519803072744249 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13622/20000] Train-Loss: 0.38663019456286335 Test-Loss: 0.1480880944228978 Train-f1: 0.5873254313605486 Test-f1: 0.5532874968169086 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13623/20000] Train-Loss: 0.39061997515977465 Test-Loss: 0.14487634729869395 Train-f1: 0.5166783151056744 Test-f1: 0.5133179723502305 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13624/20000] Train-Loss: 0.38751841335419784 Test-Loss: 0.13656640702804076 Train-f1: 0.6073239552231149 Test-f1: 0.4944577129097253 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [13625/20000] Train-Loss: 0.3882152623336261 Test-Loss: 0.14075250090179545 Train-f1: 0.5320017206798648 Test-f1: 0.5519803072744249 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13626/20000] Train-Loss: 0.38426768182442156 Test-Loss: 0.1385316013419476 Train-f1: 0.5011165435006494 Test-f1: 0.5422818042172881 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13627/20000] Train-Loss: 0.38747813179580254 Test-Loss: 0.13908377049748508 Train-f1: 0.5970828041553888 Test-f1: 0.4565609087348218 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13628/20000] Train-Loss: 0.38541476043024747 Test-Loss: 0.14222169856459085 Train-f1: 0.5170969320534538 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13629/20000] Train-Loss: 0.38948508183212505 Test-Loss: 0.13983490126642079 Train-f1: 0.611504826269927 Test-f1: 0.5262301587301588 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13630/20000] Train-Loss: 0.3890471324654304 Test-Loss: 0.13943222316101905 Train-f1: 0.5288921028051463 Test-f1: 0.5532874968169086 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13631/20000] Train-Loss: 0.3808894776874222 Test-Loss: 0.13964687559065883 Train-f1: 0.5147737591832214 Test-f1: 0.47285467739587883 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13632/20000] Train-Loss: 0.38644751454672566 Test-Loss: 0.14780226557368709 Train-f1: 0.5244719328991915 Test-f1: 0.4815458937198067 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13633/20000] Train-Loss: 0.386566023475016 Test-Loss: 0.14363500570828822 Train-f1: 0.6306117306117306 Test-f1: 0.4656685910608086 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13634/20000] Train-Loss: 0.38679427303022623 Test-Loss: 0.13876415664835137 Train-f1: 0.5619156734115618 Test-f1: 0.4565609087348218 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13635/20000] Train-Loss: 0.38645848069418004 Test-Loss: 0.1376649338431132 Train-f1: 0.4983791458768711 Test-f1: 0.4644611528822056 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13636/20000] Train-Loss: 0.3845470846291957 Test-Loss: 0.13791990096291826 Train-f1: 0.5828541876400708 Test-f1: 0.5155103444886726 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13637/20000] Train-Loss: 0.38461134772803557 Test-Loss: 0.14357761974468752 Train-f1: 0.6383007397400201 Test-f1: 0.5141038802093091 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13638/20000] Train-Loss: 0.38708969794615816 Test-Loss: 0.14147668212326364 Train-f1: 0.6335594931384405 Test-f1: 0.48343865857768786 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13639/20000] Train-Loss: 0.3860730186121455 Test-Loss: 0.13642892926232458 Train-f1: 0.5851789722999203 Test-f1: 0.5073487773487774 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13640/20000] Train-Loss: 0.3903448971875121 Test-Loss: 0.1382055960930564 Train-f1: 0.5719218615099622 Test-f1: 0.4636974789915966 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13641/20000] Train-Loss: 0.38685163989479604 Test-Loss: 0.13620071906055092 Train-f1: 0.5845343671150123 Test-f1: 0.4537451737451737 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13642/20000] Train-Loss: 0.3842469722994635 Test-Loss: 0.1427127213486561 Train-f1: 0.4949120885426618 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13643/20000] Train-Loss: 0.38218161739304757 Test-Loss: 0.1386018239236046 Train-f1: 0.5854494498789801 Test-f1: 0.515048830342948 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13644/20000] Train-Loss: 0.3824741700976782 Test-Loss: 0.14288636603587865 Train-f1: 0.5463018575896599 Test-f1: 0.4628528560711606 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13645/20000] Train-Loss: 0.38704590317430765 Test-Loss: 0.1375571266463068 Train-f1: 0.48611334052020955 Test-f1: 0.47501831501831504 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [13646/20000] Train-Loss: 0.3870872327121786 Test-Loss: 0.14672386648009256 Train-f1: 0.5053391053391054 Test-f1: 0.5381015037593986 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13647/20000] Train-Loss: 0.3913045584892071 Test-Loss: 0.1386738218905191 Train-f1: 0.4949120885426618 Test-f1: 0.5281271777003484 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13648/20000] Train-Loss: 0.38578277775564135 Test-Loss: 0.13837027285257994 Train-f1: 0.5682370962370962 Test-f1: 0.5308287764866713 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13649/20000] Train-Loss: 0.3835928233204513 Test-Loss: 0.1406557016082251 Train-f1: 0.5299294375513066 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13650/20000] Train-Loss: 0.3861336254304349 Test-Loss: 0.14320140275500087 Train-f1: 0.5659340659340659 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13651/20000] Train-Loss: 0.3859824011719118 Test-Loss: 0.13913721484490585 Train-f1: 0.5671935676189556 Test-f1: 0.47783405000796303 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13652/20000] Train-Loss: 0.39095305569166555 Test-Loss: 0.13965569392949592 Train-f1: 0.4913506035017413 Test-f1: 0.5532874968169086 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13653/20000] Train-Loss: 0.3885248440654354 Test-Loss: 0.13976000829576182 Train-f1: 0.5072046928852425 Test-f1: 0.5256318681318681 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13654/20000] Train-Loss: 0.3870787291398877 Test-Loss: 0.14251376105032093 Train-f1: 0.5791446648589507 Test-f1: 0.5309429126899965 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13655/20000] Train-Loss: 0.3902909022206275 Test-Loss: 0.14169140420933052 Train-f1: 0.5770306823843616 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13656/20000] Train-Loss: 0.3915314957574289 Test-Loss: 0.1395770749436354 Train-f1: 0.5269127530610906 Test-f1: 0.5532874968169086 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13657/20000] Train-Loss: 0.3900275250926628 Test-Loss: 0.1414746116860573 Train-f1: 0.5795930082988906 Test-f1: 0.5262301587301588 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13658/20000] Train-Loss: 0.38330373540992535 Test-Loss: 0.14469610215599216 Train-f1: 0.5605105278654168 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13659/20000] Train-Loss: 0.3868998295497219 Test-Loss: 0.13667682318457922 Train-f1: 0.585465721159085 Test-f1: 0.4944577129097253 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13660/20000] Train-Loss: 0.38801385833546226 Test-Loss: 0.14191571480854645 Train-f1: 0.6213084297356883 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [13661/20000] Train-Loss: 0.38796201823006027 Test-Loss: 0.1378090153558483 Train-f1: 0.4994463850931045 Test-f1: 0.5421972621972622 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13662/20000] Train-Loss: 0.3861853508983131 Test-Loss: 0.1426000823592342 Train-f1: 0.5294043180824622 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13663/20000] Train-Loss: 0.3876951316425185 Test-Loss: 0.1390834245534414 Train-f1: 0.6212529973582605 Test-f1: 0.5161904761904762 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13664/20000] Train-Loss: 0.38828579077008574 Test-Loss: 0.14026215015514848 Train-f1: 0.6254402438374014 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13665/20000] Train-Loss: 0.3793894605373413 Test-Loss: 0.15008751406616533 Train-f1: 0.5743385994998899 Test-f1: 0.5244444444444445 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13666/20000] Train-Loss: 0.39099589657047784 Test-Loss: 0.14460521456061692 Train-f1: 0.5718293731364488 Test-f1: 0.4656685910608086 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [13667/20000] Train-Loss: 0.38593859106420964 Test-Loss: 0.13635423530601196 Train-f1: 0.5852414252414253 Test-f1: 0.4944577129097253 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13668/20000] Train-Loss: 0.39113169169409934 Test-Loss: 0.1366633824329844 Train-f1: 0.5087443342220412 Test-f1: 0.5165079365079366 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13669/20000] Train-Loss: 0.38823641779029466 Test-Loss: 0.14444908085251515 Train-f1: 0.5167655903778471 Test-f1: 0.5161751152073732 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13670/20000] Train-Loss: 0.3806473363818332 Test-Loss: 0.13886206820509175 Train-f1: 0.5776623444451591 Test-f1: 0.4596816087138668 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13671/20000] Train-Loss: 0.3858671719559181 Test-Loss: 0.14422932052717316 Train-f1: 0.5842395756264771 Test-f1: 0.4656685910608086 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13672/20000] Train-Loss: 0.3917081885500072 Test-Loss: 0.13700084148969718 Train-f1: 0.5827990544924183 Test-f1: 0.49631578947368427 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13673/20000] Train-Loss: 0.39223634587375766 Test-Loss: 0.14114330264385347 Train-f1: 0.5110615868087292 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13674/20000] Train-Loss: 0.3796808272159023 Test-Loss: 0.13788367937020002 Train-f1: 0.5835994069268484 Test-f1: 0.4602828448452552 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13675/20000] Train-Loss: 0.3907709718045716 Test-Loss: 0.1457400523464975 Train-f1: 0.49771861248371313 Test-f1: 0.5244444444444445 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13676/20000] Train-Loss: 0.38703371713705853 Test-Loss: 0.13872589237447788 Train-f1: 0.4955866287135636 Test-f1: 0.5308287764866713 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13677/20000] Train-Loss: 0.39057798725378934 Test-Loss: 0.13975317273697052 Train-f1: 0.5268138766697282 Test-f1: 0.5625084825084825 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13678/20000] Train-Loss: 0.38518158719631634 Test-Loss: 0.13736467275291928 Train-f1: 0.4953523427975983 Test-f1: 0.5165079365079366 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13679/20000] Train-Loss: 0.38230841148743866 Test-Loss: 0.14361128781198276 Train-f1: 0.5660910518053375 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13680/20000] Train-Loss: 0.38346484638213285 Test-Loss: 0.13649990281193808 Train-f1: 0.5321295499589114 Test-f1: 0.4939348370927319 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13681/20000] Train-Loss: 0.38569565395330785 Test-Loss: 0.14040950248532058 Train-f1: 0.5202772098974393 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13682/20000] Train-Loss: 0.3883947808762018 Test-Loss: 0.13986010868227675 Train-f1: 0.49821820766093217 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13683/20000] Train-Loss: 0.3906256131810304 Test-Loss: 0.13802666444797937 Train-f1: 0.5087584545526369 Test-f1: 0.5155103444886726 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13684/20000] Train-Loss: 0.38863396746098555 Test-Loss: 0.13993145083972913 Train-f1: 0.562536800947397 Test-f1: 0.5309429126899965 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13685/20000] Train-Loss: 0.3818295645063345 Test-Loss: 0.14778813543018446 Train-f1: 0.6130642708671423 Test-f1: 0.516919615198957 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13686/20000] Train-Loss: 0.3879821368182745 Test-Loss: 0.13686971507385604 Train-f1: 0.6019202329761336 Test-f1: 0.5308287764866713 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13687/20000] Train-Loss: 0.38434894029962624 Test-Loss: 0.14031299828145244 Train-f1: 0.5149966243275543 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13688/20000] Train-Loss: 0.3860567444447955 Test-Loss: 0.13997431454219172 Train-f1: 0.593513974609865 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13689/20000] Train-Loss: 0.3885738477179028 Test-Loss: 0.13672559347675517 Train-f1: 0.6564070751276072 Test-f1: 0.5181595881595882 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [13690/20000] Train-Loss: 0.3798825491915414 Test-Loss: 0.15126288745248564 Train-f1: 0.5249336249336249 Test-f1: 0.46520247083047356 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13691/20000] Train-Loss: 0.3782734925999301 Test-Loss: 0.1388481114490316 Train-f1: 0.6126997958472341 Test-f1: 0.4630985798349032 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13692/20000] Train-Loss: 0.3843135394795122 Test-Loss: 0.13703107664281117 Train-f1: 0.49908926129961745 Test-f1: 0.5181595881595882 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13693/20000] Train-Loss: 0.3883877062773729 Test-Loss: 0.13796461296640997 Train-f1: 0.5656396936396937 Test-f1: 0.46727688787185356 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13694/20000] Train-Loss: 0.38370591126292586 Test-Loss: 0.13965154703691263 Train-f1: 0.6257746668342695 Test-f1: 0.5256318681318681 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13695/20000] Train-Loss: 0.3860882030646321 Test-Loss: 0.13961225812858558 Train-f1: 0.6073675181933134 Test-f1: 0.4746428571428572 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13696/20000] Train-Loss: 0.38570102345852386 Test-Loss: 0.13879961590187553 Train-f1: 0.5220923220923221 Test-f1: 0.4767762460233298 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13697/20000] Train-Loss: 0.38294202513667336 Test-Loss: 0.15381480229013375 Train-f1: 0.5896424877316597 Test-f1: 0.5145210340566151 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13698/20000] Train-Loss: 0.3862372921847531 Test-Loss: 0.1477687066147164 Train-f1: 0.5673822512485593 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13699/20000] Train-Loss: 0.3883599372555261 Test-Loss: 0.13888593854503542 Train-f1: 0.5798983867300354 Test-f1: 0.5255137844611528 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13700/20000] Train-Loss: 0.3903020825821107 Test-Loss: 0.13914981672317983 Train-f1: 0.5848628323080878 Test-f1: 0.5256318681318681 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13701/20000] Train-Loss: 0.38509131461099244 Test-Loss: 0.141533958922825 Train-f1: 0.6197399382307015 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13702/20000] Train-Loss: 0.384032152135299 Test-Loss: 0.13781743331248708 Train-f1: 0.6065401459854015 Test-f1: 0.5181595881595882 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13703/20000] Train-Loss: 0.3892279561746629 Test-Loss: 0.13892649155453407 Train-f1: 0.5800635965341847 Test-f1: 0.5272243672243672 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13704/20000] Train-Loss: 0.38654486950404293 Test-Loss: 0.13763085187891122 Train-f1: 0.5741208919502534 Test-f1: 0.4636974789915966 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13705/20000] Train-Loss: 0.3819551911688062 Test-Loss: 0.14773122171375913 Train-f1: 0.5170851496657949 Test-f1: 0.5381015037593986 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13706/20000] Train-Loss: 0.3816789324407601 Test-Loss: 0.14498930268358515 Train-f1: 0.5504256023012376 Test-f1: 0.4656685910608086 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13707/20000] Train-Loss: 0.3893715013372632 Test-Loss: 0.1427764413008348 Train-f1: 0.5964940376705082 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "best score_dx!!\n",
      "Epoch [13708/20000] Train-Loss: 0.39234029798098974 Test-Loss: 0.13579745662228648 Train-f1: 0.5022875334955872 Test-f1: 0.4395848595848596 Train-acc: 0.8112582781456954 Test-acc: 0.75\n",
      "Epoch [13709/20000] Train-Loss: 0.38429353838621544 Test-Loss: 0.13864210333727509 Train-f1: 0.6021656421545144 Test-f1: 0.5272243672243672 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13710/20000] Train-Loss: 0.3842624044334219 Test-Loss: 0.13688572200774488 Train-f1: 0.4906454091432962 Test-f1: 0.4939348370927319 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [13711/20000] Train-Loss: 0.3877096618734949 Test-Loss: 0.13806269897288798 Train-f1: 0.516894285152613 Test-f1: 0.5421972621972622 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13712/20000] Train-Loss: 0.38462186659066366 Test-Loss: 0.14633295563703683 Train-f1: 0.5092933732933733 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [13713/20000] Train-Loss: 0.38634067665371136 Test-Loss: 0.1412159068514895 Train-f1: 0.5859340659340659 Test-f1: 0.5624077238550923 Train-acc: 0.8178807947019867 Test-acc: 0.8157894736842105\n",
      "Epoch [13714/20000] Train-Loss: 0.3861429369792879 Test-Loss: 0.1374743159042334 Train-f1: 0.6014154584910887 Test-f1: 0.528796992481203 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13715/20000] Train-Loss: 0.3856571491736387 Test-Loss: 0.14059829652404063 Train-f1: 0.5223117418051386 Test-f1: 0.5308287764866713 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13716/20000] Train-Loss: 0.38526830965234843 Test-Loss: 0.13668169677056585 Train-f1: 0.5144718333456603 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13717/20000] Train-Loss: 0.386928504912635 Test-Loss: 0.14298950215520673 Train-f1: 0.5114654226574153 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13718/20000] Train-Loss: 0.3858491941357249 Test-Loss: 0.13892401140405963 Train-f1: 0.6254402438374014 Test-f1: 0.526984126984127 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13719/20000] Train-Loss: 0.3842020494459188 Test-Loss: 0.13913346266276513 Train-f1: 0.5852482799566934 Test-f1: 0.5262301587301588 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13720/20000] Train-Loss: 0.3889848441583212 Test-Loss: 0.13692585133901466 Train-f1: 0.5875407207171913 Test-f1: 0.5051919146036793 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13721/20000] Train-Loss: 0.38444784284982286 Test-Loss: 0.14021900178802169 Train-f1: 0.6020215385424244 Test-f1: 0.4774585921325052 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13722/20000] Train-Loss: 0.386689285810122 Test-Loss: 0.13994692324502953 Train-f1: 0.5989934266646595 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13723/20000] Train-Loss: 0.3888929854704862 Test-Loss: 0.13681199132538469 Train-f1: 0.5009701856451082 Test-f1: 0.4644611528822056 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13724/20000] Train-Loss: 0.3832633505202295 Test-Loss: 0.14053588728956376 Train-f1: 0.4973471374918029 Test-f1: 0.5256318681318681 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13725/20000] Train-Loss: 0.38275961322536767 Test-Loss: 0.13872087497967667 Train-f1: 0.5944987662755236 Test-f1: 0.4565609087348218 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13726/20000] Train-Loss: 0.3851022103347045 Test-Loss: 0.13740580140439926 Train-f1: 0.5628468176119182 Test-f1: 0.44240059457450764 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [13727/20000] Train-Loss: 0.38598011642359314 Test-Loss: 0.13969740174027379 Train-f1: 0.6528462405174734 Test-f1: 0.5281271777003484 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [13728/20000] Train-Loss: 0.3857379665432898 Test-Loss: 0.1408279501757883 Train-f1: 0.5712201001612767 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13729/20000] Train-Loss: 0.38709109022614474 Test-Loss: 0.14024177094651177 Train-f1: 0.5194675803573875 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13730/20000] Train-Loss: 0.3841982736479453 Test-Loss: 0.13834847933251587 Train-f1: 0.6052079026998936 Test-f1: 0.5155103444886726 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13731/20000] Train-Loss: 0.383939501823171 Test-Loss: 0.15102977641289334 Train-f1: 0.5781786983348466 Test-f1: 0.5603463203463204 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [13732/20000] Train-Loss: 0.3855324549770081 Test-Loss: 0.1382918342460918 Train-f1: 0.6125588697017268 Test-f1: 0.46727688787185356 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13733/20000] Train-Loss: 0.38362076236715353 Test-Loss: 0.14220132668981794 Train-f1: 0.5177217917992616 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13734/20000] Train-Loss: 0.38995118723801464 Test-Loss: 0.13867559079299457 Train-f1: 0.644911709295271 Test-f1: 0.5519803072744249 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13735/20000] Train-Loss: 0.3859134992084058 Test-Loss: 0.1436558522347123 Train-f1: 0.5898936878091137 Test-f1: 0.5290458937198068 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13736/20000] Train-Loss: 0.3757887532378062 Test-Loss: 0.13757145938385115 Train-f1: 0.5937778452231007 Test-f1: 0.4630985798349032 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13737/20000] Train-Loss: 0.3859499243228355 Test-Loss: 0.14100366390798755 Train-f1: 0.5146746860030849 Test-f1: 0.5641558441558441 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13738/20000] Train-Loss: 0.38421840294952836 Test-Loss: 0.14038573249021066 Train-f1: 0.5999096570858585 Test-f1: 0.5421972621972622 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13739/20000] Train-Loss: 0.3827882254130445 Test-Loss: 0.1385879333508851 Train-f1: 0.5198923826878306 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13740/20000] Train-Loss: 0.38443112431027315 Test-Loss: 0.14011603201582207 Train-f1: 0.6579975579975581 Test-f1: 0.5281271777003484 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [13741/20000] Train-Loss: 0.3823990576136209 Test-Loss: 0.13626221205697514 Train-f1: 0.5773651053812298 Test-f1: 0.4944577129097253 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13742/20000] Train-Loss: 0.3861045947855046 Test-Loss: 0.14407127901794095 Train-f1: 0.5840722350543389 Test-f1: 0.5133179723502305 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13743/20000] Train-Loss: 0.3824503388362672 Test-Loss: 0.13727304411530475 Train-f1: 0.569701726844584 Test-f1: 0.5165079365079366 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13744/20000] Train-Loss: 0.38633704670683383 Test-Loss: 0.1374813017842414 Train-f1: 0.6072849921569398 Test-f1: 0.5165079365079366 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13745/20000] Train-Loss: 0.3820481885222243 Test-Loss: 0.1376417317628772 Train-f1: 0.6429479815669075 Test-f1: 0.44240059457450764 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13746/20000] Train-Loss: 0.3842123975923943 Test-Loss: 0.13747033730623254 Train-f1: 0.5202150920109626 Test-f1: 0.4944577129097253 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13747/20000] Train-Loss: 0.3866724083274736 Test-Loss: 0.13831109341844539 Train-f1: 0.5489821436841305 Test-f1: 0.5422818042172881 Train-acc: 0.8112582781456954 Test-acc: 0.7894736842105263\n",
      "Epoch [13748/20000] Train-Loss: 0.3842199171535406 Test-Loss: 0.13667216686836955 Train-f1: 0.5852131922373951 Test-f1: 0.5051919146036793 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13749/20000] Train-Loss: 0.38416421809971363 Test-Loss: 0.1398094859729753 Train-f1: 0.49904919877965703 Test-f1: 0.5256318681318681 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13750/20000] Train-Loss: 0.3874535622942723 Test-Loss: 0.13850702118773098 Train-f1: 0.5177297548507029 Test-f1: 0.5421972621972622 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13751/20000] Train-Loss: 0.3835298261014796 Test-Loss: 0.1419199802300982 Train-f1: 0.6359908164868876 Test-f1: 0.5161751152073732 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13752/20000] Train-Loss: 0.38556794718289866 Test-Loss: 0.13884400950321552 Train-f1: 0.5820739534023522 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13753/20000] Train-Loss: 0.3886113244252175 Test-Loss: 0.14135943903161968 Train-f1: 0.6192549444162347 Test-f1: 0.5519803072744249 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13754/20000] Train-Loss: 0.3903111879812314 Test-Loss: 0.13759998295508113 Train-f1: 0.5768679175834734 Test-f1: 0.5161904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13755/20000] Train-Loss: 0.38692288498069816 Test-Loss: 0.14063136117165545 Train-f1: 0.536768605595259 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13756/20000] Train-Loss: 0.38597145054046633 Test-Loss: 0.15027221951331723 Train-f1: 0.5797152461427508 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13757/20000] Train-Loss: 0.38801079944766187 Test-Loss: 0.13712605134819147 Train-f1: 0.5750661892597376 Test-f1: 0.5051919146036793 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13758/20000] Train-Loss: 0.37938731330670933 Test-Loss: 0.14367664272657416 Train-f1: 0.6209641950588866 Test-f1: 0.4656685910608086 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [13759/20000] Train-Loss: 0.3816510383461338 Test-Loss: 0.14410295261729597 Train-f1: 0.6091787670909315 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13760/20000] Train-Loss: 0.38029111749599903 Test-Loss: 0.1475873293060308 Train-f1: 0.6438217059995117 Test-f1: 0.5631620553359683 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13761/20000] Train-Loss: 0.3834624313275682 Test-Loss: 0.1370243930513569 Train-f1: 0.5492569714535136 Test-f1: 0.4644611528822056 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13762/20000] Train-Loss: 0.38606021930510254 Test-Loss: 0.1360475541123416 Train-f1: 0.4955791070243626 Test-f1: 0.4428237259816208 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13763/20000] Train-Loss: 0.38424631615333726 Test-Loss: 0.13982390975882644 Train-f1: 0.6188842364917752 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13764/20000] Train-Loss: 0.3811416101883084 Test-Loss: 0.14115335736722776 Train-f1: 0.5581075268817204 Test-f1: 0.5641558441558441 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [13765/20000] Train-Loss: 0.38893720303388113 Test-Loss: 0.13984102283441827 Train-f1: 0.5268069154850596 Test-f1: 0.5256318681318681 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13766/20000] Train-Loss: 0.3857217171166253 Test-Loss: 0.13724714347681002 Train-f1: 0.5198801586091775 Test-f1: 0.4939348370927319 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13767/20000] Train-Loss: 0.3847963140938906 Test-Loss: 0.13619163612707275 Train-f1: 0.4969716552220693 Test-f1: 0.49631578947368427 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [13768/20000] Train-Loss: 0.38374282247759295 Test-Loss: 0.14217243447672415 Train-f1: 0.5991572632526041 Test-f1: 0.5256318681318681 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13769/20000] Train-Loss: 0.3809828926517198 Test-Loss: 0.13951483189706826 Train-f1: 0.564938320879892 Test-f1: 0.5256318681318681 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13770/20000] Train-Loss: 0.3841125767046831 Test-Loss: 0.1404096462212468 Train-f1: 0.517405651807429 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13771/20000] Train-Loss: 0.38368942264481076 Test-Loss: 0.13682126846488174 Train-f1: 0.6349138847804031 Test-f1: 0.4602828448452552 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13772/20000] Train-Loss: 0.3876812457839812 Test-Loss: 0.13939843808140304 Train-f1: 0.6335306206515687 Test-f1: 0.5308287764866713 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13773/20000] Train-Loss: 0.3848439603040696 Test-Loss: 0.13855153015290644 Train-f1: 0.502457054439813 Test-f1: 0.46651321398124457 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13774/20000] Train-Loss: 0.38960194221516814 Test-Loss: 0.14209374090272436 Train-f1: 0.638385775298527 Test-f1: 0.4774585921325052 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13775/20000] Train-Loss: 0.38442406873912244 Test-Loss: 0.14262083318137936 Train-f1: 0.5311674083823387 Test-f1: 0.4656685910608086 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13776/20000] Train-Loss: 0.38823628097567114 Test-Loss: 0.14827262024473636 Train-f1: 0.5148024944648986 Test-f1: 0.5237166085946574 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13777/20000] Train-Loss: 0.38853544252674577 Test-Loss: 0.1375168751343326 Train-f1: 0.555247142157933 Test-f1: 0.5421972621972622 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [13778/20000] Train-Loss: 0.38920957705665205 Test-Loss: 0.13757705130049355 Train-f1: 0.5850395508987354 Test-f1: 0.46727688787185356 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13779/20000] Train-Loss: 0.38770319255180785 Test-Loss: 0.13799082069866495 Train-f1: 0.6020728299950902 Test-f1: 0.5161904761904762 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13780/20000] Train-Loss: 0.38366420225644765 Test-Loss: 0.14437944958303656 Train-f1: 0.4904782695804367 Test-f1: 0.5624077238550923 Train-acc: 0.8178807947019867 Test-acc: 0.8157894736842105\n",
      "Epoch [13781/20000] Train-Loss: 0.3842299145470718 Test-Loss: 0.13736541603048263 Train-f1: 0.5660855614108439 Test-f1: 0.4939348370927319 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13782/20000] Train-Loss: 0.3785652998211377 Test-Loss: 0.1471132281635011 Train-f1: 0.6315934798730212 Test-f1: 0.5237166085946574 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13783/20000] Train-Loss: 0.3800045372885786 Test-Loss: 0.1467866339326787 Train-f1: 0.5687912087912088 Test-f1: 0.5495545314900154 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13784/20000] Train-Loss: 0.37506534854822726 Test-Loss: 0.14926240996246937 Train-f1: 0.5351474069177208 Test-f1: 0.5603463203463204 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13785/20000] Train-Loss: 0.3869417940029654 Test-Loss: 0.1371061140942575 Train-f1: 0.6313594289384673 Test-f1: 0.5308287764866713 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13786/20000] Train-Loss: 0.38719109082878744 Test-Loss: 0.13642186911031265 Train-f1: 0.589715046857904 Test-f1: 0.4537451737451737 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [13787/20000] Train-Loss: 0.3825790657550294 Test-Loss: 0.14610219273987793 Train-f1: 0.5064746064746065 Test-f1: 0.5264562569213732 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13788/20000] Train-Loss: 0.38563942773544696 Test-Loss: 0.1490570193917435 Train-f1: 0.5868992363539087 Test-f1: 0.475889328063241 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13789/20000] Train-Loss: 0.3895864053437996 Test-Loss: 0.1416329800643594 Train-f1: 0.5124412062293766 Test-f1: 0.4628528560711606 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13790/20000] Train-Loss: 0.3861798592173365 Test-Loss: 0.14192580083204442 Train-f1: 0.5168255168255168 Test-f1: 0.5641558441558441 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13791/20000] Train-Loss: 0.3884464303288002 Test-Loss: 0.13773238115716818 Train-f1: 0.6258848152519331 Test-f1: 0.5181595881595882 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13792/20000] Train-Loss: 0.3780483315854125 Test-Loss: 0.14935804255808033 Train-f1: 0.5890151515151516 Test-f1: 0.549891398923657 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13793/20000] Train-Loss: 0.3797405790405521 Test-Loss: 0.13773009995667543 Train-f1: 0.5492925285893862 Test-f1: 0.5051919146036793 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13794/20000] Train-Loss: 0.38709828001614066 Test-Loss: 0.14041834785267637 Train-f1: 0.6161348585189644 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13795/20000] Train-Loss: 0.38571405948042026 Test-Loss: 0.14244520608427916 Train-f1: 0.5143619691270698 Test-f1: 0.4774585921325052 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13796/20000] Train-Loss: 0.37621548136181726 Test-Loss: 0.13914115156437362 Train-f1: 0.49926739926739927 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13797/20000] Train-Loss: 0.3817489139139454 Test-Loss: 0.14125751553795016 Train-f1: 0.5306699589718458 Test-f1: 0.5641558441558441 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13798/20000] Train-Loss: 0.38899643435759695 Test-Loss: 0.1378670455489566 Train-f1: 0.5995358785600814 Test-f1: 0.5272243672243672 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13799/20000] Train-Loss: 0.38001468292833446 Test-Loss: 0.1380808005569112 Train-f1: 0.5876285680710749 Test-f1: 0.4740245261984392 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13800/20000] Train-Loss: 0.38584443075334734 Test-Loss: 0.13806007139632612 Train-f1: 0.5144658877241687 Test-f1: 0.4596816087138668 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13801/20000] Train-Loss: 0.3839564836869565 Test-Loss: 0.1366752906982527 Train-f1: 0.5200086941339737 Test-f1: 0.4537451737451737 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13802/20000] Train-Loss: 0.3853904001919888 Test-Loss: 0.15868962980562518 Train-f1: 0.6357219183025634 Test-f1: 0.46947869348907634 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13803/20000] Train-Loss: 0.3900721026118653 Test-Loss: 0.14150252444375808 Train-f1: 0.5890601525995149 Test-f1: 0.5256318681318681 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13804/20000] Train-Loss: 0.38265438751280095 Test-Loss: 0.14125349119015024 Train-f1: 0.5170465328854591 Test-f1: 0.5309429126899965 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13805/20000] Train-Loss: 0.38327873932967776 Test-Loss: 0.14534570246840925 Train-f1: 0.5197310966438483 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13806/20000] Train-Loss: 0.3858908453151591 Test-Loss: 0.13995092687814908 Train-f1: 0.5316751722199807 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13807/20000] Train-Loss: 0.3837489266459553 Test-Loss: 0.13914345679301632 Train-f1: 0.5174168735498659 Test-f1: 0.5281271777003484 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13808/20000] Train-Loss: 0.3835751742574618 Test-Loss: 0.13841675905361234 Train-f1: 0.523124183006536 Test-f1: 0.5161904761904762 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13809/20000] Train-Loss: 0.37999475186609916 Test-Loss: 0.14720307110686864 Train-f1: 0.5844774963063134 Test-f1: 0.4829545454545454 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13810/20000] Train-Loss: 0.39008265294877437 Test-Loss: 0.1377689402445394 Train-f1: 0.4874394422781519 Test-f1: 0.528796992481203 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [13811/20000] Train-Loss: 0.38692211776961877 Test-Loss: 0.1412657779086048 Train-f1: 0.5740259740259741 Test-f1: 0.5256318681318681 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [13812/20000] Train-Loss: 0.38495046386837933 Test-Loss: 0.14518511260482095 Train-f1: 0.6253116253116253 Test-f1: 0.5264562569213732 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13813/20000] Train-Loss: 0.38527391690578366 Test-Loss: 0.14665741448501698 Train-f1: 0.6083952083952084 Test-f1: 0.5237166085946574 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13814/20000] Train-Loss: 0.38568921245215426 Test-Loss: 0.13877526716690478 Train-f1: 0.520213949345445 Test-f1: 0.525909645909646 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13815/20000] Train-Loss: 0.3838234147221562 Test-Loss: 0.14050065691811792 Train-f1: 0.5848628323080878 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13816/20000] Train-Loss: 0.38085884447712504 Test-Loss: 0.14481346215164762 Train-f1: 0.5198846045595272 Test-f1: 0.5494699894699895 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13817/20000] Train-Loss: 0.38710121320488833 Test-Loss: 0.13732860043871342 Train-f1: 0.6022960939062709 Test-f1: 0.4644611528822056 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13818/20000] Train-Loss: 0.3889014342786788 Test-Loss: 0.1473876998511974 Train-f1: 0.5006141879858345 Test-f1: 0.4829545454545454 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13819/20000] Train-Loss: 0.3863521830540174 Test-Loss: 0.13860049928025772 Train-f1: 0.5798983867300354 Test-f1: 0.5255137844611528 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13820/20000] Train-Loss: 0.38202423364328053 Test-Loss: 0.14795432902517083 Train-f1: 0.5260790549845238 Test-f1: 0.475889328063241 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13821/20000] Train-Loss: 0.38270740300004796 Test-Loss: 0.13775580203818313 Train-f1: 0.5830131280374218 Test-f1: 0.4602828448452552 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13822/20000] Train-Loss: 0.38177958179488475 Test-Loss: 0.1393877986442691 Train-f1: 0.5686070686070686 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13823/20000] Train-Loss: 0.3830822378094914 Test-Loss: 0.1427453195520194 Train-f1: 0.5008431762884318 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13824/20000] Train-Loss: 0.3886079054898794 Test-Loss: 0.14508518367152354 Train-f1: 0.6509279721041735 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13825/20000] Train-Loss: 0.386574548404243 Test-Loss: 0.13666463442830135 Train-f1: 0.6022961762092197 Test-f1: 0.4644611528822056 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13826/20000] Train-Loss: 0.38452190575480594 Test-Loss: 0.14840969783411187 Train-f1: 0.5222309222309223 Test-f1: 0.5603463203463204 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13827/20000] Train-Loss: 0.38239488684536127 Test-Loss: 0.14104035617104543 Train-f1: 0.562973026973027 Test-f1: 0.5421972621972622 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13828/20000] Train-Loss: 0.3824634822926695 Test-Loss: 0.13643773452447572 Train-f1: 0.6304334554334554 Test-f1: 0.4939348370927319 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13829/20000] Train-Loss: 0.3769671055115428 Test-Loss: 0.1552276793977858 Train-f1: 0.6066066180196615 Test-f1: 0.5603463203463204 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [13830/20000] Train-Loss: 0.38691680976049975 Test-Loss: 0.13680022484622478 Train-f1: 0.561104248549504 Test-f1: 0.4644611528822056 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13831/20000] Train-Loss: 0.38890157774478035 Test-Loss: 0.13703761468265888 Train-f1: 0.5647213713438879 Test-f1: 0.5051919146036793 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13832/20000] Train-Loss: 0.386013223058741 Test-Loss: 0.1377970046795743 Train-f1: 0.6541993746684055 Test-f1: 0.46727688787185356 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [13833/20000] Train-Loss: 0.38454750787068204 Test-Loss: 0.1401255803127383 Train-f1: 0.6511898911898911 Test-f1: 0.5625084825084825 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [13834/20000] Train-Loss: 0.3807949174415109 Test-Loss: 0.14136003947853612 Train-f1: 0.5590849896008488 Test-f1: 0.5308287764866713 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [13835/20000] Train-Loss: 0.3855295620125654 Test-Loss: 0.14245715466942668 Train-f1: 0.5006804968625944 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13836/20000] Train-Loss: 0.3822642540065287 Test-Loss: 0.14100780808577837 Train-f1: 0.5653333333333334 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13837/20000] Train-Loss: 0.38163092513559527 Test-Loss: 0.14333723614096255 Train-f1: 0.6122089559333201 Test-f1: 0.549891398923657 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [13838/20000] Train-Loss: 0.3855049542595818 Test-Loss: 0.13935899191739717 Train-f1: 0.5632694016799977 Test-f1: 0.5532874968169086 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13839/20000] Train-Loss: 0.3864346383182577 Test-Loss: 0.1361436114588074 Train-f1: 0.647423975767399 Test-f1: 0.4428237259816208 Train-acc: 0.8377483443708609 Test-acc: 0.75\n",
      "Epoch [13840/20000] Train-Loss: 0.378688166830839 Test-Loss: 0.14640786338732784 Train-f1: 0.5640255692020397 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13841/20000] Train-Loss: 0.37704838910803123 Test-Loss: 0.14674050174000702 Train-f1: 0.6365317009612312 Test-f1: 0.5603463203463204 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13842/20000] Train-Loss: 0.3859231722553421 Test-Loss: 0.14388018252148177 Train-f1: 0.5557046979865772 Test-f1: 0.5624077238550923 Train-acc: 0.8178807947019867 Test-acc: 0.8157894736842105\n",
      "Epoch [13843/20000] Train-Loss: 0.38665117141127386 Test-Loss: 0.13709009230486643 Train-f1: 0.4952151116856999 Test-f1: 0.5181595881595882 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13844/20000] Train-Loss: 0.38144875228049413 Test-Loss: 0.14297352486724027 Train-f1: 0.4976147912453645 Test-f1: 0.5133179723502305 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13845/20000] Train-Loss: 0.3829433476131284 Test-Loss: 0.13696632244165854 Train-f1: 0.5935955168586747 Test-f1: 0.4537451737451737 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [13846/20000] Train-Loss: 0.38417335726953294 Test-Loss: 0.1534540831279644 Train-f1: 0.5949718891496948 Test-f1: 0.46947869348907634 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13847/20000] Train-Loss: 0.38599087998101583 Test-Loss: 0.1365826372446928 Train-f1: 0.5603756243756244 Test-f1: 0.4395848595848596 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13848/20000] Train-Loss: 0.3804817682727534 Test-Loss: 0.13951215776217918 Train-f1: 0.5167342817934265 Test-f1: 0.5625084825084825 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13849/20000] Train-Loss: 0.38188002946279637 Test-Loss: 0.136801442074322 Train-f1: 0.5915587899958761 Test-f1: 0.5213472706155633 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13850/20000] Train-Loss: 0.38596452535859926 Test-Loss: 0.13634879445244422 Train-f1: 0.5897813248978133 Test-f1: 0.5213472706155633 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13851/20000] Train-Loss: 0.38185236663657285 Test-Loss: 0.1461793840046383 Train-f1: 0.5707330818461329 Test-f1: 0.5292719919110211 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13852/20000] Train-Loss: 0.3832510929316752 Test-Loss: 0.14183476894708488 Train-f1: 0.49818865345181135 Test-f1: 0.5641558441558441 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13853/20000] Train-Loss: 0.3826431532382218 Test-Loss: 0.15092785919561816 Train-f1: 0.569439825767829 Test-f1: 0.5603463203463204 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13854/20000] Train-Loss: 0.37887629799863254 Test-Loss: 0.14100302531541772 Train-f1: 0.5786476728671219 Test-f1: 0.46036981509245384 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13855/20000] Train-Loss: 0.39108042030763407 Test-Loss: 0.1405702610525751 Train-f1: 0.6099821010579298 Test-f1: 0.46727688787185356 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13856/20000] Train-Loss: 0.3859091293200483 Test-Loss: 0.13889937021065735 Train-f1: 0.6045704610641497 Test-f1: 0.5255137844611528 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13857/20000] Train-Loss: 0.3777236832566075 Test-Loss: 0.14082149679309483 Train-f1: 0.5121860595544806 Test-f1: 0.5641558441558441 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13858/20000] Train-Loss: 0.38245843614468933 Test-Loss: 0.13903367339347542 Train-f1: 0.5799149466233601 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13859/20000] Train-Loss: 0.3811499228489694 Test-Loss: 0.1440915808331586 Train-f1: 0.5140678779000349 Test-f1: 0.4829545454545454 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13860/20000] Train-Loss: 0.38355885740569695 Test-Loss: 0.13802892677148393 Train-f1: 0.6288603486674533 Test-f1: 0.44240059457450764 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [13861/20000] Train-Loss: 0.38009448727036504 Test-Loss: 0.14772448030974267 Train-f1: 0.573035760114923 Test-f1: 0.5237166085946574 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13862/20000] Train-Loss: 0.38150906834934684 Test-Loss: 0.13900181710094411 Train-f1: 0.5643647073889102 Test-f1: 0.5161904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13863/20000] Train-Loss: 0.38224790093044386 Test-Loss: 0.13827594438039376 Train-f1: 0.6259454580507212 Test-f1: 0.5532874968169086 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13864/20000] Train-Loss: 0.3837720115349878 Test-Loss: 0.1441101807948079 Train-f1: 0.6114754050984663 Test-f1: 0.4656685910608086 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13865/20000] Train-Loss: 0.3807601655364689 Test-Loss: 0.14117859726113519 Train-f1: 0.6122892767188068 Test-f1: 0.5422818042172881 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13866/20000] Train-Loss: 0.3816112653511778 Test-Loss: 0.14769521154702006 Train-f1: 0.5013414412678323 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13867/20000] Train-Loss: 0.38316647436929485 Test-Loss: 0.13893072399633785 Train-f1: 0.5857772056611447 Test-f1: 0.5616954528992294 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13868/20000] Train-Loss: 0.38371006444930084 Test-Loss: 0.13945640305460966 Train-f1: 0.5477079878902853 Test-f1: 0.4842443064182195 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13869/20000] Train-Loss: 0.3852652498526048 Test-Loss: 0.13849038782772494 Train-f1: 0.4997874578133585 Test-f1: 0.528796992481203 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13870/20000] Train-Loss: 0.3809368830624672 Test-Loss: 0.13954356499043016 Train-f1: 0.5223785846076867 Test-f1: 0.5422818042172881 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13871/20000] Train-Loss: 0.38391629905219193 Test-Loss: 0.13620865804951937 Train-f1: 0.5947583947583948 Test-f1: 0.49631578947368427 Train-acc: 0.847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [13872/20000] Train-Loss: 0.3817152123983913 Test-Loss: 0.1427604786789697 Train-f1: 0.5853175198837235 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13873/20000] Train-Loss: 0.38140561224159414 Test-Loss: 0.1377287810079415 Train-f1: 0.6174899140797759 Test-f1: 0.4630985798349032 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13874/20000] Train-Loss: 0.38302213876378416 Test-Loss: 0.13787838961525456 Train-f1: 0.6116895411199209 Test-f1: 0.49913152446333225 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13875/20000] Train-Loss: 0.3812313591187135 Test-Loss: 0.14511818470461985 Train-f1: 0.5951706499953959 Test-f1: 0.5189908501970213 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13876/20000] Train-Loss: 0.3844561494294099 Test-Loss: 0.13745330291910374 Train-f1: 0.5346286894918185 Test-f1: 0.5181595881595882 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13877/20000] Train-Loss: 0.38088348210279055 Test-Loss: 0.14315286245463257 Train-f1: 0.6058608058608058 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13878/20000] Train-Loss: 0.38591148986407037 Test-Loss: 0.13698280827999804 Train-f1: 0.5121358277982319 Test-f1: 0.4428237259816208 Train-acc: 0.8245033112582781 Test-acc: 0.75\n",
      "Epoch [13879/20000] Train-Loss: 0.3818147922595603 Test-Loss: 0.1442429936288959 Train-f1: 0.513627920550497 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13880/20000] Train-Loss: 0.38128716915897687 Test-Loss: 0.14159275408486086 Train-f1: 0.5475234699075758 Test-f1: 0.47333333333333333 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13881/20000] Train-Loss: 0.382282932739496 Test-Loss: 0.13616399434478352 Train-f1: 0.5853915367775887 Test-f1: 0.45000000000000007 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13882/20000] Train-Loss: 0.38529684634787753 Test-Loss: 0.13978642196617916 Train-f1: 0.5196668196668197 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13883/20000] Train-Loss: 0.38594343376786644 Test-Loss: 0.13850008028883048 Train-f1: 0.5819604372118898 Test-f1: 0.5281271777003484 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13884/20000] Train-Loss: 0.38352622725247565 Test-Loss: 0.1392300081177348 Train-f1: 0.5199380717105688 Test-f1: 0.5281271777003484 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13885/20000] Train-Loss: 0.37969793746251496 Test-Loss: 0.14646966940675715 Train-f1: 0.5764436200741934 Test-f1: 0.5237166085946574 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13886/20000] Train-Loss: 0.37865470115143435 Test-Loss: 0.150574379126446 Train-f1: 0.5681435231751213 Test-f1: 0.5603463203463204 Train-acc: 0.8145695364238411 Test-acc: 0.8157894736842105\n",
      "Epoch [13887/20000] Train-Loss: 0.3873918498088242 Test-Loss: 0.14305671205385304 Train-f1: 0.5272045047684145 Test-f1: 0.48217054263565895 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13888/20000] Train-Loss: 0.3857336241799688 Test-Loss: 0.13847153956298056 Train-f1: 0.5466658422964156 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13889/20000] Train-Loss: 0.3817783329103699 Test-Loss: 0.13970725052586175 Train-f1: 0.6137100994243851 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13890/20000] Train-Loss: 0.38315871292846143 Test-Loss: 0.136757075834169 Train-f1: 0.5798441558441558 Test-f1: 0.5108403361344538 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [13891/20000] Train-Loss: 0.38567089215245365 Test-Loss: 0.14188991251480143 Train-f1: 0.6294461993405656 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13892/20000] Train-Loss: 0.3874954089576366 Test-Loss: 0.137609230186784 Train-f1: 0.4958701342807303 Test-f1: 0.5181595881595882 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13893/20000] Train-Loss: 0.3796801292223802 Test-Loss: 0.14781103247218524 Train-f1: 0.5170359359097628 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13894/20000] Train-Loss: 0.3799238031768262 Test-Loss: 0.1445532179914648 Train-f1: 0.5279782745989643 Test-f1: 0.5189908501970213 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13895/20000] Train-Loss: 0.3836958658505756 Test-Loss: 0.1387788655160608 Train-f1: 0.5977201056744311 Test-f1: 0.4767762460233298 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13896/20000] Train-Loss: 0.37905736146590974 Test-Loss: 0.13641148500091718 Train-f1: 0.5912564471388001 Test-f1: 0.5213472706155633 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13897/20000] Train-Loss: 0.38493769954224827 Test-Loss: 0.13821985062526665 Train-f1: 0.5963986425629489 Test-f1: 0.5393083387201034 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13898/20000] Train-Loss: 0.3805513293698855 Test-Loss: 0.13667010356794534 Train-f1: 0.5873495631884894 Test-f1: 0.5073487773487774 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13899/20000] Train-Loss: 0.37973599134007546 Test-Loss: 0.13725708394469693 Train-f1: 0.6174485383897149 Test-f1: 0.5308287764866713 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13900/20000] Train-Loss: 0.38214172644837163 Test-Loss: 0.14194580507868576 Train-f1: 0.5869939584225299 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13901/20000] Train-Loss: 0.3855530810074663 Test-Loss: 0.1366029683256213 Train-f1: 0.5671935676189556 Test-f1: 0.4939348370927319 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13902/20000] Train-Loss: 0.381201149807792 Test-Loss: 0.14300080381548144 Train-f1: 0.5924577903732162 Test-f1: 0.5133179723502305 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13903/20000] Train-Loss: 0.38375925424174895 Test-Loss: 0.13914486799800826 Train-f1: 0.6020855518375214 Test-f1: 0.5641558441558441 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13904/20000] Train-Loss: 0.3819497126328843 Test-Loss: 0.1424223755189882 Train-f1: 0.6282612475244054 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13905/20000] Train-Loss: 0.3834461948949175 Test-Loss: 0.13730349055984326 Train-f1: 0.604513822160881 Test-f1: 0.4644611528822056 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13906/20000] Train-Loss: 0.38170489915916217 Test-Loss: 0.14177143040219595 Train-f1: 0.5992850639909463 Test-f1: 0.5625084825084825 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13907/20000] Train-Loss: 0.38606367491240823 Test-Loss: 0.14318191832093397 Train-f1: 0.5289906084615914 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [13908/20000] Train-Loss: 0.38422099121767317 Test-Loss: 0.14024942978393964 Train-f1: 0.5388944430146375 Test-f1: 0.5308287764866713 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13909/20000] Train-Loss: 0.3844502693736891 Test-Loss: 0.1363968249013019 Train-f1: 0.5744609520838172 Test-f1: 0.4428237259816208 Train-acc: 0.8211920529801324 Test-acc: 0.75\n",
      "Epoch [13910/20000] Train-Loss: 0.3853490740877545 Test-Loss: 0.14234727331145017 Train-f1: 0.5196271780467494 Test-f1: 0.5281271777003484 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13911/20000] Train-Loss: 0.3898888692128547 Test-Loss: 0.13874589801691956 Train-f1: 0.6091787670909315 Test-f1: 0.5255137844611528 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13912/20000] Train-Loss: 0.38094028969865035 Test-Loss: 0.14061758432314542 Train-f1: 0.5846815256472639 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13913/20000] Train-Loss: 0.3855092087626389 Test-Loss: 0.14035727396765402 Train-f1: 0.5115587115587116 Test-f1: 0.574856124267889 Train-acc: 0.8245033112582781 Test-acc: 0.8289473684210527\n",
      "Epoch [13914/20000] Train-Loss: 0.37750926426542186 Test-Loss: 0.14864765048290193 Train-f1: 0.5759467905163269 Test-f1: 0.5641558441558441 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [13915/20000] Train-Loss: 0.3881056964424145 Test-Loss: 0.14316741494456045 Train-f1: 0.6605459648038724 Test-f1: 0.549891398923657 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13916/20000] Train-Loss: 0.378684329439166 Test-Loss: 0.14026097597459952 Train-f1: 0.6609230951696705 Test-f1: 0.47783405000796303 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [13917/20000] Train-Loss: 0.3847866258890965 Test-Loss: 0.14108850277612253 Train-f1: 0.5820675968326975 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13918/20000] Train-Loss: 0.37922120479523164 Test-Loss: 0.14025240592715107 Train-f1: 0.5120946197711553 Test-f1: 0.5421972621972622 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13919/20000] Train-Loss: 0.38780316137417326 Test-Loss: 0.1396086503782328 Train-f1: 0.5284844679130394 Test-f1: 0.4774585921325052 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13920/20000] Train-Loss: 0.3788190369778073 Test-Loss: 0.1416780507068363 Train-f1: 0.6024012951881657 Test-f1: 0.5256318681318681 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13921/20000] Train-Loss: 0.3835706779407799 Test-Loss: 0.14342985087287244 Train-f1: 0.5069229016597437 Test-f1: 0.5624077238550923 Train-acc: 0.8178807947019867 Test-acc: 0.8157894736842105\n",
      "Epoch [13922/20000] Train-Loss: 0.3826265930679181 Test-Loss: 0.13830945107892678 Train-f1: 0.6016792731078446 Test-f1: 0.5276190476190477 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13923/20000] Train-Loss: 0.3836662631441668 Test-Loss: 0.14159132900134944 Train-f1: 0.5847521605910868 Test-f1: 0.5625084825084825 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13924/20000] Train-Loss: 0.37980416915910264 Test-Loss: 0.13896995584350091 Train-f1: 0.5421284160414596 Test-f1: 0.526984126984127 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [13925/20000] Train-Loss: 0.382307471408235 Test-Loss: 0.13754981681018674 Train-f1: 0.5968907708038143 Test-f1: 0.47501831501831504 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13926/20000] Train-Loss: 0.381058994257019 Test-Loss: 0.1457209170236039 Train-f1: 0.6282500350816838 Test-f1: 0.5237166085946574 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13927/20000] Train-Loss: 0.3844642361260365 Test-Loss: 0.1436162685448783 Train-f1: 0.4983799936242363 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13928/20000] Train-Loss: 0.38283994679027095 Test-Loss: 0.13729771286468254 Train-f1: 0.6236111660996453 Test-f1: 0.5616954528992294 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13929/20000] Train-Loss: 0.37750711620606264 Test-Loss: 0.13855846068036157 Train-f1: 0.599196740373211 Test-f1: 0.47783405000796303 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13930/20000] Train-Loss: 0.3771260399493318 Test-Loss: 0.1455694804875529 Train-f1: 0.5060785447782352 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13931/20000] Train-Loss: 0.38636885316032693 Test-Loss: 0.14007645918226355 Train-f1: 0.5916959656090091 Test-f1: 0.5532874968169086 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [13932/20000] Train-Loss: 0.38209247229117027 Test-Loss: 0.13679422785765388 Train-f1: 0.6429479815669075 Test-f1: 0.5308287764866713 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13933/20000] Train-Loss: 0.38618281599955717 Test-Loss: 0.14088015869134665 Train-f1: 0.6437486427747672 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13934/20000] Train-Loss: 0.3838129084783096 Test-Loss: 0.13884500189961851 Train-f1: 0.6100956520271287 Test-f1: 0.531612727470851 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [13935/20000] Train-Loss: 0.3877767410788765 Test-Loss: 0.1381627913830206 Train-f1: 0.49663164925337533 Test-f1: 0.5641558441558441 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13936/20000] Train-Loss: 0.3817120156855856 Test-Loss: 0.13849907803913017 Train-f1: 0.5029962863362787 Test-f1: 0.5183710801393728 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13937/20000] Train-Loss: 0.38125094537346144 Test-Loss: 0.1381547254770509 Train-f1: 0.51835335275513 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13938/20000] Train-Loss: 0.3802229498527433 Test-Loss: 0.13971121521550353 Train-f1: 0.5921867330819429 Test-f1: 0.4774585921325052 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13939/20000] Train-Loss: 0.38105259755768295 Test-Loss: 0.137005199633513 Train-f1: 0.6281248075957906 Test-f1: 0.5616954528992294 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13940/20000] Train-Loss: 0.38103494263063126 Test-Loss: 0.13804154412721578 Train-f1: 0.581765958904778 Test-f1: 0.5161904761904762 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13941/20000] Train-Loss: 0.37820006397508604 Test-Loss: 0.14404945211925002 Train-f1: 0.5930521859093287 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [13942/20000] Train-Loss: 0.3810660997756388 Test-Loss: 0.13658780171446425 Train-f1: 0.5996647502607767 Test-f1: 0.4428237259816208 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [13943/20000] Train-Loss: 0.38306636932368315 Test-Loss: 0.14674167529005108 Train-f1: 0.6453886025378092 Test-f1: 0.47526389938134217 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13944/20000] Train-Loss: 0.3812710087850036 Test-Loss: 0.13961639779288268 Train-f1: 0.5929517154159887 Test-f1: 0.4774585921325052 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13945/20000] Train-Loss: 0.385560677627189 Test-Loss: 0.13912202961964784 Train-f1: 0.5608324661898862 Test-f1: 0.5625084825084825 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [13946/20000] Train-Loss: 0.38007070693845985 Test-Loss: 0.13811824387689428 Train-f1: 0.6113246753246754 Test-f1: 0.5422818042172881 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13947/20000] Train-Loss: 0.3812754190612628 Test-Loss: 0.14504082670657875 Train-f1: 0.541394107559895 Test-f1: 0.4656685910608086 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13948/20000] Train-Loss: 0.3814569684220731 Test-Loss: 0.1366836792321459 Train-f1: 0.5871539617817164 Test-f1: 0.49631578947368427 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [13949/20000] Train-Loss: 0.3823081258380363 Test-Loss: 0.1421771107770923 Train-f1: 0.5363050021591382 Test-f1: 0.4628528560711606 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13950/20000] Train-Loss: 0.37849199045580123 Test-Loss: 0.13635446690177289 Train-f1: 0.6530588215505206 Test-f1: 0.4428237259816208 Train-acc: 0.8443708609271523 Test-acc: 0.75\n",
      "Epoch [13951/20000] Train-Loss: 0.38528637333086185 Test-Loss: 0.14351330613797073 Train-f1: 0.5092764541410645 Test-f1: 0.5141038802093091 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13952/20000] Train-Loss: 0.3812423331755557 Test-Loss: 0.138661741075196 Train-f1: 0.6144593526608165 Test-f1: 0.574856124267889 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [13953/20000] Train-Loss: 0.38101659900093454 Test-Loss: 0.1424020935553111 Train-f1: 0.5848740540505246 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13954/20000] Train-Loss: 0.3807170072891282 Test-Loss: 0.14341705135096106 Train-f1: 0.5745616131027591 Test-f1: 0.4774585921325052 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13955/20000] Train-Loss: 0.37862487088333885 Test-Loss: 0.14468402780549858 Train-f1: 0.5845343671150123 Test-f1: 0.4656685910608086 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [13956/20000] Train-Loss: 0.3824809305542302 Test-Loss: 0.14338966866877778 Train-f1: 0.619592329239388 Test-f1: 0.5161751152073732 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13957/20000] Train-Loss: 0.3794809931519681 Test-Loss: 0.14095121273066535 Train-f1: 0.5747236573623009 Test-f1: 0.4767762460233298 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [13958/20000] Train-Loss: 0.37723078898273554 Test-Loss: 0.13789429494991354 Train-f1: 0.6162724692526017 Test-f1: 0.44240059457450764 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [13959/20000] Train-Loss: 0.3826010273610066 Test-Loss: 0.1429282699863954 Train-f1: 0.5083535765904016 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [13960/20000] Train-Loss: 0.38167692533669073 Test-Loss: 0.15476849521074415 Train-f1: 0.5864167998479709 Test-f1: 0.5603463203463204 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13961/20000] Train-Loss: 0.38213235623127634 Test-Loss: 0.1389120156866929 Train-f1: 0.6048861547085255 Test-f1: 0.4565609087348218 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [13962/20000] Train-Loss: 0.3845231155413216 Test-Loss: 0.13877332555773692 Train-f1: 0.5660855614108439 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [13963/20000] Train-Loss: 0.3810892938843824 Test-Loss: 0.14011478096642224 Train-f1: 0.5969115396725094 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13964/20000] Train-Loss: 0.38242371553493293 Test-Loss: 0.13765739911335265 Train-f1: 0.6201365469264629 Test-f1: 0.4565609087348218 Train-acc: 0.8443708609271523 Test-acc: 0.7631578947368421\n",
      "Epoch [13965/20000] Train-Loss: 0.3793072487612305 Test-Loss: 0.14096306444552908 Train-f1: 0.5673871129954409 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [13966/20000] Train-Loss: 0.3850628648253284 Test-Loss: 0.13716748480367993 Train-f1: 0.5007181065829334 Test-f1: 0.4644611528822056 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [13967/20000] Train-Loss: 0.37822956512446965 Test-Loss: 0.14847546019822422 Train-f1: 0.5678744678744678 Test-f1: 0.5603463203463204 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13968/20000] Train-Loss: 0.38538471828859316 Test-Loss: 0.1372241870014332 Train-f1: 0.613525613968805 Test-f1: 0.5161904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13969/20000] Train-Loss: 0.3813717628014333 Test-Loss: 0.13835461308912825 Train-f1: 0.563237966117296 Test-f1: 0.5272243672243672 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [13970/20000] Train-Loss: 0.37591025470693334 Test-Loss: 0.15285911600171317 Train-f1: 0.5272116589265886 Test-f1: 0.5292719919110211 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [13971/20000] Train-Loss: 0.3849305855855031 Test-Loss: 0.14691020059465656 Train-f1: 0.6394926340789686 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [13972/20000] Train-Loss: 0.38288070095673943 Test-Loss: 0.13801609845406532 Train-f1: 0.5202772098974393 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [13973/20000] Train-Loss: 0.3782544102884588 Test-Loss: 0.14937957053242681 Train-f1: 0.5819604372118898 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13974/20000] Train-Loss: 0.3813120013412722 Test-Loss: 0.14437922466851796 Train-f1: 0.6063094152224587 Test-f1: 0.549891398923657 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [13975/20000] Train-Loss: 0.37783552546868654 Test-Loss: 0.14295562746056073 Train-f1: 0.5451347467741791 Test-f1: 0.5624077238550923 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [13976/20000] Train-Loss: 0.3787571576412635 Test-Loss: 0.1420109651004433 Train-f1: 0.6162875334955873 Test-f1: 0.48343865857768786 Train-acc: 0.8145695364238411 Test-acc: 0.8026315789473685\n",
      "Epoch [13977/20000] Train-Loss: 0.3786802333608672 Test-Loss: 0.13996808378880604 Train-f1: 0.5151270483902063 Test-f1: 0.5181595881595882 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13978/20000] Train-Loss: 0.38078760058021427 Test-Loss: 0.13711538364752562 Train-f1: 0.6510909324218495 Test-f1: 0.5181595881595882 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [13979/20000] Train-Loss: 0.38153583411795866 Test-Loss: 0.13649835287863502 Train-f1: 0.5171548451548451 Test-f1: 0.4944577129097253 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [13980/20000] Train-Loss: 0.3803260371867978 Test-Loss: 0.13780656046950368 Train-f1: 0.4955866287135636 Test-f1: 0.5421972621972622 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [13981/20000] Train-Loss: 0.382199730564097 Test-Loss: 0.14223754726691593 Train-f1: 0.60198116356544 Test-f1: 0.5625084825084825 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13982/20000] Train-Loss: 0.3779770296288948 Test-Loss: 0.1419492056217113 Train-f1: 0.6287104223159506 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13983/20000] Train-Loss: 0.3850061011755724 Test-Loss: 0.1386752069249678 Train-f1: 0.5198215118215118 Test-f1: 0.5625084825084825 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [13984/20000] Train-Loss: 0.37337874434181556 Test-Loss: 0.14715805115450079 Train-f1: 0.5863615023474178 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [13985/20000] Train-Loss: 0.37973674598720647 Test-Loss: 0.146211053082093 Train-f1: 0.6480731662961985 Test-f1: 0.4656685910608086 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [13986/20000] Train-Loss: 0.3872657040218754 Test-Loss: 0.1448322329290856 Train-f1: 0.48698088828796393 Test-f1: 0.5624077238550923 Train-acc: 0.8112582781456954 Test-acc: 0.8157894736842105\n",
      "Epoch [13987/20000] Train-Loss: 0.3850814122536118 Test-Loss: 0.14137023126841572 Train-f1: 0.6229630019103704 Test-f1: 0.5308287764866713 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [13988/20000] Train-Loss: 0.38181738146992367 Test-Loss: 0.14021385325427266 Train-f1: 0.604513822160881 Test-f1: 0.5532874968169086 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [13989/20000] Train-Loss: 0.37659978846371334 Test-Loss: 0.13647852509858088 Train-f1: 0.6501180974414057 Test-f1: 0.4644611528822056 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13990/20000] Train-Loss: 0.3786712158919626 Test-Loss: 0.15048182892608344 Train-f1: 0.5427558903159926 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [13991/20000] Train-Loss: 0.3844413553356221 Test-Loss: 0.13867159231199214 Train-f1: 0.5369412839654869 Test-f1: 0.5241630056052113 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [13992/20000] Train-Loss: 0.3781873340302947 Test-Loss: 0.1414473526521765 Train-f1: 0.5161128526645768 Test-f1: 0.5421972621972622 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [13993/20000] Train-Loss: 0.37473367018844245 Test-Loss: 0.14647286699201909 Train-f1: 0.5521547169872403 Test-f1: 0.46539589442815255 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [13994/20000] Train-Loss: 0.38299220126844286 Test-Loss: 0.13875770472694465 Train-f1: 0.572363486557035 Test-f1: 0.5532874968169086 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [13995/20000] Train-Loss: 0.3774370855060023 Test-Loss: 0.1401265141931366 Train-f1: 0.5149357052790537 Test-f1: 0.574856124267889 Train-acc: 0.8278145695364238 Test-acc: 0.8289473684210527\n",
      "Epoch [13996/20000] Train-Loss: 0.3805841181075757 Test-Loss: 0.13775720030603758 Train-f1: 0.5795930082988906 Test-f1: 0.5421972621972622 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [13997/20000] Train-Loss: 0.38100084577631826 Test-Loss: 0.14183677270079276 Train-f1: 0.5317633433039397 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [13998/20000] Train-Loss: 0.38111932237940765 Test-Loss: 0.1427392629988287 Train-f1: 0.5224237950303409 Test-f1: 0.5256318681318681 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [13999/20000] Train-Loss: 0.3741580422704816 Test-Loss: 0.14537401161834118 Train-f1: 0.6158555729984301 Test-f1: 0.5495545314900154 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14000/20000] Train-Loss: 0.37748430715342407 Test-Loss: 0.16138169196987293 Train-f1: 0.6532230399686066 Test-f1: 0.48703703703703705 Train-acc: 0.8443708609271523 Test-acc: 0.7631578947368421\n",
      "Epoch [14001/20000] Train-Loss: 0.3775195351261376 Test-Loss: 0.1429610349501848 Train-f1: 0.6475761020588606 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14002/20000] Train-Loss: 0.3740547423993448 Test-Loss: 0.14282854437172474 Train-f1: 0.6418840760946025 Test-f1: 0.5422818042172881 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14003/20000] Train-Loss: 0.3842926431323673 Test-Loss: 0.14221006422822768 Train-f1: 0.5263577002707438 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14004/20000] Train-Loss: 0.3813753033567596 Test-Loss: 0.13969508332147068 Train-f1: 0.5873495631884894 Test-f1: 0.5281271777003484 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14005/20000] Train-Loss: 0.3812024410072311 Test-Loss: 0.13645877168922418 Train-f1: 0.5564359829783322 Test-f1: 0.4944577129097253 Train-acc: 0.8178807947019867 Test-acc: 0.7631578947368421\n",
      "Epoch [14006/20000] Train-Loss: 0.38340899332333006 Test-Loss: 0.14015330230188133 Train-f1: 0.6046541883878513 Test-f1: 0.4774585921325052 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14007/20000] Train-Loss: 0.3757421345209226 Test-Loss: 0.13653185207232615 Train-f1: 0.604635761589404 Test-f1: 0.45000000000000007 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [14008/20000] Train-Loss: 0.3845034055223559 Test-Loss: 0.13896402540377326 Train-f1: 0.5743119625472567 Test-f1: 0.4767762460233298 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [14009/20000] Train-Loss: 0.38192956087408897 Test-Loss: 0.14011900287708515 Train-f1: 0.6305590357355063 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14010/20000] Train-Loss: 0.3824648237316982 Test-Loss: 0.13664876180097493 Train-f1: 0.5971065504905769 Test-f1: 0.4944577129097253 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14011/20000] Train-Loss: 0.38349067791334773 Test-Loss: 0.13949440580354733 Train-f1: 0.6310726800793026 Test-f1: 0.5421240737097515 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14012/20000] Train-Loss: 0.380560677725108 Test-Loss: 0.14590516116352611 Train-f1: 0.5421284160414596 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14013/20000] Train-Loss: 0.37960054184144676 Test-Loss: 0.14737639444490452 Train-f1: 0.5284033938486494 Test-f1: 0.47125448028673833 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14014/20000] Train-Loss: 0.38138665235783464 Test-Loss: 0.14279509241967078 Train-f1: 0.49692973692973685 Test-f1: 0.5532874968169086 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14015/20000] Train-Loss: 0.3811231712295321 Test-Loss: 0.13666445329441 Train-f1: 0.6302193957966071 Test-f1: 0.4537451737451737 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [14016/20000] Train-Loss: 0.38036772365745486 Test-Loss: 0.13875188596945073 Train-f1: 0.5801901365742425 Test-f1: 0.526984126984127 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14017/20000] Train-Loss: 0.38060320075206294 Test-Loss: 0.13932775252801902 Train-f1: 0.5823326311748274 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14018/20000] Train-Loss: 0.3780596983820516 Test-Loss: 0.13989422233950838 Train-f1: 0.5830131280374218 Test-f1: 0.44240059457450764 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [14019/20000] Train-Loss: 0.3793416652059869 Test-Loss: 0.14482640270005995 Train-f1: 0.6090133495196787 Test-f1: 0.549891398923657 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14020/20000] Train-Loss: 0.37557433441495025 Test-Loss: 0.13736561982837792 Train-f1: 0.6504255143352887 Test-f1: 0.45000000000000007 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [14021/20000] Train-Loss: 0.38715468666132274 Test-Loss: 0.13988501483911653 Train-f1: 0.5225117847408869 Test-f1: 0.5308287764866713 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14022/20000] Train-Loss: 0.37959762385769946 Test-Loss: 0.14087999210252627 Train-f1: 0.5001625619460015 Test-f1: 0.5641558441558441 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14023/20000] Train-Loss: 0.38160307970741825 Test-Loss: 0.13693105850494433 Train-f1: 0.5066267066267066 Test-f1: 0.546221198156682 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [14024/20000] Train-Loss: 0.3851682703359952 Test-Loss: 0.14273669340748704 Train-f1: 0.5093284499762468 Test-f1: 0.4829545454545454 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [14025/20000] Train-Loss: 0.3831149704811107 Test-Loss: 0.13872332409538388 Train-f1: 0.4932025468938221 Test-f1: 0.5393083387201034 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14026/20000] Train-Loss: 0.3814411247897613 Test-Loss: 0.13959025917740264 Train-f1: 0.5650458125435377 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14027/20000] Train-Loss: 0.37566994597958137 Test-Loss: 0.14065368835867081 Train-f1: 0.5315199023392243 Test-f1: 0.5281271777003484 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14028/20000] Train-Loss: 0.3815354576620351 Test-Loss: 0.13764401775964716 Train-f1: 0.6485222932591354 Test-f1: 0.4644611528822056 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14029/20000] Train-Loss: 0.3738534546798379 Test-Loss: 0.13661729552621446 Train-f1: 0.5922212363206152 Test-f1: 0.5076190476190476 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14030/20000] Train-Loss: 0.3794413308323533 Test-Loss: 0.14064093289436733 Train-f1: 0.6549753694581281 Test-f1: 0.5308287764866713 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [14031/20000] Train-Loss: 0.38039871908022804 Test-Loss: 0.1415422621253419 Train-f1: 0.5899832592439285 Test-f1: 0.574856124267889 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14032/20000] Train-Loss: 0.38500741458394977 Test-Loss: 0.1432305890833085 Train-f1: 0.6096702382049377 Test-f1: 0.4770348837209302 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14033/20000] Train-Loss: 0.3800217639877457 Test-Loss: 0.13898425514088994 Train-f1: 0.6528153029945446 Test-f1: 0.5161904761904762 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14034/20000] Train-Loss: 0.3761718598296647 Test-Loss: 0.15018494258147977 Train-f1: 0.4976147912453645 Test-f1: 0.5603463203463204 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14035/20000] Train-Loss: 0.37913183324767413 Test-Loss: 0.13654619652363587 Train-f1: 0.5170203158056548 Test-f1: 0.4944577129097253 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14036/20000] Train-Loss: 0.3759872724655017 Test-Loss: 0.13770269476265407 Train-f1: 0.6047719298245614 Test-f1: 0.5308287764866713 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14037/20000] Train-Loss: 0.3789924796477023 Test-Loss: 0.1470602671385577 Train-f1: 0.5298197707660857 Test-f1: 0.475889328063241 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14038/20000] Train-Loss: 0.3779800362099454 Test-Loss: 0.13795814020329064 Train-f1: 0.596471778721301 Test-f1: 0.46727688787185356 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14039/20000] Train-Loss: 0.3796116520636748 Test-Loss: 0.14675063841575173 Train-f1: 0.505202410977643 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14040/20000] Train-Loss: 0.3775388091597004 Test-Loss: 0.14629939330118458 Train-f1: 0.6022132732691738 Test-f1: 0.5189908501970213 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14041/20000] Train-Loss: 0.3824821659998046 Test-Loss: 0.14059527568900831 Train-f1: 0.5904415584415584 Test-f1: 0.4842443064182195 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14042/20000] Train-Loss: 0.37760587181353616 Test-Loss: 0.13860787332525126 Train-f1: 0.6307391866807579 Test-f1: 0.526984126984127 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14043/20000] Train-Loss: 0.3786064453792629 Test-Loss: 0.1457968250437914 Train-f1: 0.5626076626076626 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14044/20000] Train-Loss: 0.3769318541203413 Test-Loss: 0.14646084865488881 Train-f1: 0.5995563678788737 Test-f1: 0.5189908501970213 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14045/20000] Train-Loss: 0.37688518109888597 Test-Loss: 0.14709381096379961 Train-f1: 0.5981728307534759 Test-f1: 0.475889328063241 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14046/20000] Train-Loss: 0.38280438307804265 Test-Loss: 0.1396018372300007 Train-f1: 0.5765483165483165 Test-f1: 0.4774585921325052 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14047/20000] Train-Loss: 0.3791753714042711 Test-Loss: 0.13665421771688782 Train-f1: 0.5607356114205428 Test-f1: 0.5213472706155633 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [14048/20000] Train-Loss: 0.3859671019273713 Test-Loss: 0.13855310718848168 Train-f1: 0.5061965635214043 Test-f1: 0.5532874968169086 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [14049/20000] Train-Loss: 0.3811103588768218 Test-Loss: 0.1420053490197995 Train-f1: 0.592422092429398 Test-f1: 0.574856124267889 Train-acc: 0.8245033112582781 Test-acc: 0.8289473684210527\n",
      "Epoch [14050/20000] Train-Loss: 0.38130098146456937 Test-Loss: 0.14464796162560326 Train-f1: 0.6030534390077645 Test-f1: 0.4656685910608086 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14051/20000] Train-Loss: 0.3797072174338467 Test-Loss: 0.14687122230639024 Train-f1: 0.5792510192510192 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14052/20000] Train-Loss: 0.38262106253355477 Test-Loss: 0.14424908821216764 Train-f1: 0.5900971867007673 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14053/20000] Train-Loss: 0.3792818103418012 Test-Loss: 0.15001530917259506 Train-f1: 0.5779313517607132 Test-f1: 0.5265323435843053 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14054/20000] Train-Loss: 0.38472408952188186 Test-Loss: 0.14327783919459414 Train-f1: 0.5798983867300354 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14055/20000] Train-Loss: 0.375745047093597 Test-Loss: 0.14204314556710113 Train-f1: 0.6065391614754672 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14056/20000] Train-Loss: 0.3800675529607142 Test-Loss: 0.14300767188097885 Train-f1: 0.6354384023312298 Test-f1: 0.5141038802093091 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14057/20000] Train-Loss: 0.3797640738101206 Test-Loss: 0.13928643465966806 Train-f1: 0.5146104163320387 Test-f1: 0.5281271777003484 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14058/20000] Train-Loss: 0.3759836027118628 Test-Loss: 0.14195583435033127 Train-f1: 0.5879849061203464 Test-f1: 0.43593384429205323 Train-acc: 0.8377483443708609 Test-acc: 0.7368421052631579\n",
      "Epoch [14059/20000] Train-Loss: 0.3853990159390451 Test-Loss: 0.1415680206985933 Train-f1: 0.4964918587022149 Test-f1: 0.5625084825084825 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14060/20000] Train-Loss: 0.37837389282490763 Test-Loss: 0.138936028441983 Train-f1: 0.584848484848485 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14061/20000] Train-Loss: 0.3796466139194339 Test-Loss: 0.1385489418347008 Train-f1: 0.5321937988127246 Test-f1: 0.5378013661604993 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14062/20000] Train-Loss: 0.37812973198543026 Test-Loss: 0.13766131048167354 Train-f1: 0.5998038925907629 Test-f1: 0.5181595881595882 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14063/20000] Train-Loss: 0.3766146341519753 Test-Loss: 0.1570370926984771 Train-f1: 0.5341802341802342 Test-f1: 0.4725704809286899 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14064/20000] Train-Loss: 0.38346006549872025 Test-Loss: 0.13976559287246135 Train-f1: 0.5093917105282412 Test-f1: 0.5262301587301588 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [14065/20000] Train-Loss: 0.3796919803291705 Test-Loss: 0.1377088962882646 Train-f1: 0.5731168375003992 Test-f1: 0.5181595881595882 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14066/20000] Train-Loss: 0.37803404945591484 Test-Loss: 0.13899263588461663 Train-f1: 0.5686917109735902 Test-f1: 0.5623953823953824 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14067/20000] Train-Loss: 0.3810316065786188 Test-Loss: 0.1417854647852168 Train-f1: 0.5170969320534538 Test-f1: 0.5641558441558441 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14068/20000] Train-Loss: 0.37830354490315304 Test-Loss: 0.13695962241306017 Train-f1: 0.6019867549668874 Test-f1: 0.4644611528822056 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14069/20000] Train-Loss: 0.37725778505850405 Test-Loss: 0.13735675339344247 Train-f1: 0.6108715247885754 Test-f1: 0.5616954528992294 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14070/20000] Train-Loss: 0.37824699809396306 Test-Loss: 0.13665661026539935 Train-f1: 0.6412892603850051 Test-f1: 0.4537451737451737 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14071/20000] Train-Loss: 0.38303383205819325 Test-Loss: 0.14245176848287638 Train-f1: 0.5898177149790053 Test-f1: 0.4774585921325052 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14072/20000] Train-Loss: 0.376543283657193 Test-Loss: 0.13873267302108858 Train-f1: 0.5225769464462199 Test-f1: 0.5281271777003484 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14073/20000] Train-Loss: 0.3764007342190272 Test-Loss: 0.15064299962702885 Train-f1: 0.5164608352747766 Test-f1: 0.475889328063241 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14074/20000] Train-Loss: 0.38233610456419825 Test-Loss: 0.14699587519694762 Train-f1: 0.555558789995876 Test-f1: 0.5603463203463204 Train-acc: 0.8178807947019867 Test-acc: 0.8157894736842105\n",
      "Epoch [14075/20000] Train-Loss: 0.3808700355386691 Test-Loss: 0.1429692815372296 Train-f1: 0.6025401459854015 Test-f1: 0.4829545454545454 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14076/20000] Train-Loss: 0.3807067577981744 Test-Loss: 0.1410316852250243 Train-f1: 0.534289551304948 Test-f1: 0.5161904761904762 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14077/20000] Train-Loss: 0.3785700120153646 Test-Loss: 0.13853173495439558 Train-f1: 0.5777256049971534 Test-f1: 0.5422818042172881 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14078/20000] Train-Loss: 0.3815952407410534 Test-Loss: 0.14858311324519932 Train-f1: 0.6011074337700806 Test-f1: 0.5603463203463204 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14079/20000] Train-Loss: 0.38236567953143846 Test-Loss: 0.13896578303925267 Train-f1: 0.6070840638582574 Test-f1: 0.5422818042172881 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14080/20000] Train-Loss: 0.3834187098188106 Test-Loss: 0.1381274744716763 Train-f1: 0.51944205585325 Test-f1: 0.5308287764866713 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14081/20000] Train-Loss: 0.3811483844977045 Test-Loss: 0.13672915704374558 Train-f1: 0.4941832271983676 Test-f1: 0.5213472706155633 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [14082/20000] Train-Loss: 0.3819389373596845 Test-Loss: 0.13681131988588593 Train-f1: 0.5335890919242556 Test-f1: 0.5308287764866713 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14083/20000] Train-Loss: 0.3791060926949852 Test-Loss: 0.14145280484762227 Train-f1: 0.5868308557439941 Test-f1: 0.5262301587301588 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14084/20000] Train-Loss: 0.37838139557136236 Test-Loss: 0.137455812315243 Train-f1: 0.5608498938650344 Test-f1: 0.5422818042172881 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14085/20000] Train-Loss: 0.37857903101279183 Test-Loss: 0.13870860762199175 Train-f1: 0.5177560117367364 Test-f1: 0.4630985798349032 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14086/20000] Train-Loss: 0.38431686898370104 Test-Loss: 0.13720704271501166 Train-f1: 0.578459595959596 Test-f1: 0.5181595881595882 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14087/20000] Train-Loss: 0.37926728246487673 Test-Loss: 0.13908113895254373 Train-f1: 0.5854494498789801 Test-f1: 0.4565609087348218 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [14088/20000] Train-Loss: 0.37961199591319816 Test-Loss: 0.14755229674698914 Train-f1: 0.5392966251607817 Test-f1: 0.5603463203463204 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14089/20000] Train-Loss: 0.3812914734883941 Test-Loss: 0.1403649049667415 Train-f1: 0.6029669707213091 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14090/20000] Train-Loss: 0.37408340113535116 Test-Loss: 0.1484496750400164 Train-f1: 0.5341437908496732 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14091/20000] Train-Loss: 0.38033548222385194 Test-Loss: 0.1379753487082469 Train-f1: 0.5396491707050713 Test-f1: 0.5165079365079366 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14092/20000] Train-Loss: 0.37881639693638813 Test-Loss: 0.13901307409239066 Train-f1: 0.5614471114471115 Test-f1: 0.5262301587301588 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14093/20000] Train-Loss: 0.384647340148278 Test-Loss: 0.14120735859932387 Train-f1: 0.5393365321758723 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14094/20000] Train-Loss: 0.3792181629267926 Test-Loss: 0.137510962462809 Train-f1: 0.5795930082988906 Test-f1: 0.5421972621972622 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14095/20000] Train-Loss: 0.381971033958611 Test-Loss: 0.1383424559425635 Train-f1: 0.5799149466233601 Test-f1: 0.5532874968169086 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14096/20000] Train-Loss: 0.375333793837062 Test-Loss: 0.1559391263214604 Train-f1: 0.5760816705853622 Test-f1: 0.5453954496208018 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14097/20000] Train-Loss: 0.38169737828115635 Test-Loss: 0.14057991433331404 Train-f1: 0.5968959864560187 Test-f1: 0.5625084825084825 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14098/20000] Train-Loss: 0.3797570284017074 Test-Loss: 0.13901087459910094 Train-f1: 0.592520417681708 Test-f1: 0.4746428571428572 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14099/20000] Train-Loss: 0.3808789362844242 Test-Loss: 0.14897826247231566 Train-f1: 0.4925798810847392 Test-f1: 0.4829545454545454 Train-acc: 0.8178807947019867 Test-acc: 0.8026315789473685\n",
      "Epoch [14100/20000] Train-Loss: 0.3769459549716153 Test-Loss: 0.13701822331927863 Train-f1: 0.5208689257840462 Test-f1: 0.4644611528822056 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14101/20000] Train-Loss: 0.3791970506301455 Test-Loss: 0.14016720058809404 Train-f1: 0.6073745635917545 Test-f1: 0.5532874968169086 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14102/20000] Train-Loss: 0.3805085601715567 Test-Loss: 0.1388462934318996 Train-f1: 0.5852131922373951 Test-f1: 0.5193236714975845 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14103/20000] Train-Loss: 0.3778994419216774 Test-Loss: 0.1385698384445763 Train-f1: 0.6521724765300924 Test-f1: 0.4456394609712688 Train-acc: 0.8377483443708609 Test-acc: 0.75\n",
      "Epoch [14104/20000] Train-Loss: 0.3825164655483195 Test-Loss: 0.14115585738006017 Train-f1: 0.6243749964570567 Test-f1: 0.5625084825084825 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14105/20000] Train-Loss: 0.37979160474926615 Test-Loss: 0.13985423911595482 Train-f1: 0.5851499316854007 Test-f1: 0.49437548083920735 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14106/20000] Train-Loss: 0.3749036626997055 Test-Loss: 0.13703950684438654 Train-f1: 0.6123620309050773 Test-f1: 0.4644611528822056 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [14107/20000] Train-Loss: 0.38002450454965925 Test-Loss: 0.14029002769602306 Train-f1: 0.5816117396258745 Test-f1: 0.5281271777003484 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14108/20000] Train-Loss: 0.37510758950865064 Test-Loss: 0.14617188604375753 Train-f1: 0.615539868520001 Test-f1: 0.549891398923657 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14109/20000] Train-Loss: 0.37774490296983665 Test-Loss: 0.14233732631494458 Train-f1: 0.5746178011601504 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14110/20000] Train-Loss: 0.3783591661488329 Test-Loss: 0.14330216853535052 Train-f1: 0.6318130491062821 Test-f1: 0.48217054263565895 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14111/20000] Train-Loss: 0.3789395507932335 Test-Loss: 0.1394385653372726 Train-f1: 0.5876285680710749 Test-f1: 0.5309429126899965 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14112/20000] Train-Loss: 0.37534346887298 Test-Loss: 0.14170054743542515 Train-f1: 0.5681120265315979 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14113/20000] Train-Loss: 0.37980111922319415 Test-Loss: 0.140054237776682 Train-f1: 0.612284617289905 Test-f1: 0.4740245261984392 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14114/20000] Train-Loss: 0.38117612535473633 Test-Loss: 0.15000432137941858 Train-f1: 0.5314404258498882 Test-f1: 0.47125448028673833 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14115/20000] Train-Loss: 0.3755406846472556 Test-Loss: 0.14166384023252307 Train-f1: 0.5927432216905901 Test-f1: 0.5532874968169086 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14116/20000] Train-Loss: 0.37164751422555775 Test-Loss: 0.15208353740292888 Train-f1: 0.6206708816297858 Test-f1: 0.47555555555555556 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14117/20000] Train-Loss: 0.3785525528498414 Test-Loss: 0.1408589326184547 Train-f1: 0.6285770369980896 Test-f1: 0.5309429126899965 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14118/20000] Train-Loss: 0.37842201636249284 Test-Loss: 0.13833300723757302 Train-f1: 0.5800141120037463 Test-f1: 0.5421972621972622 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14119/20000] Train-Loss: 0.3777211304003876 Test-Loss: 0.14616712989853753 Train-f1: 0.5703146716795489 Test-f1: 0.5821288515406162 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14120/20000] Train-Loss: 0.38307166458695124 Test-Loss: 0.14106649215279227 Train-f1: 0.5849048718613936 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14121/20000] Train-Loss: 0.37816398414363495 Test-Loss: 0.14853554917248682 Train-f1: 0.59823361319782 Test-f1: 0.475889328063241 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14122/20000] Train-Loss: 0.3788069253279722 Test-Loss: 0.13808885025784134 Train-f1: 0.5972068126520681 Test-f1: 0.5393083387201034 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14123/20000] Train-Loss: 0.3799198134401811 Test-Loss: 0.13802818136549344 Train-f1: 0.5822596749655573 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14124/20000] Train-Loss: 0.38076626434755695 Test-Loss: 0.1478435838813737 Train-f1: 0.604499939616428 Test-f1: 0.549891398923657 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14125/20000] Train-Loss: 0.3748427472460173 Test-Loss: 0.1379179822574947 Train-f1: 0.5768423706836656 Test-f1: 0.5308287764866713 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14126/20000] Train-Loss: 0.3775285443593281 Test-Loss: 0.13737538155619755 Train-f1: 0.5853121252487794 Test-f1: 0.4456394609712688 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14127/20000] Train-Loss: 0.37794326434436876 Test-Loss: 0.1368474899567568 Train-f1: 0.6611911077428319 Test-f1: 0.5056027456027457 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14128/20000] Train-Loss: 0.3816610110770115 Test-Loss: 0.14641719028643518 Train-f1: 0.6537754961802388 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14129/20000] Train-Loss: 0.3755024593045705 Test-Loss: 0.13677287831245816 Train-f1: 0.5823905442762187 Test-f1: 0.5073487773487774 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14130/20000] Train-Loss: 0.38385210611148923 Test-Loss: 0.1376038120484955 Train-f1: 0.5590664617748752 Test-f1: 0.46727688787185356 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [14131/20000] Train-Loss: 0.3800564877115038 Test-Loss: 0.1368744652438192 Train-f1: 0.5618817783523665 Test-f1: 0.5181595881595882 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14132/20000] Train-Loss: 0.38554048247691003 Test-Loss: 0.13843513686236134 Train-f1: 0.6361908407944215 Test-f1: 0.574856124267889 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14133/20000] Train-Loss: 0.3806973167039814 Test-Loss: 0.13822354709102413 Train-f1: 0.6071214499785929 Test-f1: 0.5776243470361118 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14134/20000] Train-Loss: 0.3790940056905784 Test-Loss: 0.13855809510622238 Train-f1: 0.5869939584225299 Test-f1: 0.5641558441558441 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14135/20000] Train-Loss: 0.37605114439134013 Test-Loss: 0.14289643475770955 Train-f1: 0.5918941468832346 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14136/20000] Train-Loss: 0.3797702316607509 Test-Loss: 0.14627435006564893 Train-f1: 0.5305195759514086 Test-f1: 0.5603463203463204 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14137/20000] Train-Loss: 0.3789557423662521 Test-Loss: 0.14173028371863286 Train-f1: 0.5351790886612504 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14138/20000] Train-Loss: 0.3733624686091349 Test-Loss: 0.13855346086789294 Train-f1: 0.6430596101381469 Test-f1: 0.5276190476190477 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14139/20000] Train-Loss: 0.3704364823692039 Test-Loss: 0.14157958296773765 Train-f1: 0.6334881380917187 Test-f1: 0.574856124267889 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14140/20000] Train-Loss: 0.3800200574574212 Test-Loss: 0.1416795578575861 Train-f1: 0.6073570060627658 Test-f1: 0.4774585921325052 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14141/20000] Train-Loss: 0.37605508497718204 Test-Loss: 0.1363349418470769 Train-f1: 0.5855031942625873 Test-f1: 0.49631578947368427 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [14142/20000] Train-Loss: 0.3766108776393108 Test-Loss: 0.14416240385371162 Train-f1: 0.4973315087239138 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14143/20000] Train-Loss: 0.37652249677152183 Test-Loss: 0.14037121011185544 Train-f1: 0.6482963931568051 Test-f1: 0.5532874968169086 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14144/20000] Train-Loss: 0.37959424674753844 Test-Loss: 0.14497206102987303 Train-f1: 0.6256296685770371 Test-f1: 0.4656685910608086 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14145/20000] Train-Loss: 0.37829938332887936 Test-Loss: 0.1388453667928818 Train-f1: 0.5587719298245614 Test-f1: 0.574856124267889 Train-acc: 0.8211920529801324 Test-acc: 0.8289473684210527\n",
      "Epoch [14146/20000] Train-Loss: 0.3774852974537072 Test-Loss: 0.15307630759548985 Train-f1: 0.587237069817715 Test-f1: 0.5603463203463204 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14147/20000] Train-Loss: 0.38416243944127815 Test-Loss: 0.14004814375083452 Train-f1: 0.5922059763774301 Test-f1: 0.5532874968169086 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [14148/20000] Train-Loss: 0.3809951733274218 Test-Loss: 0.13915041070871495 Train-f1: 0.5321937988127246 Test-f1: 0.5532874968169086 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14149/20000] Train-Loss: 0.37692242127004955 Test-Loss: 0.13971553066078238 Train-f1: 0.6412119412119412 Test-f1: 0.5624077238550923 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14150/20000] Train-Loss: 0.3789243301059146 Test-Loss: 0.14569639044195837 Train-f1: 0.620624496668863 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14151/20000] Train-Loss: 0.3755469375288461 Test-Loss: 0.1413557508489252 Train-f1: 0.5898177149790053 Test-f1: 0.4834843205574913 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14152/20000] Train-Loss: 0.3755789270358962 Test-Loss: 0.15030682847620236 Train-f1: 0.6580816725010451 Test-f1: 0.5292719919110211 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14153/20000] Train-Loss: 0.3872792936721461 Test-Loss: 0.1407160180586196 Train-f1: 0.5097160387576694 Test-f1: 0.4746428571428572 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [14154/20000] Train-Loss: 0.38354835219077443 Test-Loss: 0.14486894786710253 Train-f1: 0.5976680278779526 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14155/20000] Train-Loss: 0.3795589421588029 Test-Loss: 0.1399782130971327 Train-f1: 0.5850221232235872 Test-f1: 0.5262301587301588 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14156/20000] Train-Loss: 0.37464532588614063 Test-Loss: 0.14074790269073711 Train-f1: 0.5848628323080878 Test-f1: 0.5309429126899965 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14157/20000] Train-Loss: 0.37523720716783127 Test-Loss: 0.13716581076605983 Train-f1: 0.5237155010887731 Test-f1: 0.5213472706155633 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14158/20000] Train-Loss: 0.382967436979964 Test-Loss: 0.14122181199273373 Train-f1: 0.590525834565879 Test-f1: 0.47333333333333333 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14159/20000] Train-Loss: 0.37504047510874805 Test-Loss: 0.14212320762940478 Train-f1: 0.6332091332091332 Test-f1: 0.4774585921325052 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14160/20000] Train-Loss: 0.3785408876456092 Test-Loss: 0.14518523874742567 Train-f1: 0.5929907714664258 Test-f1: 0.549891398923657 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14161/20000] Train-Loss: 0.37724032783227546 Test-Loss: 0.1362319087841479 Train-f1: 0.6178107606679035 Test-f1: 0.4395848595848596 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14162/20000] Train-Loss: 0.3799624404057646 Test-Loss: 0.13725970869567838 Train-f1: 0.5817087674230532 Test-f1: 0.44240059457450764 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [14163/20000] Train-Loss: 0.3822667753705143 Test-Loss: 0.13889938869957646 Train-f1: 0.5871539617817164 Test-f1: 0.525909645909646 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14164/20000] Train-Loss: 0.3766766014515599 Test-Loss: 0.13760398338142657 Train-f1: 0.6306812735793665 Test-f1: 0.46727688787185356 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14165/20000] Train-Loss: 0.3775818095619842 Test-Loss: 0.13846018004706712 Train-f1: 0.5883582119735195 Test-f1: 0.4739605110336818 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14166/20000] Train-Loss: 0.3779312720950266 Test-Loss: 0.13885122544126824 Train-f1: 0.6488832319626477 Test-f1: 0.46727688787185356 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14167/20000] Train-Loss: 0.3766095718943895 Test-Loss: 0.14036627595996923 Train-f1: 0.566840305041769 Test-f1: 0.46367328426824994 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14168/20000] Train-Loss: 0.38092784944555075 Test-Loss: 0.148691138760143 Train-f1: 0.5080592809925327 Test-f1: 0.47580280416101317 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [14169/20000] Train-Loss: 0.3843037999539046 Test-Loss: 0.13930980247588762 Train-f1: 0.5199380717105688 Test-f1: 0.5281271777003484 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14170/20000] Train-Loss: 0.3767069051413179 Test-Loss: 0.13799799591764594 Train-f1: 0.6388112062214537 Test-f1: 0.5213472706155633 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14171/20000] Train-Loss: 0.37871978103557397 Test-Loss: 0.14023573616757545 Train-f1: 0.6044636546428962 Test-f1: 0.5641558441558441 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14172/20000] Train-Loss: 0.378870767458816 Test-Loss: 0.14721174203966872 Train-f1: 0.4955866287135636 Test-f1: 0.5696804511278196 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14173/20000] Train-Loss: 0.38222932691457767 Test-Loss: 0.1442375412717115 Train-f1: 0.5846815256472639 Test-f1: 0.5189908501970213 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14174/20000] Train-Loss: 0.38052103773553675 Test-Loss: 0.14087455019811498 Train-f1: 0.5654274627732085 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14175/20000] Train-Loss: 0.383654139895552 Test-Loss: 0.137661765692562 Train-f1: 0.5848740540505246 Test-f1: 0.4456394609712688 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14176/20000] Train-Loss: 0.3767599221514766 Test-Loss: 0.1387923241414611 Train-f1: 0.6580816725010451 Test-f1: 0.4746428571428572 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14177/20000] Train-Loss: 0.3834843134657605 Test-Loss: 0.14105871123672703 Train-f1: 0.5961095045305571 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14178/20000] Train-Loss: 0.3780148807190007 Test-Loss: 0.13813331735812867 Train-f1: 0.6510909324218495 Test-f1: 0.5532874968169086 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14179/20000] Train-Loss: 0.37562966100189077 Test-Loss: 0.1382386992554212 Train-f1: 0.6073745635917545 Test-f1: 0.5641558441558441 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14180/20000] Train-Loss: 0.37826158034824775 Test-Loss: 0.1442239795778721 Train-f1: 0.5844298558584273 Test-f1: 0.5303571428571429 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14181/20000] Train-Loss: 0.37431064068176023 Test-Loss: 0.13976955561530108 Train-f1: 0.6290419500939969 Test-f1: 0.4565609087348218 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14182/20000] Train-Loss: 0.37792704644739483 Test-Loss: 0.14023693633082404 Train-f1: 0.5681029786990053 Test-f1: 0.5421972621972622 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14183/20000] Train-Loss: 0.38044452699861037 Test-Loss: 0.14080876725263444 Train-f1: 0.6042326276003435 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14184/20000] Train-Loss: 0.3788729542048346 Test-Loss: 0.13954097090441586 Train-f1: 0.587961131871296 Test-f1: 0.5309429126899965 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14185/20000] Train-Loss: 0.3780177008628334 Test-Loss: 0.13646173365974062 Train-f1: 0.6172964569676899 Test-f1: 0.49631578947368427 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [14186/20000] Train-Loss: 0.37508932106727183 Test-Loss: 0.14570418896703627 Train-f1: 0.5601998001998002 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14187/20000] Train-Loss: 0.37594439843091415 Test-Loss: 0.13896409864892234 Train-f1: 0.5576341690420347 Test-f1: 0.46727688787185356 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [14188/20000] Train-Loss: 0.3778102086536071 Test-Loss: 0.14105515244652303 Train-f1: 0.5343639797055946 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14189/20000] Train-Loss: 0.3786166591531413 Test-Loss: 0.150763847614988 Train-f1: 0.5703146716795489 Test-f1: 0.5292719919110211 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14190/20000] Train-Loss: 0.3768410666897804 Test-Loss: 0.1374720031344063 Train-f1: 0.4946701463534948 Test-f1: 0.5308287764866713 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [14191/20000] Train-Loss: 0.3767283438647857 Test-Loss: 0.13792389712180703 Train-f1: 0.631015064315634 Test-f1: 0.5616954528992294 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14192/20000] Train-Loss: 0.3785409944362689 Test-Loss: 0.13858113908125602 Train-f1: 0.650281935978419 Test-f1: 0.5281271777003484 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14193/20000] Train-Loss: 0.3771026958993774 Test-Loss: 0.14468512662675306 Train-f1: 0.5848740540505246 Test-f1: 0.4656685910608086 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14194/20000] Train-Loss: 0.3809225614715138 Test-Loss: 0.13740368023243188 Train-f1: 0.6536704687599828 Test-f1: 0.5181595881595882 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14195/20000] Train-Loss: 0.37608968027931333 Test-Loss: 0.14349273183341968 Train-f1: 0.5892337012564216 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14196/20000] Train-Loss: 0.3777291349327596 Test-Loss: 0.14337872039890873 Train-f1: 0.5395884941483047 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14197/20000] Train-Loss: 0.37801705425787924 Test-Loss: 0.14415401740382466 Train-f1: 0.5420777420777421 Test-f1: 0.549891398923657 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14198/20000] Train-Loss: 0.3797146273673122 Test-Loss: 0.13912475935290008 Train-f1: 0.6567154131979931 Test-f1: 0.5262301587301588 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14199/20000] Train-Loss: 0.376610792100352 Test-Loss: 0.150283307048686 Train-f1: 0.5342855895004391 Test-f1: 0.5804381846635367 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14200/20000] Train-Loss: 0.3740233529736744 Test-Loss: 0.1381268435313238 Train-f1: 0.5658236412466872 Test-f1: 0.5213472706155633 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14201/20000] Train-Loss: 0.37609905193680243 Test-Loss: 0.13926839499985103 Train-f1: 0.6281710563040487 Test-f1: 0.574856124267889 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [14202/20000] Train-Loss: 0.373471859975553 Test-Loss: 0.13844606648977006 Train-f1: 0.634176491373211 Test-f1: 0.5616954528992294 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14203/20000] Train-Loss: 0.38084176273894893 Test-Loss: 0.14152479361072864 Train-f1: 0.5868308557439941 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14204/20000] Train-Loss: 0.3763007210182473 Test-Loss: 0.14592580886666662 Train-f1: 0.631015064315634 Test-f1: 0.549891398923657 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14205/20000] Train-Loss: 0.3792067281353157 Test-Loss: 0.13761312851683144 Train-f1: 0.5963607820750678 Test-f1: 0.528796992481203 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14206/20000] Train-Loss: 0.3772987035233317 Test-Loss: 0.14060487289817397 Train-f1: 0.5251743490436225 Test-f1: 0.574856124267889 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14207/20000] Train-Loss: 0.3785416083825756 Test-Loss: 0.13807147790075144 Train-f1: 0.6596246033506403 Test-f1: 0.5378013661604993 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14208/20000] Train-Loss: 0.3743683565629429 Test-Loss: 0.1444530443366911 Train-f1: 0.5198923826878306 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14209/20000] Train-Loss: 0.3763795083829149 Test-Loss: 0.1448500019938392 Train-f1: 0.5330487820783139 Test-f1: 0.4770348837209302 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14210/20000] Train-Loss: 0.3785728623809351 Test-Loss: 0.13663487985261522 Train-f1: 0.6312155127944601 Test-f1: 0.4537451737451737 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [14211/20000] Train-Loss: 0.37899845058386755 Test-Loss: 0.13762199439300954 Train-f1: 0.5865998287014413 Test-f1: 0.5165079365079366 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14212/20000] Train-Loss: 0.3768502408962648 Test-Loss: 0.14193247380693833 Train-f1: 0.541108034701774 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14213/20000] Train-Loss: 0.3774706103439145 Test-Loss: 0.13677158836939726 Train-f1: 0.5976680278779526 Test-f1: 0.5073487773487774 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14214/20000] Train-Loss: 0.3830512666595385 Test-Loss: 0.14211522100824797 Train-f1: 0.5876285680710749 Test-f1: 0.5290458937198068 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14215/20000] Train-Loss: 0.3726731438630754 Test-Loss: 0.15146980123798767 Train-f1: 0.6058809713493257 Test-f1: 0.5244444444444445 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14216/20000] Train-Loss: 0.38069764579863763 Test-Loss: 0.13854982630994822 Train-f1: 0.49869164146217937 Test-f1: 0.5339348370927318 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14217/20000] Train-Loss: 0.3732366771014117 Test-Loss: 0.14357074474213688 Train-f1: 0.6377986554061941 Test-f1: 0.549891398923657 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14218/20000] Train-Loss: 0.37474614720982774 Test-Loss: 0.1456988356035878 Train-f1: 0.6454291050564342 Test-f1: 0.5264562569213732 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14219/20000] Train-Loss: 0.3775994194757531 Test-Loss: 0.13939761602870898 Train-f1: 0.5155673673398512 Test-f1: 0.5625084825084825 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14220/20000] Train-Loss: 0.38057900820462953 Test-Loss: 0.14175409881583803 Train-f1: 0.6048935788066223 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14221/20000] Train-Loss: 0.379054363274914 Test-Loss: 0.1389175807482399 Train-f1: 0.5850221232235872 Test-f1: 0.5625084825084825 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14222/20000] Train-Loss: 0.37651497855345284 Test-Loss: 0.14307106248690085 Train-f1: 0.59311028674086 Test-f1: 0.4770348837209302 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14223/20000] Train-Loss: 0.3805946260905856 Test-Loss: 0.14231743131310226 Train-f1: 0.6147818717230482 Test-f1: 0.4935574229691877 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14224/20000] Train-Loss: 0.37790396968359946 Test-Loss: 0.1373993664531664 Train-f1: 0.5287988139038501 Test-f1: 0.5165079365079366 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14225/20000] Train-Loss: 0.37772109705753554 Test-Loss: 0.14119266110770012 Train-f1: 0.6256989216098765 Test-f1: 0.5532874968169086 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14226/20000] Train-Loss: 0.3767935622065891 Test-Loss: 0.1363908707279822 Train-f1: 0.6213738398655388 Test-f1: 0.4537451737451737 Train-acc: 0.8443708609271523 Test-acc: 0.7631578947368421\n",
      "Epoch [14227/20000] Train-Loss: 0.3750522554989215 Test-Loss: 0.14390314585832542 Train-f1: 0.5025998714967973 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14228/20000] Train-Loss: 0.3778725331003955 Test-Loss: 0.13834759846102984 Train-f1: 0.5120603069181177 Test-f1: 0.5561904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14229/20000] Train-Loss: 0.3829062287519978 Test-Loss: 0.13673762242309245 Train-f1: 0.6419676967327973 Test-f1: 0.5056027456027457 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14230/20000] Train-Loss: 0.3774927169321701 Test-Loss: 0.1407048440229588 Train-f1: 0.573620728931681 Test-f1: 0.4774585921325052 Train-acc: 0.8211920529801324 Test-acc: 0.7894736842105263\n",
      "Epoch [14231/20000] Train-Loss: 0.37418782496725755 Test-Loss: 0.14045854377448766 Train-f1: 0.5371199022122165 Test-f1: 0.4774585921325052 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14232/20000] Train-Loss: 0.3792631705293258 Test-Loss: 0.13707434069910546 Train-f1: 0.6334237504391472 Test-f1: 0.4644611528822056 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14233/20000] Train-Loss: 0.3781018486972729 Test-Loss: 0.143205478818469 Train-f1: 0.5767219641672197 Test-f1: 0.45666404818015194 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [14234/20000] Train-Loss: 0.3810666775633529 Test-Loss: 0.1384320202757777 Train-f1: 0.6306117306117306 Test-f1: 0.5378013661604993 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14235/20000] Train-Loss: 0.375031108516374 Test-Loss: 0.13806755966810522 Train-f1: 0.5114148330978184 Test-f1: 0.5056027456027457 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [14236/20000] Train-Loss: 0.37639122773217704 Test-Loss: 0.13802229128053992 Train-f1: 0.6073675181933134 Test-f1: 0.46727688787185356 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14237/20000] Train-Loss: 0.37610860469252116 Test-Loss: 0.1436399116103988 Train-f1: 0.6386399482870071 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14238/20000] Train-Loss: 0.3808689817218415 Test-Loss: 0.142327648351176 Train-f1: 0.5967206786724099 Test-f1: 0.574856124267889 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [14239/20000] Train-Loss: 0.37812065285744934 Test-Loss: 0.14032812672087705 Train-f1: 0.5374996588579578 Test-f1: 0.5262301587301588 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14240/20000] Train-Loss: 0.37869392277604125 Test-Loss: 0.1452482906772955 Train-f1: 0.6528969144811908 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14241/20000] Train-Loss: 0.3809897258018421 Test-Loss: 0.1410029976668376 Train-f1: 0.5988010794061214 Test-f1: 0.48343865857768786 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14242/20000] Train-Loss: 0.3768080182248274 Test-Loss: 0.13726087510529875 Train-f1: 0.4997689041553361 Test-f1: 0.4644611528822056 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14243/20000] Train-Loss: 0.37788433686482625 Test-Loss: 0.1375111795531101 Train-f1: 0.5851789722999203 Test-f1: 0.5181595881595882 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14244/20000] Train-Loss: 0.37518437031167073 Test-Loss: 0.14620094188160357 Train-f1: 0.6386647801811124 Test-f1: 0.549891398923657 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14245/20000] Train-Loss: 0.3814277364786 Test-Loss: 0.13683241012276348 Train-f1: 0.4906454091432962 Test-f1: 0.5073487773487774 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [14246/20000] Train-Loss: 0.3783784706491653 Test-Loss: 0.14014822290977166 Train-f1: 0.5844764540136209 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14247/20000] Train-Loss: 0.37815743965480525 Test-Loss: 0.147487141684948 Train-f1: 0.5848740540505246 Test-f1: 0.5292719919110211 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14248/20000] Train-Loss: 0.37694906322095234 Test-Loss: 0.14478526407943196 Train-f1: 0.5685656448814344 Test-f1: 0.5532874968169086 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14249/20000] Train-Loss: 0.3759465812744741 Test-Loss: 0.13695522910762292 Train-f1: 0.6204543557247233 Test-f1: 0.4428237259816208 Train-acc: 0.8443708609271523 Test-acc: 0.75\n",
      "Epoch [14250/20000] Train-Loss: 0.3812549538733802 Test-Loss: 0.140392548112464 Train-f1: 0.4929792555445459 Test-f1: 0.5421240737097515 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14251/20000] Train-Loss: 0.37623448514621743 Test-Loss: 0.13863465720059623 Train-f1: 0.5152840556704876 Test-f1: 0.5532874968169086 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14252/20000] Train-Loss: 0.3775282888080807 Test-Loss: 0.13785287814795869 Train-f1: 0.6343614368824453 Test-f1: 0.5393083387201034 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14253/20000] Train-Loss: 0.3756156136242573 Test-Loss: 0.13919899489868026 Train-f1: 0.5012473118279569 Test-f1: 0.574856124267889 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [14254/20000] Train-Loss: 0.3752376929542372 Test-Loss: 0.13978755159236145 Train-f1: 0.6175808720112518 Test-f1: 0.574856124267889 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14255/20000] Train-Loss: 0.3798166065631446 Test-Loss: 0.13678263872189544 Train-f1: 0.6319510552784967 Test-f1: 0.5181595881595882 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14256/20000] Train-Loss: 0.3813316598417223 Test-Loss: 0.13822432595122428 Train-f1: 0.5316879505617775 Test-f1: 0.5378013661604993 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14257/20000] Train-Loss: 0.3705260088469749 Test-Loss: 0.15036250353193303 Train-f1: 0.6066066180196615 Test-f1: 0.47555555555555556 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14258/20000] Train-Loss: 0.37803514515842823 Test-Loss: 0.13838985945262028 Train-f1: 0.5964940376705082 Test-f1: 0.48827791986359764 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14259/20000] Train-Loss: 0.3749639726288054 Test-Loss: 0.14248639377150887 Train-f1: 0.554684916570591 Test-f1: 0.46306306306306305 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14260/20000] Train-Loss: 0.3761935505388634 Test-Loss: 0.13690052574022563 Train-f1: 0.6612480351610787 Test-f1: 0.5451919146036793 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [14261/20000] Train-Loss: 0.36924948580983324 Test-Loss: 0.15813344708854027 Train-f1: 0.562973026973027 Test-f1: 0.5223696350086644 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14262/20000] Train-Loss: 0.37290224048876863 Test-Loss: 0.1439551699904578 Train-f1: 0.6048220251800325 Test-f1: 0.43593384429205323 Train-acc: 0.8410596026490066 Test-acc: 0.7368421052631579\n",
      "Epoch [14263/20000] Train-Loss: 0.37648080825400926 Test-Loss: 0.13860812275268053 Train-f1: 0.5352444740211617 Test-f1: 0.574856124267889 Train-acc: 0.8543046357615894 Test-acc: 0.8289473684210527\n",
      "Epoch [14264/20000] Train-Loss: 0.373511035253008 Test-Loss: 0.13705271000981814 Train-f1: 0.5744822821120948 Test-f1: 0.5213472706155633 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14265/20000] Train-Loss: 0.3755090475353765 Test-Loss: 0.13754502285633524 Train-f1: 0.5477498691784406 Test-f1: 0.5165079365079366 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14266/20000] Train-Loss: 0.3802803714365826 Test-Loss: 0.13720422601566892 Train-f1: 0.6037850114290604 Test-f1: 0.48546218487394965 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14267/20000] Train-Loss: 0.37839472834076854 Test-Loss: 0.13916751642207026 Train-f1: 0.553077150748966 Test-f1: 0.5308287764866713 Train-acc: 0.8145695364238411 Test-acc: 0.7763157894736842\n",
      "Epoch [14268/20000] Train-Loss: 0.3738659119768062 Test-Loss: 0.13859771432448498 Train-f1: 0.5171929217810345 Test-f1: 0.5262301587301588 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14269/20000] Train-Loss: 0.37327972266425485 Test-Loss: 0.1371077759783365 Train-f1: 0.6310726800793025 Test-f1: 0.5181595881595882 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14270/20000] Train-Loss: 0.37935843202664105 Test-Loss: 0.14127489860970646 Train-f1: 0.596061985885562 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14271/20000] Train-Loss: 0.3754902396358135 Test-Loss: 0.13643775545934625 Train-f1: 0.5537407037407037 Test-f1: 0.4644611528822056 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14272/20000] Train-Loss: 0.3811592971952926 Test-Loss: 0.13703700333961408 Train-f1: 0.599196740373211 Test-f1: 0.4644611528822056 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14273/20000] Train-Loss: 0.37969453876467146 Test-Loss: 0.13726183899387198 Train-f1: 0.5600860215053765 Test-f1: 0.5561904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14274/20000] Train-Loss: 0.36786225009515783 Test-Loss: 0.1372366379936399 Train-f1: 0.6534643360338726 Test-f1: 0.4602828448452552 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14275/20000] Train-Loss: 0.3770154954168578 Test-Loss: 0.14041712890631267 Train-f1: 0.5942732267732268 Test-f1: 0.4746428571428572 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14276/20000] Train-Loss: 0.37207077039633824 Test-Loss: 0.1466065445492133 Train-f1: 0.6706037206037205 Test-f1: 0.5697812097812098 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14277/20000] Train-Loss: 0.3765779168038843 Test-Loss: 0.14424819748992632 Train-f1: 0.5264857386368764 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14278/20000] Train-Loss: 0.3777651703157804 Test-Loss: 0.14902031283041275 Train-f1: 0.5573408573408574 Test-f1: 0.549891398923657 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [14279/20000] Train-Loss: 0.37561090131545405 Test-Loss: 0.1446740439466801 Train-f1: 0.628541784925891 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14280/20000] Train-Loss: 0.37480514281853256 Test-Loss: 0.13738355422652537 Train-f1: 0.6703542947119105 Test-f1: 0.5165079365079366 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14281/20000] Train-Loss: 0.3720584487386856 Test-Loss: 0.13744949215673347 Train-f1: 0.604513822160881 Test-f1: 0.5308287764866713 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14282/20000] Train-Loss: 0.3792963642389811 Test-Loss: 0.13691831894467174 Train-f1: 0.5792510192510192 Test-f1: 0.5181595881595882 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14283/20000] Train-Loss: 0.37978380815771523 Test-Loss: 0.13823455674561266 Train-f1: 0.5398855008126532 Test-f1: 0.5532874968169086 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14284/20000] Train-Loss: 0.37652573416613166 Test-Loss: 0.13818608029836446 Train-f1: 0.625091190668402 Test-f1: 0.5776243470361118 Train-acc: 0.8278145695364238 Test-acc: 0.8289473684210527\n",
      "Epoch [14285/20000] Train-Loss: 0.37509488319371437 Test-Loss: 0.1398640697829419 Train-f1: 0.5972889502301266 Test-f1: 0.4630985798349032 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14286/20000] Train-Loss: 0.3800107612601142 Test-Loss: 0.1368875613639755 Train-f1: 0.5819604372118898 Test-f1: 0.4644611528822056 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14287/20000] Train-Loss: 0.3704535989358265 Test-Loss: 0.1363271232710065 Train-f1: 0.6527373923814412 Test-f1: 0.49631578947368427 Train-acc: 0.8443708609271523 Test-acc: 0.7631578947368421\n",
      "Epoch [14288/20000] Train-Loss: 0.37497951197241775 Test-Loss: 0.1417833130372126 Train-f1: 0.6350959625588134 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14289/20000] Train-Loss: 0.3729337367936989 Test-Loss: 0.1448345220850923 Train-f1: 0.6332091332091332 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14290/20000] Train-Loss: 0.37628866719906445 Test-Loss: 0.14535940510587603 Train-f1: 0.620948870948871 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14291/20000] Train-Loss: 0.3757116592763909 Test-Loss: 0.14142587809808232 Train-f1: 0.6167867926257189 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14292/20000] Train-Loss: 0.37689980564544584 Test-Loss: 0.13702373562500866 Train-f1: 0.5350115733852692 Test-f1: 0.5451919146036793 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14293/20000] Train-Loss: 0.3777260447307692 Test-Loss: 0.13787278459518404 Train-f1: 0.5174168735498659 Test-f1: 0.46727688787185356 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14294/20000] Train-Loss: 0.37507745305918544 Test-Loss: 0.1371538899536557 Train-f1: 0.650722172918715 Test-f1: 0.5181595881595882 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14295/20000] Train-Loss: 0.37601830775296075 Test-Loss: 0.142163290930361 Train-f1: 0.6040988270684743 Test-f1: 0.5290458937198068 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14296/20000] Train-Loss: 0.3787833907698567 Test-Loss: 0.14134637055106367 Train-f1: 0.5928938023145973 Test-f1: 0.5532874968169086 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14297/20000] Train-Loss: 0.3791955911619846 Test-Loss: 0.13653468841797275 Train-f1: 0.64341739995296 Test-f1: 0.5339348370927318 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14298/20000] Train-Loss: 0.37650825773874785 Test-Loss: 0.13897847980925826 Train-f1: 0.5845343671150123 Test-f1: 0.577671859257537 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14299/20000] Train-Loss: 0.3719579404765345 Test-Loss: 0.14009882252710756 Train-f1: 0.6510909324218495 Test-f1: 0.4630985798349032 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14300/20000] Train-Loss: 0.37586815798757883 Test-Loss: 0.13986800691736134 Train-f1: 0.5970704295704296 Test-f1: 0.574856124267889 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [14301/20000] Train-Loss: 0.37795357731998475 Test-Loss: 0.13714431860668203 Train-f1: 0.6335594931384405 Test-f1: 0.5056027456027457 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14302/20000] Train-Loss: 0.3737932528812717 Test-Loss: 0.1381503123117777 Train-f1: 0.599196740373211 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14303/20000] Train-Loss: 0.37463873673656833 Test-Loss: 0.15616462220391814 Train-f1: 0.5275951421646786 Test-f1: 0.5603463203463204 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14304/20000] Train-Loss: 0.3782991987795248 Test-Loss: 0.14514443938357693 Train-f1: 0.5826631186262629 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14305/20000] Train-Loss: 0.3766501463498967 Test-Loss: 0.13846054232172375 Train-f1: 0.5922254465595427 Test-f1: 0.5308287764866713 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [14306/20000] Train-Loss: 0.37117858714839325 Test-Loss: 0.14425618375993887 Train-f1: 0.6250173124515412 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14307/20000] Train-Loss: 0.3769023906077115 Test-Loss: 0.13707954784968662 Train-f1: 0.6430193146454226 Test-f1: 0.5308287764866713 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14308/20000] Train-Loss: 0.37522916271996465 Test-Loss: 0.1408516790140706 Train-f1: 0.6045704610641497 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14309/20000] Train-Loss: 0.37570146071673965 Test-Loss: 0.14225958444802003 Train-f1: 0.6195715467328371 Test-f1: 0.5309429126899965 Train-acc: 0.8543046357615894 Test-acc: 0.8026315789473685\n",
      "Epoch [14310/20000] Train-Loss: 0.3731046142164749 Test-Loss: 0.14722964267905025 Train-f1: 0.6442095059929456 Test-f1: 0.46539589442815255 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14311/20000] Train-Loss: 0.3739839517626579 Test-Loss: 0.13662015527074464 Train-f1: 0.6486340553145682 Test-f1: 0.5213472706155633 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14312/20000] Train-Loss: 0.37329880351706385 Test-Loss: 0.1365836558444444 Train-f1: 0.5428445556582322 Test-f1: 0.4395848595848596 Train-acc: 0.8443708609271523 Test-acc: 0.75\n",
      "Epoch [14313/20000] Train-Loss: 0.3725836511071883 Test-Loss: 0.1403898342314438 Train-f1: 0.5951010628429984 Test-f1: 0.555110087045571 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14314/20000] Train-Loss: 0.3760770383022636 Test-Loss: 0.13786356617550444 Train-f1: 0.6285128100917575 Test-f1: 0.5209753231492362 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14315/20000] Train-Loss: 0.3712103014158676 Test-Loss: 0.14307406304386333 Train-f1: 0.6360227781977521 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14316/20000] Train-Loss: 0.37106101615526543 Test-Loss: 0.14960486062671213 Train-f1: 0.6675908494021703 Test-f1: 0.47555555555555556 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14317/20000] Train-Loss: 0.3813183804144506 Test-Loss: 0.14236174664089113 Train-f1: 0.5907889502066026 Test-f1: 0.574856124267889 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14318/20000] Train-Loss: 0.37123570094334213 Test-Loss: 0.13986165939093292 Train-f1: 0.6278348989566901 Test-f1: 0.5656318681318682 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14319/20000] Train-Loss: 0.37976363377932537 Test-Loss: 0.13954710708033966 Train-f1: 0.5879849061203464 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14320/20000] Train-Loss: 0.3758759937543655 Test-Loss: 0.138167926962695 Train-f1: 0.5968529584372348 Test-f1: 0.5101645123384253 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14321/20000] Train-Loss: 0.37800524347828673 Test-Loss: 0.13814304774395217 Train-f1: 0.5247624633431085 Test-f1: 0.5308287764866713 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14322/20000] Train-Loss: 0.37455267709834783 Test-Loss: 0.14109012790302905 Train-f1: 0.6098370682412279 Test-f1: 0.5309429126899965 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14323/20000] Train-Loss: 0.3714206778449743 Test-Loss: 0.14024064396078503 Train-f1: 0.639293726250248 Test-f1: 0.5422818042172881 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14324/20000] Train-Loss: 0.3820716668886522 Test-Loss: 0.13870053247406033 Train-f1: 0.6079423752980799 Test-f1: 0.5532874968169086 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [14325/20000] Train-Loss: 0.38101739109792343 Test-Loss: 0.13800734291326414 Train-f1: 0.5818501331902166 Test-f1: 0.5616954528992294 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14326/20000] Train-Loss: 0.37845536844741196 Test-Loss: 0.1372653818063051 Train-f1: 0.5311254955090572 Test-f1: 0.5421972621972622 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14327/20000] Train-Loss: 0.383068661459273 Test-Loss: 0.14019729729363858 Train-f1: 0.6105881964126361 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14328/20000] Train-Loss: 0.3825703182935334 Test-Loss: 0.1423018875997198 Train-f1: 0.6259454580507212 Test-f1: 0.4746428571428572 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14329/20000] Train-Loss: 0.3762401195223888 Test-Loss: 0.1446518474458119 Train-f1: 0.6404727703549179 Test-f1: 0.5624077238550923 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14330/20000] Train-Loss: 0.3692700892644191 Test-Loss: 0.1410071633176107 Train-f1: 0.6285770369980896 Test-f1: 0.5728065640103406 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14331/20000] Train-Loss: 0.37990767091356076 Test-Loss: 0.14953523534028199 Train-f1: 0.6717559771955597 Test-f1: 0.5637166085946574 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14332/20000] Train-Loss: 0.37289044782693265 Test-Loss: 0.13838823133644684 Train-f1: 0.6560560845289385 Test-f1: 0.5378013661604993 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14333/20000] Train-Loss: 0.37447970501521693 Test-Loss: 0.14173415754882907 Train-f1: 0.5929907714664258 Test-f1: 0.5290458937198068 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14334/20000] Train-Loss: 0.3728838299161142 Test-Loss: 0.14377328337296613 Train-f1: 0.5223785846076867 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14335/20000] Train-Loss: 0.37548539677918813 Test-Loss: 0.13815969215156257 Train-f1: 0.4996290114578287 Test-f1: 0.5561904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14336/20000] Train-Loss: 0.37682784438965067 Test-Loss: 0.13844502389526353 Train-f1: 0.6482650633545775 Test-f1: 0.5561904761904762 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14337/20000] Train-Loss: 0.37784020447266015 Test-Loss: 0.14435477362280258 Train-f1: 0.5084826029862946 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14338/20000] Train-Loss: 0.36800048347972114 Test-Loss: 0.14545604815410468 Train-f1: 0.658545332458376 Test-f1: 0.5141038802093091 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14339/20000] Train-Loss: 0.375758701439627 Test-Loss: 0.14455166776686684 Train-f1: 0.5776974547482385 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14340/20000] Train-Loss: 0.3758850415406466 Test-Loss: 0.13657894135276394 Train-f1: 0.6024054933339336 Test-f1: 0.4944577129097253 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [14341/20000] Train-Loss: 0.3717161962788965 Test-Loss: 0.13811250797847036 Train-f1: 0.6050604357760588 Test-f1: 0.48546218487394965 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14342/20000] Train-Loss: 0.37578815524326326 Test-Loss: 0.1390307066695388 Train-f1: 0.6105746514747699 Test-f1: 0.574856124267889 Train-acc: 0.8278145695364238 Test-acc: 0.8289473684210527\n",
      "Epoch [14343/20000] Train-Loss: 0.3770786625823398 Test-Loss: 0.13923842879775358 Train-f1: 0.5148108648108648 Test-f1: 0.5056027456027457 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14344/20000] Train-Loss: 0.3763094487822491 Test-Loss: 0.1371403337841589 Train-f1: 0.6335594931384405 Test-f1: 0.4644611528822056 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14345/20000] Train-Loss: 0.3751403855271727 Test-Loss: 0.1368296688873018 Train-f1: 0.5095214487132645 Test-f1: 0.5002197802197803 Train-acc: 0.8211920529801324 Test-acc: 0.7631578947368421\n",
      "Epoch [14346/20000] Train-Loss: 0.3747563701875431 Test-Loss: 0.14047470826945246 Train-f1: 0.5420811682272634 Test-f1: 0.48343865857768786 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14347/20000] Train-Loss: 0.37871186052061573 Test-Loss: 0.14521527129570005 Train-f1: 0.5348604654793914 Test-f1: 0.4656685910608086 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14348/20000] Train-Loss: 0.37589338800150707 Test-Loss: 0.13974408628025953 Train-f1: 0.5803495371237306 Test-f1: 0.4740245261984392 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14349/20000] Train-Loss: 0.3773092926154795 Test-Loss: 0.14110307898002156 Train-f1: 0.5246155863990258 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14350/20000] Train-Loss: 0.369090006564169 Test-Loss: 0.15235290838902163 Train-f1: 0.5220021589149105 Test-f1: 0.549891398923657 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14351/20000] Train-Loss: 0.3787211694442569 Test-Loss: 0.1380283720511994 Train-f1: 0.6485917928889423 Test-f1: 0.5421972621972622 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14352/20000] Train-Loss: 0.3707817491442223 Test-Loss: 0.1539841954370364 Train-f1: 0.6200277883036505 Test-f1: 0.47555555555555556 Train-acc: 0.8609271523178808 Test-acc: 0.7894736842105263\n",
      "Epoch [14353/20000] Train-Loss: 0.3754308566008615 Test-Loss: 0.14118649339965572 Train-f1: 0.6334237504391472 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14354/20000] Train-Loss: 0.37439419287475456 Test-Loss: 0.14081240100564663 Train-f1: 0.5876285680710749 Test-f1: 0.5309429126899965 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14355/20000] Train-Loss: 0.3747441263221013 Test-Loss: 0.13634975979113173 Train-f1: 0.5301016073703555 Test-f1: 0.49631578947368427 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [14356/20000] Train-Loss: 0.3760667140534178 Test-Loss: 0.14145011861947449 Train-f1: 0.5171528555832353 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14357/20000] Train-Loss: 0.37431950156912924 Test-Loss: 0.14127104937096857 Train-f1: 0.5793730767924317 Test-f1: 0.5281271777003484 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14358/20000] Train-Loss: 0.37346648844906977 Test-Loss: 0.14248288971785628 Train-f1: 0.5657739135729007 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14359/20000] Train-Loss: 0.3767082242840037 Test-Loss: 0.1383646588814264 Train-f1: 0.5743756243756244 Test-f1: 0.5776243470361118 Train-acc: 0.8211920529801324 Test-acc: 0.8289473684210527\n",
      "Epoch [14360/20000] Train-Loss: 0.3762641050506517 Test-Loss: 0.13723683836715533 Train-f1: 0.5873495631884894 Test-f1: 0.5073487773487774 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14361/20000] Train-Loss: 0.3771408860396829 Test-Loss: 0.14204816997790304 Train-f1: 0.5171296612290401 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14362/20000] Train-Loss: 0.37273150959510704 Test-Loss: 0.13619983999317722 Train-f1: 0.6508693567517096 Test-f1: 0.4395848595848596 Train-acc: 0.8410596026490066 Test-acc: 0.75\n",
      "Epoch [14363/20000] Train-Loss: 0.3759391205711845 Test-Loss: 0.14187872430731258 Train-f1: 0.5196271780467494 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14364/20000] Train-Loss: 0.3763973184716521 Test-Loss: 0.13685399721506988 Train-f1: 0.5873495631884894 Test-f1: 0.5165079365079366 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14365/20000] Train-Loss: 0.3756880122370074 Test-Loss: 0.13800057961529208 Train-f1: 0.5918076076472204 Test-f1: 0.48546218487394965 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14366/20000] Train-Loss: 0.37558207606623323 Test-Loss: 0.1377827422357593 Train-f1: 0.5825274581057059 Test-f1: 0.4456394609712688 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [14367/20000] Train-Loss: 0.3741997021109208 Test-Loss: 0.13946915294868095 Train-f1: 0.6044003615432187 Test-f1: 0.5532874968169086 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14368/20000] Train-Loss: 0.3745782945605659 Test-Loss: 0.14087642565075234 Train-f1: 0.635911835911836 Test-f1: 0.5290458937198068 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14369/20000] Train-Loss: 0.3765198117405204 Test-Loss: 0.14090271666846768 Train-f1: 0.6310726800793025 Test-f1: 0.4888974581445419 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14370/20000] Train-Loss: 0.3785416740713007 Test-Loss: 0.1372535576141957 Train-f1: 0.5666666666666667 Test-f1: 0.4537451737451737 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14371/20000] Train-Loss: 0.37624999010645427 Test-Loss: 0.14535963739433436 Train-f1: 0.5650458125435377 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14372/20000] Train-Loss: 0.3727166839731617 Test-Loss: 0.1433475509813873 Train-f1: 0.6364817927170868 Test-f1: 0.5533179723502304 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14373/20000] Train-Loss: 0.3757791651511515 Test-Loss: 0.14643127667285027 Train-f1: 0.5878165983520675 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14374/20000] Train-Loss: 0.37646778609612597 Test-Loss: 0.1383851789060575 Train-f1: 0.4906237250608111 Test-f1: 0.5776243470361118 Train-acc: 0.8178807947019867 Test-acc: 0.8289473684210527\n",
      "Epoch [14375/20000] Train-Loss: 0.37302635678898255 Test-Loss: 0.1381257145086016 Train-f1: 0.6020728299950902 Test-f1: 0.5532874968169086 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14376/20000] Train-Loss: 0.3719323872252007 Test-Loss: 0.1393735524562073 Train-f1: 0.6059384288747347 Test-f1: 0.5262301587301588 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14377/20000] Train-Loss: 0.3724918736948607 Test-Loss: 0.14567810514092625 Train-f1: 0.5215821040273595 Test-f1: 0.516919615198957 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14378/20000] Train-Loss: 0.37529251557176657 Test-Loss: 0.1424998320785197 Train-f1: 0.5174492174492175 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14379/20000] Train-Loss: 0.37046018270735265 Test-Loss: 0.14699056542267797 Train-f1: 0.5852358703523588 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14380/20000] Train-Loss: 0.3759821615575668 Test-Loss: 0.1459734226054308 Train-f1: 0.5589915992939977 Test-f1: 0.549891398923657 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14381/20000] Train-Loss: 0.3797857769493614 Test-Loss: 0.14007206659968183 Train-f1: 0.6712351556489489 Test-f1: 0.5290458937198068 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14382/20000] Train-Loss: 0.3764369629069575 Test-Loss: 0.14214757042914025 Train-f1: 0.6510909324218495 Test-f1: 0.4774585921325052 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14383/20000] Train-Loss: 0.37170744567106667 Test-Loss: 0.1512145456374344 Train-f1: 0.5763242428978403 Test-f1: 0.5603463203463204 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14384/20000] Train-Loss: 0.3810031538875394 Test-Loss: 0.14427771660278446 Train-f1: 0.6055611652770801 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14385/20000] Train-Loss: 0.3734182862528379 Test-Loss: 0.14908860425480416 Train-f1: 0.6146807109940751 Test-f1: 0.5603463203463204 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14386/20000] Train-Loss: 0.381511853234077 Test-Loss: 0.14473203914973126 Train-f1: 0.5121990883502262 Test-f1: 0.549891398923657 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14387/20000] Train-Loss: 0.37511406319859736 Test-Loss: 0.14492495491051047 Train-f1: 0.6094750881759279 Test-f1: 0.549891398923657 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14388/20000] Train-Loss: 0.3723183455303161 Test-Loss: 0.13916406651667104 Train-f1: 0.6108457752293368 Test-f1: 0.46367328426824994 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14389/20000] Train-Loss: 0.37330520592953537 Test-Loss: 0.1433096443905634 Train-f1: 0.5767277173755143 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14390/20000] Train-Loss: 0.3659691951452465 Test-Loss: 0.13817149175951857 Train-f1: 0.59076396677312 Test-f1: 0.5222619047619047 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14391/20000] Train-Loss: 0.3764275737096857 Test-Loss: 0.14242420316604532 Train-f1: 0.6019356350773736 Test-f1: 0.4883116883116883 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14392/20000] Train-Loss: 0.375877513349354 Test-Loss: 0.137209786982141 Train-f1: 0.6675198638639118 Test-f1: 0.5165079365079366 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14393/20000] Train-Loss: 0.37653379949480337 Test-Loss: 0.13999197858463475 Train-f1: 0.6285128100917575 Test-f1: 0.5656318681318682 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14394/20000] Train-Loss: 0.37616441780616766 Test-Loss: 0.14041950918730992 Train-f1: 0.6560660871219878 Test-f1: 0.5624077238550923 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14395/20000] Train-Loss: 0.3728914789368668 Test-Loss: 0.14187467955917146 Train-f1: 0.6361796272740853 Test-f1: 0.5641558441558441 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14396/20000] Train-Loss: 0.3725296636655647 Test-Loss: 0.14460560578473178 Train-f1: 0.6250216507250923 Test-f1: 0.5141038802093091 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14397/20000] Train-Loss: 0.3775607569345685 Test-Loss: 0.13650611299126852 Train-f1: 0.5995234699075758 Test-f1: 0.4644611528822056 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14398/20000] Train-Loss: 0.3762315664455821 Test-Loss: 0.1444589023393408 Train-f1: 0.5634408602150538 Test-f1: 0.5605602240896359 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14399/20000] Train-Loss: 0.37260522017540687 Test-Loss: 0.1370045564642879 Train-f1: 0.6363944675733861 Test-f1: 0.5339348370927318 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14400/20000] Train-Loss: 0.37197796271563666 Test-Loss: 0.1483139892936927 Train-f1: 0.6176582633053221 Test-f1: 0.549891398923657 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14401/20000] Train-Loss: 0.3766342815888504 Test-Loss: 0.1375327189067594 Train-f1: 0.6286927622718006 Test-f1: 0.4944577129097253 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14402/20000] Train-Loss: 0.3788732309151796 Test-Loss: 0.1427301675398608 Train-f1: 0.5246564246564247 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14403/20000] Train-Loss: 0.37505331098073824 Test-Loss: 0.150624903052386 Train-f1: 0.6534417054439712 Test-f1: 0.5603463203463204 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14404/20000] Train-Loss: 0.37240074623234737 Test-Loss: 0.1384557598584551 Train-f1: 0.6241724765300924 Test-f1: 0.44240059457450764 Train-acc: 0.847682119205298 Test-acc: 0.75\n",
      "Epoch [14405/20000] Train-Loss: 0.37520201182034035 Test-Loss: 0.14104492776678867 Train-f1: 0.5895580609866324 Test-f1: 0.5532874968169086 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14406/20000] Train-Loss: 0.3711700796296461 Test-Loss: 0.14911994790525843 Train-f1: 0.6094719488913215 Test-f1: 0.5264562569213732 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14407/20000] Train-Loss: 0.37768867384697385 Test-Loss: 0.13805143986524243 Train-f1: 0.6317823927115173 Test-f1: 0.5378013661604993 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14408/20000] Train-Loss: 0.37461557734080464 Test-Loss: 0.13914742251780599 Train-f1: 0.5844298558584273 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14409/20000] Train-Loss: 0.3711546302413468 Test-Loss: 0.14507843133243972 Train-f1: 0.5396175053948686 Test-f1: 0.552707133913305 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14410/20000] Train-Loss: 0.37367927828508957 Test-Loss: 0.13851399320798313 Train-f1: 0.6393890766329542 Test-f1: 0.4456394609712688 Train-acc: 0.8443708609271523 Test-acc: 0.75\n",
      "Epoch [14411/20000] Train-Loss: 0.37146357309273376 Test-Loss: 0.14310380814445542 Train-f1: 0.6528969144811908 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14412/20000] Train-Loss: 0.3733903284072489 Test-Loss: 0.13976883543645816 Train-f1: 0.6366963236528453 Test-f1: 0.5532874968169086 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14413/20000] Train-Loss: 0.3765301794083482 Test-Loss: 0.15477360594797737 Train-f1: 0.5418734793187349 Test-f1: 0.47555555555555556 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14414/20000] Train-Loss: 0.3774734832107204 Test-Loss: 0.13953128223966793 Train-f1: 0.5324640895245711 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14415/20000] Train-Loss: 0.3764913993475693 Test-Loss: 0.13793095498461103 Train-f1: 0.6341568341568341 Test-f1: 0.46727688787185356 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14416/20000] Train-Loss: 0.37455075158314116 Test-Loss: 0.1395295301350178 Train-f1: 0.587291650087098 Test-f1: 0.5532874968169086 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14417/20000] Train-Loss: 0.3754020845664598 Test-Loss: 0.13688963991120048 Train-f1: 0.6589053912225944 Test-f1: 0.4644611528822056 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [14418/20000] Train-Loss: 0.3738947392696343 Test-Loss: 0.14685457600099963 Train-f1: 0.6327815042100757 Test-f1: 0.5244444444444445 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14419/20000] Train-Loss: 0.37583489040385815 Test-Loss: 0.14385713798487104 Train-f1: 0.6048701602101525 Test-f1: 0.4560625444207534 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [14420/20000] Train-Loss: 0.37896079646109265 Test-Loss: 0.1403696645900587 Train-f1: 0.5169426142883601 Test-f1: 0.5281271777003484 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14421/20000] Train-Loss: 0.37241305193350044 Test-Loss: 0.14935220083513745 Train-f1: 0.6127110082899556 Test-f1: 0.5696804511278196 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14422/20000] Train-Loss: 0.370010259146177 Test-Loss: 0.14431710780854767 Train-f1: 0.5313762428048142 Test-f1: 0.5561337073398784 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14423/20000] Train-Loss: 0.37551905893399534 Test-Loss: 0.13754952511463617 Train-f1: 0.6310726800793025 Test-f1: 0.5561904761904762 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14424/20000] Train-Loss: 0.3746632880761924 Test-Loss: 0.14514348575926928 Train-f1: 0.5121081621081621 Test-f1: 0.5141038802093091 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14425/20000] Train-Loss: 0.37883783575755 Test-Loss: 0.1440993221805536 Train-f1: 0.5152207951184933 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14426/20000] Train-Loss: 0.3735216873796245 Test-Loss: 0.13622979080361516 Train-f1: 0.5674248366013072 Test-f1: 0.49631578947368427 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14427/20000] Train-Loss: 0.3705927730261822 Test-Loss: 0.15228268531342318 Train-f1: 0.5761523745013013 Test-f1: 0.5272601794340925 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14428/20000] Train-Loss: 0.3782857841518518 Test-Loss: 0.13723341820032803 Train-f1: 0.5775008088783833 Test-f1: 0.4537451737451737 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [14429/20000] Train-Loss: 0.37013911858366016 Test-Loss: 0.14376441745195356 Train-f1: 0.6382453361607621 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14430/20000] Train-Loss: 0.3715507484745168 Test-Loss: 0.14152884333208368 Train-f1: 0.5578615212429003 Test-f1: 0.574856124267889 Train-acc: 0.8211920529801324 Test-acc: 0.8289473684210527\n",
      "Epoch [14431/20000] Train-Loss: 0.3748650319800636 Test-Loss: 0.1374258353010994 Train-f1: 0.6425478978110558 Test-f1: 0.46727688787185356 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14432/20000] Train-Loss: 0.37687828450260413 Test-Loss: 0.13841732769069778 Train-f1: 0.6303699986649296 Test-f1: 0.5084184805923936 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14433/20000] Train-Loss: 0.377087171473075 Test-Loss: 0.13785851554074074 Train-f1: 0.5288801288801289 Test-f1: 0.5422818042172881 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14434/20000] Train-Loss: 0.37431330465732926 Test-Loss: 0.14875057738877615 Train-f1: 0.5825299656774038 Test-f1: 0.5469903926482874 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14435/20000] Train-Loss: 0.37910097090915645 Test-Loss: 0.1367578255248201 Train-f1: 0.6119846227897814 Test-f1: 0.49631578947368427 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14436/20000] Train-Loss: 0.3742082705126502 Test-Loss: 0.13747513141936735 Train-f1: 0.6116446168210874 Test-f1: 0.5308287764866713 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14437/20000] Train-Loss: 0.3722653645527024 Test-Loss: 0.14654702750075804 Train-f1: 0.6532452271582707 Test-f1: 0.549891398923657 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14438/20000] Train-Loss: 0.3762505579666994 Test-Loss: 0.1396838665030171 Train-f1: 0.6314919276148216 Test-f1: 0.5308287764866713 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14439/20000] Train-Loss: 0.37522119412177796 Test-Loss: 0.1452650783834824 Train-f1: 0.5129578305368688 Test-f1: 0.549891398923657 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14440/20000] Train-Loss: 0.3718462340190634 Test-Loss: 0.13672360500967537 Train-f1: 0.6457747897160917 Test-f1: 0.49631578947368427 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [14441/20000] Train-Loss: 0.36989098418430916 Test-Loss: 0.14660440403965572 Train-f1: 0.5973900678908934 Test-f1: 0.5637166085946574 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14442/20000] Train-Loss: 0.37594392500501916 Test-Loss: 0.13874596465175904 Train-f1: 0.5853753858007738 Test-f1: 0.538030858030858 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14443/20000] Train-Loss: 0.36721320991113776 Test-Loss: 0.15156490185807628 Train-f1: 0.6388645324951058 Test-f1: 0.5603463203463204 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14444/20000] Train-Loss: 0.3761122207049112 Test-Loss: 0.1494284689311759 Train-f1: 0.6374952633573324 Test-f1: 0.5696804511278196 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14445/20000] Train-Loss: 0.37446583500817654 Test-Loss: 0.13779117770891736 Train-f1: 0.5991628109916282 Test-f1: 0.48546218487394965 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14446/20000] Train-Loss: 0.36902260906066886 Test-Loss: 0.1378576829346147 Train-f1: 0.6419907755168337 Test-f1: 0.5213472706155633 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14447/20000] Train-Loss: 0.37066137531046756 Test-Loss: 0.14084169231172802 Train-f1: 0.6029822474780846 Test-f1: 0.5262301587301588 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14448/20000] Train-Loss: 0.37428798068097335 Test-Loss: 0.13781304210365067 Train-f1: 0.5657739135729007 Test-f1: 0.48546218487394965 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14449/20000] Train-Loss: 0.37411103436679966 Test-Loss: 0.1447618208570332 Train-f1: 0.5353095096479411 Test-f1: 0.5605602240896359 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14450/20000] Train-Loss: 0.3695529854190861 Test-Loss: 0.13677426460025893 Train-f1: 0.5198682508598899 Test-f1: 0.5451919146036793 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14451/20000] Train-Loss: 0.37364020231620415 Test-Loss: 0.14273392885564476 Train-f1: 0.6024054933339336 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14452/20000] Train-Loss: 0.3727495660970251 Test-Loss: 0.13832462165590673 Train-f1: 0.650757141685582 Test-f1: 0.5339348370927318 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14453/20000] Train-Loss: 0.3733565314969337 Test-Loss: 0.15157567761797053 Train-f1: 0.6260490084628015 Test-f1: 0.4811594202898551 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14454/20000] Train-Loss: 0.3727615937383256 Test-Loss: 0.13675996967698498 Train-f1: 0.633225702402173 Test-f1: 0.4537451737451737 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [14455/20000] Train-Loss: 0.37206191978702624 Test-Loss: 0.14470357528432215 Train-f1: 0.5820675968326975 Test-f1: 0.549891398923657 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14456/20000] Train-Loss: 0.3711145015594221 Test-Loss: 0.13622211469333065 Train-f1: 0.6560560845289385 Test-f1: 0.49631578947368427 Train-acc: 0.847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [14457/20000] Train-Loss: 0.3760958334685672 Test-Loss: 0.1438352617885457 Train-f1: 0.6592231394673087 Test-f1: 0.5161751152073732 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14458/20000] Train-Loss: 0.3716540672037263 Test-Loss: 0.14051135321754232 Train-f1: 0.6101936841067277 Test-f1: 0.441995841995842 Train-acc: 0.847682119205298 Test-acc: 0.75\n",
      "Epoch [14459/20000] Train-Loss: 0.37246789938168323 Test-Loss: 0.14242077102386366 Train-f1: 0.5622147593214502 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14460/20000] Train-Loss: 0.3786483288902732 Test-Loss: 0.1454738334879574 Train-f1: 0.5989581846724704 Test-f1: 0.5821288515406162 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14461/20000] Train-Loss: 0.3703235986760978 Test-Loss: 0.15317751724834888 Train-f1: 0.593038961038961 Test-f1: 0.5631620553359683 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14462/20000] Train-Loss: 0.3767343612073334 Test-Loss: 0.150238353150687 Train-f1: 0.5893131658577883 Test-f1: 0.5603463203463204 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14463/20000] Train-Loss: 0.372368719520898 Test-Loss: 0.1453717455238606 Train-f1: 0.6918022532497461 Test-f1: 0.43593384429205323 Train-acc: 0.8509933774834437 Test-acc: 0.7368421052631579\n",
      "Epoch [14464/20000] Train-Loss: 0.37957196104232366 Test-Loss: 0.1397768864177674 Train-f1: 0.5845541125541126 Test-f1: 0.441995841995842 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14465/20000] Train-Loss: 0.3733865627410604 Test-Loss: 0.13873907833871318 Train-f1: 0.539564313477357 Test-f1: 0.574856124267889 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14466/20000] Train-Loss: 0.374005565897899 Test-Loss: 0.13925600310594866 Train-f1: 0.4978390736779999 Test-f1: 0.574856124267889 Train-acc: 0.8278145695364238 Test-acc: 0.8289473684210527\n",
      "Epoch [14467/20000] Train-Loss: 0.37388988494079756 Test-Loss: 0.13920063444014166 Train-f1: 0.6503678394434698 Test-f1: 0.5532874968169086 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14468/20000] Train-Loss: 0.375730343043215 Test-Loss: 0.13621530895287054 Train-f1: 0.6062937104139049 Test-f1: 0.49631578947368427 Train-acc: 0.8443708609271523 Test-acc: 0.7631578947368421\n",
      "Epoch [14469/20000] Train-Loss: 0.3749156436217917 Test-Loss: 0.1366471974560475 Train-f1: 0.5148024944648986 Test-f1: 0.4602828448452552 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14470/20000] Train-Loss: 0.37663759727696755 Test-Loss: 0.13757625457420203 Train-f1: 0.6130033790206467 Test-f1: 0.5776243470361118 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [14471/20000] Train-Loss: 0.37084642224254966 Test-Loss: 0.15191592767112197 Train-f1: 0.5727202273314389 Test-f1: 0.5603463203463204 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14472/20000] Train-Loss: 0.37313711448454057 Test-Loss: 0.14059658322095628 Train-f1: 0.5828541876400708 Test-f1: 0.5308287764866713 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14473/20000] Train-Loss: 0.3759218471256331 Test-Loss: 0.1386309832683327 Train-f1: 0.5370064415945542 Test-f1: 0.574856124267889 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14474/20000] Train-Loss: 0.37110133703256737 Test-Loss: 0.14978504930032857 Train-f1: 0.5845541125541126 Test-f1: 0.5696804511278196 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14475/20000] Train-Loss: 0.37777213491253003 Test-Loss: 0.14403482650306457 Train-f1: 0.6072037260303794 Test-f1: 0.549891398923657 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14476/20000] Train-Loss: 0.3729789024032774 Test-Loss: 0.1465803370431055 Train-f1: 0.6479026687799191 Test-f1: 0.46539589442815255 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14477/20000] Train-Loss: 0.3665997299115058 Test-Loss: 0.13812655482720684 Train-f1: 0.6260437556492839 Test-f1: 0.5308287764866713 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14478/20000] Train-Loss: 0.3727385524042054 Test-Loss: 0.14719842277265102 Train-f1: 0.6328148190629121 Test-f1: 0.5292719919110211 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14479/20000] Train-Loss: 0.36964973633491705 Test-Loss: 0.14398806105251444 Train-f1: 0.5313762428048142 Test-f1: 0.4770348837209302 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14480/20000] Train-Loss: 0.3743398892455149 Test-Loss: 0.14322967255708094 Train-f1: 0.51493336377556 Test-f1: 0.5605602240896359 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14481/20000] Train-Loss: 0.37815600912139125 Test-Loss: 0.13739999762710817 Train-f1: 0.6482197802197802 Test-f1: 0.5339348370927318 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14482/20000] Train-Loss: 0.37682254069855475 Test-Loss: 0.13689444031590667 Train-f1: 0.5817612365263372 Test-f1: 0.5056027456027457 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14483/20000] Train-Loss: 0.369595199473012 Test-Loss: 0.14726766809992112 Train-f1: 0.5427558903159926 Test-f1: 0.549891398923657 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14484/20000] Train-Loss: 0.3723207084224094 Test-Loss: 0.13755842064958676 Train-f1: 0.532742248590776 Test-f1: 0.4944577129097253 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14485/20000] Train-Loss: 0.37559900329790274 Test-Loss: 0.1387874048154724 Train-f1: 0.5822021691586909 Test-f1: 0.5776243470361118 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [14486/20000] Train-Loss: 0.36962115956547986 Test-Loss: 0.13651926891611635 Train-f1: 0.6454819935745404 Test-f1: 0.4395848595848596 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14487/20000] Train-Loss: 0.3728099618922491 Test-Loss: 0.1427857989854121 Train-f1: 0.5790073382151228 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14488/20000] Train-Loss: 0.3728354166460382 Test-Loss: 0.14518772145155426 Train-f1: 0.497391234788869 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14489/20000] Train-Loss: 0.37360704085806795 Test-Loss: 0.14080848494479187 Train-f1: 0.6298713593017391 Test-f1: 0.5290458937198068 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14490/20000] Train-Loss: 0.375230489112351 Test-Loss: 0.13962111966596627 Train-f1: 0.5852131922373951 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14491/20000] Train-Loss: 0.3720193430295361 Test-Loss: 0.14517763236623124 Train-f1: 0.6328860154666607 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14492/20000] Train-Loss: 0.37262261349240783 Test-Loss: 0.14966515726483975 Train-f1: 0.5272174915231589 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14493/20000] Train-Loss: 0.3750674099723547 Test-Loss: 0.1458675614970279 Train-f1: 0.6455531135531135 Test-f1: 0.5821288515406162 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14494/20000] Train-Loss: 0.3723170125728478 Test-Loss: 0.13758276845759726 Train-f1: 0.6444665404290495 Test-f1: 0.5451919146036793 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14495/20000] Train-Loss: 0.37220949844995 Test-Loss: 0.1407384241439211 Train-f1: 0.49805470467722124 Test-f1: 0.5656318681318682 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14496/20000] Train-Loss: 0.3708664406073505 Test-Loss: 0.14243158735708544 Train-f1: 0.5959782745989642 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14497/20000] Train-Loss: 0.37157074094650466 Test-Loss: 0.14984799732525078 Train-f1: 0.5903131318294641 Test-f1: 0.5603463203463204 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14498/20000] Train-Loss: 0.3684990934663463 Test-Loss: 0.14732381272110065 Train-f1: 0.6340555062520483 Test-f1: 0.5244444444444445 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14499/20000] Train-Loss: 0.3754469068184705 Test-Loss: 0.14083592788682017 Train-f1: 0.5873495631884894 Test-f1: 0.5309429126899965 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14500/20000] Train-Loss: 0.37141759131254737 Test-Loss: 0.13966145668826135 Train-f1: 0.6447124304267161 Test-f1: 0.5281271777003484 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14501/20000] Train-Loss: 0.37104113777284153 Test-Loss: 0.14499139933382482 Train-f1: 0.6552919516733496 Test-f1: 0.5624077238550923 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14502/20000] Train-Loss: 0.3740942980731133 Test-Loss: 0.15120647037099502 Train-f1: 0.6190443200445366 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14503/20000] Train-Loss: 0.37638381634049956 Test-Loss: 0.13657846812436913 Train-f1: 0.6005369310616023 Test-f1: 0.4428237259816208 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14504/20000] Train-Loss: 0.37862179335770735 Test-Loss: 0.14377865782292618 Train-f1: 0.5740259740259741 Test-f1: 0.5624077238550923 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14505/20000] Train-Loss: 0.37113516669093854 Test-Loss: 0.15484053360551284 Train-f1: 0.6067276366556509 Test-f1: 0.5603463203463204 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14506/20000] Train-Loss: 0.3677338101176894 Test-Loss: 0.14097208383462898 Train-f1: 0.5959081517388456 Test-f1: 0.46306306306306305 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [14507/20000] Train-Loss: 0.37032332600650797 Test-Loss: 0.14349994154905907 Train-f1: 0.6585507427612691 Test-f1: 0.5605602240896359 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14508/20000] Train-Loss: 0.3740997135442865 Test-Loss: 0.13999121679574128 Train-f1: 0.6174899140797759 Test-f1: 0.5746296946296946 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14509/20000] Train-Loss: 0.3727847996895587 Test-Loss: 0.13790794942163484 Train-f1: 0.5403105162286257 Test-f1: 0.5272243672243672 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14510/20000] Train-Loss: 0.36980063239220934 Test-Loss: 0.1443550149628965 Train-f1: 0.499107246906234 Test-f1: 0.549891398923657 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14511/20000] Train-Loss: 0.37395265408400186 Test-Loss: 0.1402633595973531 Train-f1: 0.651567795721037 Test-f1: 0.48827791986359764 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14512/20000] Train-Loss: 0.3698834942153494 Test-Loss: 0.13913770179438148 Train-f1: 0.5198431498690506 Test-f1: 0.574856124267889 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14513/20000] Train-Loss: 0.37141935592159536 Test-Loss: 0.13655453415204022 Train-f1: 0.5629556038557222 Test-f1: 0.49631578947368427 Train-acc: 0.8245033112582781 Test-acc: 0.7631578947368421\n",
      "Epoch [14514/20000] Train-Loss: 0.3731915893968721 Test-Loss: 0.1393954346481447 Train-f1: 0.6097700460331883 Test-f1: 0.5281271777003484 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14515/20000] Train-Loss: 0.369688660624248 Test-Loss: 0.14631601943403494 Train-f1: 0.5837490078056116 Test-f1: 0.5292719919110211 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14516/20000] Train-Loss: 0.3730946997046183 Test-Loss: 0.14533500108677197 Train-f1: 0.6449340096398919 Test-f1: 0.5533179723502304 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14517/20000] Train-Loss: 0.37592859456590955 Test-Loss: 0.14311864561853122 Train-f1: 0.5826710816777042 Test-f1: 0.4770348837209302 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14518/20000] Train-Loss: 0.37181658178965704 Test-Loss: 0.14522694194754238 Train-f1: 0.53458748756762 Test-f1: 0.5696804511278196 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14519/20000] Train-Loss: 0.3677499406022843 Test-Loss: 0.13762339449678154 Train-f1: 0.5730568729516098 Test-f1: 0.5480076495933274 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14520/20000] Train-Loss: 0.3705550103018955 Test-Loss: 0.14291251035218852 Train-f1: 0.530758038702168 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14521/20000] Train-Loss: 0.37312320108275115 Test-Loss: 0.14022490997514103 Train-f1: 0.6133203888842986 Test-f1: 0.555110087045571 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14522/20000] Train-Loss: 0.36843213317280016 Test-Loss: 0.14421823196753578 Train-f1: 0.5947894118460156 Test-f1: 0.5189908501970213 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14523/20000] Train-Loss: 0.3741964025390993 Test-Loss: 0.1375347025105168 Train-f1: 0.6310726800793025 Test-f1: 0.46727688787185356 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14524/20000] Train-Loss: 0.37953820707074315 Test-Loss: 0.1363655820604992 Train-f1: 0.5971065504905769 Test-f1: 0.5339348370927318 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14525/20000] Train-Loss: 0.3714374953743791 Test-Loss: 0.13829998595542925 Train-f1: 0.5871539617817164 Test-f1: 0.5532874968169086 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14526/20000] Train-Loss: 0.3704998249427317 Test-Loss: 0.13939252670686486 Train-f1: 0.6479026687799191 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14527/20000] Train-Loss: 0.37415166879576534 Test-Loss: 0.14584835525445025 Train-f1: 0.589715046857904 Test-f1: 0.549891398923657 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14528/20000] Train-Loss: 0.3761361625926171 Test-Loss: 0.13815102466111054 Train-f1: 0.5455348458856725 Test-f1: 0.5776243470361118 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [14529/20000] Train-Loss: 0.3688315244202645 Test-Loss: 0.15089036026549948 Train-f1: 0.6409155527568672 Test-f1: 0.5266149870801033 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14530/20000] Train-Loss: 0.3769952080778209 Test-Loss: 0.13745381656825092 Train-f1: 0.6235442059616956 Test-f1: 0.5561904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14531/20000] Train-Loss: 0.3756988255254774 Test-Loss: 0.13899334820099674 Train-f1: 0.5117938387503604 Test-f1: 0.574856124267889 Train-acc: 0.8245033112582781 Test-acc: 0.8289473684210527\n",
      "Epoch [14532/20000] Train-Loss: 0.3747005509158669 Test-Loss: 0.1418127421385608 Train-f1: 0.5988343457985527 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14533/20000] Train-Loss: 0.37361899841556995 Test-Loss: 0.14808389214704012 Train-f1: 0.607653527715055 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14534/20000] Train-Loss: 0.376456873288023 Test-Loss: 0.1485773679932465 Train-f1: 0.6355635232347561 Test-f1: 0.5287878787878787 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14535/20000] Train-Loss: 0.3662294163827012 Test-Loss: 0.14763865720475905 Train-f1: 0.549816421770041 Test-f1: 0.549891398923657 Train-acc: 0.8543046357615894 Test-acc: 0.8026315789473685\n",
      "Epoch [14536/20000] Train-Loss: 0.3763878907646971 Test-Loss: 0.13665748682060902 Train-f1: 0.6073387521462407 Test-f1: 0.4644611528822056 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14537/20000] Train-Loss: 0.3782008568275668 Test-Loss: 0.13989454178926297 Train-f1: 0.5773969298675181 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14538/20000] Train-Loss: 0.3768235723033672 Test-Loss: 0.13754367023560368 Train-f1: 0.6124131972851449 Test-f1: 0.5561904761904762 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14539/20000] Train-Loss: 0.3738044851753166 Test-Loss: 0.13977072832421208 Train-f1: 0.655318431213168 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14540/20000] Train-Loss: 0.36726378781525704 Test-Loss: 0.13985542358013742 Train-f1: 0.6726956403426991 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14541/20000] Train-Loss: 0.3686955909433839 Test-Loss: 0.13667532364922377 Train-f1: 0.6155144724556489 Test-f1: 0.4395848595848596 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14542/20000] Train-Loss: 0.3685460521535372 Test-Loss: 0.13983936417720366 Train-f1: 0.5657739135729007 Test-f1: 0.546221198156682 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14543/20000] Train-Loss: 0.3756438589538099 Test-Loss: 0.1397572775671856 Train-f1: 0.6235401459854015 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14544/20000] Train-Loss: 0.3740670034167675 Test-Loss: 0.13625948557688386 Train-f1: 0.6015795015795016 Test-f1: 0.4428237259816208 Train-acc: 0.8377483443708609 Test-acc: 0.75\n",
      "Epoch [14545/20000] Train-Loss: 0.3739488694601229 Test-Loss: 0.13739707302987703 Train-f1: 0.5908188675517706 Test-f1: 0.5561904761904762 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14546/20000] Train-Loss: 0.37134438191659314 Test-Loss: 0.13693446292891162 Train-f1: 0.5826779756191521 Test-f1: 0.5002197802197803 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14547/20000] Train-Loss: 0.3716466263517913 Test-Loss: 0.13711385444567942 Train-f1: 0.5842180160881478 Test-f1: 0.5451919146036793 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14548/20000] Train-Loss: 0.37744424007268174 Test-Loss: 0.1386148673703887 Train-f1: 0.6454291050564342 Test-f1: 0.574856124267889 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14549/20000] Train-Loss: 0.37255897218145967 Test-Loss: 0.14281603736269768 Train-f1: 0.5991572632526041 Test-f1: 0.4770348837209302 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14550/20000] Train-Loss: 0.37825153617067414 Test-Loss: 0.13679650773728683 Train-f1: 0.5822596749655573 Test-f1: 0.5339348370927318 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14551/20000] Train-Loss: 0.37088333308908533 Test-Loss: 0.1362175076271251 Train-f1: 0.6534067366771042 Test-f1: 0.4395848595848596 Train-acc: 0.8443708609271523 Test-acc: 0.75\n",
      "Epoch [14552/20000] Train-Loss: 0.3703125585724586 Test-Loss: 0.14637644267251726 Train-f1: 0.6016888187042155 Test-f1: 0.5381015037593986 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14553/20000] Train-Loss: 0.37007183428762025 Test-Loss: 0.15037285679604645 Train-f1: 0.5847447861992583 Test-f1: 0.5657142857142857 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14554/20000] Train-Loss: 0.36784814448670516 Test-Loss: 0.13839841923795737 Train-f1: 0.613921631292338 Test-f1: 0.44240059457450764 Train-acc: 0.8311258278145696 Test-acc: 0.75\n",
      "Epoch [14555/20000] Train-Loss: 0.3712047252134896 Test-Loss: 0.13619936483741082 Train-f1: 0.5924770718003801 Test-f1: 0.4395848595848596 Train-acc: 0.8377483443708609 Test-acc: 0.75\n",
      "Epoch [14556/20000] Train-Loss: 0.37262891660003855 Test-Loss: 0.14291703129368838 Train-f1: 0.5754884269536602 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14557/20000] Train-Loss: 0.3705009062290569 Test-Loss: 0.14714799348658336 Train-f1: 0.611268893849539 Test-f1: 0.5665323435843054 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14558/20000] Train-Loss: 0.37620915740070454 Test-Loss: 0.13863308183371476 Train-f1: 0.5101189638102389 Test-f1: 0.5776349965823651 Train-acc: 0.8211920529801324 Test-acc: 0.8289473684210527\n",
      "Epoch [14559/20000] Train-Loss: 0.37273830328272123 Test-Loss: 0.13731905266832237 Train-f1: 0.5827990544924183 Test-f1: 0.5272243672243672 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14560/20000] Train-Loss: 0.3721388823261057 Test-Loss: 0.13700859849569613 Train-f1: 0.6018994430759137 Test-f1: 0.5181595881595882 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14561/20000] Train-Loss: 0.37096786513829927 Test-Loss: 0.14431115913495765 Train-f1: 0.6048935788066223 Test-f1: 0.5141038802093091 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14562/20000] Train-Loss: 0.3658058525833987 Test-Loss: 0.13693205397794533 Train-f1: 0.6155387235102948 Test-f1: 0.4602828448452552 Train-acc: 0.8543046357615894 Test-acc: 0.7763157894736842\n",
      "Epoch [14563/20000] Train-Loss: 0.37963914399248117 Test-Loss: 0.14178498446525067 Train-f1: 0.5469088615578681 Test-f1: 0.555110087045571 Train-acc: 0.8145695364238411 Test-acc: 0.7894736842105263\n",
      "Epoch [14564/20000] Train-Loss: 0.37170755698248464 Test-Loss: 0.1398035597214004 Train-f1: 0.6354792717985562 Test-f1: 0.4774585921325052 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14565/20000] Train-Loss: 0.37537443509000457 Test-Loss: 0.13908528937075407 Train-f1: 0.5679139784946237 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14566/20000] Train-Loss: 0.37100129263989307 Test-Loss: 0.14296159083816076 Train-f1: 0.6172121212121212 Test-f1: 0.5605602240896359 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14567/20000] Train-Loss: 0.3725858595297206 Test-Loss: 0.13641244838210326 Train-f1: 0.5202150920109626 Test-f1: 0.5339348370927318 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14568/20000] Train-Loss: 0.367696103988064 Test-Loss: 0.14171103447770225 Train-f1: 0.627246488901468 Test-f1: 0.4452347083926031 Train-acc: 0.8509933774834437 Test-acc: 0.75\n",
      "Epoch [14569/20000] Train-Loss: 0.3678322036890392 Test-Loss: 0.1376974996666267 Train-f1: 0.613779111124857 Test-f1: 0.5776243470361118 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14570/20000] Train-Loss: 0.3710797263760236 Test-Loss: 0.1393224321169905 Train-f1: 0.6190443200445366 Test-f1: 0.5281271777003484 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14571/20000] Train-Loss: 0.3726819199679099 Test-Loss: 0.1450566777800336 Train-f1: 0.5925736164428899 Test-f1: 0.5821288515406162 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14572/20000] Train-Loss: 0.36608149134081136 Test-Loss: 0.13956369666615454 Train-f1: 0.6744240315442591 Test-f1: 0.4531469684787762 Train-acc: 0.8543046357615894 Test-acc: 0.7631578947368421\n",
      "Epoch [14573/20000] Train-Loss: 0.37892542491157255 Test-Loss: 0.13951867532773998 Train-f1: 0.5239091812130107 Test-f1: 0.5656318681318682 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14574/20000] Train-Loss: 0.3685550240122487 Test-Loss: 0.13976028037220845 Train-f1: 0.6410703390569162 Test-f1: 0.5381015037593986 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [14575/20000] Train-Loss: 0.3714578475816191 Test-Loss: 0.136369984816358 Train-f1: 0.5966860710220828 Test-f1: 0.5451919146036793 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14576/20000] Train-Loss: 0.37460774060923874 Test-Loss: 0.1406980118462061 Train-f1: 0.6329506438262198 Test-f1: 0.5309429126899965 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14577/20000] Train-Loss: 0.3724928904578257 Test-Loss: 0.14351468032631248 Train-f1: 0.6061502055389911 Test-f1: 0.4774585921325052 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14578/20000] Train-Loss: 0.37136067235681514 Test-Loss: 0.13782264456161314 Train-f1: 0.6369857533540398 Test-f1: 0.5683710801393728 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14579/20000] Train-Loss: 0.37604298882577575 Test-Loss: 0.1379826908766441 Train-f1: 0.6004257004257004 Test-f1: 0.5339348370927318 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [14580/20000] Train-Loss: 0.3718851162153913 Test-Loss: 0.14975920285264877 Train-f1: 0.5365380044711265 Test-f1: 0.5696804511278196 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14581/20000] Train-Loss: 0.36317570836325924 Test-Loss: 0.15744964862295938 Train-f1: 0.5245509116300745 Test-f1: 0.5603463203463204 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14582/20000] Train-Loss: 0.3734452083158438 Test-Loss: 0.14374665847818113 Train-f1: 0.6549852339441381 Test-f1: 0.4774585921325052 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14583/20000] Train-Loss: 0.3688972977315196 Test-Loss: 0.1489488296921753 Train-f1: 0.6695968091416611 Test-f1: 0.5292719919110211 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14584/20000] Train-Loss: 0.3717178251870949 Test-Loss: 0.14251623271884892 Train-f1: 0.5015651874785002 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14585/20000] Train-Loss: 0.3735631134882383 Test-Loss: 0.1430458823927058 Train-f1: 0.5799149466233601 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14586/20000] Train-Loss: 0.36432832415142125 Test-Loss: 0.15446886485170264 Train-f1: 0.5406007326007326 Test-f1: 0.4725704809286899 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14587/20000] Train-Loss: 0.3715447228947724 Test-Loss: 0.1446686920448421 Train-f1: 0.5933224056683234 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14588/20000] Train-Loss: 0.3732173339991197 Test-Loss: 0.13999451506308966 Train-f1: 0.5719687945376338 Test-f1: 0.5624077238550923 Train-acc: 0.8178807947019867 Test-acc: 0.8157894736842105\n",
      "Epoch [14589/20000] Train-Loss: 0.3697687737704853 Test-Loss: 0.14435803739206435 Train-f1: 0.5364999396164281 Test-f1: 0.5561337073398784 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14590/20000] Train-Loss: 0.37110083306894026 Test-Loss: 0.14151717607003836 Train-f1: 0.6048741119636261 Test-f1: 0.5494699894699895 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14591/20000] Train-Loss: 0.372234250468256 Test-Loss: 0.14254297266785634 Train-f1: 0.6473981265304959 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14592/20000] Train-Loss: 0.364802150899969 Test-Loss: 0.15562059742239068 Train-f1: 0.5653637985233397 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14593/20000] Train-Loss: 0.3774962680915284 Test-Loss: 0.13720879651843637 Train-f1: 0.6510421871415694 Test-f1: 0.5561904761904762 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14594/20000] Train-Loss: 0.37145175367160127 Test-Loss: 0.1468944162709261 Train-f1: 0.6076164650949585 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14595/20000] Train-Loss: 0.36957631487179443 Test-Loss: 0.1443117278012651 Train-f1: 0.5722777396879871 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14596/20000] Train-Loss: 0.3744158036017296 Test-Loss: 0.14084922338011052 Train-f1: 0.6564070751276072 Test-f1: 0.5309429126899965 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14597/20000] Train-Loss: 0.3698747308404879 Test-Loss: 0.14301918023542085 Train-f1: 0.5947345037825799 Test-f1: 0.5684476031215162 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14598/20000] Train-Loss: 0.3708471486283909 Test-Loss: 0.15008612326202214 Train-f1: 0.5007181065829334 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14599/20000] Train-Loss: 0.37740988196708763 Test-Loss: 0.13853525951627774 Train-f1: 0.6419907755168337 Test-f1: 0.5561904761904762 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14600/20000] Train-Loss: 0.36706304206412466 Test-Loss: 0.13661031767724138 Train-f1: 0.6200233481563405 Test-f1: 0.5683710801393728 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14601/20000] Train-Loss: 0.37157842961116044 Test-Loss: 0.14139231567384422 Train-f1: 0.625316712273234 Test-f1: 0.5290458937198068 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14602/20000] Train-Loss: 0.36849777752722934 Test-Loss: 0.1371836276568491 Train-f1: 0.6363706933243357 Test-f1: 0.4602828448452552 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14603/20000] Train-Loss: 0.3748316014222727 Test-Loss: 0.13716087012236122 Train-f1: 0.6040988270684743 Test-f1: 0.4602828448452552 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14604/20000] Train-Loss: 0.3703844520531592 Test-Loss: 0.1394166717285907 Train-f1: 0.5657739135729007 Test-f1: 0.574856124267889 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [14605/20000] Train-Loss: 0.3680294832158887 Test-Loss: 0.14999044903999592 Train-f1: 0.6651171677770116 Test-f1: 0.5637166085946574 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14606/20000] Train-Loss: 0.37286931730750583 Test-Loss: 0.1412570595564435 Train-f1: 0.6172121212121212 Test-f1: 0.5821288515406162 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14607/20000] Train-Loss: 0.3686105606044055 Test-Loss: 0.14020524899172188 Train-f1: 0.6285770369980896 Test-f1: 0.5408465930205061 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14608/20000] Train-Loss: 0.3735010133828019 Test-Loss: 0.14012850594539414 Train-f1: 0.5892068126520681 Test-f1: 0.4630985798349032 Train-acc: 0.8211920529801324 Test-acc: 0.7763157894736842\n",
      "Epoch [14609/20000] Train-Loss: 0.371232125130096 Test-Loss: 0.14114276145436858 Train-f1: 0.5713097713097712 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14610/20000] Train-Loss: 0.3721089726475712 Test-Loss: 0.13990500183732665 Train-f1: 0.6315594315594316 Test-f1: 0.4875179340028694 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14611/20000] Train-Loss: 0.3713482993822915 Test-Loss: 0.13886233884418114 Train-f1: 0.5845343671150123 Test-f1: 0.5776349965823651 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14612/20000] Train-Loss: 0.36661457815168413 Test-Loss: 0.1386427856062423 Train-f1: 0.6249801504179384 Test-f1: 0.5339348370927318 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14613/20000] Train-Loss: 0.3716750567873475 Test-Loss: 0.1375793737490109 Train-f1: 0.5387800878366915 Test-f1: 0.5561904761904762 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14614/20000] Train-Loss: 0.375111057225268 Test-Loss: 0.14032218085366738 Train-f1: 0.5318028204245465 Test-f1: 0.4774585921325052 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14615/20000] Train-Loss: 0.37169290231652596 Test-Loss: 0.14015807944505568 Train-f1: 0.5148699902248289 Test-f1: 0.4774585921325052 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14616/20000] Train-Loss: 0.37264275441385136 Test-Loss: 0.1367094225390699 Train-f1: 0.6019867549668874 Test-f1: 0.5181595881595882 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14617/20000] Train-Loss: 0.3687546257162591 Test-Loss: 0.13637898530044243 Train-f1: 0.5894915894915895 Test-f1: 0.5451190476190477 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14618/20000] Train-Loss: 0.3662021257845141 Test-Loss: 0.1440890776912317 Train-f1: 0.5031896551724138 Test-f1: 0.5381015037593986 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14619/20000] Train-Loss: 0.37285912893658985 Test-Loss: 0.1466500910393321 Train-f1: 0.5874880503205736 Test-f1: 0.5605602240896359 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14620/20000] Train-Loss: 0.37399445333961195 Test-Loss: 0.14106023895965544 Train-f1: 0.6164311878597593 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14621/20000] Train-Loss: 0.36980725651073215 Test-Loss: 0.1478322289263067 Train-f1: 0.6038094152224588 Test-f1: 0.5272601794340925 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14622/20000] Train-Loss: 0.37172631085999497 Test-Loss: 0.13652193503264368 Train-f1: 0.6004044323852479 Test-f1: 0.5339348370927318 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14623/20000] Train-Loss: 0.3728405220161837 Test-Loss: 0.13697580052884004 Train-f1: 0.6396124672353324 Test-f1: 0.5561904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14624/20000] Train-Loss: 0.3743560472056393 Test-Loss: 0.14101141824625782 Train-f1: 0.5795870339705956 Test-f1: 0.5656318681318682 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14625/20000] Train-Loss: 0.37676260153887453 Test-Loss: 0.1407745065714934 Train-f1: 0.6290533429703936 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14626/20000] Train-Loss: 0.36885649205985127 Test-Loss: 0.1411129822435992 Train-f1: 0.6862336503032253 Test-f1: 0.4774585921325052 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [14627/20000] Train-Loss: 0.36934077613323746 Test-Loss: 0.14513487437301323 Train-f1: 0.5710372992803846 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14628/20000] Train-Loss: 0.3693481128601616 Test-Loss: 0.15164946130499463 Train-f1: 0.5950437967829272 Test-f1: 0.5272601794340925 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14629/20000] Train-Loss: 0.36878962662758213 Test-Loss: 0.14167679031473115 Train-f1: 0.6340373911802484 Test-f1: 0.5381015037593986 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14630/20000] Train-Loss: 0.37056353052205543 Test-Loss: 0.13914682237169143 Train-f1: 0.6207262852090437 Test-f1: 0.5776349965823651 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14631/20000] Train-Loss: 0.3703509378510949 Test-Loss: 0.14357697105566902 Train-f1: 0.6510909324218495 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14632/20000] Train-Loss: 0.36678826256311153 Test-Loss: 0.1374728198975819 Train-f1: 0.66165110443832 Test-f1: 0.5213472706155633 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [14633/20000] Train-Loss: 0.3739292565314826 Test-Loss: 0.1437800246690314 Train-f1: 0.5897598018776139 Test-f1: 0.5821288515406162 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14634/20000] Train-Loss: 0.3714746835710638 Test-Loss: 0.138015145112103 Train-f1: 0.6646830652790918 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14635/20000] Train-Loss: 0.3735659824421022 Test-Loss: 0.1402565832744479 Train-f1: 0.5876711298461037 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14636/20000] Train-Loss: 0.3723016249125526 Test-Loss: 0.14174650898195737 Train-f1: 0.5930495703084905 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14637/20000] Train-Loss: 0.36912950147410567 Test-Loss: 0.14814156974482964 Train-f1: 0.6578267365275762 Test-f1: 0.5603463203463204 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14638/20000] Train-Loss: 0.3728127682620294 Test-Loss: 0.14198614377456928 Train-f1: 0.5954748882617587 Test-f1: 0.4770348837209302 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14639/20000] Train-Loss: 0.3720341792007078 Test-Loss: 0.1407547188667971 Train-f1: 0.5734778121775026 Test-f1: 0.574856124267889 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14640/20000] Train-Loss: 0.3637253227051317 Test-Loss: 0.15424491185039071 Train-f1: 0.6408272113616389 Test-f1: 0.5536203326907975 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14641/20000] Train-Loss: 0.3756275850206737 Test-Loss: 0.141820233884893 Train-f1: 0.580807287950145 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14642/20000] Train-Loss: 0.3697131359737324 Test-Loss: 0.13651583294855457 Train-f1: 0.5905114378514302 Test-f1: 0.5056027456027457 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14643/20000] Train-Loss: 0.36800025231003397 Test-Loss: 0.14283053051881892 Train-f1: 0.6663296791367318 Test-f1: 0.5656318681318682 Train-acc: 0.8609271523178808 Test-acc: 0.8157894736842105\n",
      "Epoch [14644/20000] Train-Loss: 0.36999460039123155 Test-Loss: 0.15209579511684398 Train-f1: 0.5631595344879334 Test-f1: 0.5603463203463204 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14645/20000] Train-Loss: 0.3719162695854688 Test-Loss: 0.1450254209312836 Train-f1: 0.6048220251800325 Test-f1: 0.552707133913305 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14646/20000] Train-Loss: 0.36940443744802537 Test-Loss: 0.13830236553089814 Train-f1: 0.5878165983520675 Test-f1: 0.5776243470361118 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14647/20000] Train-Loss: 0.3644665280803599 Test-Loss: 0.14208477484705553 Train-f1: 0.6284236734567861 Test-f1: 0.43593384429205323 Train-acc: 0.8311258278145696 Test-acc: 0.7368421052631579\n",
      "Epoch [14648/20000] Train-Loss: 0.3700309215931238 Test-Loss: 0.1372844562747429 Train-f1: 0.6310867936918358 Test-f1: 0.4565609087348218 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [14649/20000] Train-Loss: 0.37066150509640083 Test-Loss: 0.1466076425640226 Train-f1: 0.5889417664643258 Test-f1: 0.549891398923657 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14650/20000] Train-Loss: 0.36827209791586446 Test-Loss: 0.1466596003375889 Train-f1: 0.6080014428416386 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14651/20000] Train-Loss: 0.37012749710325915 Test-Loss: 0.14893791926782432 Train-f1: 0.5403310974739546 Test-f1: 0.4770348837209302 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14652/20000] Train-Loss: 0.3684509227600061 Test-Loss: 0.13910896578104823 Train-f1: 0.5665345604589688 Test-f1: 0.46727688787185356 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14653/20000] Train-Loss: 0.37010494973574615 Test-Loss: 0.14741108263685285 Train-f1: 0.5949762181240856 Test-f1: 0.5696804511278196 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14654/20000] Train-Loss: 0.3733064165432745 Test-Loss: 0.142442906709539 Train-f1: 0.6680552975005529 Test-f1: 0.5309429126899965 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14655/20000] Train-Loss: 0.3722393072761069 Test-Loss: 0.1452917304640559 Train-f1: 0.5981361626878868 Test-f1: 0.516919615198957 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14656/20000] Train-Loss: 0.36860569542197913 Test-Loss: 0.13865126892411123 Train-f1: 0.5998734793187348 Test-f1: 0.5209753231492362 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14657/20000] Train-Loss: 0.370321874801821 Test-Loss: 0.14149736867996843 Train-f1: 0.6611427350557786 Test-f1: 0.5624077238550923 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14658/20000] Train-Loss: 0.3668578640367837 Test-Loss: 0.14970040284685482 Train-f1: 0.5484907948484107 Test-f1: 0.5665323435843054 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14659/20000] Train-Loss: 0.3688733924053562 Test-Loss: 0.140050690689851 Train-f1: 0.6054886044542795 Test-f1: 0.4888974581445419 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14660/20000] Train-Loss: 0.36672165168386306 Test-Loss: 0.13931894844810397 Train-f1: 0.5576101602591669 Test-f1: 0.5641558441558441 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [14661/20000] Train-Loss: 0.3714451562777806 Test-Loss: 0.14054427719199583 Train-f1: 0.566840305041769 Test-f1: 0.5656318681318682 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14662/20000] Train-Loss: 0.36835106119299577 Test-Loss: 0.1378751688305825 Train-f1: 0.6334237504391472 Test-f1: 0.5393083387201034 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14663/20000] Train-Loss: 0.37118857877615796 Test-Loss: 0.14136664155703682 Train-f1: 0.6482650633545775 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14664/20000] Train-Loss: 0.3708529213289921 Test-Loss: 0.1471022709239046 Train-f1: 0.532515737137194 Test-f1: 0.4770348837209302 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14665/20000] Train-Loss: 0.37332434038827683 Test-Loss: 0.13720792167621335 Train-f1: 0.6538402528059278 Test-f1: 0.4537451737451737 Train-acc: 0.8443708609271523 Test-acc: 0.7631578947368421\n",
      "Epoch [14666/20000] Train-Loss: 0.37526887427254396 Test-Loss: 0.14219437718956726 Train-f1: 0.5963986425629489 Test-f1: 0.5656318681318682 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14667/20000] Train-Loss: 0.3718034651871332 Test-Loss: 0.13754667647370525 Train-f1: 0.6023314169274434 Test-f1: 0.46727688787185356 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14668/20000] Train-Loss: 0.3718669790251205 Test-Loss: 0.14682683330257795 Train-f1: 0.5255200664152763 Test-f1: 0.5665323435843054 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14669/20000] Train-Loss: 0.37307329136396444 Test-Loss: 0.1399991844356886 Train-f1: 0.6489418023001012 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14670/20000] Train-Loss: 0.3733791107861113 Test-Loss: 0.1417153995551463 Train-f1: 0.6197850114290604 Test-f1: 0.4942917547568711 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14671/20000] Train-Loss: 0.3723880684354306 Test-Loss: 0.13993700447175794 Train-f1: 0.5999916529328294 Test-f1: 0.47333333333333333 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14672/20000] Train-Loss: 0.3730003130889166 Test-Loss: 0.13816558224683004 Train-f1: 0.6335594931384405 Test-f1: 0.48827791986359764 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14673/20000] Train-Loss: 0.3663287973938373 Test-Loss: 0.14581568022279695 Train-f1: 0.6739505443038161 Test-f1: 0.4656685910608086 Train-acc: 0.8543046357615894 Test-acc: 0.7763157894736842\n",
      "Epoch [14674/20000] Train-Loss: 0.3688971604967002 Test-Loss: 0.1367951560566831 Train-f1: 0.5177217917992616 Test-f1: 0.4537451737451737 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14675/20000] Train-Loss: 0.37410645820734606 Test-Loss: 0.14124418809675243 Train-f1: 0.5321995535578525 Test-f1: 0.4834843205574913 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14676/20000] Train-Loss: 0.36878202459897785 Test-Loss: 0.15059659013458113 Train-f1: 0.6358065358065359 Test-f1: 0.5665323435843054 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14677/20000] Train-Loss: 0.37421720654220686 Test-Loss: 0.13992712691292783 Train-f1: 0.6510909324218495 Test-f1: 0.4935574229691877 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14678/20000] Train-Loss: 0.3683054927825495 Test-Loss: 0.14543931049432315 Train-f1: 0.5196271780467494 Test-f1: 0.549891398923657 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14679/20000] Train-Loss: 0.3705451359621007 Test-Loss: 0.14318250527214546 Train-f1: 0.5876143594102301 Test-f1: 0.5494699894699895 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14680/20000] Train-Loss: 0.36906530684892086 Test-Loss: 0.1373300421138534 Train-f1: 0.6477534603585025 Test-f1: 0.5451919146036793 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14681/20000] Train-Loss: 0.3738568755904746 Test-Loss: 0.14571378232956078 Train-f1: 0.6941811382376841 Test-f1: 0.5624077238550923 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14682/20000] Train-Loss: 0.37033659282828146 Test-Loss: 0.14136417203817253 Train-f1: 0.5825300737065443 Test-f1: 0.5605602240896359 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14683/20000] Train-Loss: 0.3630147049885333 Test-Loss: 0.14770684176451576 Train-f1: 0.6149447604791879 Test-f1: 0.5665323435843054 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14684/20000] Train-Loss: 0.3718752216767758 Test-Loss: 0.13822057144023225 Train-f1: 0.5845343671150123 Test-f1: 0.5776243470361118 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14685/20000] Train-Loss: 0.3716920547947011 Test-Loss: 0.138194995176822 Train-f1: 0.5964369327281457 Test-f1: 0.5393083387201034 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14686/20000] Train-Loss: 0.37025568876309756 Test-Loss: 0.1366227590590913 Train-f1: 0.6158634070398776 Test-f1: 0.5339348370927318 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14687/20000] Train-Loss: 0.3730951531977118 Test-Loss: 0.14280674378156533 Train-f1: 0.6381693633306537 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14688/20000] Train-Loss: 0.3730678101513184 Test-Loss: 0.14216358061400278 Train-f1: 0.6121607263595094 Test-f1: 0.4774585921325052 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14689/20000] Train-Loss: 0.3708464906622418 Test-Loss: 0.1408594594481484 Train-f1: 0.5871791567163236 Test-f1: 0.4746428571428572 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14690/20000] Train-Loss: 0.3664526612377243 Test-Loss: 0.14573026977026726 Train-f1: 0.6444138607388036 Test-f1: 0.5584434203789043 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14691/20000] Train-Loss: 0.37592350337747443 Test-Loss: 0.13865202571852325 Train-f1: 0.6303699986649296 Test-f1: 0.4531469684787762 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [14692/20000] Train-Loss: 0.3692075462271183 Test-Loss: 0.1399403894822113 Train-f1: 0.6409831778959296 Test-f1: 0.5381015037593986 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [14693/20000] Train-Loss: 0.3691222742664875 Test-Loss: 0.14474828632310582 Train-f1: 0.6003844107069913 Test-f1: 0.5624077238550923 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14694/20000] Train-Loss: 0.37131274362438477 Test-Loss: 0.14358530104483097 Train-f1: 0.5199380717105688 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14695/20000] Train-Loss: 0.3725481464390721 Test-Loss: 0.13961231751671332 Train-f1: 0.6564070751276072 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14696/20000] Train-Loss: 0.3653118553561034 Test-Loss: 0.14583595984052966 Train-f1: 0.5878471554942143 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14697/20000] Train-Loss: 0.36925907601806585 Test-Loss: 0.13770996539131536 Train-f1: 0.6676544361899053 Test-f1: 0.5422818042172881 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14698/20000] Train-Loss: 0.3731334097399298 Test-Loss: 0.14067741308580786 Train-f1: 0.6178802796637192 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14699/20000] Train-Loss: 0.3683970516300717 Test-Loss: 0.14111816922338433 Train-f1: 0.6656481053193383 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14700/20000] Train-Loss: 0.36999995639051253 Test-Loss: 0.14231764335735564 Train-f1: 0.6528654705125294 Test-f1: 0.5309429126899965 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14701/20000] Train-Loss: 0.36971724575697495 Test-Loss: 0.1459768163100821 Train-f1: 0.5633402633402633 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14702/20000] Train-Loss: 0.3732221111012979 Test-Loss: 0.13690787936448445 Train-f1: 0.5987368952886194 Test-f1: 0.5165079365079366 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14703/20000] Train-Loss: 0.36899858026733795 Test-Loss: 0.142773404079764 Train-f1: 0.60198116356544 Test-f1: 0.5821288515406162 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14704/20000] Train-Loss: 0.37131698359601484 Test-Loss: 0.139612706414444 Train-f1: 0.5661130517597712 Test-f1: 0.5684476031215162 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14705/20000] Train-Loss: 0.3667557472340818 Test-Loss: 0.1517651115944703 Train-f1: 0.5986350669299978 Test-f1: 0.5603463203463204 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14706/20000] Train-Loss: 0.37264060915176217 Test-Loss: 0.1364564724622428 Train-f1: 0.6369857533540398 Test-f1: 0.5339348370927318 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14707/20000] Train-Loss: 0.37283152020492555 Test-Loss: 0.13770927602490424 Train-f1: 0.6630226476032928 Test-f1: 0.47501831501831504 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14708/20000] Train-Loss: 0.3712904490366502 Test-Loss: 0.1388741771913776 Train-f1: 0.5844774963063134 Test-f1: 0.5425559947299078 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14709/20000] Train-Loss: 0.36811000790401655 Test-Loss: 0.13782889746278532 Train-f1: 0.6388577387524756 Test-f1: 0.5776243470361118 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14710/20000] Train-Loss: 0.365055330813257 Test-Loss: 0.1401700266858841 Train-f1: 0.6355887181693634 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14711/20000] Train-Loss: 0.3730780874487846 Test-Loss: 0.13927908206970735 Train-f1: 0.5798516860713657 Test-f1: 0.5421972621972622 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14712/20000] Train-Loss: 0.36820092349358835 Test-Loss: 0.14176614282212988 Train-f1: 0.658545332458376 Test-f1: 0.4774585921325052 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14713/20000] Train-Loss: 0.3726322546794736 Test-Loss: 0.13735741485242342 Train-f1: 0.6028944986104134 Test-f1: 0.48546218487394965 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14714/20000] Train-Loss: 0.3699463092417785 Test-Loss: 0.13869589983934127 Train-f1: 0.6098538089281427 Test-f1: 0.5776349965823651 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [14715/20000] Train-Loss: 0.37014727988209073 Test-Loss: 0.1366263850127976 Train-f1: 0.631633988131684 Test-f1: 0.5339348370927318 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14716/20000] Train-Loss: 0.371879530973759 Test-Loss: 0.14094465411737614 Train-f1: 0.6070114253435331 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14717/20000] Train-Loss: 0.36943578142908506 Test-Loss: 0.13984977150097413 Train-f1: 0.6701062378481734 Test-f1: 0.4888974581445419 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14718/20000] Train-Loss: 0.36775659624693985 Test-Loss: 0.14712464579499213 Train-f1: 0.5954361455537927 Test-f1: 0.5603463203463204 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14719/20000] Train-Loss: 0.3702043985056946 Test-Loss: 0.13817552150286538 Train-f1: 0.6556904004978891 Test-f1: 0.48546218487394965 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14720/20000] Train-Loss: 0.3749206648483953 Test-Loss: 0.14012334409118346 Train-f1: 0.5246564246564247 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14721/20000] Train-Loss: 0.3689260775597025 Test-Loss: 0.1406182700862444 Train-f1: 0.6459159887731316 Test-f1: 0.5656318681318682 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14722/20000] Train-Loss: 0.36849899594612145 Test-Loss: 0.1364665967166814 Train-f1: 0.5977377657185812 Test-f1: 0.5073487773487774 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [14723/20000] Train-Loss: 0.37237578205131266 Test-Loss: 0.14076836937830972 Train-f1: 0.6122598503557749 Test-f1: 0.5776243470361118 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [14724/20000] Train-Loss: 0.365419244843844 Test-Loss: 0.14357573242662167 Train-f1: 0.6961881977671451 Test-f1: 0.5821288515406162 Train-acc: 0.8543046357615894 Test-acc: 0.8289473684210527\n",
      "Epoch [14725/20000] Train-Loss: 0.37260293382806586 Test-Loss: 0.14491913292422476 Train-f1: 0.6218480444286897 Test-f1: 0.5141038802093091 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14726/20000] Train-Loss: 0.3708842424455213 Test-Loss: 0.13905195824121397 Train-f1: 0.6028944986104134 Test-f1: 0.47333333333333333 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14727/20000] Train-Loss: 0.36705254321747405 Test-Loss: 0.14585646061267504 Train-f1: 0.6411211272387743 Test-f1: 0.5533179723502304 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14728/20000] Train-Loss: 0.37221275754070154 Test-Loss: 0.13746668700802908 Train-f1: 0.6027034295966056 Test-f1: 0.4537451737451737 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [14729/20000] Train-Loss: 0.36901982918267007 Test-Loss: 0.1389055177610556 Train-f1: 0.610745337984951 Test-f1: 0.5776243470361118 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [14730/20000] Train-Loss: 0.3694003527607642 Test-Loss: 0.14258293942713768 Train-f1: 0.5315642647238059 Test-f1: 0.4746428571428572 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14731/20000] Train-Loss: 0.3716203363720121 Test-Loss: 0.1395353082194773 Train-f1: 0.6678642284867451 Test-f1: 0.4888974581445419 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14732/20000] Train-Loss: 0.3672723604935394 Test-Loss: 0.13887657530068298 Train-f1: 0.6560560845289385 Test-f1: 0.5540079365079366 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14733/20000] Train-Loss: 0.374723019476618 Test-Loss: 0.13958950978818338 Train-f1: 0.6824141615903629 Test-f1: 0.5290458937198068 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14734/20000] Train-Loss: 0.36924148940941404 Test-Loss: 0.14004290015160262 Train-f1: 0.6958481368255804 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14735/20000] Train-Loss: 0.368200565445926 Test-Loss: 0.14654529378296355 Train-f1: 0.6574133934039017 Test-f1: 0.475889328063241 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14736/20000] Train-Loss: 0.3697894187946023 Test-Loss: 0.1368498191377734 Train-f1: 0.5622283505737246 Test-f1: 0.5561904761904762 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14737/20000] Train-Loss: 0.3717608881183611 Test-Loss: 0.1428011176525347 Train-f1: 0.6146955478691547 Test-f1: 0.4770348837209302 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [14738/20000] Train-Loss: 0.3674901828503883 Test-Loss: 0.1445946769369102 Train-f1: 0.6077044361772901 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14739/20000] Train-Loss: 0.37245088086833517 Test-Loss: 0.14013018239481428 Train-f1: 0.6338752826121248 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14740/20000] Train-Loss: 0.3702341416954087 Test-Loss: 0.13638484191435546 Train-f1: 0.6495560146194184 Test-f1: 0.4537451737451737 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [14741/20000] Train-Loss: 0.3675283051905947 Test-Loss: 0.14928344252361087 Train-f1: 0.6116446168210874 Test-f1: 0.5637166085946574 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14742/20000] Train-Loss: 0.3629340912482264 Test-Loss: 0.14043269804905578 Train-f1: 0.6509677660572801 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14743/20000] Train-Loss: 0.37119642808909387 Test-Loss: 0.13900336295909746 Train-f1: 0.6151919533934173 Test-f1: 0.5776349965823651 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14744/20000] Train-Loss: 0.3690714535822991 Test-Loss: 0.14578757338845838 Train-f1: 0.6747433248990535 Test-f1: 0.4770348837209302 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [14745/20000] Train-Loss: 0.3689723830569333 Test-Loss: 0.1378734186046155 Train-f1: 0.5906256880374527 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14746/20000] Train-Loss: 0.3704292600527211 Test-Loss: 0.13904330038924073 Train-f1: 0.5988767988767989 Test-f1: 0.5776349965823651 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14747/20000] Train-Loss: 0.37165595227388815 Test-Loss: 0.1393737563276577 Train-f1: 0.5995411299517259 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14748/20000] Train-Loss: 0.36695138027141605 Test-Loss: 0.144210428144499 Train-f1: 0.6497671068427371 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14749/20000] Train-Loss: 0.37532827832994403 Test-Loss: 0.13789331039809985 Train-f1: 0.5973043833685646 Test-f1: 0.566984126984127 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14750/20000] Train-Loss: 0.37250235118613906 Test-Loss: 0.13981483441361528 Train-f1: 0.6363312070213918 Test-f1: 0.5281271777003484 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14751/20000] Train-Loss: 0.36337375158672586 Test-Loss: 0.15281212354100554 Train-f1: 0.5973900678908934 Test-f1: 0.5287878787878787 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14752/20000] Train-Loss: 0.37094075935731996 Test-Loss: 0.13661311377612698 Train-f1: 0.5073474638228914 Test-f1: 0.5056027456027457 Train-acc: 0.8178807947019867 Test-acc: 0.7763157894736842\n",
      "Epoch [14753/20000] Train-Loss: 0.3652018020525865 Test-Loss: 0.14341996333502693 Train-f1: 0.6069403033217012 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14754/20000] Train-Loss: 0.36967848584881235 Test-Loss: 0.1453953857241788 Train-f1: 0.6073570060627658 Test-f1: 0.46539589442815255 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14755/20000] Train-Loss: 0.36556700912130385 Test-Loss: 0.13647283884067046 Train-f1: 0.6391156151247686 Test-f1: 0.49631578947368427 Train-acc: 0.8443708609271523 Test-acc: 0.7631578947368421\n",
      "Epoch [14756/20000] Train-Loss: 0.37326090248494465 Test-Loss: 0.13637725846101983 Train-f1: 0.6282612475244054 Test-f1: 0.4537451737451737 Train-acc: 0.8311258278145696 Test-acc: 0.7631578947368421\n",
      "Epoch [14757/20000] Train-Loss: 0.36884710829029405 Test-Loss: 0.13896120839848666 Train-f1: 0.5716358967971871 Test-f1: 0.5776349965823651 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14758/20000] Train-Loss: 0.3678503119801975 Test-Loss: 0.1431562211272223 Train-f1: 0.6563757575224751 Test-f1: 0.4883116883116883 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14759/20000] Train-Loss: 0.3695350509717479 Test-Loss: 0.13687004052026241 Train-f1: 0.616925518925519 Test-f1: 0.4537451737451737 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [14760/20000] Train-Loss: 0.3692455391152577 Test-Loss: 0.13739306818651134 Train-f1: 0.672485015971001 Test-f1: 0.5451919146036793 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14761/20000] Train-Loss: 0.3656945779182205 Test-Loss: 0.14185871028041938 Train-f1: 0.6831168831168831 Test-f1: 0.4770348837209302 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [14762/20000] Train-Loss: 0.37208196919241554 Test-Loss: 0.14132815031309054 Train-f1: 0.5321937988127246 Test-f1: 0.5656318681318682 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14763/20000] Train-Loss: 0.36957326356248194 Test-Loss: 0.13696534519233464 Train-f1: 0.6567167359113668 Test-f1: 0.4644611528822056 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14764/20000] Train-Loss: 0.370616294683754 Test-Loss: 0.13789783008254125 Train-f1: 0.5929517154159887 Test-f1: 0.5561904761904762 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14765/20000] Train-Loss: 0.37310460405625223 Test-Loss: 0.13748798669536028 Train-f1: 0.656617561916314 Test-f1: 0.5339348370927318 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [14766/20000] Train-Loss: 0.37535572081531077 Test-Loss: 0.1402441507360402 Train-f1: 0.588091448781104 Test-f1: 0.5281271777003484 Train-acc: 0.8211920529801324 Test-acc: 0.8026315789473685\n",
      "Epoch [14767/20000] Train-Loss: 0.37056708187020404 Test-Loss: 0.1423266903664659 Train-f1: 0.6853833412963847 Test-f1: 0.4875179340028694 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14768/20000] Train-Loss: 0.3679752030051392 Test-Loss: 0.13760200493461824 Train-f1: 0.6447428785192598 Test-f1: 0.5393083387201034 Train-acc: 0.8576158940397351 Test-acc: 0.8157894736842105\n",
      "Epoch [14769/20000] Train-Loss: 0.37118184476404215 Test-Loss: 0.13735795044598315 Train-f1: 0.6267117103251557 Test-f1: 0.5561904761904762 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [14770/20000] Train-Loss: 0.37590203628274066 Test-Loss: 0.13660758399734782 Train-f1: 0.60955919604417 Test-f1: 0.5451190476190477 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14771/20000] Train-Loss: 0.37102448674998467 Test-Loss: 0.1385008096347557 Train-f1: 0.6503742577921313 Test-f1: 0.4860817231548939 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14772/20000] Train-Loss: 0.3684018390045811 Test-Loss: 0.13990484844989026 Train-f1: 0.6284236734567861 Test-f1: 0.5425559947299078 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14773/20000] Train-Loss: 0.368538158396188 Test-Loss: 0.13997558751835526 Train-f1: 0.5343639797055946 Test-f1: 0.5605602240896359 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14774/20000] Train-Loss: 0.3701559700457616 Test-Loss: 0.14126535847889185 Train-f1: 0.5758565635277965 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14775/20000] Train-Loss: 0.364015161018277 Test-Loss: 0.13617279043083064 Train-f1: 0.6041025641025641 Test-f1: 0.4395848595848596 Train-acc: 0.8410596026490066 Test-acc: 0.75\n",
      "Epoch [14776/20000] Train-Loss: 0.37428591554781926 Test-Loss: 0.1369878323286505 Train-f1: 0.6454610195980044 Test-f1: 0.4395848595848596 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14777/20000] Train-Loss: 0.3660742372449967 Test-Loss: 0.14134115050597093 Train-f1: 0.5703665462054723 Test-f1: 0.5262301587301588 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14778/20000] Train-Loss: 0.37278920632319473 Test-Loss: 0.13776821857237645 Train-f1: 0.588076100535978 Test-f1: 0.5451919146036793 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14779/20000] Train-Loss: 0.37208398262927467 Test-Loss: 0.14283824484191887 Train-f1: 0.5847457086923379 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14780/20000] Train-Loss: 0.3692131561328022 Test-Loss: 0.13920342742438013 Train-f1: 0.656353091193287 Test-f1: 0.5776349965823651 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [14781/20000] Train-Loss: 0.3673734991370061 Test-Loss: 0.14547090367577747 Train-f1: 0.5368333602010761 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14782/20000] Train-Loss: 0.3637279571163329 Test-Loss: 0.14512625726376746 Train-f1: 0.5834407995666718 Test-f1: 0.5494699894699895 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14783/20000] Train-Loss: 0.37203636788398076 Test-Loss: 0.1370467466159566 Train-f1: 0.6504244783467386 Test-f1: 0.5451919146036793 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14784/20000] Train-Loss: 0.3731691922794224 Test-Loss: 0.14126132435619762 Train-f1: 0.5507721137310179 Test-f1: 0.4666666666666667 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14785/20000] Train-Loss: 0.36352539570572906 Test-Loss: 0.14395736514646987 Train-f1: 0.6490515360080578 Test-f1: 0.5624077238550923 Train-acc: 0.8576158940397351 Test-acc: 0.8157894736842105\n",
      "Epoch [14786/20000] Train-Loss: 0.369366033012754 Test-Loss: 0.13959736778950238 Train-f1: 0.6016888187042155 Test-f1: 0.5776349965823651 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14787/20000] Train-Loss: 0.37041459154227296 Test-Loss: 0.14116361858473128 Train-f1: 0.5873495631884894 Test-f1: 0.5746296946296946 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14788/20000] Train-Loss: 0.3676763148939086 Test-Loss: 0.14044767294436536 Train-f1: 0.658545332458376 Test-f1: 0.5425559947299078 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14789/20000] Train-Loss: 0.36936801147602183 Test-Loss: 0.14991448972655072 Train-f1: 0.5822169059011164 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14790/20000] Train-Loss: 0.3653240224539689 Test-Loss: 0.13814846263585415 Train-f1: 0.6882057482181738 Test-f1: 0.5683710801393728 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14791/20000] Train-Loss: 0.36744468430218935 Test-Loss: 0.1456052078032347 Train-f1: 0.6111706499953959 Test-f1: 0.4770348837209302 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14792/20000] Train-Loss: 0.36893212396891273 Test-Loss: 0.14690635432253227 Train-f1: 0.5901550998021586 Test-f1: 0.5637166085946574 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14793/20000] Train-Loss: 0.36431959355184634 Test-Loss: 0.1471467353611727 Train-f1: 0.6767757772710993 Test-f1: 0.5264562569213732 Train-acc: 0.8741721854304636 Test-acc: 0.8026315789473685\n",
      "Epoch [14794/20000] Train-Loss: 0.36780475879427327 Test-Loss: 0.13898955861904214 Train-f1: 0.4963063603063603 Test-f1: 0.5776349965823651 Train-acc: 0.8245033112582781 Test-acc: 0.8289473684210527\n",
      "Epoch [14795/20000] Train-Loss: 0.3720087188437924 Test-Loss: 0.14083458884650515 Train-f1: 0.6361908407944215 Test-f1: 0.4875179340028694 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14796/20000] Train-Loss: 0.3664386958114035 Test-Loss: 0.13604530437892942 Train-f1: 0.6483576233831801 Test-f1: 0.4428237259816208 Train-acc: 0.8377483443708609 Test-acc: 0.75\n",
      "Epoch [14797/20000] Train-Loss: 0.36741467920016435 Test-Loss: 0.1392282250509759 Train-f1: 0.6287219358711426 Test-f1: 0.49493694711086017 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14798/20000] Train-Loss: 0.36833198872915907 Test-Loss: 0.14356121929537893 Train-f1: 0.60606167708588 Test-f1: 0.5684476031215162 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14799/20000] Train-Loss: 0.3689837551226788 Test-Loss: 0.13989165456226352 Train-f1: 0.6337216867018192 Test-f1: 0.5605602240896359 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14800/20000] Train-Loss: 0.3690514042317043 Test-Loss: 0.1366760399294865 Train-f1: 0.6172964569676899 Test-f1: 0.5073487773487774 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14801/20000] Train-Loss: 0.3662607010504194 Test-Loss: 0.13867615878912093 Train-f1: 0.66165110443832 Test-f1: 0.5776243470361118 Train-acc: 0.8543046357615894 Test-acc: 0.8289473684210527\n",
      "Epoch [14802/20000] Train-Loss: 0.3608279603854694 Test-Loss: 0.15326186150112986 Train-f1: 0.6001700089522002 Test-f1: 0.5603463203463204 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14803/20000] Train-Loss: 0.36988334257166666 Test-Loss: 0.14275282774330603 Train-f1: 0.6382986141375404 Test-f1: 0.5309429126899965 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14804/20000] Train-Loss: 0.3664207624711776 Test-Loss: 0.13717054914837631 Train-f1: 0.6052068126520681 Test-f1: 0.5683710801393728 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14805/20000] Train-Loss: 0.3712558346786414 Test-Loss: 0.1455703117430048 Train-f1: 0.5206271651100132 Test-f1: 0.5533179723502304 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14806/20000] Train-Loss: 0.37159033180444645 Test-Loss: 0.13926676028635668 Train-f1: 0.588076100535978 Test-f1: 0.47333333333333333 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14807/20000] Train-Loss: 0.3680859019849218 Test-Loss: 0.13640405551134352 Train-f1: 0.5898177149790053 Test-f1: 0.4537451737451737 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [14808/20000] Train-Loss: 0.36552733398576087 Test-Loss: 0.13950167101383293 Train-f1: 0.5149413268270013 Test-f1: 0.5617857142857143 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14809/20000] Train-Loss: 0.3752090369646101 Test-Loss: 0.13898152029055946 Train-f1: 0.660244869825515 Test-f1: 0.5428237259816207 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [14810/20000] Train-Loss: 0.37018610718546285 Test-Loss: 0.14017837418968465 Train-f1: 0.5852131922373951 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14811/20000] Train-Loss: 0.3719058766781304 Test-Loss: 0.14424219743210573 Train-f1: 0.6077044361772901 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14812/20000] Train-Loss: 0.3648749541198056 Test-Loss: 0.14317616767186397 Train-f1: 0.6540493120325054 Test-f1: 0.46674465510286395 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14813/20000] Train-Loss: 0.37311499525949876 Test-Loss: 0.15010830417908472 Train-f1: 0.5171548451548451 Test-f1: 0.57121145793681 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14814/20000] Train-Loss: 0.3640846306671929 Test-Loss: 0.14181486609741928 Train-f1: 0.6154362034668941 Test-f1: 0.5728065640103406 Train-acc: 0.8543046357615894 Test-acc: 0.8026315789473685\n",
      "Epoch [14815/20000] Train-Loss: 0.3708670291126541 Test-Loss: 0.13683921902199409 Train-f1: 0.6328237972073589 Test-f1: 0.5451919146036793 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14816/20000] Train-Loss: 0.367980850148747 Test-Loss: 0.13723045129392072 Train-f1: 0.6487507332862933 Test-f1: 0.5272243672243672 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14817/20000] Train-Loss: 0.36676842131564164 Test-Loss: 0.13899803496907523 Train-f1: 0.6534686845245852 Test-f1: 0.5101645123384253 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14818/20000] Train-Loss: 0.36909732830728914 Test-Loss: 0.13660072022964703 Train-f1: 0.604513822160881 Test-f1: 0.5339348370927318 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14819/20000] Train-Loss: 0.3687279641872202 Test-Loss: 0.14729156534887305 Train-f1: 0.6559479298609734 Test-f1: 0.5624077238550923 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14820/20000] Train-Loss: 0.36702474230560456 Test-Loss: 0.1488004445192887 Train-f1: 0.647734165277782 Test-f1: 0.4770348837209302 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14821/20000] Train-Loss: 0.36760564413132873 Test-Loss: 0.1389591011319536 Train-f1: 0.6047719298245614 Test-f1: 0.5308287764866713 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14822/20000] Train-Loss: 0.35890446887602917 Test-Loss: 0.13862458390088744 Train-f1: 0.6890902718351908 Test-f1: 0.4602828448452552 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [14823/20000] Train-Loss: 0.3656344543387043 Test-Loss: 0.14028843248350253 Train-f1: 0.5926007248173201 Test-f1: 0.5776349965823651 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14824/20000] Train-Loss: 0.3709383486273869 Test-Loss: 0.14181309180170928 Train-f1: 0.5767219641672197 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14825/20000] Train-Loss: 0.3680801017842677 Test-Loss: 0.13904435745460916 Train-f1: 0.6509279721041735 Test-f1: 0.5495545314900154 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14826/20000] Train-Loss: 0.3682150279168936 Test-Loss: 0.14331121224013155 Train-f1: 0.5529545396394051 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14827/20000] Train-Loss: 0.3669920350465418 Test-Loss: 0.1403135987092976 Train-f1: 0.5825300737065443 Test-f1: 0.5656318681318682 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14828/20000] Train-Loss: 0.3637403113668023 Test-Loss: 0.13672460228188654 Train-f1: 0.6239514348785872 Test-f1: 0.4944577129097253 Train-acc: 0.847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [14829/20000] Train-Loss: 0.3700905031029265 Test-Loss: 0.13865967402077842 Train-f1: 0.6725858304805673 Test-f1: 0.5776349965823651 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [14830/20000] Train-Loss: 0.370181033105105 Test-Loss: 0.14465137741743095 Train-f1: 0.6357012115401377 Test-f1: 0.5561337073398784 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14831/20000] Train-Loss: 0.36961205093057836 Test-Loss: 0.13748340475332044 Train-f1: 0.6512407995114589 Test-f1: 0.5561904761904762 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14832/20000] Train-Loss: 0.37122380753939666 Test-Loss: 0.13924926205639404 Train-f1: 0.650722172918715 Test-f1: 0.5540808034925682 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14833/20000] Train-Loss: 0.36731885598229114 Test-Loss: 0.14404467614590818 Train-f1: 0.6073745635917545 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14834/20000] Train-Loss: 0.37036449414017514 Test-Loss: 0.14215651176957114 Train-f1: 0.5954630429082984 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14835/20000] Train-Loss: 0.3737377606758086 Test-Loss: 0.1454513764253293 Train-f1: 0.6525063653388885 Test-f1: 0.4829545454545454 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14836/20000] Train-Loss: 0.36407894594004425 Test-Loss: 0.14506507322771678 Train-f1: 0.6338184338184337 Test-f1: 0.555110087045571 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14837/20000] Train-Loss: 0.36782652394962745 Test-Loss: 0.13704105770091318 Train-f1: 0.6704856927792558 Test-f1: 0.5451919146036793 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14838/20000] Train-Loss: 0.37358136106379414 Test-Loss: 0.13827084260309924 Train-f1: 0.5993910304931146 Test-f1: 0.5776349965823651 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14839/20000] Train-Loss: 0.3678753562382476 Test-Loss: 0.13886869667702134 Train-f1: 0.654880377936282 Test-f1: 0.5101645123384253 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14840/20000] Train-Loss: 0.3620303223987095 Test-Loss: 0.13826588774926363 Train-f1: 0.590401644768414 Test-f1: 0.5826567944250871 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14841/20000] Train-Loss: 0.3688868372622761 Test-Loss: 0.14146954900250464 Train-f1: 0.5898177149790053 Test-f1: 0.4834843205574913 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14842/20000] Train-Loss: 0.3681094041544296 Test-Loss: 0.13860671346933057 Train-f1: 0.5340915032679738 Test-f1: 0.5561904761904762 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14843/20000] Train-Loss: 0.36783003852999974 Test-Loss: 0.1385452826102401 Train-f1: 0.5876711298461037 Test-f1: 0.5561904761904762 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14844/20000] Train-Loss: 0.3648895379863348 Test-Loss: 0.13625440551142237 Train-f1: 0.5137216406781624 Test-f1: 0.49631578947368427 Train-acc: 0.8278145695364238 Test-acc: 0.7631578947368421\n",
      "Epoch [14845/20000] Train-Loss: 0.3676401038690066 Test-Loss: 0.13694350959361182 Train-f1: 0.6040772438007461 Test-f1: 0.5561904761904762 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14846/20000] Train-Loss: 0.3677574615572636 Test-Loss: 0.1374403930487302 Train-f1: 0.6281710563040487 Test-f1: 0.566984126984127 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14847/20000] Train-Loss: 0.3682659640487116 Test-Loss: 0.14018847832023212 Train-f1: 0.6363706933243357 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14848/20000] Train-Loss: 0.3658412543968733 Test-Loss: 0.14136996924642758 Train-f1: 0.6047701155959108 Test-f1: 0.5865132359250007 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14849/20000] Train-Loss: 0.36951348081221164 Test-Loss: 0.14256414113982252 Train-f1: 0.6388112062214537 Test-f1: 0.5714285714285714 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14850/20000] Train-Loss: 0.36939725606592594 Test-Loss: 0.14712969015575678 Train-f1: 0.552547238183983 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14851/20000] Train-Loss: 0.3737522805098443 Test-Loss: 0.13803342928355944 Train-f1: 0.6168761598873851 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14852/20000] Train-Loss: 0.3678712953389888 Test-Loss: 0.13881141012012918 Train-f1: 0.5851082251082251 Test-f1: 0.5617857142857143 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14853/20000] Train-Loss: 0.3664154298782049 Test-Loss: 0.14517800542943102 Train-f1: 0.5473455121097123 Test-f1: 0.5624077238550923 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14854/20000] Train-Loss: 0.3689414552202945 Test-Loss: 0.14004154218346976 Train-f1: 0.6555681732152321 Test-f1: 0.5495545314900154 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14855/20000] Train-Loss: 0.36646718715897625 Test-Loss: 0.13706809241213222 Train-f1: 0.5146104163320387 Test-f1: 0.5451919146036793 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14856/20000] Train-Loss: 0.3657216632697315 Test-Loss: 0.14364649343325406 Train-f1: 0.597506427320092 Test-f1: 0.5624077238550923 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14857/20000] Train-Loss: 0.36532991624103245 Test-Loss: 0.1384588524961639 Train-f1: 0.6510421871415694 Test-f1: 0.5683710801393728 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14858/20000] Train-Loss: 0.36094157575451197 Test-Loss: 0.14367056792331967 Train-f1: 0.6146807109940751 Test-f1: 0.5684476031215162 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14859/20000] Train-Loss: 0.3639186156343392 Test-Loss: 0.1378770293345145 Train-f1: 0.6683507829203194 Test-f1: 0.5776243470361118 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [14860/20000] Train-Loss: 0.36434141749091725 Test-Loss: 0.1363595945052918 Train-f1: 0.5289764145462995 Test-f1: 0.5339348370927318 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [14861/20000] Train-Loss: 0.3645318195973767 Test-Loss: 0.13657375371440109 Train-f1: 0.526371473354232 Test-f1: 0.5339348370927318 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [14862/20000] Train-Loss: 0.3665754555904161 Test-Loss: 0.14519230450471973 Train-f1: 0.6537040845707704 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14863/20000] Train-Loss: 0.36650119647188534 Test-Loss: 0.14769084864224669 Train-f1: 0.5877700658210456 Test-f1: 0.5637166085946574 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14864/20000] Train-Loss: 0.36349755834409314 Test-Loss: 0.1371515136741262 Train-f1: 0.675304625199362 Test-f1: 0.5506746031746033 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14865/20000] Train-Loss: 0.36870105819012344 Test-Loss: 0.14125957499301023 Train-f1: 0.5495092024539877 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14866/20000] Train-Loss: 0.36642331001357475 Test-Loss: 0.14059658022043878 Train-f1: 0.5769020638585856 Test-f1: 0.4774585921325052 Train-acc: 0.8245033112582781 Test-acc: 0.7894736842105263\n",
      "Epoch [14867/20000] Train-Loss: 0.36886825334035916 Test-Loss: 0.14582015886216917 Train-f1: 0.5466212100136151 Test-f1: 0.5272601794340925 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14868/20000] Train-Loss: 0.35981247932437566 Test-Loss: 0.14594683628080785 Train-f1: 0.6873259876571134 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [14869/20000] Train-Loss: 0.3622202052648622 Test-Loss: 0.13690140789960134 Train-f1: 0.6809856412902769 Test-f1: 0.4602828448452552 Train-acc: 0.8543046357615894 Test-acc: 0.7763157894736842\n",
      "Epoch [14870/20000] Train-Loss: 0.37073481888376053 Test-Loss: 0.13942761702785086 Train-f1: 0.6300604157747015 Test-f1: 0.580450731572013 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14871/20000] Train-Loss: 0.37007045401341065 Test-Loss: 0.14835236220942807 Train-f1: 0.5879011082693947 Test-f1: 0.5624077238550923 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14872/20000] Train-Loss: 0.3642962715407898 Test-Loss: 0.13682755854126083 Train-f1: 0.6805788982259571 Test-f1: 0.5451919146036793 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14873/20000] Train-Loss: 0.3689261084016168 Test-Loss: 0.13625890377873476 Train-f1: 0.6006235546327712 Test-f1: 0.5073487773487774 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [14874/20000] Train-Loss: 0.3677456357885648 Test-Loss: 0.1452584418699996 Train-f1: 0.6510909324218495 Test-f1: 0.5533179723502304 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14875/20000] Train-Loss: 0.36411753251931406 Test-Loss: 0.14481290769102856 Train-f1: 0.65139973990881 Test-f1: 0.5821288515406162 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14876/20000] Train-Loss: 0.3732962982152795 Test-Loss: 0.14130456748613676 Train-f1: 0.616840077207554 Test-f1: 0.5309429126899965 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14877/20000] Train-Loss: 0.3714196783833932 Test-Loss: 0.14024176084943035 Train-f1: 0.5732977838938104 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14878/20000] Train-Loss: 0.36623628064143726 Test-Loss: 0.1388618447892255 Train-f1: 0.6532164611111979 Test-f1: 0.580450731572013 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14879/20000] Train-Loss: 0.3654249740562065 Test-Loss: 0.13633727635324133 Train-f1: 0.5946109263258561 Test-f1: 0.5451919146036793 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14880/20000] Train-Loss: 0.37007669850997643 Test-Loss: 0.14012578091790734 Train-f1: 0.6385092385092386 Test-f1: 0.5684476031215162 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14881/20000] Train-Loss: 0.3590414672033223 Test-Loss: 0.139418271531596 Train-f1: 0.60061288039632 Test-f1: 0.4746428571428572 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14882/20000] Train-Loss: 0.3664696897345692 Test-Loss: 0.14562173111501323 Train-f1: 0.50153927414582 Test-f1: 0.5656318681318682 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14883/20000] Train-Loss: 0.37129574051301917 Test-Loss: 0.14152628877541482 Train-f1: 0.5745810411086907 Test-f1: 0.5309429126899965 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14884/20000] Train-Loss: 0.3614049469012745 Test-Loss: 0.1535969946167282 Train-f1: 0.6415917154740685 Test-f1: 0.5368571035824556 Train-acc: 0.847682119205298 Test-acc: 0.7631578947368421\n",
      "Epoch [14885/20000] Train-Loss: 0.3706382116724913 Test-Loss: 0.13717705315217568 Train-f1: 0.5918592258123138 Test-f1: 0.5339348370927318 Train-acc: 0.8245033112582781 Test-acc: 0.7763157894736842\n",
      "Epoch [14886/20000] Train-Loss: 0.36357702126174796 Test-Loss: 0.13700913760319822 Train-f1: 0.5294628832922447 Test-f1: 0.5451919146036793 Train-acc: 0.8278145695364238 Test-acc: 0.7894736842105263\n",
      "Epoch [14887/20000] Train-Loss: 0.3702027025284543 Test-Loss: 0.13912434910115812 Train-f1: 0.641998738568275 Test-f1: 0.5425559947299078 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14888/20000] Train-Loss: 0.37291284663532037 Test-Loss: 0.1480780259166097 Train-f1: 0.5395730862284733 Test-f1: 0.5637166085946574 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14889/20000] Train-Loss: 0.3684615673425845 Test-Loss: 0.14085014297880122 Train-f1: 0.5561470061470061 Test-f1: 0.4883116883116883 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14890/20000] Train-Loss: 0.36660474090817724 Test-Loss: 0.13713729817227815 Train-f1: 0.5227123753439542 Test-f1: 0.5561904761904762 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14891/20000] Train-Loss: 0.36789744751510867 Test-Loss: 0.14178844063884263 Train-f1: 0.6557033069638112 Test-f1: 0.5309429126899965 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14892/20000] Train-Loss: 0.36821434677704906 Test-Loss: 0.1393440372510263 Train-f1: 0.6482650633545775 Test-f1: 0.5776243470361118 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14893/20000] Train-Loss: 0.367023012140864 Test-Loss: 0.14585538240352777 Train-f1: 0.5880435374342879 Test-f1: 0.5561337073398784 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14894/20000] Train-Loss: 0.36617240572318843 Test-Loss: 0.15109909444084893 Train-f1: 0.6123336291038155 Test-f1: 0.5637166085946574 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14895/20000] Train-Loss: 0.3687603537287448 Test-Loss: 0.15161512232045202 Train-f1: 0.5772833676759916 Test-f1: 0.4811594202898551 Train-acc: 0.8245033112582781 Test-acc: 0.8026315789473685\n",
      "Epoch [14896/20000] Train-Loss: 0.3646092299392449 Test-Loss: 0.13751795872100958 Train-f1: 0.5718745312921837 Test-f1: 0.5393083387201034 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14897/20000] Train-Loss: 0.3666974571935865 Test-Loss: 0.1416035290355015 Train-f1: 0.6052079026998936 Test-f1: 0.4883116883116883 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [14898/20000] Train-Loss: 0.36724733820371325 Test-Loss: 0.14194210657170347 Train-f1: 0.6356717322615941 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14899/20000] Train-Loss: 0.3662253465999402 Test-Loss: 0.13678292226755256 Train-f1: 0.6562848478449501 Test-f1: 0.4602828448452552 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [14900/20000] Train-Loss: 0.370393262674123 Test-Loss: 0.13896355045073566 Train-f1: 0.6602163583055303 Test-f1: 0.5776243470361118 Train-acc: 0.8543046357615894 Test-acc: 0.8289473684210527\n",
      "Epoch [14901/20000] Train-Loss: 0.36743464915185475 Test-Loss: 0.1443836751164243 Train-f1: 0.5196271780467494 Test-f1: 0.5624077238550923 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14902/20000] Train-Loss: 0.36269361444351866 Test-Loss: 0.13715181519073893 Train-f1: 0.6453886025378092 Test-f1: 0.5683710801393728 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14903/20000] Train-Loss: 0.3701624537955573 Test-Loss: 0.14076128707863253 Train-f1: 0.6416687065693688 Test-f1: 0.574856124267889 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [14904/20000] Train-Loss: 0.3685022389486764 Test-Loss: 0.1398396691060542 Train-f1: 0.631015064315634 Test-f1: 0.5746296946296946 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14905/20000] Train-Loss: 0.36381399160477246 Test-Loss: 0.14274201388839075 Train-f1: 0.6777839588143576 Test-f1: 0.5656318681318682 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14906/20000] Train-Loss: 0.36629182281352013 Test-Loss: 0.13741772239158118 Train-f1: 0.6368080703205793 Test-f1: 0.5451919146036793 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14907/20000] Train-Loss: 0.37585731723198945 Test-Loss: 0.1393762352408709 Train-f1: 0.5873495631884894 Test-f1: 0.5776349965823651 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14908/20000] Train-Loss: 0.36449902772733134 Test-Loss: 0.15177856057731684 Train-f1: 0.6170862163850088 Test-f1: 0.5584762906812692 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14909/20000] Train-Loss: 0.3641788160567896 Test-Loss: 0.15466016267220722 Train-f1: 0.5505376996011033 Test-f1: 0.5657142857142857 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14910/20000] Train-Loss: 0.3677322179104696 Test-Loss: 0.14042801087612442 Train-f1: 0.6316686715663697 Test-f1: 0.574856124267889 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14911/20000] Train-Loss: 0.3681855122595713 Test-Loss: 0.13689401492943468 Train-f1: 0.6756204146730462 Test-f1: 0.5561904761904762 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14912/20000] Train-Loss: 0.3678233860080626 Test-Loss: 0.13968284701055472 Train-f1: 0.590979290979291 Test-f1: 0.5421240737097515 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14913/20000] Train-Loss: 0.36865283985677094 Test-Loss: 0.13917335529455993 Train-f1: 0.6610279455400674 Test-f1: 0.5776349965823651 Train-acc: 0.8543046357615894 Test-acc: 0.8289473684210527\n",
      "Epoch [14914/20000] Train-Loss: 0.36454010827386135 Test-Loss: 0.1415494210582551 Train-f1: 0.6948321486814979 Test-f1: 0.5656318681318682 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [14915/20000] Train-Loss: 0.36516973560565935 Test-Loss: 0.14313071291264773 Train-f1: 0.7229478575116158 Test-f1: 0.4834843205574913 Train-acc: 0.8576158940397351 Test-acc: 0.8026315789473685\n",
      "Epoch [14916/20000] Train-Loss: 0.3721495259861349 Test-Loss: 0.14149016319527158 Train-f1: 0.5376052536932711 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14917/20000] Train-Loss: 0.3675757566314435 Test-Loss: 0.15130324372012935 Train-f1: 0.6353456067741783 Test-f1: 0.5665323435843054 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14918/20000] Train-Loss: 0.36915936150397816 Test-Loss: 0.13788132699625305 Train-f1: 0.6459927560767897 Test-f1: 0.44240059457450764 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [14919/20000] Train-Loss: 0.370995900153418 Test-Loss: 0.13740330935168163 Train-f1: 0.6016792731078446 Test-f1: 0.5776243470361118 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14920/20000] Train-Loss: 0.3653241279282518 Test-Loss: 0.1386448384245147 Train-f1: 0.612214814203325 Test-f1: 0.5428237259816207 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [14921/20000] Train-Loss: 0.36966126102683217 Test-Loss: 0.1396569700152707 Train-f1: 0.5900996770561988 Test-f1: 0.4746428571428572 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14922/20000] Train-Loss: 0.3663478797054699 Test-Loss: 0.13703421630946847 Train-f1: 0.6100233159046299 Test-f1: 0.5561904761904762 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14923/20000] Train-Loss: 0.36398214516320804 Test-Loss: 0.1393063418290902 Train-f1: 0.5876918286230691 Test-f1: 0.5641558441558441 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14924/20000] Train-Loss: 0.3672599095269606 Test-Loss: 0.15552595010860545 Train-f1: 0.6373276723276724 Test-f1: 0.5556872268091306 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14925/20000] Train-Loss: 0.37028337116767046 Test-Loss: 0.13717532905133736 Train-f1: 0.6354210388054062 Test-f1: 0.566984126984127 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14926/20000] Train-Loss: 0.36608156993567503 Test-Loss: 0.13955067433807058 Train-f1: 0.6332091332091332 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14927/20000] Train-Loss: 0.3689555365758999 Test-Loss: 0.13800955532174297 Train-f1: 0.5823326311748274 Test-f1: 0.48827791986359764 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14928/20000] Train-Loss: 0.36621616589547384 Test-Loss: 0.1487232603542918 Train-f1: 0.6437514307079525 Test-f1: 0.549891398923657 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [14929/20000] Train-Loss: 0.36489860371983 Test-Loss: 0.14293131578280382 Train-f1: 0.6692897706320525 Test-f1: 0.4770348837209302 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14930/20000] Train-Loss: 0.3659893849326529 Test-Loss: 0.14500944845573374 Train-f1: 0.6534417054439712 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14931/20000] Train-Loss: 0.36581707630632676 Test-Loss: 0.13796628126484267 Train-f1: 0.6338752826121248 Test-f1: 0.5804400820257598 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [14932/20000] Train-Loss: 0.3642061051150359 Test-Loss: 0.1438307333702277 Train-f1: 0.5605105278654168 Test-f1: 0.5624077238550923 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [14933/20000] Train-Loss: 0.3636949392417371 Test-Loss: 0.13751445116816094 Train-f1: 0.6208123080044262 Test-f1: 0.4531469684787762 Train-acc: 0.8443708609271523 Test-acc: 0.7631578947368421\n",
      "Epoch [14934/20000] Train-Loss: 0.36943035529692014 Test-Loss: 0.13694676439108863 Train-f1: 0.5931085644923219 Test-f1: 0.5561904761904762 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14935/20000] Train-Loss: 0.3706756704740181 Test-Loss: 0.1452649328015058 Train-f1: 0.6534067366771042 Test-f1: 0.4829545454545454 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14936/20000] Train-Loss: 0.3700155078524196 Test-Loss: 0.13937237764480545 Train-f1: 0.6413370723674712 Test-f1: 0.5561904761904762 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14937/20000] Train-Loss: 0.3703460298797733 Test-Loss: 0.13918277922410965 Train-f1: 0.5404806494178958 Test-f1: 0.5421972621972622 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14938/20000] Train-Loss: 0.36716733523896355 Test-Loss: 0.1471734596534844 Train-f1: 0.6694040590481081 Test-f1: 0.4770348837209302 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [14939/20000] Train-Loss: 0.3675100417388385 Test-Loss: 0.14001800666419267 Train-f1: 0.5393917369572615 Test-f1: 0.580450731572013 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14940/20000] Train-Loss: 0.3686304493429649 Test-Loss: 0.14007915374231286 Train-f1: 0.5950218929373188 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14941/20000] Train-Loss: 0.3610763191724034 Test-Loss: 0.13802048065553169 Train-f1: 0.6428668680404157 Test-f1: 0.5683710801393728 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [14942/20000] Train-Loss: 0.36707862051822293 Test-Loss: 0.1407049371820614 Train-f1: 0.5392386872083345 Test-f1: 0.580450731572013 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14943/20000] Train-Loss: 0.36562087247705155 Test-Loss: 0.14218818616256415 Train-f1: 0.6412119412119412 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14944/20000] Train-Loss: 0.36678897029980384 Test-Loss: 0.1398114111648937 Train-f1: 0.6028944986104134 Test-f1: 0.5408465930205061 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14945/20000] Train-Loss: 0.37003265930622925 Test-Loss: 0.13814418449316657 Train-f1: 0.6363106697224344 Test-f1: 0.5428237259816207 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14946/20000] Train-Loss: 0.3695644236045851 Test-Loss: 0.13762444632099294 Train-f1: 0.5346789100464256 Test-f1: 0.566984126984127 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14947/20000] Train-Loss: 0.36440848322903924 Test-Loss: 0.1380697055494723 Train-f1: 0.6354666606279509 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14948/20000] Train-Loss: 0.3653649348448515 Test-Loss: 0.14871390006146196 Train-f1: 0.6037613836976894 Test-f1: 0.5637166085946574 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14949/20000] Train-Loss: 0.3723318024866409 Test-Loss: 0.14162451101039278 Train-f1: 0.5977286883512049 Test-f1: 0.5417960088691796 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [14950/20000] Train-Loss: 0.3707706803980503 Test-Loss: 0.14317247467921998 Train-f1: 0.531793715056873 Test-f1: 0.4770348837209302 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [14951/20000] Train-Loss: 0.3624341404480911 Test-Loss: 0.13666395139807097 Train-f1: 0.6932193043762254 Test-f1: 0.5073487773487774 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [14952/20000] Train-Loss: 0.3697410572952821 Test-Loss: 0.1403643611050695 Train-f1: 0.5992414375380426 Test-f1: 0.4774585921325052 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14953/20000] Train-Loss: 0.3666965166448721 Test-Loss: 0.14243272888753397 Train-f1: 0.6557033069638112 Test-f1: 0.5684476031215162 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14954/20000] Train-Loss: 0.3713325748857566 Test-Loss: 0.14316901651237857 Train-f1: 0.607653527715055 Test-f1: 0.5745207570207571 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14955/20000] Train-Loss: 0.36816686818367883 Test-Loss: 0.14425188336843436 Train-f1: 0.5371513461808781 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14956/20000] Train-Loss: 0.36631105430388927 Test-Loss: 0.14091825411913075 Train-f1: 0.6539074661781256 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14957/20000] Train-Loss: 0.3689467966100718 Test-Loss: 0.1469885965343909 Train-f1: 0.6416473497271495 Test-f1: 0.5665323435843054 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14958/20000] Train-Loss: 0.36761547217890506 Test-Loss: 0.1481728441407095 Train-f1: 0.5793830330743083 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14959/20000] Train-Loss: 0.36534239312286826 Test-Loss: 0.13952771245980344 Train-f1: 0.6313708487415555 Test-f1: 0.5650793650793651 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [14960/20000] Train-Loss: 0.36051025750869425 Test-Loss: 0.1456677118579107 Train-f1: 0.6473677576903384 Test-f1: 0.4766897221211018 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [14961/20000] Train-Loss: 0.37112445811360595 Test-Loss: 0.1445219064473801 Train-f1: 0.533568914870721 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14962/20000] Train-Loss: 0.3675525559153507 Test-Loss: 0.14362564775280914 Train-f1: 0.667519566526189 Test-f1: 0.5684476031215162 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14963/20000] Train-Loss: 0.35952745443662315 Test-Loss: 0.14503924418452716 Train-f1: 0.6124561403508773 Test-f1: 0.5819024219024219 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [14964/20000] Train-Loss: 0.3615169830485306 Test-Loss: 0.14055703595404448 Train-f1: 0.6485917928889423 Test-f1: 0.4834843205574913 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [14965/20000] Train-Loss: 0.3635527855875803 Test-Loss: 0.15175465316415807 Train-f1: 0.6165887445887446 Test-f1: 0.5603463203463204 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14966/20000] Train-Loss: 0.37146404100456076 Test-Loss: 0.14077591404595013 Train-f1: 0.655880230880231 Test-f1: 0.5746296946296946 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [14967/20000] Train-Loss: 0.36602948112161426 Test-Loss: 0.1378676629148935 Train-f1: 0.5940748877873568 Test-f1: 0.566984126984127 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14968/20000] Train-Loss: 0.36892597152384676 Test-Loss: 0.13879041111174004 Train-f1: 0.585465721159085 Test-f1: 0.566984126984127 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14969/20000] Train-Loss: 0.36647047100594815 Test-Loss: 0.14791648994430312 Train-f1: 0.6387998256510843 Test-f1: 0.5266149870801033 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [14970/20000] Train-Loss: 0.3630514943322005 Test-Loss: 0.1363875799634707 Train-f1: 0.6366118784835206 Test-f1: 0.5395848595848596 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14971/20000] Train-Loss: 0.3706171685884057 Test-Loss: 0.14116807050113428 Train-f1: 0.6048391431967591 Test-f1: 0.5684476031215162 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14972/20000] Train-Loss: 0.3680476405166973 Test-Loss: 0.1426179081860751 Train-f1: 0.658545332458376 Test-f1: 0.5624077238550923 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [14973/20000] Train-Loss: 0.3601887468192588 Test-Loss: 0.13677091102273992 Train-f1: 0.6784376619637201 Test-f1: 0.5451919146036793 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [14974/20000] Train-Loss: 0.3734435668888178 Test-Loss: 0.1394569145071055 Train-f1: 0.6664705782540178 Test-f1: 0.5804400820257598 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14975/20000] Train-Loss: 0.3696362426778732 Test-Loss: 0.14184587143033478 Train-f1: 0.6204725389633022 Test-f1: 0.5494699894699895 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14976/20000] Train-Loss: 0.36974681505995244 Test-Loss: 0.1377011562232335 Train-f1: 0.582207387383858 Test-f1: 0.5561904761904762 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14977/20000] Train-Loss: 0.3627254277129534 Test-Loss: 0.141007484375716 Train-f1: 0.625881231144389 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [14978/20000] Train-Loss: 0.3665612394026603 Test-Loss: 0.13921000224521196 Train-f1: 0.5177217917992616 Test-f1: 0.48827791986359764 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [14979/20000] Train-Loss: 0.36069087088445567 Test-Loss: 0.14826080538314965 Train-f1: 0.5219898265538869 Test-f1: 0.4770348837209302 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [14980/20000] Train-Loss: 0.36823676675549316 Test-Loss: 0.1384364551457834 Train-f1: 0.6529810713121972 Test-f1: 0.5776243470361118 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14981/20000] Train-Loss: 0.3695580989998546 Test-Loss: 0.14151816290839042 Train-f1: 0.6473981265304959 Test-f1: 0.5684476031215162 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14982/20000] Train-Loss: 0.3663229837236381 Test-Loss: 0.14200370234021342 Train-f1: 0.6381693633306537 Test-f1: 0.5684476031215162 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14983/20000] Train-Loss: 0.36408443472852164 Test-Loss: 0.14215442996149572 Train-f1: 0.6144372097624922 Test-f1: 0.5494699894699895 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14984/20000] Train-Loss: 0.366119627917903 Test-Loss: 0.13822936303303035 Train-f1: 0.64565068426961 Test-f1: 0.566984126984127 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [14985/20000] Train-Loss: 0.36105614490410926 Test-Loss: 0.14358801631646276 Train-f1: 0.6383928206969681 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [14986/20000] Train-Loss: 0.36544988461038036 Test-Loss: 0.13765720380457497 Train-f1: 0.6528462405174734 Test-f1: 0.5776243470361118 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14987/20000] Train-Loss: 0.3678477966952863 Test-Loss: 0.13961609416459939 Train-f1: 0.5983394706177692 Test-f1: 0.4746428571428572 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [14988/20000] Train-Loss: 0.36853965184555815 Test-Loss: 0.13772632876579544 Train-f1: 0.6361910720858089 Test-f1: 0.566984126984127 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14989/20000] Train-Loss: 0.3687017738057991 Test-Loss: 0.14148309285343952 Train-f1: 0.6037996289424861 Test-f1: 0.5821288515406162 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14990/20000] Train-Loss: 0.3659617960102567 Test-Loss: 0.13739297678822912 Train-f1: 0.656617561916314 Test-f1: 0.566984126984127 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14991/20000] Train-Loss: 0.36571474261761483 Test-Loss: 0.14574651580993583 Train-f1: 0.6438985850750557 Test-f1: 0.4942917547568711 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [14992/20000] Train-Loss: 0.3684625753671181 Test-Loss: 0.13932370129117785 Train-f1: 0.6363706933243357 Test-f1: 0.5776349965823651 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [14993/20000] Train-Loss: 0.3652225991704868 Test-Loss: 0.13836887651983024 Train-f1: 0.6388577387524756 Test-f1: 0.5776243470361118 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [14994/20000] Train-Loss: 0.362492668518925 Test-Loss: 0.13943839390641946 Train-f1: 0.6411211272387743 Test-f1: 0.5776349965823651 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [14995/20000] Train-Loss: 0.3653116069836727 Test-Loss: 0.13881010305910219 Train-f1: 0.6040361829950871 Test-f1: 0.47783405000796303 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [14996/20000] Train-Loss: 0.36286817874140226 Test-Loss: 0.14953021283728843 Train-f1: 0.6620904916679564 Test-f1: 0.46539589442815255 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [14997/20000] Train-Loss: 0.36763413036015674 Test-Loss: 0.14293838828790847 Train-f1: 0.5898181818181818 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [14998/20000] Train-Loss: 0.3625563122557478 Test-Loss: 0.14605614745703435 Train-f1: 0.633225702402173 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [14999/20000] Train-Loss: 0.36395911721674484 Test-Loss: 0.13906033484714636 Train-f1: 0.6270280323680247 Test-f1: 0.4746428571428572 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [15000/20000] Train-Loss: 0.3646826054872851 Test-Loss: 0.13944758808075894 Train-f1: 0.6197399382307015 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15001/20000] Train-Loss: 0.3630213000289305 Test-Loss: 0.13864389504293143 Train-f1: 0.6917726956388214 Test-f1: 0.5804400820257598 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [15002/20000] Train-Loss: 0.36612653342498563 Test-Loss: 0.1365188399178632 Train-f1: 0.6363706933243357 Test-f1: 0.4395848595848596 Train-acc: 0.8410596026490066 Test-acc: 0.75\n",
      "Epoch [15003/20000] Train-Loss: 0.3666544484540413 Test-Loss: 0.14057204795025172 Train-f1: 0.5922791494220065 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15004/20000] Train-Loss: 0.36376504693769457 Test-Loss: 0.1416872289118202 Train-f1: 0.5901550998021586 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15005/20000] Train-Loss: 0.3702459845020492 Test-Loss: 0.14249260326001348 Train-f1: 0.61015515771526 Test-f1: 0.4770348837209302 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15006/20000] Train-Loss: 0.3651368817594516 Test-Loss: 0.1463793848263014 Train-f1: 0.5795930082988906 Test-f1: 0.5533179723502304 Train-acc: 0.8278145695364238 Test-acc: 0.8026315789473685\n",
      "Epoch [15007/20000] Train-Loss: 0.36616881828093517 Test-Loss: 0.1396695202055658 Train-f1: 0.609830903948551 Test-f1: 0.5281271777003484 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [15008/20000] Train-Loss: 0.3658511003808261 Test-Loss: 0.14129001172762143 Train-f1: 0.6584506647775015 Test-f1: 0.5656318681318682 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15009/20000] Train-Loss: 0.36085104067955803 Test-Loss: 0.14253741333801756 Train-f1: 0.6669236828334573 Test-f1: 0.5417960088691796 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15010/20000] Train-Loss: 0.37007041918404365 Test-Loss: 0.13752522063752254 Train-f1: 0.6587406482873278 Test-f1: 0.5451919146036793 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [15011/20000] Train-Loss: 0.3675098731633789 Test-Loss: 0.14483212253570227 Train-f1: 0.6174485383897149 Test-f1: 0.5696804511278196 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15012/20000] Train-Loss: 0.3645131469597796 Test-Loss: 0.13763651325964743 Train-f1: 0.5750479653601918 Test-f1: 0.566984126984127 Train-acc: 0.8211920529801324 Test-acc: 0.8157894736842105\n",
      "Epoch [15013/20000] Train-Loss: 0.3657257411547776 Test-Loss: 0.14641491779733484 Train-f1: 0.5764235764235764 Test-f1: 0.5637166085946574 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [15014/20000] Train-Loss: 0.3695228811728099 Test-Loss: 0.13706211167819782 Train-f1: 0.5940037653361637 Test-f1: 0.5056027456027457 Train-acc: 0.8278145695364238 Test-acc: 0.7763157894736842\n",
      "Epoch [15015/20000] Train-Loss: 0.36816629731929534 Test-Loss: 0.14452704502265182 Train-f1: 0.6335594931384405 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15016/20000] Train-Loss: 0.3631942858042182 Test-Loss: 0.13829246579285828 Train-f1: 0.5901550998021586 Test-f1: 0.4860817231548939 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [15017/20000] Train-Loss: 0.3662019506523473 Test-Loss: 0.13851705397401162 Train-f1: 0.671928882524909 Test-f1: 0.48827791986359764 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [15018/20000] Train-Loss: 0.368263614864287 Test-Loss: 0.14058288808899957 Train-f1: 0.6441302647185 Test-f1: 0.4875179340028694 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [15019/20000] Train-Loss: 0.3687676233296592 Test-Loss: 0.13853900771216499 Train-f1: 0.49485937543156594 Test-f1: 0.569799861973775 Train-acc: 0.8245033112582781 Test-acc: 0.8157894736842105\n",
      "Epoch [15020/20000] Train-Loss: 0.36357978382099837 Test-Loss: 0.14165587809347466 Train-f1: 0.5417331526920568 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15021/20000] Train-Loss: 0.3622380211180562 Test-Loss: 0.13697565200669135 Train-f1: 0.6551025743630785 Test-f1: 0.5339348370927318 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [15022/20000] Train-Loss: 0.36672686598949084 Test-Loss: 0.14359176556711425 Train-f1: 0.6835470085470086 Test-f1: 0.4703482587064677 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [15023/20000] Train-Loss: 0.36123952876815457 Test-Loss: 0.1380433270543758 Train-f1: 0.6317135039918025 Test-f1: 0.44240059457450764 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [15024/20000] Train-Loss: 0.36274345880751946 Test-Loss: 0.14358738239012345 Train-f1: 0.653559551051542 Test-f1: 0.5865238854712539 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [15025/20000] Train-Loss: 0.37220379668227904 Test-Loss: 0.13961633252018488 Train-f1: 0.6099510521269618 Test-f1: 0.5776349965823651 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15026/20000] Train-Loss: 0.366177716668135 Test-Loss: 0.1371479488071218 Train-f1: 0.5823509737027462 Test-f1: 0.47501831501831504 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [15027/20000] Train-Loss: 0.3657429670586996 Test-Loss: 0.14076055477123167 Train-f1: 0.6329415075692623 Test-f1: 0.5776349965823651 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15028/20000] Train-Loss: 0.3645409572605197 Test-Loss: 0.13711471757424393 Train-f1: 0.6313708487415555 Test-f1: 0.5451919146036793 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [15029/20000] Train-Loss: 0.35898743935104505 Test-Loss: 0.1388059118225649 Train-f1: 0.6001573498964803 Test-f1: 0.4860817231548939 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [15030/20000] Train-Loss: 0.36477610395170307 Test-Loss: 0.14234886082857628 Train-f1: 0.536245491987897 Test-f1: 0.4942917547568711 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15031/20000] Train-Loss: 0.3633632709453828 Test-Loss: 0.15011291123017775 Train-f1: 0.6458838445396758 Test-f1: 0.47555555555555556 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [15032/20000] Train-Loss: 0.3665183935917245 Test-Loss: 0.14027068719134705 Train-f1: 0.587959021370786 Test-f1: 0.4935574229691877 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15033/20000] Train-Loss: 0.366729774084866 Test-Loss: 0.14359183512755114 Train-f1: 0.6012644889357219 Test-f1: 0.4770348837209302 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15034/20000] Train-Loss: 0.35755871102802483 Test-Loss: 0.14840059088065427 Train-f1: 0.6394584537882475 Test-f1: 0.5721373200442967 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15035/20000] Train-Loss: 0.36254194325673395 Test-Loss: 0.15548456252611811 Train-f1: 0.6685979416007517 Test-f1: 0.5603463203463204 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15036/20000] Train-Loss: 0.35782655806184416 Test-Loss: 0.14348662072110852 Train-f1: 0.6532164611111979 Test-f1: 0.4774585921325052 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15037/20000] Train-Loss: 0.3675877693527698 Test-Loss: 0.14096006953255721 Train-f1: 0.5954361455537927 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15038/20000] Train-Loss: 0.36184074175109365 Test-Loss: 0.14954969536157572 Train-f1: 0.5927432216905901 Test-f1: 0.5804381846635367 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15039/20000] Train-Loss: 0.36993422329894665 Test-Loss: 0.14400441574876868 Train-f1: 0.6197031246158764 Test-f1: 0.5675000000000001 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15040/20000] Train-Loss: 0.36912303397552904 Test-Loss: 0.1368211424728618 Train-f1: 0.6033280540981177 Test-f1: 0.5451919146036793 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15041/20000] Train-Loss: 0.3697796707380925 Test-Loss: 0.13934203648225413 Train-f1: 0.5812429181556699 Test-f1: 0.5776349965823651 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15042/20000] Train-Loss: 0.369127128716408 Test-Loss: 0.15193957196111962 Train-f1: 0.6603109766945383 Test-f1: 0.5657142857142857 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15043/20000] Train-Loss: 0.3642325776992847 Test-Loss: 0.13928658649188216 Train-f1: 0.5931148439661025 Test-f1: 0.5425559947299078 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15044/20000] Train-Loss: 0.365041957141464 Test-Loss: 0.1364193128290548 Train-f1: 0.6395531135531136 Test-f1: 0.4602828448452552 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [15045/20000] Train-Loss: 0.3632488211641813 Test-Loss: 0.13940418514497405 Train-f1: 0.6014154584910887 Test-f1: 0.5281271777003484 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [15046/20000] Train-Loss: 0.36574774172720614 Test-Loss: 0.13852334256661422 Train-f1: 0.5279755332386911 Test-f1: 0.5776243470361118 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [15047/20000] Train-Loss: 0.36464211950657033 Test-Loss: 0.1373971896960537 Train-f1: 0.6313708487415555 Test-f1: 0.5424005945745076 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [15048/20000] Train-Loss: 0.3582247499283731 Test-Loss: 0.15589115907970508 Train-f1: 0.6057744914659808 Test-f1: 0.5529751403368084 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [15049/20000] Train-Loss: 0.35565088470123063 Test-Loss: 0.13638226893650732 Train-f1: 0.6126924735831072 Test-f1: 0.4395848595848596 Train-acc: 0.8509933774834437 Test-acc: 0.75\n",
      "Epoch [15050/20000] Train-Loss: 0.3657835228936359 Test-Loss: 0.1395350455024042 Train-f1: 0.6840431743214728 Test-f1: 0.46306306306306305 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [15051/20000] Train-Loss: 0.3697884477941009 Test-Loss: 0.14399852451393777 Train-f1: 0.584371942757036 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15052/20000] Train-Loss: 0.36381682944595106 Test-Loss: 0.13956587817791685 Train-f1: 0.690905057675244 Test-f1: 0.566984126984127 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15053/20000] Train-Loss: 0.36074667255432774 Test-Loss: 0.1420794824156172 Train-f1: 0.6413033637676371 Test-f1: 0.5495545314900154 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15054/20000] Train-Loss: 0.36933002174635066 Test-Loss: 0.1412049581598881 Train-f1: 0.589906076521839 Test-f1: 0.5684476031215162 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15055/20000] Train-Loss: 0.35935153274363574 Test-Loss: 0.15393942081481765 Train-f1: 0.6385590357355063 Test-f1: 0.5529751403368084 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [15056/20000] Train-Loss: 0.3705480299972953 Test-Loss: 0.14119330481217884 Train-f1: 0.6485917928889423 Test-f1: 0.5865132359250007 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15057/20000] Train-Loss: 0.3612838659163932 Test-Loss: 0.15065023023237561 Train-f1: 0.5321995535578525 Test-f1: 0.5657142857142857 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15058/20000] Train-Loss: 0.36065661294913387 Test-Loss: 0.1373913518087627 Train-f1: 0.6155058847429346 Test-f1: 0.47501831501831504 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [15059/20000] Train-Loss: 0.36407272807222235 Test-Loss: 0.13809790123760232 Train-f1: 0.6190443200445366 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15060/20000] Train-Loss: 0.36620948360098027 Test-Loss: 0.1382705358246762 Train-f1: 0.6530588215505206 Test-f1: 0.5339348370927318 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [15061/20000] Train-Loss: 0.36349636463776047 Test-Loss: 0.1410013177121845 Train-f1: 0.6707173049419991 Test-f1: 0.5656318681318682 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15062/20000] Train-Loss: 0.3637882222272874 Test-Loss: 0.13693028797988277 Train-f1: 0.6454291050564342 Test-f1: 0.5561904761904762 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [15063/20000] Train-Loss: 0.35851214088734334 Test-Loss: 0.1403327332346987 Train-f1: 0.6340554124999447 Test-f1: 0.5535328501285949 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15064/20000] Train-Loss: 0.36682134972529856 Test-Loss: 0.1432858824689846 Train-f1: 0.6123336291038155 Test-f1: 0.5675000000000001 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15065/20000] Train-Loss: 0.3643394749686761 Test-Loss: 0.1397682153459721 Train-f1: 0.6002454919878969 Test-f1: 0.5641558441558441 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15066/20000] Train-Loss: 0.3680865742565112 Test-Loss: 0.13903797218805608 Train-f1: 0.6284236734567861 Test-f1: 0.5776349965823651 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [15067/20000] Train-Loss: 0.3604689807346276 Test-Loss: 0.13762453365676172 Train-f1: 0.6413370723674712 Test-f1: 0.566984126984127 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15068/20000] Train-Loss: 0.36236918351657194 Test-Loss: 0.13856574343328928 Train-f1: 0.6233374731138543 Test-f1: 0.5776243470361118 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [15069/20000] Train-Loss: 0.36461028218026326 Test-Loss: 0.14042186031114856 Train-f1: 0.5876143594102301 Test-f1: 0.5793006993006993 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15070/20000] Train-Loss: 0.3665681246722738 Test-Loss: 0.1395218650676634 Train-f1: 0.6247654905910133 Test-f1: 0.5201754385964913 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15071/20000] Train-Loss: 0.3631573624371013 Test-Loss: 0.14249011723525057 Train-f1: 0.6900098015940779 Test-f1: 0.5675000000000001 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15072/20000] Train-Loss: 0.36412472982385896 Test-Loss: 0.1440056885790445 Train-f1: 0.6900290315891339 Test-f1: 0.5684476031215162 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15073/20000] Train-Loss: 0.3685904033152894 Test-Loss: 0.13776248484646933 Train-f1: 0.6001903588402444 Test-f1: 0.5480076495933274 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [15074/20000] Train-Loss: 0.36200098897556227 Test-Loss: 0.1411239601997511 Train-f1: 0.6766731382574147 Test-f1: 0.5746296946296946 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [15075/20000] Train-Loss: 0.36259267284286106 Test-Loss: 0.1380970218827571 Train-f1: 0.6147902978923819 Test-f1: 0.5776243470361118 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15076/20000] Train-Loss: 0.36217060070270307 Test-Loss: 0.14383671812558718 Train-f1: 0.5851789722999203 Test-f1: 0.5469903926482874 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [15077/20000] Train-Loss: 0.3727634066577431 Test-Loss: 0.14087123383460134 Train-f1: 0.6875314005200742 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15078/20000] Train-Loss: 0.36229800592198974 Test-Loss: 0.1555408434110738 Train-f1: 0.6860013659376716 Test-f1: 0.47555555555555556 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15079/20000] Train-Loss: 0.369197752819397 Test-Loss: 0.14825147971167663 Train-f1: 0.5036433793317201 Test-f1: 0.5637166085946574 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15080/20000] Train-Loss: 0.36321085701841865 Test-Loss: 0.1484043163078274 Train-f1: 0.593934065934066 Test-f1: 0.5665323435843054 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [15081/20000] Train-Loss: 0.36266996461832274 Test-Loss: 0.13916844569183645 Train-f1: 0.6587047499221373 Test-f1: 0.5776349965823651 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [15082/20000] Train-Loss: 0.36365171757947024 Test-Loss: 0.13863490405824114 Train-f1: 0.5817087674230532 Test-f1: 0.5776243470361118 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [15083/20000] Train-Loss: 0.35823724798239376 Test-Loss: 0.1420341954513237 Train-f1: 0.542670908039329 Test-f1: 0.5819024219024219 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [15084/20000] Train-Loss: 0.36228436967252026 Test-Loss: 0.14211298984623377 Train-f1: 0.6477534603585025 Test-f1: 0.5290458937198068 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [15085/20000] Train-Loss: 0.3625661276630305 Test-Loss: 0.14070003181990143 Train-f1: 0.6245559324506692 Test-f1: 0.5865238854712539 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [15086/20000] Train-Loss: 0.36798381443780337 Test-Loss: 0.13782192630805876 Train-f1: 0.5982716542132255 Test-f1: 0.47783405000796303 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [15087/20000] Train-Loss: 0.3596910683268547 Test-Loss: 0.14503614821674643 Train-f1: 0.6345630278582275 Test-f1: 0.5821288515406162 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15088/20000] Train-Loss: 0.3587560489935238 Test-Loss: 0.15336213095150875 Train-f1: 0.6843313700687801 Test-f1: 0.5657142857142857 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [15089/20000] Train-Loss: 0.3690779213503503 Test-Loss: 0.13715761410365582 Train-f1: 0.5722105263157895 Test-f1: 0.5506746031746033 Train-acc: 0.8178807947019867 Test-acc: 0.7894736842105263\n",
      "Epoch [15090/20000] Train-Loss: 0.365242160300971 Test-Loss: 0.14053539706882687 Train-f1: 0.6036845011528555 Test-f1: 0.5793006993006993 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15091/20000] Train-Loss: 0.365443870002293 Test-Loss: 0.14385633218994404 Train-f1: 0.5823296252277183 Test-f1: 0.5624077238550923 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15092/20000] Train-Loss: 0.3627496842257074 Test-Loss: 0.14395615467229836 Train-f1: 0.6572354129248594 Test-f1: 0.4770348837209302 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15093/20000] Train-Loss: 0.3661201799030542 Test-Loss: 0.1427178944826746 Train-f1: 0.5956885318229856 Test-f1: 0.5281271777003484 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [15094/20000] Train-Loss: 0.35791192647798475 Test-Loss: 0.14459764075154238 Train-f1: 0.5081157380520438 Test-f1: 0.5684476031215162 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15095/20000] Train-Loss: 0.3673788425057507 Test-Loss: 0.14515273407827456 Train-f1: 0.630141440740519 Test-f1: 0.5624077238550923 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15096/20000] Train-Loss: 0.3616400275904207 Test-Loss: 0.14485248531428715 Train-f1: 0.6610147673416039 Test-f1: 0.5684476031215162 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [15097/20000] Train-Loss: 0.3643812502611342 Test-Loss: 0.1411654164409165 Train-f1: 0.5886142075603636 Test-f1: 0.5793006993006993 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [15098/20000] Train-Loss: 0.36329487803343036 Test-Loss: 0.14781510807770942 Train-f1: 0.6560560845289385 Test-f1: 0.5696804511278196 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15099/20000] Train-Loss: 0.3648912558425369 Test-Loss: 0.14340783897361387 Train-f1: 0.6722526332115374 Test-f1: 0.5684476031215162 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15100/20000] Train-Loss: 0.36278702410673597 Test-Loss: 0.14561719751523264 Train-f1: 0.6702047952047953 Test-f1: 0.4829545454545454 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [15101/20000] Train-Loss: 0.3620433671411697 Test-Loss: 0.1392014110163751 Train-f1: 0.5720815478509285 Test-f1: 0.5793650793650794 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15102/20000] Train-Loss: 0.36859503419956285 Test-Loss: 0.13906343386473372 Train-f1: 0.6499787716917862 Test-f1: 0.5776349965823651 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15103/20000] Train-Loss: 0.36127979188928244 Test-Loss: 0.14308457737688743 Train-f1: 0.7169339990638164 Test-f1: 0.4935574229691877 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15104/20000] Train-Loss: 0.36243138070361874 Test-Loss: 0.13934154769816132 Train-f1: 0.49869164146217937 Test-f1: 0.441995841995842 Train-acc: 0.8278145695364238 Test-acc: 0.75\n",
      "Epoch [15105/20000] Train-Loss: 0.3676051605823905 Test-Loss: 0.13749345416899356 Train-f1: 0.6046534751563805 Test-f1: 0.4602828448452552 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [15106/20000] Train-Loss: 0.3691981917218951 Test-Loss: 0.14100138755016434 Train-f1: 0.6684881952781112 Test-f1: 0.5262301587301588 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [15107/20000] Train-Loss: 0.36421142659512395 Test-Loss: 0.13906857455358082 Train-f1: 0.5901550998021586 Test-f1: 0.5776349965823651 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15108/20000] Train-Loss: 0.3628315451240236 Test-Loss: 0.1426077004546938 Train-f1: 0.7127670250896057 Test-f1: 0.4942917547568711 Train-acc: 0.8609271523178808 Test-acc: 0.8157894736842105\n",
      "Epoch [15109/20000] Train-Loss: 0.3674804757057128 Test-Loss: 0.13758997822377042 Train-f1: 0.5550585999232103 Test-f1: 0.566984126984127 Train-acc: 0.8178807947019867 Test-acc: 0.8157894736842105\n",
      "Epoch [15110/20000] Train-Loss: 0.35876863556027094 Test-Loss: 0.13798457913043533 Train-f1: 0.6021724765300924 Test-f1: 0.5776243470361118 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15111/20000] Train-Loss: 0.36370807289139895 Test-Loss: 0.14132166612334407 Train-f1: 0.69 Test-f1: 0.5758730158730159 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15112/20000] Train-Loss: 0.36718609168735905 Test-Loss: 0.13739971285138236 Train-f1: 0.6016792731078446 Test-f1: 0.5451919146036793 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15113/20000] Train-Loss: 0.35805747034351704 Test-Loss: 0.1400210527626393 Train-f1: 0.6515109304698347 Test-f1: 0.47714285714285715 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15114/20000] Train-Loss: 0.364130463281114 Test-Loss: 0.14619662951450899 Train-f1: 0.6015795015795016 Test-f1: 0.5675000000000001 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15115/20000] Train-Loss: 0.3643859449354408 Test-Loss: 0.14964955009534542 Train-f1: 0.584848484848485 Test-f1: 0.5696804511278196 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15116/20000] Train-Loss: 0.3617450263252345 Test-Loss: 0.1441565664526671 Train-f1: 0.6444589380895114 Test-f1: 0.4770348837209302 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [15117/20000] Train-Loss: 0.3641878453903474 Test-Loss: 0.13872786925636515 Train-f1: 0.5483546172284441 Test-f1: 0.5776243470361118 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [15118/20000] Train-Loss: 0.36374929444522563 Test-Loss: 0.14261325771168168 Train-f1: 0.6532452271582707 Test-f1: 0.5651190476190476 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15119/20000] Train-Loss: 0.36621016431729364 Test-Loss: 0.14366026785777414 Train-f1: 0.6500566823048894 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15120/20000] Train-Loss: 0.3630919461740065 Test-Loss: 0.14142721837876185 Train-f1: 0.6540031828313467 Test-f1: 0.5605602240896359 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [15121/20000] Train-Loss: 0.36730979389477453 Test-Loss: 0.13913892418154075 Train-f1: 0.6150492264416315 Test-f1: 0.5540808034925682 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15122/20000] Train-Loss: 0.36200162907426336 Test-Loss: 0.14818578142453012 Train-f1: 0.6605664625549734 Test-f1: 0.5624077238550923 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [15123/20000] Train-Loss: 0.3651474740420275 Test-Loss: 0.14655327492084239 Train-f1: 0.5822915180058037 Test-f1: 0.5561337073398784 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [15124/20000] Train-Loss: 0.35812440906800747 Test-Loss: 0.14083554900343023 Train-f1: 0.676322362568938 Test-f1: 0.5656318681318682 Train-acc: 0.8576158940397351 Test-acc: 0.8157894736842105\n",
      "Epoch [15125/20000] Train-Loss: 0.3592796899328735 Test-Loss: 0.1382815896561017 Train-f1: 0.5788648792850474 Test-f1: 0.5776243470361118 Train-acc: 0.8278145695364238 Test-acc: 0.8289473684210527\n",
      "Epoch [15126/20000] Train-Loss: 0.3662657455455569 Test-Loss: 0.14615484836583156 Train-f1: 0.6088621762170652 Test-f1: 0.5624077238550923 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [15127/20000] Train-Loss: 0.3645983823193909 Test-Loss: 0.14469675822931088 Train-f1: 0.6070114253435331 Test-f1: 0.5675000000000001 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15128/20000] Train-Loss: 0.3621473182505373 Test-Loss: 0.147674366232737 Train-f1: 0.6656481053193383 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15129/20000] Train-Loss: 0.3702949909482307 Test-Loss: 0.13958047674209764 Train-f1: 0.5967064667347253 Test-f1: 0.5776349965823651 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [15130/20000] Train-Loss: 0.36077111454421523 Test-Loss: 0.13808774322082248 Train-f1: 0.6636219859994814 Test-f1: 0.44240059457450764 Train-acc: 0.8443708609271523 Test-acc: 0.75\n",
      "Epoch [15131/20000] Train-Loss: 0.3622444915604886 Test-Loss: 0.14816006416612024 Train-f1: 0.5951010628429984 Test-f1: 0.5603463203463204 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15132/20000] Train-Loss: 0.36232546995849635 Test-Loss: 0.1445998103740506 Train-f1: 0.6306812735793665 Test-f1: 0.5675000000000001 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15133/20000] Train-Loss: 0.36103734941259236 Test-Loss: 0.15024282599306937 Train-f1: 0.6588992269299175 Test-f1: 0.4727272727272728 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [15134/20000] Train-Loss: 0.36787428987119086 Test-Loss: 0.14783453336473695 Train-f1: 0.6306352033060107 Test-f1: 0.4770348837209302 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [15135/20000] Train-Loss: 0.3590940606850856 Test-Loss: 0.14286617799226975 Train-f1: 0.6104245280636085 Test-f1: 0.5714285714285714 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15136/20000] Train-Loss: 0.361210672642605 Test-Loss: 0.1409276624479074 Train-f1: 0.5952231203844106 Test-f1: 0.5494699894699895 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15137/20000] Train-Loss: 0.36339346326116145 Test-Loss: 0.1455046647221539 Train-f1: 0.6560660871219878 Test-f1: 0.5684476031215162 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15138/20000] Train-Loss: 0.36356400780264747 Test-Loss: 0.1471179200506989 Train-f1: 0.6391156151247686 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15139/20000] Train-Loss: 0.3620356842785651 Test-Loss: 0.14637413711981653 Train-f1: 0.7086095661846497 Test-f1: 0.5675000000000001 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [15140/20000] Train-Loss: 0.3672290330027626 Test-Loss: 0.13753509430006344 Train-f1: 0.653559551051542 Test-f1: 0.5101645123384253 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [15141/20000] Train-Loss: 0.36487951694204 Test-Loss: 0.1419862881073656 Train-f1: 0.6052079026998936 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15142/20000] Train-Loss: 0.35739133448578264 Test-Loss: 0.14560541774049063 Train-f1: 0.6376718824087245 Test-f1: 0.5675000000000001 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15143/20000] Train-Loss: 0.36078999387391103 Test-Loss: 0.1391530763211122 Train-f1: 0.6474724336954658 Test-f1: 0.5826567944250871 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [15144/20000] Train-Loss: 0.363727608187555 Test-Loss: 0.1490225268567112 Train-f1: 0.655636640508588 Test-f1: 0.5637166085946574 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15145/20000] Train-Loss: 0.3676031904327433 Test-Loss: 0.15463143566210988 Train-f1: 0.6132973321112736 Test-f1: 0.5556872268091306 Train-acc: 0.8344370860927153 Test-acc: 0.8026315789473685\n",
      "Epoch [15146/20000] Train-Loss: 0.36393379357847694 Test-Loss: 0.15140405971681495 Train-f1: 0.5871020126826578 Test-f1: 0.5726054974835463 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15147/20000] Train-Loss: 0.360305238329081 Test-Loss: 0.14921398281561754 Train-f1: 0.6179857637886352 Test-f1: 0.5675000000000001 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15148/20000] Train-Loss: 0.36335075337091427 Test-Loss: 0.13809942036135286 Train-f1: 0.6147708147708147 Test-f1: 0.5617857142857143 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [15149/20000] Train-Loss: 0.36625658025694763 Test-Loss: 0.15017055184411124 Train-f1: 0.6383928206969681 Test-f1: 0.5665323435843054 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15150/20000] Train-Loss: 0.3600587925111162 Test-Loss: 0.1402621997729361 Train-f1: 0.6510421871415694 Test-f1: 0.5776349965823651 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15151/20000] Train-Loss: 0.3643546475967963 Test-Loss: 0.14103957759238586 Train-f1: 0.6072847682119205 Test-f1: 0.5684476031215162 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15152/20000] Train-Loss: 0.3598410118676876 Test-Loss: 0.1436012030953457 Train-f1: 0.6050219487676867 Test-f1: 0.5819024219024219 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15153/20000] Train-Loss: 0.3657230900141059 Test-Loss: 0.14364422315033715 Train-f1: 0.6068025841841453 Test-f1: 0.5684476031215162 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15154/20000] Train-Loss: 0.3622038119697024 Test-Loss: 0.13713212464354985 Train-f1: 0.6363127802229445 Test-f1: 0.5339348370927318 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [15155/20000] Train-Loss: 0.37223103399325574 Test-Loss: 0.13892284274764713 Train-f1: 0.6335594931384405 Test-f1: 0.5776349965823651 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15156/20000] Train-Loss: 0.3626916296408315 Test-Loss: 0.15063236352025042 Train-f1: 0.5639481102614743 Test-f1: 0.4770348837209302 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [15157/20000] Train-Loss: 0.3631879342376852 Test-Loss: 0.1377285052955284 Train-f1: 0.6289266592842753 Test-f1: 0.566984126984127 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15158/20000] Train-Loss: 0.3649977785789981 Test-Loss: 0.13701199163636 Train-f1: 0.5990833605753514 Test-f1: 0.5339348370927318 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [15159/20000] Train-Loss: 0.3607049886337862 Test-Loss: 0.13747406192966788 Train-f1: 0.6511440940012369 Test-f1: 0.5451190476190477 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [15160/20000] Train-Loss: 0.3633932098235733 Test-Loss: 0.14001011403202168 Train-f1: 0.6528297066790559 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15161/20000] Train-Loss: 0.36366205298890125 Test-Loss: 0.14141308483506024 Train-f1: 0.6556904004978891 Test-f1: 0.5684476031215162 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15162/20000] Train-Loss: 0.3641674437754081 Test-Loss: 0.13724389018270108 Train-f1: 0.5833249862661628 Test-f1: 0.5339348370927318 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [15163/20000] Train-Loss: 0.36124942622889594 Test-Loss: 0.14517166897958686 Train-f1: 0.646275199454923 Test-f1: 0.5624077238550923 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [15164/20000] Train-Loss: 0.3625009007556028 Test-Loss: 0.1407642606623621 Train-f1: 0.6953469302188778 Test-f1: 0.5684476031215162 Train-acc: 0.8543046357615894 Test-acc: 0.8157894736842105\n",
      "Epoch [15165/20000] Train-Loss: 0.35788436473486007 Test-Loss: 0.14532379123005634 Train-f1: 0.6276140816025924 Test-f1: 0.5533179723502304 Train-acc: 0.8543046357615894 Test-acc: 0.8026315789473685\n",
      "Epoch [15166/20000] Train-Loss: 0.36225294730432994 Test-Loss: 0.14074562672090304 Train-f1: 0.59076396677312 Test-f1: 0.5776349965823651 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15167/20000] Train-Loss: 0.36385269111455687 Test-Loss: 0.14347588085862525 Train-f1: 0.6478751182592242 Test-f1: 0.5819024219024219 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15168/20000] Train-Loss: 0.36555984029293376 Test-Loss: 0.14073076037094134 Train-f1: 0.5173236926256014 Test-f1: 0.4935574229691877 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15169/20000] Train-Loss: 0.362119175169582 Test-Loss: 0.14684965629587357 Train-f1: 0.6747406646679471 Test-f1: 0.4829545454545454 Train-acc: 0.8576158940397351 Test-acc: 0.8026315789473685\n",
      "Epoch [15170/20000] Train-Loss: 0.361886092494888 Test-Loss: 0.14231330317516497 Train-f1: 0.6180092539039908 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15171/20000] Train-Loss: 0.3624956654216938 Test-Loss: 0.14726388014219133 Train-f1: 0.6919317154433137 Test-f1: 0.475889328063241 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [15172/20000] Train-Loss: 0.3624770528945109 Test-Loss: 0.14482830287448759 Train-f1: 0.6390830645864203 Test-f1: 0.5819024219024219 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [15173/20000] Train-Loss: 0.3677890382863891 Test-Loss: 0.14901306335061087 Train-f1: 0.653938483191519 Test-f1: 0.5665323435843054 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15174/20000] Train-Loss: 0.3622637935819798 Test-Loss: 0.13997351621391138 Train-f1: 0.670073403343771 Test-f1: 0.5790712074303406 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [15175/20000] Train-Loss: 0.3607489485920809 Test-Loss: 0.1444593868010408 Train-f1: 0.6684881952781112 Test-f1: 0.4766897221211018 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15176/20000] Train-Loss: 0.3666266771772388 Test-Loss: 0.1496102244315371 Train-f1: 0.6277893535036392 Test-f1: 0.5637166085946574 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15177/20000] Train-Loss: 0.36239420927886334 Test-Loss: 0.150838931535794 Train-f1: 0.6388577387524756 Test-f1: 0.5584762906812692 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [15178/20000] Train-Loss: 0.3658354022133995 Test-Loss: 0.13796962590305092 Train-f1: 0.634176491373211 Test-f1: 0.566984126984127 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15179/20000] Train-Loss: 0.36622663593905175 Test-Loss: 0.13912244464886933 Train-f1: 0.6070484381043387 Test-f1: 0.5776349965823651 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [15180/20000] Train-Loss: 0.36067302859949374 Test-Loss: 0.13728580502755233 Train-f1: 0.6183408716838817 Test-f1: 0.47501831501831504 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [15181/20000] Train-Loss: 0.36403008778543067 Test-Loss: 0.13732166955597783 Train-f1: 0.6478052182699858 Test-f1: 0.4503312334891282 Train-acc: 0.8377483443708609 Test-acc: 0.7631578947368421\n",
      "Epoch [15182/20000] Train-Loss: 0.3629757352185016 Test-Loss: 0.1371061334575287 Train-f1: 0.5903722943722943 Test-f1: 0.528796992481203 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [15183/20000] Train-Loss: 0.36621164823850444 Test-Loss: 0.14193316051781943 Train-f1: 0.6978314646016509 Test-f1: 0.5684476031215162 Train-acc: 0.8576158940397351 Test-acc: 0.8157894736842105\n",
      "Epoch [15184/20000] Train-Loss: 0.36169843456060924 Test-Loss: 0.14023539580691405 Train-f1: 0.6497671068427371 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15185/20000] Train-Loss: 0.35923163078205983 Test-Loss: 0.1406292924555161 Train-f1: 0.6172323218359024 Test-f1: 0.5684476031215162 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15186/20000] Train-Loss: 0.3632867710281897 Test-Loss: 0.145005071917777 Train-f1: 0.6440980712651193 Test-f1: 0.5605602240896359 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [15187/20000] Train-Loss: 0.36165598093279416 Test-Loss: 0.13680509245571426 Train-f1: 0.6844250559284116 Test-f1: 0.5683710801393728 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [15188/20000] Train-Loss: 0.36615837636301 Test-Loss: 0.13734648532989185 Train-f1: 0.6304485174050392 Test-f1: 0.566984126984127 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15189/20000] Train-Loss: 0.35741736428784476 Test-Loss: 0.148255253121851 Train-f1: 0.6091536778780421 Test-f1: 0.5637166085946574 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15190/20000] Train-Loss: 0.366839513230766 Test-Loss: 0.16024270374821342 Train-f1: 0.6623085185745032 Test-f1: 0.45720775975648537 Train-acc: 0.8543046357615894 Test-acc: 0.75\n",
      "Epoch [15191/20000] Train-Loss: 0.3696011553435123 Test-Loss: 0.14171897180636542 Train-f1: 0.584848484848485 Test-f1: 0.5714285714285714 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15192/20000] Train-Loss: 0.3628465061628021 Test-Loss: 0.14016026975463075 Train-f1: 0.6528462405174734 Test-f1: 0.4746428571428572 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15193/20000] Train-Loss: 0.36258652829316074 Test-Loss: 0.14705093915469494 Train-f1: 0.620006379585327 Test-f1: 0.47555555555555556 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15194/20000] Train-Loss: 0.364312546929003 Test-Loss: 0.14628941630490896 Train-f1: 0.5906256880374527 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15195/20000] Train-Loss: 0.3616729384171299 Test-Loss: 0.13811476431966127 Train-f1: 0.6028570255069111 Test-f1: 0.5428237259816207 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [15196/20000] Train-Loss: 0.3615105457831271 Test-Loss: 0.13856264210909192 Train-f1: 0.6073675181933134 Test-f1: 0.47714285714285715 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15197/20000] Train-Loss: 0.3656470673510939 Test-Loss: 0.1436285071361157 Train-f1: 0.6015740718259499 Test-f1: 0.5684476031215162 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15198/20000] Train-Loss: 0.3616666241958436 Test-Loss: 0.1432881295614471 Train-f1: 0.6073675181933134 Test-f1: 0.5819024219024219 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [15199/20000] Train-Loss: 0.3619985573403016 Test-Loss: 0.13842926723118273 Train-f1: 0.6733334917905934 Test-f1: 0.441995841995842 Train-acc: 0.8543046357615894 Test-acc: 0.75\n",
      "Epoch [15200/20000] Train-Loss: 0.36027005187636896 Test-Loss: 0.14063442040756333 Train-f1: 0.6562262177182087 Test-f1: 0.5776349965823651 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [15201/20000] Train-Loss: 0.37075359596611185 Test-Loss: 0.1410083533155521 Train-f1: 0.6065391614754672 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15202/20000] Train-Loss: 0.36339037573740524 Test-Loss: 0.1445698864821971 Train-f1: 0.6532452271582707 Test-f1: 0.4774585921325052 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15203/20000] Train-Loss: 0.3596774786866548 Test-Loss: 0.14903692851986275 Train-f1: 0.5891314742710616 Test-f1: 0.5637166085946574 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15204/20000] Train-Loss: 0.3601187314398407 Test-Loss: 0.13847543812703597 Train-f1: 0.6540181295214852 Test-f1: 0.5540808034925682 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [15205/20000] Train-Loss: 0.3671765672460031 Test-Loss: 0.14076288287369085 Train-f1: 0.5899544388389747 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15206/20000] Train-Loss: 0.36177401672853854 Test-Loss: 0.1400974362097041 Train-f1: 0.6580705009276439 Test-f1: 0.5656318681318682 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15207/20000] Train-Loss: 0.36245068517068124 Test-Loss: 0.1401625103863657 Train-f1: 0.6613845781547645 Test-f1: 0.5776349965823651 Train-acc: 0.8543046357615894 Test-acc: 0.8289473684210527\n",
      "Epoch [15208/20000] Train-Loss: 0.3644518500881887 Test-Loss: 0.13973852555307392 Train-f1: 0.6107415544540234 Test-f1: 0.5656318681318682 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15209/20000] Train-Loss: 0.3565021387908664 Test-Loss: 0.13829430900178527 Train-f1: 0.5902429471560422 Test-f1: 0.4488311688311688 Train-acc: 0.8410596026490066 Test-acc: 0.7631578947368421\n",
      "Epoch [15210/20000] Train-Loss: 0.36433232207622746 Test-Loss: 0.14448639988092873 Train-f1: 0.6381693633306537 Test-f1: 0.5745207570207571 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15211/20000] Train-Loss: 0.36349375064718265 Test-Loss: 0.14234788080108082 Train-f1: 0.5370850681409688 Test-f1: 0.4774585921325052 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15212/20000] Train-Loss: 0.3624233288311663 Test-Loss: 0.14360124601392243 Train-f1: 0.695346113376804 Test-f1: 0.4770348837209302 Train-acc: 0.8543046357615894 Test-acc: 0.7894736842105263\n",
      "Epoch [15213/20000] Train-Loss: 0.3619496261355631 Test-Loss: 0.14336005248571135 Train-f1: 0.6301128538528151 Test-f1: 0.4770348837209302 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [15214/20000] Train-Loss: 0.36090899282522576 Test-Loss: 0.15137197842553363 Train-f1: 0.6105061483139286 Test-f1: 0.4770348837209302 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15215/20000] Train-Loss: 0.3596882861642539 Test-Loss: 0.14231858920099305 Train-f1: 0.6170862163850088 Test-f1: 0.5821288515406162 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15216/20000] Train-Loss: 0.36675883393716446 Test-Loss: 0.13953460296804335 Train-f1: 0.6564070751276072 Test-f1: 0.5684942084942085 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15217/20000] Train-Loss: 0.36053181643690896 Test-Loss: 0.14976465880377693 Train-f1: 0.5142595499738357 Test-f1: 0.5696804511278196 Train-acc: 0.8278145695364238 Test-acc: 0.8157894736842105\n",
      "Epoch [15218/20000] Train-Loss: 0.362003245309552 Test-Loss: 0.1428613564194676 Train-f1: 0.7107953950811093 Test-f1: 0.570937019969278 Train-acc: 0.8576158940397351 Test-acc: 0.7894736842105263\n",
      "Epoch [15219/20000] Train-Loss: 0.3661941559284025 Test-Loss: 0.14141593244702255 Train-f1: 0.6722590619825642 Test-f1: 0.5381015037593986 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [15220/20000] Train-Loss: 0.3595315372054288 Test-Loss: 0.14031158260060383 Train-f1: 0.6475089116042525 Test-f1: 0.5381015037593986 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [15221/20000] Train-Loss: 0.3615434897757821 Test-Loss: 0.1402188947435796 Train-f1: 0.6532164611111979 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15222/20000] Train-Loss: 0.3617208737670053 Test-Loss: 0.1406345054483962 Train-f1: 0.6505241248817407 Test-f1: 0.4774585921325052 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [15223/20000] Train-Loss: 0.3625281627486762 Test-Loss: 0.1392175449596452 Train-f1: 0.6487507332862933 Test-f1: 0.5776243470361118 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15224/20000] Train-Loss: 0.3616494796319951 Test-Loss: 0.13965583354153618 Train-f1: 0.5971065504905769 Test-f1: 0.5865238854712539 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [15225/20000] Train-Loss: 0.3658585061511442 Test-Loss: 0.14793129673003136 Train-f1: 0.6528462405174734 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15226/20000] Train-Loss: 0.36461036696589827 Test-Loss: 0.14489849762871992 Train-f1: 0.6071282012458483 Test-f1: 0.5282859078590786 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [15227/20000] Train-Loss: 0.3653932557582263 Test-Loss: 0.14045477166152795 Train-f1: 0.5999096570858585 Test-f1: 0.580450731572013 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [15228/20000] Train-Loss: 0.36146939110915216 Test-Loss: 0.13846896546734988 Train-f1: 0.6878857339953195 Test-f1: 0.5776243470361118 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [15229/20000] Train-Loss: 0.3614637741305096 Test-Loss: 0.144094931627319 Train-f1: 0.5551497121700512 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15230/20000] Train-Loss: 0.36131156778179185 Test-Loss: 0.13867361500893127 Train-f1: 0.5903027087934721 Test-f1: 0.566984126984127 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15231/20000] Train-Loss: 0.3601545969110214 Test-Loss: 0.1425456909439436 Train-f1: 0.61015515771526 Test-f1: 0.5381015037593986 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [15232/20000] Train-Loss: 0.35981644663370704 Test-Loss: 0.14340099344544063 Train-f1: 0.6602980927118858 Test-f1: 0.5303571428571429 Train-acc: 0.8543046357615894 Test-acc: 0.8026315789473685\n",
      "Epoch [15233/20000] Train-Loss: 0.36166362163115806 Test-Loss: 0.15591360065372775 Train-f1: 0.6552460913662859 Test-f1: 0.5529751403368084 Train-acc: 0.847682119205298 Test-acc: 0.8026315789473685\n",
      "Epoch [15234/20000] Train-Loss: 0.36722031150795054 Test-Loss: 0.14324896949189872 Train-f1: 0.5282370962370962 Test-f1: 0.4770348837209302 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15235/20000] Train-Loss: 0.35791760489775526 Test-Loss: 0.15141742399208225 Train-f1: 0.6900098015940779 Test-f1: 0.5657142857142857 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15236/20000] Train-Loss: 0.367109489420676 Test-Loss: 0.1396756338263046 Train-f1: 0.5324209890326168 Test-f1: 0.574856124267889 Train-acc: 0.8311258278145696 Test-acc: 0.8289473684210527\n",
      "Epoch [15237/20000] Train-Loss: 0.36278234208927523 Test-Loss: 0.1457193416394105 Train-f1: 0.6333144333144334 Test-f1: 0.5696804511278196 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15238/20000] Train-Loss: 0.36482085441108714 Test-Loss: 0.1407550482371716 Train-f1: 0.6564070751276072 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15239/20000] Train-Loss: 0.35905093110304287 Test-Loss: 0.13691761893396365 Train-f1: 0.6102384756068966 Test-f1: 0.5339348370927318 Train-acc: 0.847682119205298 Test-acc: 0.7763157894736842\n",
      "Epoch [15240/20000] Train-Loss: 0.36903106073480046 Test-Loss: 0.14487265119523446 Train-f1: 0.658545332458376 Test-f1: 0.5624077238550923 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15241/20000] Train-Loss: 0.3636043129883536 Test-Loss: 0.1378652556961726 Train-f1: 0.6338010982306284 Test-f1: 0.5451190476190477 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15242/20000] Train-Loss: 0.3664268535948756 Test-Loss: 0.1426154554661856 Train-f1: 0.638385775298527 Test-f1: 0.5684476031215162 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15243/20000] Train-Loss: 0.365941382704339 Test-Loss: 0.13734342170724667 Train-f1: 0.6503742577921313 Test-f1: 0.5073487773487774 Train-acc: 0.8410596026490066 Test-acc: 0.7763157894736842\n",
      "Epoch [15244/20000] Train-Loss: 0.3667471301665921 Test-Loss: 0.13839722853953795 Train-f1: 0.6506423948073163 Test-f1: 0.566984126984127 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15245/20000] Train-Loss: 0.36072867061793973 Test-Loss: 0.14506852826700967 Train-f1: 0.6535705970565819 Test-f1: 0.5605602240896359 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [15246/20000] Train-Loss: 0.36244745888980145 Test-Loss: 0.14291931375904224 Train-f1: 0.6097894736842105 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15247/20000] Train-Loss: 0.3640774476911337 Test-Loss: 0.1419731773302981 Train-f1: 0.6052079026998936 Test-f1: 0.4875179340028694 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [15248/20000] Train-Loss: 0.3601512480129285 Test-Loss: 0.14383611869353188 Train-f1: 0.6578267365275762 Test-f1: 0.5745207570207571 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15249/20000] Train-Loss: 0.36058239504387735 Test-Loss: 0.1471253648545916 Train-f1: 0.6851168642930656 Test-f1: 0.5637166085946574 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15250/20000] Train-Loss: 0.35579305143655143 Test-Loss: 0.14330108586809137 Train-f1: 0.6481327468134065 Test-f1: 0.4875179340028694 Train-acc: 0.8543046357615894 Test-acc: 0.8026315789473685\n",
      "Epoch [15251/20000] Train-Loss: 0.3570568241716047 Test-Loss: 0.1381592115188214 Train-f1: 0.598044899119068 Test-f1: 0.5776243470361118 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [15252/20000] Train-Loss: 0.3645181694785324 Test-Loss: 0.1366333053725093 Train-f1: 0.5847521605910868 Test-f1: 0.4395848595848596 Train-acc: 0.8344370860927153 Test-acc: 0.75\n",
      "Epoch [15253/20000] Train-Loss: 0.35591110486203265 Test-Loss: 0.14602533081964844 Train-f1: 0.6386525086090818 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15254/20000] Train-Loss: 0.361824878053366 Test-Loss: 0.1475140296529617 Train-f1: 0.5898177149790053 Test-f1: 0.5637166085946574 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15255/20000] Train-Loss: 0.36509083611249016 Test-Loss: 0.14040162652376095 Train-f1: 0.6511439172328704 Test-f1: 0.5714285714285714 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15256/20000] Train-Loss: 0.3638703084310324 Test-Loss: 0.14737294396513684 Train-f1: 0.6497179142696384 Test-f1: 0.4829545454545454 Train-acc: 0.8410596026490066 Test-acc: 0.8026315789473685\n",
      "Epoch [15257/20000] Train-Loss: 0.36257819626417964 Test-Loss: 0.14150735376277346 Train-f1: 0.6089944341372913 Test-f1: 0.5656318681318682 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15258/20000] Train-Loss: 0.35924119069749144 Test-Loss: 0.14534242687706048 Train-f1: 0.6073387521462407 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15259/20000] Train-Loss: 0.36156931469263903 Test-Loss: 0.1383376041807288 Train-f1: 0.585645851046905 Test-f1: 0.4560455192034139 Train-acc: 0.8344370860927153 Test-acc: 0.7631578947368421\n",
      "Epoch [15260/20000] Train-Loss: 0.360404475218521 Test-Loss: 0.14459417444916775 Train-f1: 0.5273591273591274 Test-f1: 0.5624077238550923 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15261/20000] Train-Loss: 0.3632154362967871 Test-Loss: 0.14283134564035732 Train-f1: 0.6728864468864468 Test-f1: 0.5656318681318682 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15262/20000] Train-Loss: 0.36135549023788777 Test-Loss: 0.1436760172655067 Train-f1: 0.6413370723674712 Test-f1: 0.5696804511278196 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15263/20000] Train-Loss: 0.35966996965972337 Test-Loss: 0.14252599031739346 Train-f1: 0.6902524889554484 Test-f1: 0.5584434203789043 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15264/20000] Train-Loss: 0.3635938412137991 Test-Loss: 0.14188012210709622 Train-f1: 0.6875378188687359 Test-f1: 0.5303571428571429 Train-acc: 0.8443708609271523 Test-acc: 0.8026315789473685\n",
      "Epoch [15265/20000] Train-Loss: 0.36044859477532887 Test-Loss: 0.13712178877102116 Train-f1: 0.6266899825712966 Test-f1: 0.528796992481203 Train-acc: 0.8509933774834437 Test-acc: 0.8026315789473685\n",
      "Epoch [15266/20000] Train-Loss: 0.36332319951562764 Test-Loss: 0.1410227647403474 Train-f1: 0.6519783664611251 Test-f1: 0.4746428571428572 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15267/20000] Train-Loss: 0.3589269169036798 Test-Loss: 0.14648908778457662 Train-f1: 0.6047939184304643 Test-f1: 0.5656318681318682 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15268/20000] Train-Loss: 0.36257719455699966 Test-Loss: 0.1370242773589408 Train-f1: 0.6454291050564342 Test-f1: 0.4644611528822056 Train-acc: 0.8344370860927153 Test-acc: 0.7763157894736842\n",
      "Epoch [15269/20000] Train-Loss: 0.361043467442482 Test-Loss: 0.14113064485091734 Train-f1: 0.6876876859583453 Test-f1: 0.5793006993006993 Train-acc: 0.8443708609271523 Test-acc: 0.8289473684210527\n",
      "Epoch [15270/20000] Train-Loss: 0.35777204368394866 Test-Loss: 0.1389105378367391 Train-f1: 0.6721626797377632 Test-f1: 0.4526315789473684 Train-acc: 0.8509933774834437 Test-acc: 0.7631578947368421\n",
      "Epoch [15271/20000] Train-Loss: 0.36291390809524393 Test-Loss: 0.13780192434246086 Train-f1: 0.6875314005200742 Test-f1: 0.566984126984127 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15272/20000] Train-Loss: 0.36222386899315473 Test-Loss: 0.1424077312539462 Train-f1: 0.6132994560866717 Test-f1: 0.5428237259816207 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [15273/20000] Train-Loss: 0.3595940432471983 Test-Loss: 0.1408816148792631 Train-f1: 0.6971041898405635 Test-f1: 0.5776349965823651 Train-acc: 0.8576158940397351 Test-acc: 0.8289473684210527\n",
      "Epoch [15274/20000] Train-Loss: 0.3611200741686903 Test-Loss: 0.13942712875994984 Train-f1: 0.6426450165580601 Test-f1: 0.5540808034925682 Train-acc: 0.8311258278145696 Test-acc: 0.7894736842105263\n",
      "Epoch [15275/20000] Train-Loss: 0.36333972647580287 Test-Loss: 0.1384539782066175 Train-f1: 0.5603731754932104 Test-f1: 0.5776243470361118 Train-acc: 0.8245033112582781 Test-acc: 0.8289473684210527\n",
      "Epoch [15276/20000] Train-Loss: 0.3634827999441532 Test-Loss: 0.14141981214347607 Train-f1: 0.6151919533934173 Test-f1: 0.5540079365079366 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [15277/20000] Train-Loss: 0.3589790074685771 Test-Loss: 0.15259264837889333 Train-f1: 0.6482446249402771 Test-f1: 0.5665323435843054 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15278/20000] Train-Loss: 0.3667557996091848 Test-Loss: 0.13811222213158056 Train-f1: 0.5831216402644974 Test-f1: 0.566984126984127 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15279/20000] Train-Loss: 0.36001997927809065 Test-Loss: 0.13729927410162301 Train-f1: 0.6311982197412196 Test-f1: 0.5428237259816207 Train-acc: 0.8576158940397351 Test-acc: 0.7763157894736842\n",
      "Epoch [15280/20000] Train-Loss: 0.3647828432011041 Test-Loss: 0.13678837351085962 Train-f1: 0.6286985565551968 Test-f1: 0.5339348370927318 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [15281/20000] Train-Loss: 0.35980107263914995 Test-Loss: 0.15443562989331344 Train-f1: 0.6272236619501256 Test-f1: 0.5511509180474697 Train-acc: 0.8311258278145696 Test-acc: 0.8026315789473685\n",
      "Epoch [15282/20000] Train-Loss: 0.3642588932889529 Test-Loss: 0.14262501629085786 Train-f1: 0.5971217971217971 Test-f1: 0.5714285714285714 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15283/20000] Train-Loss: 0.35783836164259475 Test-Loss: 0.14062812098329683 Train-f1: 0.6197850114290604 Test-f1: 0.5656318681318682 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15284/20000] Train-Loss: 0.35779644203437766 Test-Loss: 0.14135373778282578 Train-f1: 0.6678177351884418 Test-f1: 0.5381015037593986 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [15285/20000] Train-Loss: 0.3611519858743098 Test-Loss: 0.14626086248570724 Train-f1: 0.661125604756178 Test-f1: 0.5561337073398784 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [15286/20000] Train-Loss: 0.36242768906711076 Test-Loss: 0.13915407364441137 Train-f1: 0.6313962183527401 Test-f1: 0.5776349965823651 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [15287/20000] Train-Loss: 0.3628045644041808 Test-Loss: 0.14013825711494687 Train-f1: 0.6836531529033332 Test-f1: 0.47714285714285715 Train-acc: 0.8410596026490066 Test-acc: 0.7894736842105263\n",
      "Epoch [15288/20000] Train-Loss: 0.3591439872624362 Test-Loss: 0.13781852792093802 Train-f1: 0.5875022744587962 Test-f1: 0.5776243470361118 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15289/20000] Train-Loss: 0.3630932951062433 Test-Loss: 0.13751450581941882 Train-f1: 0.6048935788066223 Test-f1: 0.566984126984127 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15290/20000] Train-Loss: 0.3524428593880806 Test-Loss: 0.16096258306769393 Train-f1: 0.6279738377416739 Test-f1: 0.5259259259259259 Train-acc: 0.8543046357615894 Test-acc: 0.7763157894736842\n",
      "Epoch [15291/20000] Train-Loss: 0.3688713316659101 Test-Loss: 0.13719573385172232 Train-f1: 0.6855865666424672 Test-f1: 0.566984126984127 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15292/20000] Train-Loss: 0.35980573323209114 Test-Loss: 0.13946686550934 Train-f1: 0.6437877939054409 Test-f1: 0.5776349965823651 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [15293/20000] Train-Loss: 0.3610769733431615 Test-Loss: 0.13910934895861893 Train-f1: 0.570903762903763 Test-f1: 0.5776349965823651 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15294/20000] Train-Loss: 0.36313574190026865 Test-Loss: 0.13864161081291157 Train-f1: 0.6025331415384994 Test-f1: 0.5776243470361118 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15295/20000] Train-Loss: 0.3618663717763808 Test-Loss: 0.1395545933406208 Train-f1: 0.6553494124922696 Test-f1: 0.5865132359250007 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [15296/20000] Train-Loss: 0.36360456197697205 Test-Loss: 0.1379985221885352 Train-f1: 0.5957786163668517 Test-f1: 0.569799861973775 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15297/20000] Train-Loss: 0.35746164521363805 Test-Loss: 0.14751167018116348 Train-f1: 0.650281935978419 Test-f1: 0.5624077238550923 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15298/20000] Train-Loss: 0.3634355188330957 Test-Loss: 0.14096633050808988 Train-f1: 0.6172121212121212 Test-f1: 0.5865238854712539 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15299/20000] Train-Loss: 0.3621606362134872 Test-Loss: 0.13869885630452883 Train-f1: 0.658545332458376 Test-f1: 0.5804400820257598 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [15300/20000] Train-Loss: 0.36027157163366774 Test-Loss: 0.13782098294364614 Train-f1: 0.6585434111572943 Test-f1: 0.5213472706155633 Train-acc: 0.8509933774834437 Test-acc: 0.7894736842105263\n",
      "Epoch [15301/20000] Train-Loss: 0.3616629827369295 Test-Loss: 0.15237196639133732 Train-f1: 0.6276026676026676 Test-f1: 0.5603463203463204 Train-acc: 0.8311258278145696 Test-acc: 0.8157894736842105\n",
      "Epoch [15302/20000] Train-Loss: 0.36357916457734163 Test-Loss: 0.137573271689348 Train-f1: 0.6080294566214297 Test-f1: 0.5451190476190477 Train-acc: 0.8443708609271523 Test-acc: 0.7894736842105263\n",
      "Epoch [15303/20000] Train-Loss: 0.36073319354372496 Test-Loss: 0.1414563790081338 Train-f1: 0.6185692036856921 Test-f1: 0.5675000000000001 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15304/20000] Train-Loss: 0.3627478405693631 Test-Loss: 0.13837440932777462 Train-f1: 0.6804855264675355 Test-f1: 0.5776243470361118 Train-acc: 0.8344370860927153 Test-acc: 0.8289473684210527\n",
      "Epoch [15305/20000] Train-Loss: 0.3563321613645189 Test-Loss: 0.13936088151392567 Train-f1: 0.676712983535112 Test-f1: 0.4935574229691877 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15306/20000] Train-Loss: 0.3639913505881165 Test-Loss: 0.14174722193130493 Train-f1: 0.5347969561552551 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15307/20000] Train-Loss: 0.36338978783099163 Test-Loss: 0.1490520908308201 Train-f1: 0.6312155127944601 Test-f1: 0.5657142857142857 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15308/20000] Train-Loss: 0.35475401305257687 Test-Loss: 0.140811844094092 Train-f1: 0.6438935383510471 Test-f1: 0.5656318681318682 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15309/20000] Train-Loss: 0.3634497937460277 Test-Loss: 0.13914850890396155 Train-f1: 0.6026114114718234 Test-f1: 0.4875179340028694 Train-acc: 0.8377483443708609 Test-acc: 0.8026315789473685\n",
      "Epoch [15310/20000] Train-Loss: 0.3619086845851727 Test-Loss: 0.14479811027677236 Train-f1: 0.6437514307079525 Test-f1: 0.5684476031215162 Train-acc: 0.8509933774834437 Test-acc: 0.8157894736842105\n",
      "Epoch [15311/20000] Train-Loss: 0.36095681870745083 Test-Loss: 0.13806744895468323 Train-f1: 0.6510909324218495 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15312/20000] Train-Loss: 0.3592757249072921 Test-Loss: 0.1383901064067825 Train-f1: 0.6041769041769042 Test-f1: 0.5776243470361118 Train-acc: 0.8410596026490066 Test-acc: 0.8289473684210527\n",
      "Epoch [15313/20000] Train-Loss: 0.35906632342898975 Test-Loss: 0.1446108685487915 Train-f1: 0.6678642284867451 Test-f1: 0.4770348837209302 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n",
      "Epoch [15314/20000] Train-Loss: 0.36578048268928665 Test-Loss: 0.13765939409898448 Train-f1: 0.6116541353383459 Test-f1: 0.5339348370927318 Train-acc: 0.8311258278145696 Test-acc: 0.7763157894736842\n",
      "Epoch [15315/20000] Train-Loss: 0.35880519777043635 Test-Loss: 0.15418944975097537 Train-f1: 0.6024054933339336 Test-f1: 0.4727272727272728 Train-acc: 0.8377483443708609 Test-acc: 0.7763157894736842\n",
      "Epoch [15316/20000] Train-Loss: 0.35505282708867486 Test-Loss: 0.13830513309725265 Train-f1: 0.6731166780317984 Test-f1: 0.566984126984127 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15317/20000] Train-Loss: 0.3603379293677148 Test-Loss: 0.13814804823698365 Train-f1: 0.6553494124922696 Test-f1: 0.5540079365079366 Train-acc: 0.847682119205298 Test-acc: 0.7894736842105263\n",
      "Epoch [15318/20000] Train-Loss: 0.36130158597178497 Test-Loss: 0.14427906644805324 Train-f1: 0.6793355443444782 Test-f1: 0.5656318681318682 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15319/20000] Train-Loss: 0.3647304357646282 Test-Loss: 0.14405513228679534 Train-f1: 0.5997209295755443 Test-f1: 0.5411764705882354 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15320/20000] Train-Loss: 0.3571713843826328 Test-Loss: 0.13879218590438674 Train-f1: 0.5975568632853759 Test-f1: 0.5428237259816207 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [15321/20000] Train-Loss: 0.35487507934894946 Test-Loss: 0.14737789844429705 Train-f1: 0.6899155709714716 Test-f1: 0.5665323435843054 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15322/20000] Train-Loss: 0.3621397016616064 Test-Loss: 0.14309081583214764 Train-f1: 0.6681654372478312 Test-f1: 0.5819024219024219 Train-acc: 0.8377483443708609 Test-acc: 0.8289473684210527\n",
      "Epoch [15323/20000] Train-Loss: 0.35881735280445115 Test-Loss: 0.13757125223237152 Train-f1: 0.612214814203325 Test-f1: 0.5339348370927318 Train-acc: 0.8509933774834437 Test-acc: 0.7763157894736842\n",
      "Epoch [15324/20000] Train-Loss: 0.36030489465616106 Test-Loss: 0.1383321118402734 Train-f1: 0.6091324200913243 Test-f1: 0.5776243470361118 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [15325/20000] Train-Loss: 0.35920823941214686 Test-Loss: 0.14058004772796148 Train-f1: 0.6557262119434029 Test-f1: 0.5776349965823651 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [15326/20000] Train-Loss: 0.35622411836107454 Test-Loss: 0.15199408344529158 Train-f1: 0.6449563875000043 Test-f1: 0.5657142857142857 Train-acc: 0.8344370860927153 Test-acc: 0.8157894736842105\n",
      "Epoch [15327/20000] Train-Loss: 0.36211073603046895 Test-Loss: 0.13829353491235347 Train-f1: 0.610868427638614 Test-f1: 0.5776243470361118 Train-acc: 0.847682119205298 Test-acc: 0.8289473684210527\n",
      "Epoch [15328/20000] Train-Loss: 0.3638904678075247 Test-Loss: 0.13716556385765089 Train-f1: 0.6361796272740853 Test-f1: 0.566984126984127 Train-acc: 0.8410596026490066 Test-acc: 0.8157894736842105\n",
      "Epoch [15329/20000] Train-Loss: 0.36309098631607045 Test-Loss: 0.13861882478422338 Train-f1: 0.658545332458376 Test-f1: 0.5776243470361118 Train-acc: 0.8509933774834437 Test-acc: 0.8289473684210527\n",
      "Epoch [15330/20000] Train-Loss: 0.3625306975083643 Test-Loss: 0.13773791358135573 Train-f1: 0.5858301904721654 Test-f1: 0.5540808034925682 Train-acc: 0.8344370860927153 Test-acc: 0.7894736842105263\n",
      "Epoch [15331/20000] Train-Loss: 0.3612275698754247 Test-Loss: 0.14941612698176965 Train-f1: 0.609964920790716 Test-f1: 0.5637166085946574 Train-acc: 0.847682119205298 Test-acc: 0.8157894736842105\n",
      "Epoch [15332/20000] Train-Loss: 0.3584323412738806 Test-Loss: 0.1408463670934405 Train-f1: 0.6398520716284747 Test-f1: 0.5381015037593986 Train-acc: 0.8443708609271523 Test-acc: 0.7763157894736842\n",
      "Epoch [15333/20000] Train-Loss: 0.36169224643195474 Test-Loss: 0.1435374579466534 Train-f1: 0.6334372322225712 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15334/20000] Train-Loss: 0.3647420576026532 Test-Loss: 0.13820385881978559 Train-f1: 0.6071228070175438 Test-f1: 0.566984126984127 Train-acc: 0.8443708609271523 Test-acc: 0.8157894736842105\n",
      "Epoch [15335/20000] Train-Loss: 0.363337490264944 Test-Loss: 0.14340653812221718 Train-f1: 0.5476142389751433 Test-f1: 0.5656318681318682 Train-acc: 0.8377483443708609 Test-acc: 0.8157894736842105\n",
      "Epoch [15336/20000] Train-Loss: 0.35700029636630265 Test-Loss: 0.15092337921522817 Train-f1: 0.6482613054375068 Test-f1: 0.5590936468484108 Train-acc: 0.8377483443708609 Test-acc: 0.7894736842105263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/notebooks/E2E_diagnosis/test2.ipynb セル 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6969646174616b616869726f33227d@ssh-remote%2B192.168.0.102/root/notebooks/E2E_diagnosis/test2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m running_loss_dx, running_loss_test_dx \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6969646174616b616869726f33227d@ssh-remote%2B192.168.0.102/root/notebooks/E2E_diagnosis/test2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m preds_test_dx,label_test_dx \u001b[39m=\u001b[39m [], []\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6969646174616b616869726f33227d@ssh-remote%2B192.168.0.102/root/notebooks/E2E_diagnosis/test2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m ecg_image, labels, label_lead, patients, num_leads \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6969646174616b616869726f33227d@ssh-remote%2B192.168.0.102/root/notebooks/E2E_diagnosis/test2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     image \u001b[39m=\u001b[39m ecg_image\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m112\u001b[39m, \u001b[39m224\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6969646174616b616869726f33227d@ssh-remote%2B192.168.0.102/root/notebooks/E2E_diagnosis/test2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)  \u001b[39m# ラベルを1次元に変形\u001b[39;00m\n",
      "File \u001b[0;32m/python/build/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/python/build/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/python/build/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/python/build/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/python/build/lib/python3.10/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/notebooks/E2E_diagnosis/dataloader_e2e.py:65\u001b[0m, in \u001b[0;36mCustomTensorDataset_.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     63\u001b[0m     tuple1 \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors) \n\u001b[1;32m     64\u001b[0m     \u001b[39mreturn\u001b[39;00m tuple1   \n\u001b[0;32m---> 65\u001b[0m tuple1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(\u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(tensor[index]) \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors))\n\u001b[1;32m     66\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[index]\n\u001b[1;32m     67\u001b[0m label_lead \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_lead[index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 診断 (事前学習なし)\n",
    "\n",
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "bestscore_dx = 1000.0\n",
    "loss_list_dx,loss_list_test_dx = [], []\n",
    "f1s_dx, f1s_test_dx = [],[]\n",
    "acc_dx, acc_test_dx = [], []\n",
    "\n",
    "num_epochs_dx = 20000\n",
    "\n",
    "# モデル訓練　5クラス分類\n",
    "\n",
    "for epoch in range(num_epochs_dx):\n",
    "    model.train()\n",
    "    running_loss_dx = 0.0\n",
    "    preds_dx, label_dx = [], []\n",
    "    running_loss_dx, running_loss_test_dx = 0.0, 0.0\n",
    "    preds_test_dx,label_test_dx = [], []\n",
    "\n",
    "    for ecg_image, labels, label_lead, patients, num_leads in train_dataloader:\n",
    "        image = ecg_image.reshape(-1, 3, 112, 224).to(device)\n",
    "        labels = labels.reshape(-1).to(device)  # ラベルを1次元に変形\n",
    "        labels = labels.to(torch.long)  # CrossEntropyLossではクラスのインデックスはlong型である必要がある\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "        outputs = model(image).to(torch.float64)  # モデルの出力を計算\n",
    "\n",
    "        prob, pred = torch.max(outputs, 1)\n",
    "        preds_dx.extend(pred.tolist())\n",
    "        label_dx.extend(labels.tolist())\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算（CrossEntropyLossの引数は予測とラベル）\n",
    "        loss.backward()  # 勾配を計算\n",
    "        optimizer.step()  # パラメータを更新\n",
    "\n",
    "        running_loss_dx += loss.item()\n",
    "\n",
    "    for ecg_image,labels,label_lead,patients,num_leads in test_dataloader:\n",
    "        image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "        labels = labels.reshape(-1).to(device)\n",
    "\n",
    "        outputs = model(image)  # モデルの出力を計算\n",
    "        outputs = outputs.to(torch.float64)\n",
    "        prob, pred=torch.max(outputs,1)\n",
    "        preds_test_dx.extend(pred.tolist())\n",
    "        label_test_dx.extend(labels.tolist())\n",
    "        \n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "        running_loss_test_dx += loss.item()\n",
    "    \n",
    "    score_dx = running_loss_test_dx/len(test_dataloader)\n",
    "    if score_dx < bestscore_dx:\n",
    "        bestscore_dx = score_dx\n",
    "        torch.save(model.state_dict(), '/root/notebooks/sample/models/bestmodel_dx_no_pretrain.pth')\n",
    "        print('best score_dx!!')\n",
    "    loss_list_dx.append(running_loss_dx/len(train_dataloader))\n",
    "    loss_list_test_dx.append(running_loss_test_dx/len(test_dataloader))\n",
    "    f1_dx = f1_score(preds_dx,label_dx,average='macro')\n",
    "    f1_test_dx = f1_score(preds_test_dx,label_test_dx,average='macro')\n",
    "    f1s_dx.append(f1_dx)\n",
    "    f1s_test_dx.append(f1_test_dx)\n",
    "    accuracy_dx = accuracy_score(preds_dx,label_dx)\n",
    "    accuracy_test_dx = accuracy_score(preds_test_dx,label_test_dx)\n",
    "    acc_dx.append(accuracy_dx)\n",
    "    acc_test_dx.append(accuracy_test_dx)\n",
    "\n",
    "    # 1エポック終了時の損失を表示\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_dx}] Train-Loss: {running_loss_dx/len(train_dataloader)} Test-Loss: {running_loss_test_dx/len(train_dataloader)} Train-f1: {f1_dx} Test-f1: {f1_test_dx} Train-acc: {accuracy_dx} Test-acc: {accuracy_test_dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/python/build/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/python/build/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# モデル評価 ５クラス診断\n",
    "pred_dx,label_dx = [],[]\n",
    "model.load_state_dict(torch.load('/root/notebooks/sample/models/bestmodel_dx_no_pretrain.pth'))\n",
    "model.eval()\n",
    "\n",
    "for ecg_image,labels,label_lead,patients,num_leads in test_dataloader:\n",
    "    image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "    labels = labels.reshape(-1).to(device)\n",
    "    label_dx.extend(labels.tolist())\n",
    "    outputs = model(image)  # モデルの出力を計算\n",
    "    prob_dx, preds_dx=torch.max(outputs,1)\n",
    "    pred_dx.extend(preds_dx.tolist())\n",
    "    f1_test_dx = f1_score(preds_dx.tolist(),labels.tolist(),average='macro')\n",
    "    accuracy_test_dx = accuracy_score(preds_test_dx,label_test_dx)\n",
    "    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "    running_loss_test_dx += loss.item()\n",
    "\n",
    "print(f1_test_dx)\n",
    "print(accuracy_test_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学習曲線 (損失関数) 事前学習\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(loss_list_dx,label='train_dx', lw=3, c='b')\n",
    "plt.plot(loss_list_test_dx,label='test_dx', lw=3, c='r')\n",
    "plt.title('Loss_dx')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# 学習曲線 (精度)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(acc_dx,label='acc_train_dx', lw=3, c='b')\n",
    "plt.plot(acc_test_dx,label='acc_test_dx', lw=3, c='r')\n",
    "plt.title('accuracy_dx')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
