{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Classification lab : {'Lateral': 0, 'Posterior': 1, 'Anterior': 2, 'Inferior': 3, 'Normal': 4}\n",
      "Number of datas 368\n",
      "torch.Size([12, 368, 3, 112, 224])\n",
      "368\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader_e2e import create_dataset_12\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "path = os.getcwd()\n",
    "path = '/root/notebooks/sample/dataset/ECG100_224_new/'\n",
    "train_dataloader,test_dataloader,label_lead, labels=create_dataset_12(path,1,None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルクラス\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # 畳み込み層や全結合層などを定義する\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.faltten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16,16)\n",
    "        self.fc2 = nn.Linear(16, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.gap(x)\n",
    "        x = self.faltten(x)\n",
    "        #x = x.view(-1, 16 * 56 * 112)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model = CNNModel().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル訓練　事前学習12クラス分類\n",
    "\n",
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "bestscore = 1000.0\n",
    "loss_list,loss_list_test = [], []\n",
    "f1s, f1s_test = [],[]\n",
    "acc, acc_test = [], []\n",
    "\n",
    "num_epochs = 4000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, running_loss_test = 0.0, 0.0\n",
    "    preds, label =[], []\n",
    "    preds_test,label_test = [], []\n",
    "\n",
    "    for ecg_image,labels,label_lead,patients,num_leads in train_dataloader:\n",
    "        image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "        \n",
    "        label_lead = label_lead.reshape(-1).to(device)\n",
    "        optimizer.zero_grad()  # 勾配を初期化\n",
    "        \n",
    "        outputs = model(image)  # モデルの出力を計算\n",
    "\n",
    "        prob, pred=torch.max(outputs,1)\n",
    "        preds.extend(pred.tolist())\n",
    "        label.extend(label_lead.tolist())\n",
    "        loss = criterion(outputs, label_lead)  # 損失を計算\n",
    "        loss.backward()  # 勾配を計算\n",
    "        optimizer.step()  # パラメータを更新\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    for ecg_image,labels,label_lead,patients,num_leads in test_dataloader:\n",
    "        image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "\n",
    "        label_lead = label_lead.reshape(-1).to(device)\n",
    "\n",
    "        outputs = model(image)  # モデルの出力を計算\n",
    "        \n",
    "        prob, pred=torch.max(outputs,1)\n",
    "        preds_test.extend(pred.tolist())\n",
    "        label_test.extend(label_lead.tolist())\n",
    "        \n",
    "        loss = criterion(outputs, label_lead)  # 損失を計算\n",
    "\n",
    "        running_loss_test += loss.item()\n",
    "    \n",
    "    score = running_loss_test/len(test_dataloader)\n",
    "    if score < bestscore:\n",
    "        bestscore = score\n",
    "        torch.save(model.state_dict(), '/root/notebooks/sample/models/bestmodel.pth')\n",
    "        print('best score!!')\n",
    "    loss_list.append(running_loss/len(train_dataloader))\n",
    "    loss_list_test.append(running_loss_test/len(test_dataloader))\n",
    "    f1 = f1_score(preds,label,average='macro')\n",
    "    f1_test = f1_score(preds_test,label_test,average='macro')\n",
    "    f1s.append(f1)\n",
    "    f1s_test.append(f1_test)\n",
    "    accuracy = accuracy_score(preds,label)\n",
    "    accuracy_test = accuracy_score(preds_test,label_test)\n",
    "    acc.append(accuracy)\n",
    "    acc_test.append(accuracy_test)\n",
    "\n",
    "    # 1エポック終了時の損失を表示\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train-Loss: {running_loss/len(train_dataloader)} Test-Loss: {running_loss_test/len(train_dataloader)} Train-f1: {f1} Test-f1: {f1_test} Train-acc: {accuracy} Test-acc: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル評価\n",
    "pred,label = [],[]\n",
    "model.load_state_dict(torch.load('/root/notebooks/sample/models/bestmodel.pth'))\n",
    "model.eval()\n",
    "\n",
    "for ecg_image,labels,label_lead,patients,num_leads in test_dataloader:\n",
    "    image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "    label_lead = label_lead.reshape(-1).to(device)\n",
    "    label.extend(label_lead.tolist())\n",
    "    outputs = model(image)  # モデルの出力を計算\n",
    "    prob, preds=torch.max(outputs,1)\n",
    "    pred.extend(preds.tolist())\n",
    "    f1_test = f1_score(preds.tolist(),label_lead.tolist(),average='macro')\n",
    "    \n",
    "    loss = criterion(outputs, label_lead)  # 損失を計算\n",
    "\n",
    "    running_loss_test += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線 (損失関数) 事前学習\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(loss_list,label='train', lw=3, c='b')\n",
    "plt.plot(loss_list_test,label='test', lw=3, c='r')\n",
    "plt.title('Loss')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# 学習曲線 (accuracy)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(acc,label='acc_train', lw=3, c='b')\n",
    "plt.plot(acc_test,label='acc_test', lw=3, c='r')\n",
    "plt.title('accuracy')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "           \"\"\"\n",
    "           This function prints and plots the confusion matrix.\n",
    "           Normalization can be applied by setting `normalize=True`.\n",
    "           \"\"\"\n",
    "           if normalize:\n",
    "               cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "               print(\"Normalized confusion matrix\")\n",
    "           else:\n",
    "               print('Confusion matrix, without normalization')\n",
    "           print(cm)\n",
    "           plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "           plt.title(title)\n",
    "           plt.colorbar()\n",
    "           tick_marks = np.arange(len(classes))\n",
    "           plt.xticks(tick_marks, classes, rotation=45)\n",
    "           plt.yticks(tick_marks, classes)\n",
    "           fmt = '.2f' if normalize else 'd'\n",
    "           thresh = cm.max() / 2.\n",
    "           for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "               plt.text(j, i, format(cm[i, j], fmt),\n",
    "                        horizontalalignment=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "           plt.tight_layout()\n",
    "           plt.ylabel('True label')\n",
    "           plt.xlabel('Predicted label')\n",
    "           #plt.savefig(output_file)\n",
    "\n",
    "\n",
    "labels=[0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "label2=['Ⅰ','Ⅱ','Ⅲ','aVR','aVL','aVF','V1', 'V2', 'V3', 'V4', 'V5','V6']\n",
    "cnf_matrix=confusion_matrix(label,pred,labels=labels)\n",
    "title=\"confusion matrix\"\n",
    "plt.figure(figsize=(4,4),dpi=400)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label2,title=title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNモデルの特徴抽出器を取り出す\n",
    "feature_extractor = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "# 新しい全結合層を追加\n",
    "feature_extractor.to(device)\n",
    "print(feature_extractor)\n",
    "\n",
    "\n",
    "# カスタムレイヤーを定義\n",
    "class MeanLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=0, keepdim=True)\n",
    "\n",
    "# nn.Sequential内にカスタムレイヤーを含むモデルを定義\n",
    "CNN_diagnosis = nn.Sequential(\n",
    "    feature_extractor,\n",
    "    MeanLayer(),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.Linear(16, 5)\n",
    ")\n",
    "\n",
    "CNN_diagnosis.to(device)\n",
    "\n",
    "print(CNN_diagnosis)\n",
    "\n",
    "input_shape = (12, 3, 112, 224)\n",
    "\n",
    "# ランダムなテンソルを生成\n",
    "random_input = torch.randn(input_shape).to(device)\n",
    "\n",
    "# モデルにランダムなテンソルを渡して出力を得る\n",
    "random_output = CNN_diagnosis(random_input).to(torch.float64)\n",
    "feature_output = feature_extractor(random_input).to(torch.float64)\n",
    "print(random_output.shape)\n",
    "print(feature_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 診断モデル\n",
    "\n",
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = optim.Adam(CNN_diagnosis.parameters(), lr=0.0001)\n",
    "bestscore_dx = 1000.0\n",
    "loss_list_dx,loss_list_test_dx = [], []\n",
    "f1s_dx, f1s_test_dx = [],[]\n",
    "acc_dx, acc_test_dx = [], []\n",
    "\n",
    "num_epochs_dx = 2000\n",
    "\n",
    "# モデル訓練　5クラス分類\n",
    "\n",
    "for epoch in range(num_epochs_dx):\n",
    "    CNN_diagnosis.train()\n",
    "    running_loss_dx = 0.0\n",
    "    preds_dx, label_dx = [], []\n",
    "    running_loss_dx, running_loss_test_dx = 0.0, 0.0\n",
    "    preds_test_dx,label_test_dx = [], []\n",
    "\n",
    "    for ecg_image, labels, label_lead, patients, num_leads in train_dataloader:\n",
    "        image = ecg_image.reshape(-1, 3, 112, 224).to(device)\n",
    "        labels = labels.reshape(-1).to(device)  # ラベルを1次元に変形\n",
    "        labels = labels.to(torch.long)  # CrossEntropyLossではクラスのインデックスはlong型である必要がある\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "        outputs = CNN_diagnosis(image).to(torch.float64)  # モデルの出力を計算\n",
    "\n",
    "        prob, pred = torch.max(outputs, 1)\n",
    "        preds_dx.extend(pred.tolist())\n",
    "        label_dx.extend(labels.tolist())\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算（CrossEntropyLossの引数は予測とラベル）\n",
    "        loss.backward()  # 勾配を計算\n",
    "        optimizer.step()  # パラメータを更新\n",
    "\n",
    "        running_loss_dx += loss.item()\n",
    "\n",
    "    for ecg_image,labels,label_lead,patients,num_leads in test_dataloader:\n",
    "        image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "        labels = labels.reshape(-1).to(device)\n",
    "\n",
    "        outputs = CNN_diagnosis(image)  # モデルの出力を計算\n",
    "        outputs = outputs.to(torch.float64)\n",
    "        prob, pred=torch.max(outputs,1)\n",
    "        preds_test_dx.extend(pred.tolist())\n",
    "        label_test_dx.extend(labels.tolist())\n",
    "        \n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "        running_loss_test_dx += loss.item()\n",
    "    \n",
    "    score_dx = running_loss_test_dx/len(test_dataloader)\n",
    "    if score_dx < bestscore_dx:\n",
    "        bestscore_dx = score_dx\n",
    "        torch.save(CNN_diagnosis.state_dict(), '/root/notebooks/sample/models/bestmodel_dx.pth')\n",
    "        print('best score_dx!!')\n",
    "    loss_list_dx.append(running_loss_dx/len(train_dataloader))\n",
    "    loss_list_test_dx.append(running_loss_test_dx/len(test_dataloader))\n",
    "    f1_dx = f1_score(preds_dx,label_dx,average='macro')\n",
    "    f1_test_dx = f1_score(preds_test_dx,label_test_dx,average='macro')\n",
    "    f1s_dx.append(f1_dx)\n",
    "    f1s_test_dx.append(f1_test_dx)\n",
    "    accuracy_dx = accuracy_score(preds_dx,label_dx)\n",
    "    accuracy_test_dx = accuracy_score(preds_test_dx,label_test_dx)\n",
    "    acc_dx.append(accuracy_dx)\n",
    "    acc_test_dx.append(accuracy_test_dx)\n",
    "\n",
    "    # 1エポック終了時の損失を表示\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_dx}] Train-Loss: {running_loss_dx/len(train_dataloader)} Test-Loss: {running_loss_test_dx/len(train_dataloader)} Train-f1: {f1_dx} Test-f1: {f1_test_dx} Train-acc: {accuracy_dx} Test-acc: {accuracy_test_dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル評価 ５クラス診断\n",
    "pred_dx,label_dx = [],[]\n",
    "CNN_diagnosis.load_state_dict(torch.load('/root/notebooks/sample/models/bestmodel_dx.pth'))\n",
    "CNN_diagnosis.eval()\n",
    "\n",
    "for ecg_image,labels,label_lead,patients,num_leads in test_dataloader:\n",
    "    image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "    labels = labels.reshape(-1).to(device)\n",
    "    label_dx.extend(labels.tolist())\n",
    "    outputs = CNN_diagnosis(image)  # モデルの出力を計算\n",
    "    prob_dx, preds_dx=torch.max(outputs,1)\n",
    "    pred_dx.extend(preds_dx.tolist())\n",
    "    f1_test_dx = f1_score(preds_dx.tolist(),labels.tolist(),average='macro')\n",
    "    accuracy_test_dx = accuracy_score(preds_test_dx,label_test_dx)\n",
    "    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "    running_loss_test_dx += loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線 (損失関数) 診断モデル\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(loss_list_dx,label='train_dx', lw=3, c='b')\n",
    "plt.plot(loss_list_test_dx,label='test_dx', lw=3, c='r')\n",
    "plt.title('Loss_dx')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# 学習曲線 (accuracy)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(acc_dx,label='acc_train_dx', lw=3, c='b')\n",
    "plt.plot(acc_test_dx,label='acc_test_dx', lw=3, c='r')\n",
    "plt.title('accuracy_dx')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# 学習曲線 (f1)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(f1s_dx,label='f1_train_dx', lw=3, c='b')\n",
    "plt.plot(f1s_test_dx,label='f1_test_dx', lw=3, c='r')\n",
    "plt.title('f1_dx')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dx=[0,1,2,3,4]\n",
    "label2_dx=['Lateral','Posterior', 'Anterior', 'Inferior', 'Normal']\n",
    "cnf_matrix=confusion_matrix(label_dx,pred_dx,labels=labels_dx)\n",
    "title=\"confusion matrix\"\n",
    "plt.figure(figsize=(4,4),dpi=400)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label2_dx,title=title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのインスタンスを作成\n",
    "model_np = CNNModel().to(device)\n",
    "\n",
    "model_np = nn.Sequential(*list(model_np.children())[:-2])\n",
    "\n",
    "model_np = nn.Sequential(\n",
    "    model_np,\n",
    "    MeanLayer(),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.Linear(16, 5)\n",
    ")\n",
    "\n",
    "model_np.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 診断 (事前学習なし)\n",
    "\n",
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = optim.Adam(model_np.parameters(), lr=0.0001)\n",
    "bestscore_np = 1000.0\n",
    "loss_list_np,loss_list_test_np = [], []\n",
    "f1s_np, f1s_test_np = [],[]\n",
    "acc_np, acc_test_np = [], []\n",
    "\n",
    "num_epochs_np = 7000\n",
    "\n",
    "# モデル訓練　5クラス分類\n",
    "\n",
    "for epoch in range(num_epochs_np):\n",
    "    model_np.train()\n",
    "    running_loss_np = 0.0\n",
    "    preds_np, label_np = [], []\n",
    "    running_loss_np, running_loss_test_np = 0.0, 0.0\n",
    "    preds_test_np,label_test_np = [], []\n",
    "\n",
    "    for ecg_image, labels, label_lead, patients, num_leads in train_dataloader:\n",
    "        image = ecg_image.reshape(-1, 3, 112, 224).to(device)\n",
    "        labels = labels.reshape(-1).to(device)  # ラベルを1次元に変形\n",
    "        labels = labels.to(torch.long)  # CrossEntropyLossではクラスのインデックスはlong型である必要がある\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "        outputs = model_np(image).to(torch.float64)  # モデルの出力を計算\n",
    "\n",
    "        prob, pred = torch.max(outputs, 1)\n",
    "        preds_np.extend(pred.tolist())\n",
    "        label_np.extend(labels.tolist())\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算（CrossEntropyLossの引数は予測とラベル）\n",
    "        loss.backward()  # 勾配を計算\n",
    "        optimizer.step()  # パラメータを更新\n",
    "\n",
    "        running_loss_np += loss.item()\n",
    "\n",
    "    for ecg_image,labels,label_lead,patients,num_leads in test_dataloader:\n",
    "        image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "        labels = labels.reshape(-1).to(device)\n",
    "\n",
    "        outputs = model_np(image)  # モデルの出力を計算\n",
    "        outputs = outputs.to(torch.float64)\n",
    "        prob, pred=torch.max(outputs,1)\n",
    "        preds_test_np.extend(pred.tolist())\n",
    "        label_test_np.extend(labels.tolist())\n",
    "        \n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "        running_loss_test_np += loss.item()\n",
    "    \n",
    "    score_np = running_loss_test_np/len(test_dataloader)\n",
    "    if score_np < bestscore_np:\n",
    "        bestscore_np = score_np\n",
    "        torch.save(model_np.state_dict(), '/root/notebooks/sample/models/bestmodel_np.pth')\n",
    "        print('best score_np!!')\n",
    "    loss_list_np.append(running_loss_np/len(train_dataloader))\n",
    "    loss_list_test_np.append(running_loss_test_np/len(test_dataloader))\n",
    "    f1_np = f1_score(preds_np,label_np,average='macro')\n",
    "    f1_test_np = f1_score(preds_test_np,label_test_np,average='macro')\n",
    "    f1s_np.append(f1_np)\n",
    "    f1s_test_np.append(f1_test_np)\n",
    "    accuracy_np = accuracy_score(preds_np,label_np)\n",
    "    accuracy_test_np = accuracy_score(preds_test_np,label_test_np)\n",
    "    acc_np.append(accuracy_np)\n",
    "    acc_test_np.append(accuracy_test_np)\n",
    "\n",
    "    # 1エポック終了時の損失を表示\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_np}] Train-Loss: {running_loss_np/len(train_dataloader)} Test-Loss: {running_loss_test_np/len(train_dataloader)} Train-f1: {f1_np} Test-f1: {f1_test_np} Train-acc: {accuracy_np} Test-acc: {accuracy_test_np}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル評価 ５クラス診断(事前学習なし)\n",
    "pred_np,label_np = [],[]\n",
    "model_np.load_state_dict(torch.load('/root/notebooks/sample/models/bestmodel_np.pth'))\n",
    "model_np.eval()\n",
    "\n",
    "for ecg_image,labels,label_lead,patients,num_leads in test_dataloader:\n",
    "    image =ecg_image.reshape(-1,3,112,224).to(device)\n",
    "    labels = labels.reshape(-1).to(device)\n",
    "    label_np.extend(labels.tolist())\n",
    "    outputs = model_np(image)  # モデルの出力を計算\n",
    "    prob_np, preds_np=torch.max(outputs,1)\n",
    "    pred_np.extend(preds_np.tolist())\n",
    "    f1_test_np = f1_score(preds_np.tolist(),labels.tolist(),average='macro')\n",
    "    accuracy_test_np = accuracy_score(preds_test_np,label_test_np)\n",
    "    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "    running_loss_test_np += loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線 (損失関数) 事前学習\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(loss_list_np,label='train_np', lw=3, c='b')\n",
    "plt.plot(loss_list_test_np,label='test_np', lw=3, c='r')\n",
    "plt.title('Loss_np')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# 学習曲線 (accuracy)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(acc_np,label='acc_train_np', lw=3, c='b')\n",
    "plt.plot(acc_test_np,label='acc_test_np', lw=3, c='r')\n",
    "plt.title('accuracy_np')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# 学習曲線 (F1)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(f1s_np,label='f1_train_np', lw=3, c='b')\n",
    "plt.plot(f1s_test_np,label='f1_test_np', lw=3, c='r')\n",
    "plt.title('f1_np')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.grid(lw=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np=[0,1,2,3,4]\n",
    "label2_np=['Lateral','Posterior', 'Anterior', 'Inferior', 'Normal']\n",
    "cnf_matrix=confusion_matrix(label_np,pred_np,labels=labels_np)\n",
    "title=\"confusion matrix\"\n",
    "plt.figure(figsize=(4,4),dpi=400)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label2_np,title=title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
